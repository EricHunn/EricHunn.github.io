<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Springboot]]></title>
    <url>%2F2019%2F11%2F09%2FSpringboot%2F</url>
    <content type="text"><![CDATA[首先构建一个springboot，需要注意的一点，在下面这个地方不能用default应该用Http://sstart.spring.io 里面的依赖通过parent继承，Parent里面都是写好的依赖。 starter-web把spring和springmvc都已经包好了 Springboot的入口类的@SpringBootApplication这个注解是声明该类是一个springboot的引导类，通过这个注解可以实现很多功能，比如； 上面这个图片就是显示@SpringBootApplication这个类里面的原注解，主要的就是下面的三个@SpringBootConfiguration这个是相当于Configuration一样，就是标注改类是Spring的配置类，@EnableAutoConfiguration这个是自动注入配置，@ComponentScan这个是组件扫描，也就是跟这个文件统一同一个级别下的都会被扫描。 这三个注解主要是自动注入配置比较麻烦，下面看一下源码： 这个Import是当前这个注解下引入其他配置，然后定位到这一行，引入其他的配置： 然后再进去定位到这一样：这就不用进去了，因为进去里面很麻烦，所以可以往下面两行去看如果是当前出问题的话就暴露信息No auto Configuration classes found in META-INF….这个就是没有在META-INF下面发现。默认就在当前的jar包下面。也就是org.springframework.boot.autoconfigure里面通过这个里面的配置可以使用springboot首先里面的自动配置。 然后发现一行熟悉的内容，通过下面这个进去到这个类里面：在定位到这一行：进入到里面就是这样：这个就是通过sever.port来配置的，那这个value在哪里呢？ 就在这个里面 springboot自动配置注解@autoEnconfiguration这个注解的底层就是配置这个文件： 这个文件我们可以通过配置把里面的内容覆盖。那么在哪里覆盖呢？ 在这里面有一个加载配置文件的地方。在Yml或者properties配置即可。 可以在resources目录下面配置properties文件修改配置。 Springboot配置文件配置文件有一个加载顺序，先加载yml在加载Yaml最后加载Properties 在yaml文件中添加配置：和配置的写法#普通数据的配置 name: zhangsan #对象的配置 person: name: zhnagsan age: 18 addr: beijing # 对象行内配置，和上面一样只不过配置在一行了 person1: {name: zhangsan,age: 18,addr: beijing} #端口配置 server: port: 8082 # 集合或者数组的配置(普通字符串) city: - beijing - tianjin - chongqing city1: [bijing,tiananjin,chongqing] #配置数据、集合（对象是数据）studeng下面就是一个对象，三个属性 student: - name: tom age: 18 addr: beijing - name: luccy age: 19 addr: tianjin # 这种也有一行的情况 student1: [{name: tom,age: 18 ,addr: beijing},{name: luccy,age: 13 ,addr: nanjing}] #关于map的集合 map: key1: value1 key2: value2 、使用@Value注解可以拿到属性放到字段上， package com.itheima.controller; import org.springframework.beans.factory.annotation.Value; import org.springframework.stereotype.Controller; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.ResponseBody; /** * @Description TODO * @Author TT Hun * @Data 2019/11/11 23:37 */ @Controller public class Quick2Controller { @Value(&quot;${name}&quot;) private String name; @RequestMapping(&quot;/quick1&quot;) @ResponseBody public String quick2(){ // 获得内部的配置文件信息 return &quot;name:&quot;+ name; } } 使用注解@ConfigurationProperties映射 package com.itheima.controller; import org.springframework.beans.factory.annotation.Value; import org.springframework.boot.context.properties.ConfigurationProperties; import org.springframework.stereotype.Controller; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.ResponseBody; /** * @Description TODO * @Author TT Hun * @Data 2019/11/11 23:37 */ @Controller @ConfigurationProperties(prefix = &quot;person&quot;) public class Quick3Controller { private String name; private String addr; // 想获得person3的全部数据 @RequestMapping(&quot;/quick3&quot;) @ResponseBody public String quick2() { // 获得内部的配置文件信息 return &quot;name:&quot; + name; } public String getName() { return name; } public void setName(String name) { this.name = name; } public String getAddr() { return addr; } public void setAddr(String addr) { this.addr = addr; } } 在Pom当中配置这个可以给出相应的提示：安装这个之后在yml之中有这种东西： 以后程序需要配置的时候，可以先写属性，然后在在yml文件中去写配置就有相应的提示了。 Srpingboot集成Springboot整合Mybatis添加Mybatis起步依赖。 package com.itheima.controller; import com.itheima.domain.User; import com.itheima.mapper.UserMapper; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.stereotype.Controller; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.ResponseBody; import java.util.List; @Controller public class MybatisController { @Autowired private UserMapper userMapper; @RequestMapping(&quot;/query&quot;) @ResponseBody public List&lt;User&gt; queryUserList(){ List&lt;User&gt; users = userMapper.queryUserList(); return users; } } package com.itheima.domain; public class User { private Long id; private String username; private String password; private String name; public Long getId() { return id; } public void setId(Long id) { this.id = id; } public String getUsername() { return username; } public void setUsername(String username) { this.username = username; } public String getPassword() { return password; } public void setPassword(String password) { this.password = password; } public String getName() { return name; } public void setName(String name) { this.name = name; } @Override public String toString() { return &quot;User{&quot; + &quot;id=&quot; + id + &quot;, username=&apos;&quot; + username + &apos;\&apos;&apos; + &quot;, password=&apos;&quot; + password + &apos;\&apos;&apos; + &quot;, name=&apos;&quot; + name + &apos;\&apos;&apos; + &apos;}&apos;; } } package com.itheima.mapper; import com.itheima.domain.User; import org.apache.ibatis.annotations.Mapper; import java.util.List; @Mapper public interface UserMapper { public List&lt;User&gt; queryUserList(); } package com.itheima; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; @SpringBootApplication public class SpringbootMybatisApplication { public static void main(String[] args) { SpringApplication.run(SpringbootMybatisApplication.class, args); } } &lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot; ?&gt; &lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot; &gt; &lt;mapper namespace=&quot;com.itheima.mapper.UserMapper&quot;&gt; &lt;select id=&quot;queryUserList&quot; resultType=&quot;user&quot;&gt; select * from user &lt;/select&gt; &lt;/mapper&gt; spring.datasource.driverClassName=com.mysql.jdbc.Driver spring.datasource.url=jdbc:mysql://127.0.0.1:3306/test?useUnicode=true&amp;characterEncoding=utf8 spring.datasource.username=root spring.datasource.password=root mybatis.type-aliases-package=com.itheima.domain mybatis.mapper-locations=classpath:mapper/*Mapper.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.itheima&lt;/groupId&gt; &lt;artifactId&gt;springboot_mybatis&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;springboot_mybatis&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.1.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--SpingBoot集成junit测试的起步依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!--mybatis起步依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- MySQL连接驱动 --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/project&gt; 整合jpa和redis package com.itheima.domain; import javax.persistence.Entity; import javax.persistence.GeneratedValue; import javax.persistence.GenerationType; import javax.persistence.Id; /** * @Description TODO * @Author TT Hun * @Data 2019/11/12 17:52 */ //@Entity ：表明是一个实体类 @Entity public class User { //@Id 标注用于声明一个实体类的属性映射为数据库的主键列。该属性通常置于属性声明语句之前，可与声明语句同行，也可写在单独行上。 @Id //主键生成策略，因为使用Mysql所以使用IDENTITY @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; private String username; private String password; private String name; public Long getId() { return id; } public void setId(Long id) { this.id = id; } public String getUsername() { return username; } public void setUsername(String username) { this.username = username; } public String getPassword() { return password; } public void setPassword(String password) { this.password = password; } public String getName() { return name; } public void setName(String name) { this.name = name; } @Override public String toString() { return &quot;User{&quot; + &quot;id=&quot; + id + &quot;, username=&apos;&quot; + username + &apos;\&apos;&apos; + &quot;, password=&apos;&quot; + password + &apos;\&apos;&apos; + &quot;, name=&apos;&quot; + name + &apos;\&apos;&apos; + &apos;}&apos;; } } package com.itheima.repository; import com.itheima.domain.User; import org.springframework.data.jpa.repository.JpaRepository; import java.util.List; /** * @Description TODO * @Author TT Hun * @Data 2019/11/12 17:56 */ public interface UserRepository extends JpaRepository&lt;User,Long&gt; { /** * * @return */ public List&lt;User&gt; findAll(); } package com.itheima; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; @SpringBootApplication public class SpringbootJpaApplication { public static void main(String[] args) { SpringApplication.run(SpringbootJpaApplication.class, args); } } #DB Configuration: spring.datasource.driverClassName=com.mysql.jdbc.Driver spring.datasource.url=jdbc:mysql://127.0.0.1:3306/test?useUnicode=true&amp;characterEncoding=utf8 spring.datasource.username=root spring.datasource.password=root #JPA Configuration: spring.jpa.database=MySQL spring.jpa.show-sql=true spring.jpa.generate-ddl=true spring.jpa.hibernate.ddl-auto=update spring.jpa.hibernate.naming_strategy=org.hibernate.cfg.ImprovedNamingStrategy #Redis spring.redis.host=127.0.0.1 spring.redis.port=6379 package com.itheima; import com.itheima.domain.User; import com.itheima.repository.UserRepository; import org.junit.Test; import org.junit.runner.RunWith; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.test.context.SpringBootTest; import org.springframework.test.context.junit4.SpringRunner; import javax.swing.*; import java.util.List; /** * @Description TODO * @Author TT Hun * @Data 2019/11/12 18:00 */ @RunWith(SpringRunner.class) @SpringBootTest(classes = SpringbootJpaApplication.class) public class JPATest { // 测试userrepository是否可以成功注入 @Autowired private UserRepository userRepository; @Test public void test() { List&lt;User&gt; all = userRepository.findAll(); System.out.println(all); } } package com.itheima; import com.fasterxml.jackson.databind.ObjectMapper; import com.itheima.domain.User; import com.itheima.repository.UserRepository; import org.junit.Test; import org.junit.runner.RunWith; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.test.context.SpringBootTest; import org.springframework.data.redis.core.RedisTemplate; import org.springframework.test.context.junit4.SpringRunner; import java.util.List; /** * @Description TODO * @Author TT Hun * @Data 2019/11/12 21:31 */ @RunWith(SpringRunner.class) @SpringBootTest(classes =SpringbootJpaApplication.class) public class RedisTest { @Autowired private RedisTemplate&lt;String,String&gt; redisTemplate; @Autowired private UserRepository userRepository; @Test public void test() throws Exception{ // 1从redis中获得数据，数据形式一般json字符串 String userListJson = redisTemplate.boundValueOps(&quot;user.findAll&quot;).get(); // 2判断redis中是否存在数据 if(null==userListJson) { // 3判断是否存在想要数据如果不存在从数据库中查询 List&lt;User&gt; all = userRepository.findAll(); // 4.将查询的数据存储到redis中 // 向将List集合转换成json格式的字符串，使用jackson进行一个对象和json和对象的转化 ObjectMapper objectMapper = new ObjectMapper(); userListJson = objectMapper.writeValueAsString(all); redisTemplate.boundValueOps(&quot;user.findAll&quot;).set(userListJson); System.out.println(&quot;=====================从数据库中获得user的数据==================&quot;); }else{ System.out.println(&quot;===================从redis缓存中获得数据===============&quot;); } // 4、将数据在控制台打印 System.out.println(userListJson); // } } package com.itheima; import org.springframework.boot.test.context.SpringBootTest; @SpringBootTest class SpringbootJpaApplicationTests { void contextLoads() { } } &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.1.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;com.itheima&lt;/groupId&gt; &lt;artifactId&gt;springboot_jpa&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;springboot_jpa&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--jpa起步依赖配置--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--mysql驱动--&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--redis--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.junit.vintage&lt;/groupId&gt; &lt;artifactId&gt;junit-vintage-engine&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/project&gt; https://www.jianguoyun.com/p/DaYAQ50QnbDxBxinpJgC]]></content>
  </entry>
  <entry>
    <title><![CDATA[Maven高级之父子工程SSM案例]]></title>
    <url>%2F2019%2F11%2F08%2FMaven%E9%AB%98%E7%BA%A7%E4%B9%8B%E7%88%B6%E5%AD%90%E5%B7%A5%E7%A8%8BSSM%E6%A1%88%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[Maven高级之父子工程SSM案例结合三种启动方式 先看一下目录结构： 然后上代码： package com.itheima.dao; import com.itheima.domain.Items; /** * @Description TODO * @Author TT Hun * @Data 2019/11/7 17:17 */ public interface ItemsDao { /** * 根据ID查找Items * @param id * @return */ public Items findById(Integer id); } package com.itheima.domain; import java.util.Date; /** * @Description TODO * @Author TT Hun * @Data 2019/11/7 17:15 */ public class Items { private Integer id; private String name; private Double price; private String prc; private Date createtime; private String detail; public Integer getId() { return id; } public void setId(Integer id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public Double getPrice() { return price; } public void setPrice(Double price) { this.price = price; } public String getPrc() { return prc; } public void setPrc(String prc) { this.prc = prc; } public Date getCreatetime() { return createtime; } public void setCreatetime(Date createtime) { this.createtime = createtime; } public String getDetail() { return detail; } public void setDetail(String detail) { this.detail = detail; } @Override public String toString() { return &quot;items{&quot; + &quot;id=&quot; + id + &quot;, name=&apos;&quot; + name + &apos;\&apos;&apos; + &quot;, price=&quot; + price + &quot;, prc=&apos;&quot; + prc + &apos;\&apos;&apos; + &quot;, createtime=&quot; + createtime + &quot;, detail=&apos;&quot; + detail + &apos;\&apos;&apos; + &apos;}&apos;; } } &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt; &lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt; &lt;mapper namespace=&quot;com.itheima.dao.ItemsDao&quot;&gt; &lt;!-- 返回值是resulttype，因为我们事先已经把实体类中所有的属性和我们数据库中的列保持一致了 所以我们只需要写resultType就可以了，如果不一致可以写resultMap 这个地方已经改成了小写的i，这个地方既然已经写了别名了，写配置文件的时候切记要扫描POJO这个包的路径--&gt; &lt;select id=&quot;findById&quot; parameterType=&quot;int&quot; resultType=&quot;items&quot;&gt; select * from items where id = #{id} &lt;/select&gt; &lt;/mapper&gt; ItemsDao.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd&quot;&gt; &lt;!--dao层配置文件开始--&gt; &lt;!--配置连接池--&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;com.mysql.cj.jdbc.Driver&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/maven?useUnicode=true&amp;amp;characterEncoding=utf-8&amp;amp;useSSL=false&amp;amp;serverTimezone=GMT%2B8&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;root&quot;/&gt; &lt;/bean&gt; &lt;!--配置生产SqlSession对象的工厂--&gt; &lt;bean id=&quot;sqlSessionFactory&quot; class=&quot;org.mybatis.spring.SqlSessionFactoryBean&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt; &lt;!--扫描pojo包，给包下所有pojo对象起别名--&gt; &lt;property name=&quot;typeAliasesPackage&quot; value=&quot;com.itheima.domain&quot;/&gt; &lt;/bean&gt; &lt;!--扫描接口包路径，生成包下所有接口的代理对象，并且放入spring容器中--&gt; &lt;bean class=&quot;org.mybatis.spring.mapper.MapperScannerConfigurer&quot;&gt; &lt;property name=&quot;basePackage&quot; value=&quot;com.itheima.dao&quot;/&gt; &lt;/bean&gt; &lt;!--dao层配置文件结束--&gt; &lt;/beans&gt; dao.pom: &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;!-- 子工程可以和父工程共用groupid和version，但是他必须要有自己的artifactId，下面这套就是共用的--&gt; &lt;parent&gt; &lt;artifactId&gt;maven_day02_parent&lt;/artifactId&gt; &lt;groupId&gt;com.itheima&lt;/groupId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;maven_day02_dao&lt;/artifactId&gt; &lt;/project&gt; package com.itheima.service.impl; import com.itheima.dao.ItemsDao; import com.itheima.domain.Items; import com.itheima.service.ItemService; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.stereotype.Service; /** * 有一个公司开发的常用原则，自己写的类尽量使用注解，如果是框架使用的类，尽量使用配置文件 * @Description TODO * @Author TT Hun * @Data 2019/11/7 20:46 */ @Service public class ItemServiceImpl implements ItemService { @Autowired private ItemsDao itemsDao; public Items findById(Integer id) { return itemsDao.findById(id); } } package com.itheima.service; import com.itheima.domain.Items; /** * @Description TODO * @Author TT Hun * @Data 2019/11/7 20:45 */ public interface ItemService { public Items findById(Integer id); } &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd&quot;&gt; &lt;!--service层配置文件开始--&gt; &lt;!--组件扫描配置--&gt; &lt;context:component-scan base-package=&quot;com.itheima.service&quot;/&gt; &lt;!--aop面向切面编程，切面就是切入点和通知的组合--&gt; &lt;!--配置事务管理器--&gt; &lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt; &lt;/bean&gt; &lt;!--配置事务的通知--&gt; &lt;tx:advice id=&quot;advice&quot;&gt; &lt;tx:attributes&gt; &lt;tx:method name=&quot;save*&quot; propagation=&quot;REQUIRED&quot;/&gt; &lt;tx:method name=&quot;update*&quot; propagation=&quot;REQUIRED&quot;/&gt; &lt;tx:method name=&quot;delete*&quot; propagation=&quot;REQUIRED&quot;/&gt; &lt;tx:method name=&quot;find*&quot; read-only=&quot;true&quot;/&gt; &lt;tx:method name=&quot;*&quot; propagation=&quot;REQUIRED&quot;/&gt; &lt;/tx:attributes&gt; &lt;/tx:advice&gt; &lt;!--配置切面--&gt; &lt;aop:config&gt; &lt;aop:pointcut id=&quot;pointcut&quot; expression=&quot;execution(* com.itheima.service.impl.*.*(..))&quot;/&gt; &lt;aop:advisor advice-ref=&quot;advice&quot; pointcut-ref=&quot;pointcut&quot;/&gt; &lt;/aop:config&gt; &lt;!--service层配置文件结束--&gt; &lt;/beans&gt; service.pom: &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;parent&gt; &lt;artifactId&gt;maven_day02_parent&lt;/artifactId&gt; &lt;groupId&gt;com.itheima&lt;/groupId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;maven_day02_service&lt;/artifactId&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.itheima&lt;/groupId&gt; &lt;artifactId&gt;maven_day02_dao&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; package com.itheima.controller; import com.itheima.domain.Items; import com.itheima.service.ItemService; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.stereotype.Controller; import org.springframework.ui.Model; import org.springframework.web.bind.annotation.RequestMapping; /** * @Description TODO * @Author TT Hun * @Data 2019/11/7 21:36 */ @Controller @RequestMapping(&quot;/items&quot;) public class ItemController { @Autowired private ItemService itemsService; @RequestMapping(&quot;/findDetail&quot;) public String findDetail(Model model){ Items items = itemsService.findById(1); // 这个addAttribute里面参数，结合看Pages下面的jsp来看都是item.price，item.detail，说明都是用这个key来取值的。，那么就配置即可 model.addAttribute(&quot;item&quot;,items); return &quot;itemDetail&quot;; } } &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd&quot;&gt; &lt;import resource=&quot;classpath:spring/applicationContext_dao.xml&quot;/&gt; &lt;import resource=&quot;classpath:spring/applicationContext_service.xml&quot;/&gt; &lt;/beans&gt; # Set root category priority to INFO and its only appender to CONSOLE. #log4j.rootCategory=INFO, CONSOLE debug info warn error fatal log4j.rootCategory=debug, CONSOLE, LOGFILE # Set the enterprise logger category to FATAL and its only appender to CONSOLE. log4j.logger.org.apache.axis.enterprise=FATAL, CONSOLE # CONSOLE is set to be a ConsoleAppender using a PatternLayout. log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} %-6r [%15.15t] %-5p %30.30c %x - %m\n # LOGFILE is set to be a File appender using a PatternLayout. log4j.appender.LOGFILE=org.apache.log4j.FileAppender log4j.appender.LOGFILE.File=d:\axis.log log4j.appender.LOGFILE.Append=true log4j.appender.LOGFILE.layout=org.apache.log4j.PatternLayout log4j.appender.LOGFILE.layout.ConversionPattern=%d{ISO8601} %-6r [%15.15t] %-5p %30.30c %x - %m\n &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd&quot;&gt; &lt;!-- 配置组件扫描--&gt; &lt;context:component-scan base-package=&quot;com.itheima.controller&quot;/&gt; &lt;!-- 处理器映射器，处理器适配器--&gt; &lt;mvc:annotation-driven/&gt; &lt;!-- 配置视图解析器--&gt; &lt;bean id=&quot;internalResourceViewResolver&quot; class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt; &lt;property name=&quot;prefix&quot; value=&quot;/WEB-INF/pages/&quot;/&gt; &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot;/&gt; &lt;/bean&gt; &lt;!-- 释放静态资源 凡事遇到静态资源就会默认交给deafault去处理，也就是之前写的httpservlet而不是现在的动态--&gt; &lt;mvc:default-servlet-handler/&gt; &lt;/beans&gt; &lt;%@ page language=&quot;java&quot; contentType=&quot;text/html; charset=UTF-8&quot; pageEncoding=&quot;UTF-8&quot;%&gt; &lt;%@ taglib uri=&quot;http://java.sun.com/jsp/jstl/core&quot; prefix=&quot;c&quot; %&gt; &lt;%@ taglib uri=&quot;http://java.sun.com/jsp/jstl/fmt&quot; prefix=&quot;fmt&quot;%&gt; &lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD HTML 4.01 Transitional//EN&quot; &quot;http://www.w3.org/TR/html4/loose.dtd&quot;&gt; &lt;html&gt; &lt;head&gt; &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot;&gt; &lt;title&gt;Insert title here&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;form&gt; &lt;table width=&quot;100%&quot; border=1&gt; &lt;tr&gt; &lt;td&gt;商品名称&lt;/td&gt; &lt;td&gt; ${item.name } &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;商品价格&lt;/td&gt; &lt;td&gt; ${item.price } &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;生成日期&lt;/td&gt; &lt;td&gt; &lt;fmt:formatDate value=&quot;${item.createtime}&quot; pattern=&quot;yyyy-MM-dd HH:mm:ss&quot;/&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;商品简介&lt;/td&gt; &lt;td&gt;${item.detail} &lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/form&gt; &lt;/body&gt; &lt;/html&gt; &lt;!DOCTYPE web-app PUBLIC &quot;-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN&quot; &quot;http://java.sun.com/dtd/web-app_2_3.dtd&quot;&gt; &lt;web-app xmlns=&quot;http://java.sun.com/xml/ns/javaee&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_3_0.xsd&quot; version=&quot;3.0&quot;&gt; &lt;!-- 首先配置编码过滤器--&gt; &lt;filter&gt; &lt;filter-name&gt;encoding&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;forceEncoding&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;encoding&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt; &lt;!-- 配置Spring核心监听器--&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt; &lt;!-- spring默认监听器只能监听WEB_INF路径下的名字叫applicationContext.xml的spring配置文件，我们的文件名称可以对上但是路径对不上 所以我们重新指定spring配置路径--&gt; &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:applicationContext.xml&lt;/param-value&gt; &lt;/context-param&gt; &lt;!-- 配置springmvc的核心servlet--&gt; &lt;servlet&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;!-- classpath的默认路径就是这个resources--&gt; &lt;param-value&gt;classpath:springmvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;/web-app&gt; &lt;html&gt; &lt;body&gt; &lt;h2&gt;Hello World!&lt;/h2&gt; &lt;/body&gt; &lt;/html&gt; web.pom &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;parent&gt; &lt;artifactId&gt;maven_day02_parent&lt;/artifactId&gt; &lt;groupId&gt;com.itheima&lt;/groupId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;maven_day02_web&lt;/artifactId&gt; &lt;packaging&gt;war&lt;/packaging&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.itheima&lt;/groupId&gt; &lt;artifactId&gt;maven_day02_service&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; parent.pom &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;!-- 工程和模块的区别： 工程不等于完整的项目，模块也不等于完整的项目，一个完整的项目看的是代码，代码完整就可以说是一个完整的项目，和工程和 项目没关系的。--&gt; &lt;!-- 那么工程和模块啥区别呢？工程天生只能使用自己的内部资源，工程是天生独立的，后天可以和其他模块建立关联关系，模块天生是属于 父工程的，也就是模块一旦创建，所有父工程的资源都可以使用--&gt; &lt;!-- 子模块天生继承父工程，可以使用付工程所有资源， 子模块之间天生没任何关系--&gt; &lt;!-- 父子工程之间不用建立关系，继承关系是先天的，不需要手动建立 平级之间的关系叫依赖关系。依赖不是先天的，是需要后天建立的--&gt; &lt;groupId&gt;com.itheima&lt;/groupId&gt; &lt;artifactId&gt;maven_day02_parent&lt;/artifactId&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;!-- 下面有几个字模块会显示到这里--&gt; &lt;modules&gt; &lt;module&gt;maven_day02_dao&lt;/module&gt; &lt;module&gt;maven_day02_service&lt;/module&gt; &lt;module&gt;maven_day02_web&lt;/module&gt; &lt;/modules&gt; &lt;!--maven工程要导入jar包的坐标，就必须要考虑解决jar包冲突。 解决jar包冲突的方式一： 第一声明优先原则：哪个jar包的坐标在靠上的位置，这个jar包就是先声明的。 先声明的jar包坐标下的依赖包，可以优先进入项目中。 maven导入jar包中的一些概念： 直接依赖：项目中直接导入的jar包，就是该项目的直接依赖包。 传递依赖：项目中没有直接导入的jar包，可以通过项目直接依赖jar包传递到项目中去。 解决jar包冲突的方式二： 路径近者优先原则。直接依赖路径比传递依赖路径近，那么最终项目进入的jar包会是路径近的直接依赖包。 解决jar包冲突的方式三【推荐使用】： 直接排除法。 当我们要排除某个jar包下依赖包，在配置exclusions标签的时候，内部可以不写版本号。 因为此时依赖包使用的版本和默认和本jar包一样。 --&gt; &lt;!-- 统一管理jar包版本 --&gt; &lt;properties&gt; &lt;spring.version&gt;5.0.2.RELEASE&lt;/spring.version&gt; &lt;slf4j.version&gt;1.6.6&lt;/slf4j.version&gt; &lt;log4j.version&gt;1.2.12&lt;/log4j.version&gt; &lt;shiro.version&gt;1.2.3&lt;/shiro.version&gt; &lt;mysql.version&gt;8.0.11&lt;/mysql.version&gt; &lt;mybatis.version&gt;3.4.5&lt;/mybatis.version&gt; &lt;spring.security.version&gt;5.0.1.RELEASE&lt;/spring.security.version&gt; &lt;/properties&gt; &lt;!-- maven工程是可以分父子依赖关系的。 凡是依赖别的项目后，拿到的别的项目的依赖包，都属于传递依赖。 比如：当前A项目，被B项目依赖。那么我们A项目中所有jar包都会传递到B项目中。 B项目开发者，如果再在B项目中导入一套ssm框架的jar包，对于B项目是直接依赖。 那么直接依赖的jar包就会把我们A项目传递过去的jar包覆盖掉。 为了防止以上情况的出现。我们可以把A项目中主要jar包的坐标锁住，那么其他依赖该项目的项目中， 即便是有同名jar包直接依赖，也无法覆盖。 --&gt; &lt;!-- 锁定jar包版本 --&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-web&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;${mybatis.version}&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;!-- 项目依赖jar包 --&gt; &lt;dependencies&gt; &lt;!-- spring --&gt; &lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt; &lt;version&gt;1.6.8&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aop&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-web&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-orm&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;${mysql.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet.jsp&lt;/groupId&gt; &lt;artifactId&gt;jsp-api&lt;/artifactId&gt; &lt;version&gt;2.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;jstl&lt;/groupId&gt; &lt;artifactId&gt;jstl&lt;/artifactId&gt; &lt;version&gt;1.2&lt;/version&gt; &lt;/dependency&gt; &lt;!-- log start --&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;${log4j.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;${slf4j.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;${slf4j.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- log end --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;${mybatis.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt; &lt;version&gt;1.3.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;c3p0&lt;/groupId&gt; &lt;artifactId&gt;c3p0&lt;/artifactId&gt; &lt;version&gt;0.9.1.2&lt;/version&gt; &lt;type&gt;jar&lt;/type&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.pagehelper&lt;/groupId&gt; &lt;artifactId&gt;pagehelper&lt;/artifactId&gt; &lt;version&gt;5.1.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.security&lt;/groupId&gt; &lt;artifactId&gt;spring-security-web&lt;/artifactId&gt; &lt;version&gt;${spring.security.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.security&lt;/groupId&gt; &lt;artifactId&gt;spring-security-config&lt;/artifactId&gt; &lt;version&gt;${spring.security.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.security&lt;/groupId&gt; &lt;artifactId&gt;spring-security-core&lt;/artifactId&gt; &lt;version&gt;${spring.security.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.security&lt;/groupId&gt; &lt;artifactId&gt;spring-security-taglibs&lt;/artifactId&gt; &lt;version&gt;${spring.security.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.0.9&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.tomcat.maven&lt;/groupId&gt; &lt;artifactId&gt;tomcat7-maven-plugin&lt;/artifactId&gt; &lt;version&gt;2.2&lt;/version&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/project&gt; 有三种启动方式，一种是在Parent工程里面使用插件tomcat:run来启动 一种是在这里添加模块通过tomcat启动： 还有一种是使用web的tomcat插件来启动，但是必须要在Parent的插件功能里面将三个子模块Install在本地仓库里面形成jar包，才可以。 为什么要这样，因为在web的Pom文件当中他使用了service的jar包。这样的话maven会首先去本地仓库去找，然后找不到的话去中央仓库，因为都没有，所以要先打成jar包扔进本地仓库才可以。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Maven高级]]></title>
    <url>%2F2019%2F11%2F07%2FMaven%E9%AB%98%E7%BA%A7%2F</url>
    <content type="text"><![CDATA[执行SQL文件导入进去了一个表和内容供这章节学习使用： Maven导入Jar包时候依赖冲突的解决 Maven工程要导入Jar包的坐标，就必须考虑解决jar包的冲突 解决jar包冲突的方式一： 第一声明优先原则：哪个Jar包的坐标靠上的位置，这个Jar包就是先声明的。 先声明的jar包坐标下的依赖，可以有优先进入到项目当中，也就是说spring-context和spring-bean都依赖于spring-core这个， 但是content和bean版本不同 所以就会优先使用前面的 解决jar包冲突的方式二： maven导入jar包的一些概念： 直接依赖：项目中直接导入的jar包就是该项目的直接依赖包。 传递依赖：项目中并没有直接导入jar包，可以通过项目直接依赖jar包传递到项目当中去 如果有传递依赖，也有直接依赖，那么就是用最近的直接依赖 解决jar包冲突的方式三：【推荐使用】 直接排除法： 当我们要排除某个依赖包下面的依赖包的时候，在配置exclusion标签的时候，内部可以不写版本号， 因为此时依赖包使用的版本号默认和jar包一样的 在我们平时公司中使用的时候一般是复制别人的依赖包，不用在这个上面纠结太久，一旦说可能出现冲突问题，解决办法就是使用Exclusion标签做排除法，可以查看一下依赖关系，在IDEA中使用这个按钮查看 下面是常用的SSM使用的Pom文件。一般使用这些依赖包就够用了： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.itheima&lt;/groupId&gt; &lt;artifactId&gt;maven_day02_1&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;war&lt;/packaging&gt; &lt;!-- 统一管理jar包版本 --&gt; &lt;properties&gt; &lt;spring.version&gt;5.0.2.RELEASE&lt;/spring.version&gt; &lt;slf4j.version&gt;1.6.6&lt;/slf4j.version&gt; &lt;log4j.version&gt;1.2.12&lt;/log4j.version&gt; &lt;shiro.version&gt;1.2.3&lt;/shiro.version&gt; &lt;mysql.version&gt;5.1.6&lt;/mysql.version&gt; &lt;mybatis.version&gt;3.4.5&lt;/mybatis.version&gt; &lt;spring.security.version&gt;5.0.1.RELEASE&lt;/spring.security.version&gt; &lt;/properties&gt; &lt;!-- maven工程是可以分父子依赖关系的。 凡是依赖别的项目后，拿到的别的项目的依赖包，都属于传递依赖。 比如：当前A项目，被B项目依赖。那么我们A项目中所有jar包都会传递到B项目中。 B项目开发者，如果再在B项目中导入一套ssm框架的jar包，对于B项目是直接依赖。 那么直接依赖的jar包就会把我们A项目传递过去的jar包覆盖掉。 为了防止以上情况的出现。我们可以把A项目中主要jar包的坐标锁住，那么其他依赖该项目的项目中， 即便是有同名jar包直接依赖，也无法覆盖。 --&gt; &lt;!-- 锁定jar包版本 --&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-web&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;${mybatis.version}&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;!-- 项目依赖jar包 --&gt; &lt;dependencies&gt; &lt;!-- spring --&gt; &lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt; &lt;version&gt;1.6.8&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aop&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-web&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-orm&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;${mysql.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet.jsp&lt;/groupId&gt; &lt;artifactId&gt;jsp-api&lt;/artifactId&gt; &lt;version&gt;2.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;jstl&lt;/groupId&gt; &lt;artifactId&gt;jstl&lt;/artifactId&gt; &lt;version&gt;1.2&lt;/version&gt; &lt;/dependency&gt; &lt;!-- log start --&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;${log4j.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;${slf4j.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;${slf4j.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- log end --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;${mybatis.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt; &lt;version&gt;1.3.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;c3p0&lt;/groupId&gt; &lt;artifactId&gt;c3p0&lt;/artifactId&gt; &lt;version&gt;0.9.1.2&lt;/version&gt; &lt;type&gt;jar&lt;/type&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.pagehelper&lt;/groupId&gt; &lt;artifactId&gt;pagehelper&lt;/artifactId&gt; &lt;version&gt;5.1.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.security&lt;/groupId&gt; &lt;artifactId&gt;spring-security-web&lt;/artifactId&gt; &lt;version&gt;${spring.security.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.security&lt;/groupId&gt; &lt;artifactId&gt;spring-security-config&lt;/artifactId&gt; &lt;version&gt;${spring.security.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.security&lt;/groupId&gt; &lt;artifactId&gt;spring-security-core&lt;/artifactId&gt; &lt;version&gt;${spring.security.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.security&lt;/groupId&gt; &lt;artifactId&gt;spring-security-taglibs&lt;/artifactId&gt; &lt;version&gt;${spring.security.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.0.9&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;!-- 添加tomcat7插件 --&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.tomcat.maven&lt;/groupId&gt; &lt;artifactId&gt;tomcat7-maven-plugin&lt;/artifactId&gt; &lt;version&gt;2.2&lt;/version&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/project&gt; maven工程是可以分成父子关系的，一个项目可以依赖另外一个项目，凡事依赖别人的项目后，拿到的别人的I项目依赖之后都属于传递依赖。 比如当前A项目被B项目依赖，那么我们A项目中所有的Jar包都会传递到B项目中。B项目中得到的项目依赖都属于传递依赖。可能A项目是比较新的项目，但是B项目用的是比较老的依赖。B项目开发者如果再在B项目中导入一套SSM框架的依赖包，对于B项目就是直接依赖。那么直接依赖的jar包就会把我们A项目传递过去的jar包覆盖掉。为了防止这种情况出现，我们可以把A项目中的主要jar包的坐标锁定住，那么其他依赖该项目的项目中即便是有同名jar包直接依赖也无法覆盖。 这个标签起到了一个锁定的作用。但是仅仅是锁定作用，没有导入坐标作用，还是得需要解决导入jar包。上面这段SSM的依赖通过EL表达式，把配置文件都卸载上面来统一管理jar包版本。*# 案例一整套SSM的编写 # package com.itheima.controller; import com.itheima.domain.Items; import com.itheima.service.ItemService; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.stereotype.Controller; import org.springframework.ui.Model; import org.springframework.web.bind.annotation.RequestMapping; / @Description TODO @Author TT Hun @Data 2019/11/7 21:36 / @Controller @RequestMapping(“/items”) public class ItemController { @Autowired private ItemService itemsService; @RequestMapping(“/findDetail”) public String findDetail(Model model){ Items items = itemsService.findById(1); // 这个addAttribute里面参数，结合看Pages下面的jsp来看都是item.price，item.detail，说明都是用这个key来取值的。，那么就配置即可 model.addAttribute(“item”,items); return “itemDetail”; } }* package com.itheima.dao; import com.itheima.domain.Items; / @Description TODO @Author TT Hun @Data 2019/11/7 17:17 / public interface ItemsDao { / 根据ID查找Items @param id @return / public Items findById(Integer id); }* package com.itheima.domain; import java.util.Date; / @Description TODO @Author TT Hun @Data 2019/11/7 17:15 / public class Items { private Integer id; private String name; private Double price; private String prc; private Date createtime; private String detail; public Integer getId() { return id; } public void setId(Integer id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public Double getPrice() { return price; } public void setPrice(Double price) { this.price = price; } public String getPrc() { return prc; } public void setPrc(String prc) { this.prc = prc; } public Date getCreatetime() { return createtime; } public void setCreatetime(Date createtime) { this.createtime = createtime; } public String getDetail() { return detail; } public void setDetail(String detail) { this.detail = detail; } @Override public String toString() { return “items{“ + “id=” + id + “, name=’” + name + ‘\’’ + “, price=” + price + “, prc=’” + prc + ‘\’’ + “, createtime=” + createtime + “, detail=’” + detail + ‘\’’ + ‘}’; } }* package com.itheima.service.impl; import com.itheima.dao.ItemsDao; import com.itheima.domain.Items; import com.itheima.service.ItemService; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.stereotype.Service; / 有一个公司开发的常用原则，自己写的类尽量使用注解，如果是框架使用的类，尽量使用配置文件 @Description TODO @Author TT Hun @Data 2019/11/7 20:46 / @Service public class ItemServiceImpl implements ItemService { @Autowired private ItemsDao itemsDao; public Items findById(Integer id) { return itemsDao.findById(id); } } package com.itheima.service; import com.itheima.domain.Items; / @Description TODO @Author TT Hun @Data 2019/11/7 20:45 / public interface ItemService { public Items findById(Integer id); }* &lt;?xml version=”1.0” encoding=”UTF-8” ?&gt; &lt;!DOCTYPE mapper PUBLIC “-//mybatis.org//DTD Mapper 3.0//EN” “http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt; select from items where id = #{id} **applicationContext.xml &lt;?xml version=”1.0” encoding=”UTF-8”?&gt; &lt;context:component-scan base-package=”com.itheima.service”/&gt; &lt;tx:advice id=”advice”&gt; tx:attributes &lt;tx:method name=”save“ propagation=”REQUIRED”/&gt; &lt;tx:method name=”update“ propagation=”REQUIRED”/&gt; &lt;tx:method name=”delete“ propagation=”REQUIRED”/&gt; &lt;tx:method name=”find“ read-only=”true”/&gt; &lt;tx:method name=”“ propagation=”REQUIRED”/&gt; &lt;/tx:attributes&gt; &lt;/tx:advice&gt; aop:config &lt;aop:pointcut id=”pointcut” expression=”execution( com.itheima.service.impl..(..))”/&gt; &lt;aop:advisor advice-ref=”advice” pointcut-ref=”pointcut”/&gt; &lt;/aop:config&gt; log4j.properties # Set root category priority to INFO and its only appender to CONSOLE. #log4j.rootCategory=INFO, CONSOLE debug info warn error fatal log4j.rootCategory=debug, CONSOLE, LOGFILE # Set the enterprise logger category to FATAL and its only appender to CONSOLE. log4j.logger.org.apache.axis.enterprise=FATAL, CONSOLE # CONSOLE is set to be a ConsoleAppender using a PatternLayout. log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} %-6r [%15.15t] %-5p %30.30c %x - %m\n # LOGFILE is set to be a File appender using a PatternLayout. log4j.appender.LOGFILE=org.apache.log4j.FileAppender log4j.appender.LOGFILE.File=d:\axis.log log4j.appender.LOGFILE.Append=true log4j.appender.LOGFILE.layout=org.apache.log4j.PatternLayout log4j.appender.LOGFILE.layout.ConversionPattern=%d{ISO8601} %-6r [%15.15t] %-5p %30.30c %x - %m\n springmvc.xml &lt;?xml version=”1.0” encoding=”UTF-8”?&gt; &lt;context:component-scan base-package=”com.itheima.controller”/&gt; mvc:annotation-driven/ mvc:default-servlet-handler/ itemDetail.jsp &lt;%@ page language=”java” contentType=”text/html; charset=UTF-8” pageEncoding=”UTF-8”%&gt; &lt;%@ taglib uri=”http://java.sun.com/jsp/jstl/core&quot; prefix=”c” %&gt; &lt;%@ taglib uri=”http://java.sun.com/jsp/jstl/fmt&quot; prefix=”fmt”%&gt; &lt;!DOCTYPE html PUBLIC “-//W3C//DTD HTML 4.01 Transitional//EN” “http://www.w3.org/TR/html4/loose.dtd&quot;&gt; Insert title here 商品名称 ${item.name } 商品价格 ${item.price } 生成日期 &lt;fmt:formatDate value=”${item.createtime}” pattern=”yyyy-MM-dd HH:mm:ss”/&gt; 商品简介 ${item.detail} web.xml &lt;!DOCTYPE web-app PUBLIC “-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN” “http://java.sun.com/dtd/web-app_2_3.dtd&quot;&gt; encoding org.springframework.web.filter.CharacterEncodingFilter encoding UTF-8 forceEncoding true encoding / org.springframework.web.context.ContextLoaderListener contextConfigLocation classpath:applicationContext.xml springmvc org.springframework.web.servlet.DispatcherServlet contextConfigLocation classpath:springmvc.xml 1 springmvc / **ItemTest package com.itheima.test; import com.itheima.dao.ItemsDao; import com.itheima.domain.Items; import com.itheima.service.ItemService; import org.junit.Test; import org.springframework.context.ApplicationContext; import org.springframework.context.support.ClassPathXmlApplicationContext; / @Description TODO @Author TT Hun @Data 2019/11/7 20:12 / public class ItemsTest { @Test public void findById(){ // 获取Spring容器 ApplicationContext ac = new ClassPathXmlApplicationContext(“applicationContext.xml”); // 从容器中直接拿到dao的代理对象 ItemsDao itemsDao = ac.getBean(ItemsDao.class); // 调用方法 Items items = itemsDao.findById(1); System.out.println(items.getName()); } / service测试 / @Test public void findById1(){ // 获取Spring容器 ApplicationContext ac = new ClassPathXmlApplicationContext(“applicationContext.xml”); ItemService itemService = ac.getBean(ItemService.class); // 拿到Service之后直接去调用方法 Items items = itemService.findById(1); System.out.println(items.getName()); } }***pom文件： &lt;?xml version=”1.0” encoding=”UTF-8”?&gt; 4.0.0 com.itheima maven_day02_1 1.0-SNAPSHOT war &lt;spring.version&gt;5.0.2.RELEASE&lt;/spring.version&gt; &lt;slf4j.version&gt;1.6.6&lt;/slf4j.version&gt; &lt;log4j.version&gt;1.2.12&lt;/log4j.version&gt; &lt;shiro.version&gt;1.2.3&lt;/shiro.version&gt; &lt;mysql.version&gt;8.0.11&lt;/mysql.version&gt; &lt;mybatis.version&gt;3.4.5&lt;/mybatis.version&gt; &lt;spring.security.version&gt;5.0.1.RELEASE&lt;/spring.security.version&gt; org.springframework spring-context ${spring.version} org.springframework spring-web ${spring.version} org.springframework spring-webmvc ${spring.version} org.springframework spring-tx ${spring.version} org.springframework spring-test ${spring.version} org.mybatis mybatis ${mybatis.version} &lt;!-- 项目依赖jar包 --&gt; &lt;dependencies&gt; &lt;!-- spring --&gt; &lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt; &lt;version&gt;1.6.8&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aop&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-web&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-orm&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;${mysql.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet.jsp&lt;/groupId&gt; &lt;artifactId&gt;jsp-api&lt;/artifactId&gt; &lt;version&gt;2.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;jstl&lt;/groupId&gt; &lt;artifactId&gt;jstl&lt;/artifactId&gt; &lt;version&gt;1.2&lt;/version&gt; &lt;/dependency&gt; &lt;!-- log start --&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;${log4j.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;${slf4j.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;${slf4j.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- log end --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;${mybatis.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt; &lt;version&gt;1.3.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;c3p0&lt;/groupId&gt; &lt;artifactId&gt;c3p0&lt;/artifactId&gt; &lt;version&gt;0.9.1.2&lt;/version&gt; &lt;type&gt;jar&lt;/type&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.pagehelper&lt;/groupId&gt; &lt;artifactId&gt;pagehelper&lt;/artifactId&gt; &lt;version&gt;5.1.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.security&lt;/groupId&gt; &lt;artifactId&gt;spring-security-web&lt;/artifactId&gt; &lt;version&gt;${spring.security.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.security&lt;/groupId&gt; &lt;artifactId&gt;spring-security-config&lt;/artifactId&gt; &lt;version&gt;${spring.security.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.security&lt;/groupId&gt; &lt;artifactId&gt;spring-security-core&lt;/artifactId&gt; &lt;version&gt;${spring.security.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.security&lt;/groupId&gt; &lt;artifactId&gt;spring-security-taglibs&lt;/artifactId&gt; &lt;version&gt;${spring.security.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.0.9&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.tomcat.maven&lt;/groupId&gt; &lt;artifactId&gt;tomcat7-maven-plugin&lt;/artifactId&gt; &lt;version&gt;2.2&lt;/version&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/project&gt; Maven工程拆分与聚合的思想 #Maven父子工程的创建#首先父工程的pom： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;!-- 工程和模块的区别： 工程不等于完整的项目，模块也不等于完整的项目，一个完整的项目看的是代码，代码完整就可以说是一个完整的项目，和工程和 项目没关系的。--&gt; &lt;!-- 那么工程和模块啥区别呢？工程天生只能使用自己的内部资源，工程是天生独立的，后天可以和其他模块建立关联关系，模块天生是属于 父工程的，也就是模块一旦创建，所有父工程的资源都可以使用--&gt; &lt;!-- 子模块天生继承父工程，可以使用付工程所有资源， 子模块之间天生没任何关系--&gt; &lt;!-- 父子工程之间不用建立关系，继承关系是先天的，不需要手动建立 平级之间的关系叫依赖关系。依赖不是先天的，是需要后天建立的--&gt; &lt;groupId&gt;com.itheima&lt;/groupId&gt; &lt;artifactId&gt;maven_day02_parent&lt;/artifactId&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;!-- 下面有几个字模块会显示到这里--&gt; &lt;modules&gt; &lt;module&gt;maven_day02_dao&lt;/module&gt; &lt;module&gt;maven_day02_service&lt;/module&gt; &lt;module&gt;maven_day02_web&lt;/module&gt; &lt;/modules&gt; &lt;!--maven工程要导入jar包的坐标，就必须要考虑解决jar包冲突。 解决jar包冲突的方式一： 第一声明优先原则：哪个jar包的坐标在靠上的位置，这个jar包就是先声明的。 先声明的jar包坐标下的依赖包，可以优先进入项目中。 maven导入jar包中的一些概念： 直接依赖：项目中直接导入的jar包，就是该项目的直接依赖包。 传递依赖：项目中没有直接导入的jar包，可以通过项目直接依赖jar包传递到项目中去。 解决jar包冲突的方式二： 路径近者优先原则。直接依赖路径比传递依赖路径近，那么最终项目进入的jar包会是路径近的直接依赖包。 解决jar包冲突的方式三【推荐使用】： 直接排除法。 当我们要排除某个jar包下依赖包，在配置exclusions标签的时候，内部可以不写版本号。 因为此时依赖包使用的版本和默认和本jar包一样。 --&gt; &lt;!-- 统一管理jar包版本 --&gt; &lt;properties&gt; &lt;spring.version&gt;5.0.2.RELEASE&lt;/spring.version&gt; &lt;slf4j.version&gt;1.6.6&lt;/slf4j.version&gt; &lt;log4j.version&gt;1.2.12&lt;/log4j.version&gt; &lt;shiro.version&gt;1.2.3&lt;/shiro.version&gt; &lt;mysql.version&gt;8.0.11&lt;/mysql.version&gt; &lt;mybatis.version&gt;3.4.5&lt;/mybatis.version&gt; &lt;spring.security.version&gt;5.0.1.RELEASE&lt;/spring.security.version&gt; &lt;/properties&gt; &lt;!-- maven工程是可以分父子依赖关系的。 凡是依赖别的项目后，拿到的别的项目的依赖包，都属于传递依赖。 比如：当前A项目，被B项目依赖。那么我们A项目中所有jar包都会传递到B项目中。 B项目开发者，如果再在B项目中导入一套ssm框架的jar包，对于B项目是直接依赖。 那么直接依赖的jar包就会把我们A项目传递过去的jar包覆盖掉。 为了防止以上情况的出现。我们可以把A项目中主要jar包的坐标锁住，那么其他依赖该项目的项目中， 即便是有同名jar包直接依赖，也无法覆盖。 --&gt; &lt;!-- 锁定jar包版本 --&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-web&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;${mybatis.version}&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;!-- 项目依赖jar包 --&gt; &lt;dependencies&gt; &lt;!-- spring --&gt; &lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt; &lt;version&gt;1.6.8&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aop&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-web&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-orm&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;${mysql.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet.jsp&lt;/groupId&gt; &lt;artifactId&gt;jsp-api&lt;/artifactId&gt; &lt;version&gt;2.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;jstl&lt;/groupId&gt; &lt;artifactId&gt;jstl&lt;/artifactId&gt; &lt;version&gt;1.2&lt;/version&gt; &lt;/dependency&gt; &lt;!-- log start --&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;${log4j.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;${slf4j.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;${slf4j.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- log end --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;${mybatis.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt; &lt;version&gt;1.3.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;c3p0&lt;/groupId&gt; &lt;artifactId&gt;c3p0&lt;/artifactId&gt; &lt;version&gt;0.9.1.2&lt;/version&gt; &lt;type&gt;jar&lt;/type&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.pagehelper&lt;/groupId&gt; &lt;artifactId&gt;pagehelper&lt;/artifactId&gt; &lt;version&gt;5.1.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.security&lt;/groupId&gt; &lt;artifactId&gt;spring-security-web&lt;/artifactId&gt; &lt;version&gt;${spring.security.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.security&lt;/groupId&gt; &lt;artifactId&gt;spring-security-config&lt;/artifactId&gt; &lt;version&gt;${spring.security.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.security&lt;/groupId&gt; &lt;artifactId&gt;spring-security-core&lt;/artifactId&gt; &lt;version&gt;${spring.security.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.security&lt;/groupId&gt; &lt;artifactId&gt;spring-security-taglibs&lt;/artifactId&gt; &lt;version&gt;${spring.security.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.0.9&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.tomcat.maven&lt;/groupId&gt; &lt;artifactId&gt;tomcat7-maven-plugin&lt;/artifactId&gt; &lt;version&gt;2.2&lt;/version&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/project&gt; service层的pom: &lt;?xml version=”1.0” encoding=”UTF-8”?&gt; maven_day02_parent com.itheima 1.0-SNAPSHOT 4.0.0 &lt;artifactId&gt;maven_day02_service&lt;/artifactId&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.itheima&lt;/groupId&gt; &lt;artifactId&gt;maven_day02_dao&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; dao层类似。 传递下来的包是否能用如果说在parent使用了scope这个作用域的标签就不一定在子工程可以使用： Maven工程打开SSM项目的三种启动方式（包括完整代码）首先看一下目录结构，目录结构是父工程结合子工程的]]></content>
  </entry>
  <entry>
    <title><![CDATA[SSM整合案例]]></title>
    <url>%2F2019%2F11%2F07%2FSSM%E6%95%B4%E5%90%88%E6%A1%88%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[SSM：就是springmvc,spring,ssm 首先整合一下spring框架里面的东西。 以下是目录结构 首先是写spring里面的结构，不包括springmvc和mybatis: package cn.itcast.controller; /** * @Description 账户web * @Author TT Hun * @Data 2019/11/9 16:04 */ public class AccountController { } package cn.itcast.dao; import java.util.List; /** * @Description TODO * @Author TT Hun * @Data 2019/11/9 15:54 */ public interface Account { /** * 查询所有账户 * @return */ public List&lt;Account&gt; findAll(); /** * 保存账户信息 * @param account */ public void saveAccount(Account account); } package cn.itcast.domain; import java.io.Serializable; /** * @Description TODO * @Author TT Hun * @Data 2019/11/9 15:52 */ public class Account implements Serializable { private Integer id; private String name; private Double money; public Integer getId() { return id; } public void setId(Integer id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public Double getMoney() { return money; } public void setMoney(Double money) { this.money = money; } @Override public String toString() { return &quot;Account{&quot; + &quot;id=&quot; + id + &quot;, name=&apos;&quot; + name + &apos;\&apos;&apos; + &quot;, money=&quot; + money + &apos;}&apos;; } } package cn.itcast.service.impl; import cn.itcast.dao.Account; import cn.itcast.service.AccountService; import org.springframework.stereotype.Service; import java.util.List; /** * @Description TODO * @Author TT Hun * @Data 2019/11/9 16:02 */ @Service(&quot;accountService&quot;) public class AccountServiceImpl implements AccountService { @Override public List&lt;Account&gt; findAll() { System.out.println(&quot;业务层查询所有账户信息&quot;); return null; } @Override public void saveAccount(Account account) { System.out.println(&quot;业务层保存账户&quot;); } } package cn.itcast.service; import cn.itcast.dao.Account; import java.util.List; /** * @Description TODO * @Author TT Hun * @Data 2019/11/9 15:58 */ public interface AccountService { /** * 查询所有账户 * @return */ public List&lt;Account&gt; findAll(); /** * 保存账户信息 * @param account */ public void saveAccount(Account account); } package cn.itcast.test; import cn.itcast.service.AccountService; import org.junit.Test; import org.springframework.context.ApplicationContext; import org.springframework.context.support.ClassPathXmlApplicationContext; /** * @Description TODO * @Author TT Hun * @Data 2019/11/9 16:21 */ public class testSpring { @Test public void run1(){ // 加载配置文件 ApplicationContext ac = new ClassPathXmlApplicationContext(&quot;classpath:applicationContext.xml&quot;); // 获取对象 AccountService as = (AccountService)ac.getBean(&quot;accountService&quot;); // 调用方法 as.findAll(); } } &lt;?xml version=”1.0” encoding=”UTF-8”?&gt; &lt;!--开启注解的扫描，希望处理service和dao，controller不需要Spring框架去处理--&gt; &lt;context:component-scan base-package=&quot;cn.itcast&quot; &gt; &lt;!--配置哪些注解不扫描--&gt; &lt;context:exclude-filter type=&quot;annotation&quot; expression=&quot;org.springframework.stereotype.Controller&quot; /&gt; &lt;/context:component-scan&gt; &lt;/beans&gt; web.xml和index.jsp先不动 然后再看这个springmvc怎么写： package cn.itcast.controller; import org.springframework.stereotype.Controller; import org.springframework.web.bind.annotation.RequestMapping; /** * @Description 账户web * @Author TT Hun * @Data 2019/11/9 16:04 */ @Controller @RequestMapping(&quot;/account&quot;) public class AccountController { @RequestMapping(&quot;/findAll&quot;) public String findAll(){ System.out.println(&quot;查询所有帐户信息&quot;); return &quot;list&quot;; } } testSpring类 package cn.itcast.test; import cn.itcast.service.AccountService; import org.junit.Test; import org.springframework.context.ApplicationContext; import org.springframework.context.support.ClassPathXmlApplicationContext; /** * @Description TODO * @Author TT Hun * @Data 2019/11/9 16:21 */ public class testSpring { @Test public void run1(){ // 加载配置文件 ApplicationContext ac = new ClassPathXmlApplicationContext(&quot;classpath:applicationContext.xml&quot;); // 获取对象 AccountService as = (AccountService)ac.getBean(&quot;accountService&quot;); // 调用方法 as.findAll(); } } springmvc.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot; http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;!--开启注解扫描，只扫描Controller注解--&gt; &lt;context:component-scan base-package=&quot;cn.itcast&quot;&gt; &lt;context:include-filter type=&quot;annotation&quot; expression=&quot;org.springframework.stereotype.Controller&quot; /&gt; &lt;/context:component-scan&gt; &lt;!--配置的视图解析器对象--&gt; &lt;bean id=&quot;internalResourceViewResolver&quot; class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt; &lt;property name=&quot;prefix&quot; value=&quot;/WEB-INF/pages/&quot;/&gt; &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot;/&gt; &lt;/bean&gt; &lt;!--过滤静态资源--&gt; &lt;mvc:resources location=&quot;/css/&quot; mapping=&quot;/css/**&quot; /&gt; &lt;mvc:resources location=&quot;/images/&quot; mapping=&quot;/images/**&quot; /&gt; &lt;mvc:resources location=&quot;/js/&quot; mapping=&quot;/js/**&quot; /&gt; &lt;!--开启SpringMVC注解的支持--&gt; &lt;mvc:annotation-driven/&gt; &lt;/beans&gt; list.jsp &lt;%-- Created by IntelliJ IDEA. User: Hun Date: 2019/11/9 Time: 17:27 To change this template use File | Settings | File Templates. --%&gt; &lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Title&lt;/title&gt; &lt;h2&gt;查询所有账户信息&lt;/h2&gt; &lt;/head&gt; &lt;body&gt; &lt;/body&gt; &lt;/html&gt; web.xml &lt;!DOCTYPE web-app PUBLIC &quot;-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN&quot; &quot;http://java.sun.com/dtd/web-app_2_3.dtd&quot; &gt; &lt;web-app&gt; &lt;display-name&gt;Archetype Created Web Application&lt;/display-name&gt; &lt;!--配置前端控制器--&gt; &lt;servlet&gt; &lt;servlet-name&gt;dispatcherServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;!--加载springmvc.xml配置文件--&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:springmvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;!--启动服务器，创建该servlet--&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;dispatcherServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;!--解决中文乱码的过滤器--&gt; &lt;filter&gt; &lt;filter-name&gt;characterEncodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;characterEncodingFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt; &lt;/web-app&gt;]]></content>
  </entry>
  <entry>
    <title><![CDATA[Oracle使用]]></title>
    <url>%2F2019%2F11%2F06%2FOracle%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Oracle的查询单行查询upper代表将小写转换成大写，lower大写变小写round四舍五入，后面参数保留几位。trunc直接截取几位。 转换函数：通用函数：nvl函数：如果是空的话就按照后面的给定的数值来计算： 多行函数（聚合函数） 分组查询分组查询中，出现在group by后面的原始列才能出现在select后面 ，没有出现在group by后面的列想在select后面出现必须加上聚合函数，也就是下面的e.sal必须要使用完聚合函数之后才能用。下面这个SQL在经历了group by之后已经变成了三行两列的状况，也就是有几个部门就有几组数据了。所以如果在聚合完了之后的再去查询e.ename，一个部门肯定不止一个人，怎么都没法查询，所以说select后面出现的必须是一个组的共同特性。聚合函数是作用于多列返回一个值，也就是聚合函数有一个特性，可以把多行记录变成一个值。 如果说把avg(e.sal) 设置一个别名asal那么在写having条件的时候能否直接使用asal&gt;200呢，不行的 所有条件都不能使用别名判断，因为在查询的时候都是分先后顺序的。 说一个错误的案例 多表查询 内连接和等值连接一样 左外连接和右外连接的区别 oracle中专用连接 继续多表查询首先如果写一个复杂的查询先把架子搭起来，也就是： select from where 然后在继续往里面写 首先看上面这张图，要写这个SQL首先考虑员工在哪一张表，领导姓名在哪张表，然后放在from后面 首先这个数据库下EMP表可以看到多个信息，可以看见员工也可以看见这个员工对应的领导 自连接其中mgr是领导编号，empno是员工编号 子查询下面这个查询的前提是这个ename是主键，也就是唯一可以写等号 要不然下面这种写法比较准确 分页查询 这个分页查询的仔细解释，涉及到一个rownum的问题。 首先看下面这样一个错误的案例：下面这个错误的案例的原因是由于顺序的问题首先是select 的内容，做完之后rownum就是在select之后就排序了，这个rownum也就相当于一个独立于这个表之外的一个列的感觉。这样的话然后再去使用where条件的话rownum已经是乱序的了，所以，这样的话是拿不到前几个的 那么rownum乱序怎么理解呢？看如下图，也就是说上面的where条件是在下面这种情况下加上的。 所以得出来一个结论，也就是排序操作会影响rownum的顺序，我们可以先排序在去加行号。 这样就已经不乱了。 也就是说如果涉及到排序，但是还需要使用rownum的话，我们可以再次嵌套查询，就可以避开rownum乱的顺序了。 接着来说分页查询的一个失败的案例 看上面的这个发现这样子查询出来就一条数据都没有了，什么原因呢 原因就是因为rownum是在select之后按照顺序排列的，rownum是按照顺序依次递增不能跳着顺序走。那么这句SQL就设计到一个问题，也就是select先执行还是where先执行的问题。我们知道是where先走，当where先的时候，查询第一条记录本来要在该记录上加上一个行号1但是一看要通过rownum=1来判断rownum&gt;5的条件，这样一来怎么都不能满足，第一个rownum就加不上，也不能跳跃着加，所以就查不到了，只不过是条件不满足而已。 所以rownum不能直接写大于一个正数，但是我们可以通过嵌套间接写大于也就是如下 分页查询结束 Oracle的对象视图 假如总部一个查询语句，分部一个查询语句各自查一套，总部突然卖出了十万雨伞，但是分部没有及时去查询，就出现总部已经没货了，所以就让总部从表中查询，分部看视图，因为视图里面根本就没数据，只要总部的数据改变了，分部立马就能看到最新的数据。这是视图经常使用的地方。 索引 Oracle编程 这个地方没有看感觉用处不大]]></content>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC第二天]]></title>
    <url>%2F2019%2F11%2F03%2FSpringMVC%E7%AC%AC%E4%BA%8C%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[第一章：相应数据和结果视图响应之返回值是String类型的 首先说一个以后开发常见的方式：首先从response.jsp发请求，然后我从Controller后台查出来，存到model，存到request里，然后在转发到页面。再从request域里取出来。 package cn.itcast.controller; import cn.itcast.domain.User; import org.springframework.stereotype.Controller; import org.springframework.ui.Model; import org.springframework.web.bind.annotation.RequestMapping; /** * @Description TODO * @Author TT Hun * @Data 2019/11/3 21:26 */ @Controller @RequestMapping(&quot;/user&quot;) public class UserController { @RequestMapping(&quot;/testString&quot;) public String testString(Model model){ System.out.println(&quot;testString方法执行了...&quot;); // 模拟从数据库中查询对象 User user = new User(); user.setUsername(&quot;美美&quot;); user.setPassword(&quot;123&quot;); user.setAge(30); // 把对象存起来 model.addAttribute(&quot;user&quot;,user); return &quot;success&quot;; } } package cn.itcast.domain; import java.io.Serializable; /** * @Description TODO * @Author TT Hun * @Data 2019/11/3 22:00 */ public class User implements Serializable { private String password; private Integer age; private String username; public String getPassword() { return password; } public void setPassword(String password) { this.password = password; } public Integer getAge() { return age; } public void setAge(Integer age) { this.age = age; } public String getUsername() { return username; } public void setUsername(String username) { this.username = username; } } springmvc.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot; http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;!-- 开启注解扫描 扫描这个路径下的包，就可以在--&gt; &lt;context:component-scan base-package=&quot;cn.itcast&quot;/&gt; &lt;!-- 视图解析器对象 视图解析器的类名是固定的，然后配置通过这个视图解析器就可以让controller里面return的东西直接跳转指定的jsp页面 但是还需要配置2个属性，一个是Prefix就是配置找文件的路径 suffix指的是配置的后缀名字。最后也就是找/web-if/pages/下的后缀是jsp的文件--&gt; &lt;bean id=&quot;internalResourceViewResolver&quot; class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt; &lt;property name=&quot;prefix&quot; value=&quot;/WEB-INF/pages/&quot;/&gt; &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot;/&gt; &lt;/bean&gt; &lt;!-- 开启SpringMVC框架注解的支持 标签是固定的--&gt; &lt;mvc:annotation-driven /&gt; &lt;/beans&gt; success.jsp &lt;%-- Created by IntelliJ IDEA. User: Hun Date: 2019/11/3 Time: 21:22 To change this template use File | Settings | File Templates. --%&gt; &lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; isELIgnored=&quot;false&quot; %&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Title&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h2&gt;执行成功&lt;/h2&gt; ${user.username} ${user.password} &lt;/body&gt; &lt;/html&gt; web.xml &lt;!DOCTYPE web-app PUBLIC &quot;-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN&quot; &quot;http://java.sun.com/dtd/web-app_2_3.dtd&quot; &gt; &lt;web-app&gt; &lt;display-name&gt;Archetype Created Web Application&lt;/display-name&gt; &lt;!--配置前端控制器--&gt; &lt;servlet&gt; &lt;servlet-name&gt;dispatcherServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:springmvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;dispatcherServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;!--配置解决中文乱码的过滤器--&gt; &lt;filter&gt; &lt;filter-name&gt;characterEncodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;characterEncodingFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt; &lt;/web-app&gt; &lt;%-- Created by IntelliJ IDEA. User: Hun Date: 2019/11/3 Time: 21:25 To change this template use File | Settings | File Templates. --%&gt; &lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Title&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;a href=&quot;user/testString&quot; &gt;testString&lt;/a&gt; &lt;/body&gt; &lt;/html&gt; 响应值值返回值是void类型第一种方法：编写请求转发程序跳转到jsp界面package cn.itcast.controller; import cn.itcast.domain.User; import org.springframework.stereotype.Controller; import org.springframework.ui.Model; import org.springframework.web.bind.annotation.RequestMapping; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpServletResponse; /** * @Description TODO * @Author TT Hun * @Data 2019/11/3 21:26 */ @Controller @RequestMapping(&quot;/user&quot;) public class UserController { /** * 返回值类型是一个字符串 * @param model * @return */ @RequestMapping(&quot;/testString&quot;) public String testString(Model model){ System.out.println(&quot;testString方法执行了...&quot;); // 模拟从数据库中查询对象 User user = new User(); user.setUsername(&quot;美美&quot;); user.setPassword(&quot;123&quot;); user.setAge(30); // 把对象存起来 model.addAttribute(&quot;user&quot;,user); return &quot;success&quot;; } /** * 返回值类型是Void * 请求转发是一次请求，请求路径不用编写项目名字 * @param * @return */ @RequestMapping(&quot;/testVoid&quot;) public void testVoid(HttpServletRequest request ,HttpServletResponse response)throws Exception{ System.out.println(&quot;testVoid方法执行了...&quot;); // 便携请求转发的程序，手动便携转发程序不会通过视图解析器，所以要自己配置WEB-INF/pages/success.jsp需要自己提供完整目录 request.getRequestDispatcher(&quot;/WEB-INF/pages/success.jsp&quot;).forward(request,response); return ; } } package cn.itcast.domain; import java.io.Serializable; /** * @Description TODO * @Author TT Hun * @Data 2019/11/3 22:00 */ public class User implements Serializable { private String password; private Integer age; private String username; public String getPassword() { return password; } public void setPassword(String password) { this.password = password; } public Integer getAge() { return age; } public void setAge(Integer age) { this.age = age; } public String getUsername() { return username; } public void setUsername(String username) { this.username = username; } } &lt;%-- Created by IntelliJ IDEA. User: Hun Date: 2019/11/3 Time: 21:25 To change this template use File | Settings | File Templates. --%&gt; &lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Title&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;a href=&quot;user/testString&quot; &gt;testString&lt;/a&gt; &lt;br&gt; &lt;a href=&quot;user/testVoid&quot;&gt;testVoid&lt;/a&gt; &lt;/body&gt; &lt;/html&gt; success.jsp &lt;%-- Created by IntelliJ IDEA. User: Hun Date: 2019/11/3 Time: 21:22 To change this template use File | Settings | File Templates. --%&gt; &lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; isELIgnored=&quot;false&quot; %&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Title&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h2&gt;执行成功&lt;/h2&gt; ${user.username} ${user.password} &lt;/body&gt; &lt;/html&gt; 第二种方法：重定向 第三种方法：直接响应 响应之返回值是ModelAndView类型这个和返回值是String基本是一样的，只不过写法不同，返回值String底层也是调用这个的。 转发和重定向有个问题没搞懂，重定向智能定位到根目录下的文件吗。为什么写和上面一样的路径不能跳转。是一个问题，尚未认真考虑解决呢。 ResponseBody注解响应json数据页面发送一个axis请求，是一个异步请求，后台需要把对象转换成json的字符串响应回去可以通过ResponseBody这个注解完成这个任务。 这个地方需要告诉前端控制器不是所有请求都拦截的，静态资源比如.js和图片是不需要拦截的。下面这地方配置为/证明都拦截还需要在springmvc.xml中配置哪些不拦截 先看目录结构：里面的js文件视频中是从资料中直接拿到的。 下面是整个代码的结构： response.jsp界面 &lt;%-- Created by IntelliJ IDEA. User: Hun Date: 2019/11/3 Time: 21:25 To change this template use File | Settings | File Templates. --%&gt; &lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Title&lt;/title&gt; &lt;%--下面这个是吧js引入到了jsp里面了--%&gt; &lt;script src=&quot;js/jquery.min.js&quot;&gt;&lt;/script&gt; &lt;script&gt; // 页面加载，绑定点击时间 $(function(){ $(&quot;#btn&quot;).click(function(){ alert(&quot;hello btn&quot;); }); }); &lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;a href=&quot;user/testString&quot; &gt;testString&lt;/a&gt; &lt;br&gt; &lt;a href=&quot;user/testVoid&quot;&gt;testVoid&lt;/a&gt; 5&lt;br&gt; &lt;%--转发或者重定向--%&gt; &lt;a href=&quot;user/testForwardOrRedirect&quot;&gt;testForwardOrRedirect&lt;/a&gt; &lt;%--写一个按钮--%&gt; &lt;button id=&quot;btn&quot;&gt;发送ajax的请求&lt;/button&gt; &lt;/body&gt; &lt;/html&gt; 在springmvc.xml文件中设置哪些不拦截； &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot; http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;!-- 开启注解扫描 扫描这个路径下的包，就可以在--&gt; &lt;context:component-scan base-package=&quot;cn.itcast&quot;/&gt; &lt;!-- 视图解析器对象 视图解析器的类名是固定的，然后配置通过这个视图解析器就可以让controller里面return的东西直接跳转指定的jsp页面 但是还需要配置2个属性，一个是Prefix就是配置找文件的路径 suffix指的是配置的后缀名字。最后也就是找/web-if/pages/下的后缀是jsp的文件--&gt; &lt;bean id=&quot;internalResourceViewResolver&quot; class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt; &lt;property name=&quot;prefix&quot; value=&quot;/WEB-INF/pages/&quot;/&gt; &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot;/&gt; &lt;/bean&gt; &lt;!--前端控制器，哪些静态资源不拦截，location写法就是以后js下所有的请求都不拦截，mapping就是和映射请求有关的,以后jss-图片也需要配置上不拦截--&gt; &lt;mvc:resources mapping=&quot;/js/&quot; location=&quot;/js/**&quot;/&gt; &lt;!-- 开启SpringMVC框架注解的支持 标签是固定的--&gt; &lt;mvc:annotation-driven /&gt; &lt;/beans&gt; 响应json数据之发送ajax请求package cn.itcast.controller; import cn.itcast.domain.User;import org.springframework.stereotype.Controller;import org.springframework.ui.Model;import org.springframework.web.bind.annotation.RequestBody;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.servlet.ModelAndView; import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse; /** @Description TODO @Author TT Hun @Data 2019/11/3 21:26*/@Controller@RequestMapping(“/user”)public class UserController { /** 模拟异步请求和响应过程 @param @return*/@RequestMapping(“/testAjax”)public void testAjax(@RequestBody String body){ System.out.println(“testForwardOrRedirect开始执行”); System.out.println(body);}} &lt;%-- Created by IntelliJ IDEA. User: Hun Date: 2019/11/3 Time: 21:25 To change this template use File | Settings | File Templates. --%&gt; &lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Title&lt;/title&gt; &lt;%--下面这个是吧js引入到了jsp里面了--%&gt; &lt;script src=&quot;js/jquery.min.js&quot;&gt;&lt;/script&gt; &lt;script&gt; // 页面加载，绑定点击时间 $(function(){ $(&quot;#btn&quot;).click(function(){ // alert(&quot;hello btn&quot;); // 发送ajax请求 $.ajax({ // 编写json格式 url:&quot;user/testAjax&quot;, //给服务器端传递的类型 contentType:&quot;application/json;charset=utf-8&quot;, data:&apos;{&quot;username&quot;:&quot;呵呵&quot;,&quot;password&quot;:&quot;123&quot;,&quot;age&quot;:&quot;30&quot;}&apos;, dataType:&quot;json&quot;, type:&quot;post&quot;, success:function (data){ // data指的是服务器端Json相应的数据，进行解析 } }) }); }); &lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;a href=&quot;user/testString&quot; &gt;testString&lt;/a&gt; &lt;br&gt; &lt;a href=&quot;user/testVoid&quot;&gt;testVoid&lt;/a&gt; 5&lt;br&gt; &lt;%--转发或者重定向--%&gt; &lt;a href=&quot;user/testForwardOrRedirect&quot;&gt;testForwardOrRedirect&lt;/a&gt; &lt;%--写一个按钮--%&gt; &lt;button id=&quot;btn&quot;&gt;发送ajax的请求&lt;/button&gt; &lt;/body&gt; &lt;/html&gt; 结果直接拿到请求体 怎么返回的json数据直接封装到对象当中 springmvc框架已经帮我们做好了很多，如果Key和javabean中属性相同，可以直接封装到javabean里面，但是需要额外的jar包做类型转换。把串转成对象或者把对象转成串。 package cn.itcast.controller; import cn.itcast.domain.User; import org.springframework.stereotype.Controller; import org.springframework.ui.Model; import org.springframework.web.bind.annotation.RequestBody; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.ResponseBody; import org.springframework.web.servlet.ModelAndView; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpServletResponse; /** * @Description TODO * @Author TT Hun * @Data 2019/11/3 21:26 */ @Controller @RequestMapping(&quot;/user&quot;) public class UserController { @RequestMapping(&quot;/testAjax&quot;) public @ResponseBody User testAjax(@RequestBody User user){ System.out.println(&quot;testAjax开始执行&quot;); // 客户端发送的ajax请求，传递的是json字符串，后台已经把字符串封装到user对象当中去了 System.out.println(user); // 做一个响应 user.setUsername(&quot;哈哈&quot;); user.setAge(40); user.setPassword(&quot;123&quot;); return user; } } package cn.itcast.domain; import java.io.Serializable; /** * @Description TODO * @Author TT Hun * @Data 2019/11/3 22:00 */ public class User implements Serializable { private String password; private Integer age; private String username; public String getPassword() { return password; } public void setPassword(String password) { this.password = password; } public Integer getAge() { return age; } public void setAge(Integer age) { this.age = age; } public String getUsername() { return username; } public void setUsername(String username) { this.username = username; } @Override public String toString() { return &quot;User{&quot; + &quot;password=&apos;&quot; + password + &apos;\&apos;&apos; + &quot;, age=&quot; + age + &quot;, username=&apos;&quot; + username + &apos;\&apos;&apos; + &apos;}&apos;; } } &lt;%-- Created by IntelliJ IDEA. User: Hun Date: 2019/11/3 Time: 21:25 To change this template use File | Settings | File Templates. --%&gt; &lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Title&lt;/title&gt; &lt;%--下面这个是吧js引入到了jsp里面了--%&gt; &lt;script src=&quot;js/jquery.min.js&quot;&gt;&lt;/script&gt; &lt;script&gt; // 页面加载，绑定点击时间 $(function(){ $(&quot;#btn&quot;).click(function(){ // alert(&quot;hello btn&quot;); // 发送ajax请求 $.ajax({ // 编写json格式 url:&quot;user/testAjax&quot;, //给服务器端传递的类型 contentType:&quot;application/json;charset=utf-8&quot;, data:&apos;{&quot;username&quot;:&quot;呵呵&quot;,&quot;password&quot;:&quot;123&quot;,&quot;age&quot;:&quot;30&quot;}&apos;, dataType:&quot;json&quot;, type:&quot;post&quot;, success:function (data){ // data指的是服务器端Json相应的数据，进行解析 alert(data); alert(data.username); alert(data.age); alert(data.password); } }) }); }); &lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;a href=&quot;user/testString&quot; &gt;testString&lt;/a&gt; &lt;br&gt; &lt;a href=&quot;user/testVoid&quot;&gt;testVoid&lt;/a&gt; 5&lt;br&gt; &lt;%--转发或者重定向--%&gt; &lt;a href=&quot;user/testForwardOrRedirect&quot;&gt;testForwardOrRedirect&lt;/a&gt; &lt;%--写一个按钮--%&gt; &lt;button id=&quot;btn&quot;&gt;发送ajax的请求&lt;/button&gt; &lt;/body&gt; &lt;/html&gt; 第二章：SpringMVC实现文件上传文件上传回顾默认表单提交数据的格式是一组键值对的方式username=hehe&amp;password=123但是一旦把取值从默认变成multipart会把提交的表单分成几个部分，里面存储数据，可能包含文本框的值也可能包含上传选择文件的内容。 然后请求正文的内容变成： 这时候表单用分隔符的方式把它分成了若干个部分。那么怎么去解析呢？通过一个jar包的组件 首先环境搭建配置springmvc.xml配置web.xml配置Pom.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;!-- $Id: pom.xml 642118 2008-03-28 08:04:16Z reinhard $ --&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;packaging&gt;war&lt;/packaging&gt; &lt;name&gt;springmvc_day02_02_fileupload&lt;/name&gt; &lt;groupId&gt;cn.itcast&lt;/groupId&gt; &lt;artifactId&gt;springmvc_day02_02_fileupload&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;spring.version&gt;5.0.2.RELEASE&lt;/spring.version&gt; &lt;/properties&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.mortbay.jetty&lt;/groupId&gt; &lt;artifactId&gt;maven-jetty-plugin&lt;/artifactId&gt; &lt;version&gt;6.1.7&lt;/version&gt; &lt;configuration&gt; &lt;connectors&gt; &lt;connector implementation=&quot;org.mortbay.jetty.nio.SelectChannelConnector&quot;&gt; &lt;port&gt;8888&lt;/port&gt; &lt;maxIdleTime&gt;30000&lt;/maxIdleTime&gt; &lt;/connector&gt; &lt;/connectors&gt; &lt;webAppSourceDirectory&gt;${project.build.directory}/${pom.artifactId}-${pom.version}&lt;/webAppSourceDirectory&gt; &lt;contextPath&gt;/&lt;/contextPath&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-web&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;servlet-api&lt;/artifactId&gt; &lt;version&gt;2.5&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;commons-fileupload&lt;/groupId&gt; &lt;artifactId&gt;commons-fileupload&lt;/artifactId&gt; &lt;version&gt;1.3.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;commons-io&lt;/groupId&gt; &lt;artifactId&gt;commons-io&lt;/artifactId&gt; &lt;version&gt;2.4&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet.jsp&lt;/groupId&gt; &lt;artifactId&gt;jsp-api&lt;/artifactId&gt; &lt;version&gt;2.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; web.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;web-app version=&quot;2.4&quot; xmlns=&quot;http://java.sun.com/xml/ns/j2ee&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://java.sun.com/xml/ns/j2ee http://java.sun.com/xml/ns/j2ee/web-app_2_4.xsd&quot;&gt; &lt;!--配置前端控制器--&gt; &lt;servlet&gt; &lt;servlet-name&gt;dispatcherServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:springmvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;dispatcherServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;!--配置解决中文乱码的过滤器--&gt; &lt;filter&gt; &lt;filter-name&gt;characterEncodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;characterEncodingFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt; &lt;/web-app&gt; springmvc.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot; http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;!-- 开启注解扫描 扫描这个路径下的包，就可以在--&gt; &lt;context:component-scan base-package=&quot;cn.itcast&quot;/&gt; &lt;!-- 视图解析器对象 视图解析器的类名是固定的，然后配置通过这个视图解析器就可以让controller里面return的东西直接跳转指定的jsp页面 但是还需要配置2个属性，一个是Prefix就是配置找文件的路径 suffix指的是配置的后缀名字。最后也就是找/web-if/pages/下的后缀是jsp的文件--&gt; &lt;bean id=&quot;internalResourceViewResolver&quot; class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt; &lt;property name=&quot;prefix&quot; value=&quot;/WEB-INF/pages/&quot;/&gt; &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot;/&gt; &lt;/bean&gt; &lt;!--前端控制器，哪些静态资源不拦截，location写法就是以后js下所有的请求都不拦截，mapping就是和映射请求有关的,以后jss-图片也需要配置上不拦截--&gt; &lt;mvc:resources mapping=&quot;/js/&quot; location=&quot;/js/**&quot;/&gt; &lt;!-- 开启SpringMVC框架注解的支持 标签是固定的--&gt; &lt;mvc:annotation-driven /&gt; &lt;/beans&gt; 文件上传之传统方式上传代码index.jsp文件： &lt;%– Created by IntelliJ IDEA. User: Hun Date: 2019/11/5 Time: 11:25 To change this template use File | Settings | File Templates. –%&gt; &lt;%@ page contentType=”text/html;charset=UTF-8” language=”java” %&gt; 文件上传 文件上传 选择上传文件： success.jsp &lt;%– Created by IntelliJ IDEA. User: Hun Date: 2019/11/5 Time: 11:28 To change this template use File | Settings | File Templates. –%&gt; &lt;%@ page contentType=”text/html;charset=UTF-8” language=”java” %&gt; Title 上传文件成功 package cn.itcast.controller; import org.apache.commons.fileupload.FileItem; import org.apache.commons.fileupload.disk.DiskFileItemFactory; import org.apache.commons.fileupload.servlet.ServletFileUpload; import org.springframework.stereotype.Controller; import org.springframework.web.bind.annotation.RequestMapping; import javax.servlet.http.HttpServletRequest; import java.io.File; import java.util.List; import java.util.UUID; /** * @Description TODO * @Author TT Hun * @Data 2019/11/5 11:26 */ @Controller @RequestMapping(&quot;/user&quot;) public class UserController { /** * 文件上传 * @return */ @RequestMapping(&quot;/fileupload1&quot;) public String fileUpload1(HttpServletRequest request) throws Exception{ System.out.println(&quot;文件上传&quot;); // 可以使用fileupload这个组件完成上传 // 上传的位置 String path = request.getSession().getServletContext().getRealPath(&quot;/uploads/&quot;); // 判断改路径是否存在： File file = new File(path); if(!file.exists()){ file.mkdirs(); } // 解析request对象获取到上传的文件项 DiskFileItemFactory factory=new DiskFileItemFactory(); ServletFileUpload upload = new ServletFileUpload(factory); // 解析request List&lt;FileItem&gt; items = upload.parseRequest(request); // 遍历 for(FileItem item:items){ // 进行判断，判断item对象是否是上传文件项目 if(item.isFormField()){ // 如果是true则是普通表单项目，如果是false则是上传文件项 }else{ // 说明是上传文件项 // 获取到上传文件的名称 String filename = item.getName(); // 把文件名设置成唯一值，uuid String uuid = UUID.randomUUID().toString().replace(&quot;-&quot;,&quot;&quot;); filename = uuid+&quot;_&quot;+filename; // 完成文件上传 item.write(new File(path,filename)); // 删除临时文件。一旦大于10KB就生成临时文件 item.delete(); } } return &quot;success&quot;; } } 最终结果的图片落在这里： SpringMVC上传文件首先看一下整体原理：解释一下下面的图：也就是上传的文件通过写在域里面通过request传递到前端控制器，前端控制器通过配置好的文件解析器解析request,然后拿到上传文件。再传递给Controller里面和文件解析器中相同参数名的参数。然后再调用他的方法对他进行使用。完整代码如下： Controller package cn.itcast.controller; import org.apache.commons.fileupload.FileItem; import org.apache.commons.fileupload.disk.DiskFileItemFactory; import org.apache.commons.fileupload.servlet.ServletFileUpload; import org.springframework.stereotype.Controller; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.multipart.MultipartFile; import javax.servlet.http.HttpServletRequest; import java.io.File; import java.util.List; import java.util.UUID; /** * @Description TODO * @Author TT Hun * @Data 2019/11/5 11:26 */ @Controller @RequestMapping(&quot;/user&quot;) public class UserController { /** * 传统方式文件上传 * @return */ @RequestMapping(&quot;/fileupload1&quot;) public String fileUpload1(HttpServletRequest request) throws Exception{ System.out.println(&quot;文件上传&quot;); // 可以使用fileupload这个组件完成上传 // 上传的位置 String path = request.getSession().getServletContext().getRealPath(&quot;/uploads/&quot;); // 判断改路径是否存在： File file = new File(path); if(!file.exists()){ file.mkdirs(); } // 解析request对象获取到上传的文件项 DiskFileItemFactory factory=new DiskFileItemFactory(); ServletFileUpload upload = new ServletFileUpload(factory); // 解析request List&lt;FileItem&gt; items = upload.parseRequest(request); // 遍历 for(FileItem item:items){ // 进行判断，判断item对象是否是上传文件项目 if(item.isFormField()){ // 如果是true则是普通表单项目，如果是false则是上传文件项 }else{ // 说明是上传文件项 // 获取到上传文件的名称 String filename = item.getName(); // 把文件名设置成唯一值，uuid String uuid = UUID.randomUUID().toString().replace(&quot;-&quot;,&quot;&quot;); filename = uuid+&quot;_&quot;+filename; // 完成文件上传 item.write(new File(path,filename)); // 删除临时文件。一旦大于10KB就生成临时文件 item.delete(); } } return &quot;success&quot;; } /** * SpringMVC方式文件上传 * @return */ @RequestMapping(&quot;/fileupload2&quot;) public String fileUpload2(HttpServletRequest request ,MultipartFile upload) throws Exception{ System.out.println(&quot;文件上传&quot;); // 可以使用fileupload这个组件完成上传 // 上传的位置 String path = request.getSession().getServletContext().getRealPath(&quot;/uploads/&quot;); // 判断改路径是否存在： File file = new File(path); if(!file.exists()){ file.mkdirs(); } // 说明是上传文件项 // 获取到上传文件的名称 String filename = upload.getOriginalFilename(); // 把文件名设置成唯一值，uuid String uuid = UUID.randomUUID().toString().replace(&quot;-&quot;,&quot;&quot;); filename = uuid+&quot;_&quot;+filename; // 完成文件上传 upload.transferTo(new File(path,filename)); return &quot;success&quot;; } } springmvc.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot; http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;!-- 开启注解扫描 扫描这个路径下的包，就可以在--&gt; &lt;context:component-scan base-package=&quot;cn.itcast&quot;/&gt; &lt;!-- 视图解析器对象 视图解析器的类名是固定的，然后配置通过这个视图解析器就可以让controller里面return的东西直接跳转指定的jsp页面 但是还需要配置2个属性，一个是Prefix就是配置找文件的路径 suffix指的是配置的后缀名字。最后也就是找/web-if/pages/下的后缀是jsp的文件--&gt; &lt;bean id=&quot;internalResourceViewResolver&quot; class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt; &lt;property name=&quot;prefix&quot; value=&quot;/WEB-INF/pages/&quot;/&gt; &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot;/&gt; &lt;/bean&gt; &lt;!--前端控制器，哪些静态资源不拦截，location写法就是以后js下所有的请求都不拦截，mapping就是和映射请求有关的,以后jss-图片也需要配置上不拦截--&gt; &lt;mvc:resources mapping=&quot;/js/&quot; location=&quot;/js/**&quot;/&gt; &lt;!-- 配置文件解析器对象--&gt; &lt;bean id=&quot;multipartResolver&quot; class=&quot;org.springframework.web.multipart.commons.CommonsMultipartResolver&quot;&gt; &lt;!-- 这个是配置最大上传文件。value是字节为单位，下面这个也就是10M --&gt; &lt;property name=&quot;maxUploadSize&quot; value=&quot;10485760&quot; /&gt; &lt;/bean&gt; &lt;!-- 开启SpringMVC框架注解的支持 标签是固定的--&gt; &lt;mvc:annotation-driven /&gt; &lt;/beans&gt; index.jsp &lt;%-- Created by IntelliJ IDEA. User: Hun Date: 2019/11/5 Time: 11:25 To change this template use File | Settings | File Templates. --%&gt; &lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;文件上传&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h3&gt;文件上传&lt;/h3&gt; &lt;form action=&quot;user/fileupload1&quot; method=&quot;post&quot; enctype=&quot;multipart/form-data&quot;&gt; 选择上传文件：&lt;input type=&quot;file&quot; name=&quot;upload&quot;/&gt; &lt;br/&gt; &lt;input type=&quot;submit&quot; value=&quot;上传&quot;/&gt; &lt;/form&gt; &lt;form action=&quot;user/fileupload2&quot; method=&quot;post&quot; enctype=&quot;multipart/form-data&quot;&gt; 选择上传文件：&lt;input type=&quot;file&quot; name=&quot;upload&quot;/&gt; &lt;br/&gt; &lt;input type=&quot;submit&quot; value=&quot;上传&quot;/&gt; &lt;/form&gt; &lt;/body&gt; &lt;/html&gt; web.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;!-- - This is the Cocoon web-app configurations file - - $Id$ --&gt; &lt;web-app version=&quot;2.4&quot; xmlns=&quot;http://java.sun.com/xml/ns/j2ee&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://java.sun.com/xml/ns/j2ee http://java.sun.com/xml/ns/j2ee/web-app_2_4.xsd&quot;&gt; &lt;!--配置前端控制器--&gt; &lt;servlet&gt; &lt;servlet-name&gt;dispatcherServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:springmvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;dispatcherServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;!--配置解决中文乱码的过滤器--&gt; &lt;filter&gt; &lt;filter-name&gt;characterEncodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;characterEncodingFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt; &lt;/web-app&gt; SpringMVC跨服务器方式的文件上传目前跨服务器的内容由于图片服务器没有打开所以目前还没成功，可能是因为图片服务器的配置问题。代码肯定没问题： package cn.itcast.controller; import com.sun.jersey.api.client.Client; import com.sun.jersey.api.client.WebResource; import org.apache.commons.fileupload.FileItem; import org.apache.commons.fileupload.disk.DiskFileItemFactory; import org.apache.commons.fileupload.servlet.ServletFileUpload; import org.springframework.stereotype.Controller; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.multipart.MultipartFile; import javax.servlet.http.HttpServletRequest; import java.io.File; import java.util.List; import java.util.UUID; /** * @Description TODO * @Author TT Hun * @Data 2019/11/5 11:26 */ @Controller @RequestMapping(&quot;/user&quot;) public class UserController { /** * 跨服务器文件上传 * @return */ @RequestMapping(&quot;/fileupload3&quot;) public String fileuoload3(MultipartFile upload) throws Exception { System.out.println(&quot;跨服务器文件上传...&quot;); // 定义上传文件服务器路径 String path = &quot;http://localhost:9999/uploads/&quot;; // 说明上传文件项 // 获取上传文件的名称 String filename = upload.getOriginalFilename(); // 把文件的名称设置唯一值，uuid String uuid = UUID.randomUUID().toString().replace(&quot;-&quot;, &quot;&quot;); filename = uuid+&quot;_&quot;+filename; // 创建客户端的对象 Client client = Client.create(); // 和图片服务器进行连接 WebResource webResource = client.resource(path + filename); // 上传文件 webResource.put(upload.getBytes()); return &quot;success&quot;; } } springmvc.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot; http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;!-- 开启注解扫描 --&gt; &lt;context:component-scan base-package=&quot;cn.itcast&quot;/&gt; &lt;!-- 视图解析器对象 --&gt; &lt;bean id=&quot;internalResourceViewResolver&quot; class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt; &lt;property name=&quot;prefix&quot; value=&quot;/WEB-INF/pages/&quot;/&gt; &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot;/&gt; &lt;/bean&gt; &lt;!--前端控制器，哪些静态资源不拦截--&gt; &lt;mvc:resources location=&quot;/css/&quot; mapping=&quot;/css/**&quot;/&gt; &lt;mvc:resources location=&quot;/images/&quot; mapping=&quot;/images/**&quot;/&gt; &lt;mvc:resources location=&quot;/js/&quot; mapping=&quot;/js/**&quot;/&gt; &lt;!--配置文件解析器对象--&gt; &lt;bean id=&quot;multipartResolver&quot; class=&quot;org.springframework.web.multipart.commons.CommonsMultipartResolver&quot;&gt; &lt;property name=&quot;maxUploadSize&quot; value=&quot;10485760&quot; /&gt; &lt;/bean&gt; &lt;!-- 开启SpringMVC框架注解的支持 --&gt; &lt;mvc:annotation-driven /&gt; &lt;/beans&gt; web.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;!-- Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to You under the Apache License, Version 2.0 (the &quot;License&quot;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. --&gt; &lt;!-- - This is the Cocoon web-app configurations file - - $Id$ --&gt; &lt;web-app version=&quot;2.4&quot; xmlns=&quot;http://java.sun.com/xml/ns/j2ee&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://java.sun.com/xml/ns/j2ee http://java.sun.com/xml/ns/j2ee/web-app_2_4.xsd&quot;&gt; &lt;!--配置前端控制器--&gt; &lt;servlet&gt; &lt;servlet-name&gt;dispatcherServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:springmvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;dispatcherServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;!--配置解决中文乱码的过滤器--&gt; &lt;filter&gt; &lt;filter-name&gt;characterEncodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;characterEncodingFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt; &lt;/web-app&gt; index.jsp &lt;%-- Created by IntelliJ IDEA. User: Hun Date: 2019/11/5 Time: 11:25 To change this template use File | Settings | File Templates. --%&gt; &lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;文件上传&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h3&gt;文件上传&lt;/h3&gt; &lt;form action=&quot;user/fileupload1&quot; method=&quot;post&quot; enctype=&quot;multipart/form-data&quot;&gt; 选择上传文件：&lt;input type=&quot;file&quot; name=&quot;upload&quot;/&gt; &lt;br/&gt; &lt;input type=&quot;submit&quot; value=&quot;上传&quot;/&gt; &lt;/form&gt; &lt;form action=&quot;user/fileupload2&quot; method=&quot;post&quot; enctype=&quot;multipart/form-data&quot;&gt; 选择上传文件：&lt;input type=&quot;file&quot; name=&quot;upload&quot;/&gt; &lt;br/&gt; &lt;input type=&quot;submit&quot; value=&quot;上传&quot;/&gt; &lt;/form&gt; &lt;h2&gt;跨服务器文件上传&lt;/h2&gt; &lt;form action=&quot;user/fileupload3&quot; method=&quot;post&quot; enctype=&quot;multipart/form-data&quot;&gt; 选择上传文件：&lt;input type=&quot;file&quot; name=&quot;upload&quot;/&gt; &lt;br/&gt; &lt;input type=&quot;submit&quot; value=&quot;上传&quot;/&gt; &lt;/form&gt; &lt;/body&gt; &lt;/html&gt; *** success.jsp &lt;%-- Created by IntelliJ IDEA. User: Hun Date: 2019/11/5 Time: 11:28 To change this template use File | Settings | File Templates. --%&gt; &lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Title&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h3&gt;上传文件成功&lt;/h3&gt; &lt;/body&gt; &lt;/html&gt; 图片服务器略。 第三章SpringMVC异常处理SpringMVC异常处理之处理代码编写 package cn.itcast.controller; import cn.itcast.exception.SysException; import org.springframework.stereotype.Controller; import org.springframework.web.bind.annotation.RequestMapping; /** * @Description TODO * @Author TT Hun * @Data 2019/11/5 22:38 */ @Controller @RequestMapping(&quot;/user&quot;) public class UserController { @RequestMapping(&quot;/testException&quot;) public String testException() throws SysException { System.out.println(&quot;testException 执行了&quot;); try { // 模拟异常 int a = 10 / 0; } catch (Exception e) { // 控制台打印异常信息 e.printStackTrace(); // 向上抛出异常信息 throw new SysException(&quot;查询所有用户出错了。。。&quot;); } return &quot;success&quot;; } } package cn.itcast.exception; /** * @Description 自定义的异常类 * @Author TT Hun * @Data 2019/11/5 23:06 */ public class SysException extends Exception { // 用来存储提示信息的 private String message; @Override public String getMessage() { return message; } public void setMessage(String message) { this.message = message; } public SysException(String message) { this.message = message; } } package cn.itcast.exception; import org.springframework.web.servlet.HandlerExceptionResolver; import org.springframework.web.servlet.ModelAndView; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpServletResponse; /** * @Description 异常处理器 * @Author TT Hun * @Data 2019/11/5 23:10 */ public class SysExceptionResolver implements HandlerExceptionResolver { /** * 处理异常逻辑 * @param request * @param response * @param handler * @param ex * @return */ public ModelAndView resolveException(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) { // 获取到异常对象 SysException e = null; // 判断是否是ex异常 if(ex instanceof SysException) { e = (SysException) ex; } else{//如果不是的话 e = new SysException(&quot;系统正在维护....&quot;); } // 创建ModelAndView对象，这个modelandview可以帮助跳转到那个界面 ModelAndView mv = new ModelAndView(); // 向里面存入提示信息，存了一组键值对 mv.addObject(&quot;errorMsg&quot;,e.getMessage()); // 往哪里跳转 mv.setViewName(&quot;error&quot;); return mv; } } &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot; http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;!-- 开启注解扫描 --&gt; &lt;context:component-scan base-package=&quot;cn.itcast&quot;/&gt; &lt;!-- 视图解析器对象 --&gt; &lt;bean id=&quot;internalResourceViewResolver&quot; class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt; &lt;property name=&quot;prefix&quot; value=&quot;/WEB-INF/pages/&quot;/&gt; &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot;/&gt; &lt;/bean&gt; &lt;!--前端控制器，哪些静态资源不拦截--&gt; &lt;mvc:resources location=&quot;/css/&quot; mapping=&quot;/css/**&quot;/&gt; &lt;mvc:resources location=&quot;/images/&quot; mapping=&quot;/images/**&quot;/&gt; &lt;mvc:resources location=&quot;/js/&quot; mapping=&quot;/js/**&quot;/&gt; &lt;!--配置异常处理器--&gt; &lt;bean id=&quot;sysExceptionResolver&quot; class=&quot;cn.itcast.exception.SysExceptionResolver&quot;/&gt; &lt;!-- 开启SpringMVC框架注解的支持 --&gt; &lt;mvc:annotation-driven /&gt; &lt;/beans&gt; error.jsp &lt;%-- Created by IntelliJ IDEA. User: Hun Date: 2019/11/6 Time: 9:35 To change this template use File | Settings | File Templates. --%&gt; &lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; isELIgnored=&quot;false&quot;%&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Title&lt;/title&gt; &lt;/head&gt; &lt;body&gt; ${errorMsg} &lt;/body&gt; &lt;/html&gt; success.jsp &lt;%-- Created by IntelliJ IDEA. User: Hun Date: 2019/11/5 Time: 22:41 To change this template use File | Settings | File Templates. --%&gt; &lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Title&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h3&gt;测试成功&lt;/h3&gt; &lt;/body&gt; &lt;/html&gt; &lt;!DOCTYPE web-app PUBLIC &quot;-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN&quot; &quot;http://java.sun.com/dtd/web-app_2_3.dtd&quot; &gt; &lt;web-app&gt; &lt;display-name&gt;Archetype Created Web Application&lt;/display-name&gt; &lt;servlet&gt; &lt;servlet-name&gt;dispatcherServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:springmvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;dispatcherServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;!--配置解决中文乱码的过滤器--&gt; &lt;filter&gt; &lt;filter-name&gt;characterEncodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;characterEncodingFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt; &lt;/web-app&gt; index.jsp &lt;%-- Created by IntelliJ IDEA. User: Administrator Date: 2018/5/5 Time: 22:08 To change this template use File | Settings | File Templates. --%&gt; &lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Title&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h3&gt;异常处理&lt;/h3&gt; &lt;a href=&quot;user/testException&quot; &gt;异常处理&lt;/a&gt; &lt;/body&gt; &lt;/html&gt; pom文件： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;!-- Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to you under the Apache License, Version 2.0 (the &quot;License&quot;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. --&gt; &lt;!-- $Id: pom.xml 642118 2008-03-28 08:04:16Z reinhard $ --&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd&quot;&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;spring.version&gt;5.0.2.RELEASE&lt;/spring.version&gt; &lt;/properties&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;packaging&gt;war&lt;/packaging&gt; &lt;name&gt;springmvc_day02_03_exception1&lt;/name&gt; &lt;artifactId&gt;springmvc_day02_03_exception1&lt;/artifactId&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.mortbay.jetty&lt;/groupId&gt; &lt;artifactId&gt;maven-jetty-plugin&lt;/artifactId&gt; &lt;version&gt;6.1.7&lt;/version&gt; &lt;configuration&gt; &lt;connectors&gt; &lt;connector implementation=&quot;org.mortbay.jetty.nio.SelectChannelConnector&quot;&gt; &lt;port&gt;8888&lt;/port&gt; &lt;maxIdleTime&gt;30000&lt;/maxIdleTime&gt; &lt;/connector&gt; &lt;/connectors&gt; &lt;webAppSourceDirectory&gt;${project.build.directory}/${pom.artifactId}-${pom.version} &lt;/webAppSourceDirectory&gt; &lt;contextPath&gt;/&lt;/contextPath&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-web&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;servlet-api&lt;/artifactId&gt; &lt;version&gt;2.5&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet.jsp&lt;/groupId&gt; &lt;artifactId&gt;jsp-api&lt;/artifactId&gt; &lt;version&gt;2.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; SpringMVC中的拦截器拦截器只会拦截控制器中的方法，也就是Controller中的方法。JSP之类的资源不会拦截的。过滤器什么都可以拦截。也就是拦截器可以做的事情过滤器都可以做。但是过滤器可以做的事情拦截器不一定会做。拦截器完整代码注意Pom中的依赖使用那一些依赖： 先看一下目录； package cn.itcast.controller.cn.itcast.interceptor; import org.springframework.web.servlet.HandlerInterceptor; import org.springframework.web.servlet.ModelAndView; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpServletResponse; /** * 自定义拦截器 */ public class MyInterceptor1 implements HandlerInterceptor{ /** * 预处理，controller方法执行前 * return true 放行，执行下一个拦截器，如果没有，执行controller中的方法 * return false不放行 * @param request * @param response * @param handler * @return * @throws Exception */ @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { System.out.println(&quot;MyInterceptor1执行了...前1111&quot;); // request.getRequestDispatcher(&quot;/WEB-INF/pages/error.jsp&quot;).forward(request,response); return true; } /** * 后处理方法，controller方法执行后，success.jsp执行之前 * @param request * @param response * @param handler * @param modelAndView * @throws Exception */ @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception { System.out.println(&quot;MyInterceptor1执行了...后1111&quot;); // request.getRequestDispatcher(&quot;/WEB-INF/pages/error.jsp&quot;).forward(request,response); } /** * success.jsp页面执行后，该方法会执行 * @param request * @param response * @param handler * @param ex * @throws Exception */ @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception { System.out.println(&quot;MyInterceptor1执行了...最后1111&quot;); } } package cn.itcast.controller.cn.itcast.interceptor; import org.springframework.web.servlet.HandlerInterceptor; import org.springframework.web.servlet.ModelAndView; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpServletResponse; /** * 自定义拦截器 */ public class MyInterceptor2 implements HandlerInterceptor{ /** * 预处理，controller方法执行前 * return true 放行，执行下一个拦截器，如果没有，执行controller中的方法 * return false不放行 * @param request * @param response * @param handler * @return * @throws Exception */ @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { System.out.println(&quot;MyInterceptor1执行了...前2222&quot;); // request.getRequestDispatcher(&quot;/WEB-INF/pages/error.jsp&quot;).forward(request,response); return true; } /** * 后处理方法，controller方法执行后，success.jsp执行之前 * @param request * @param response * @param handler * @param modelAndView * @throws Exception */ @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception { System.out.println(&quot;MyInterceptor1执行了...后2222&quot;); // request.getRequestDispatcher(&quot;/WEB-INF/pages/error.jsp&quot;).forward(request,response); } /** * success.jsp页面执行后，该方法会执行 * @param request * @param response * @param handler * @param ex * @throws Exception */ @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception { System.out.println(&quot;MyInterceptor1执行了...最后2222&quot;); } } package cn.itcast.controller; import org.springframework.stereotype.Controller; import org.springframework.web.bind.annotation.RequestMapping; @Controller @RequestMapping(&quot;/user&quot;) public class UserController { @RequestMapping(&quot;/testInterceptor&quot;) public String testInterceptor(){ System.out.println(&quot;testInterceptor执行了...&quot;); return &quot;success&quot;; } } springmvc.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot; http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;!-- 开启注解扫描 --&gt; &lt;context:component-scan base-package=&quot;cn.itcast&quot;/&gt; &lt;!-- 视图解析器对象 --&gt; &lt;bean id=&quot;internalResourceViewResolver&quot; class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt; &lt;property name=&quot;prefix&quot; value=&quot;/WEB-INF/pages/&quot;/&gt; &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot;/&gt; &lt;/bean&gt; &lt;!--前端控制器，哪些静态资源不拦截--&gt; &lt;mvc:resources location=&quot;/css/&quot; mapping=&quot;/css/**&quot;/&gt; &lt;mvc:resources location=&quot;/images/&quot; mapping=&quot;/images/**&quot;/&gt; &lt;mvc:resources location=&quot;/js/&quot; mapping=&quot;/js/**&quot;/&gt; &lt;!--配置拦截器--&gt; &lt;mvc:interceptors&gt; &lt;!--配置拦截器--&gt; &lt;mvc:interceptor&gt; &lt;!--要拦截的具体的方法--&gt; &lt;mvc:mapping path=&quot;/user/*&quot;/&gt; &lt;!--不要拦截的方法 &lt;mvc:exclude-mapping path=&quot;&quot;/&gt; --&gt; &lt;!--配置拦截器对象--&gt; &lt;bean class=&quot;cn.itcast.controller.cn.itcast.interceptor.MyInterceptor1&quot; /&gt; &lt;/mvc:interceptor&gt; &lt;!--配置第二个拦截器--&gt; &lt;mvc:interceptor&gt; &lt;!--要拦截的具体的方法--&gt; &lt;mvc:mapping path=&quot;/**&quot;/&gt; &lt;!--不要拦截的方法 &lt;mvc:exclude-mapping path=&quot;&quot;/&gt; --&gt; &lt;!--配置拦截器对象--&gt; &lt;bean class=&quot;cn.itcast.controller.cn.itcast.interceptor.MyInterceptor2&quot; /&gt; &lt;/mvc:interceptor&gt; &lt;/mvc:interceptors&gt; &lt;!-- 开启SpringMVC框架注解的支持 --&gt; &lt;mvc:annotation-driven /&gt; &lt;/beans&gt; error.jsp &lt;%-- Created by IntelliJ IDEA. User: Hun Date: 2019/11/6 Time: 9:35 To change this template use File | Settings | File Templates. --%&gt; &lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; isELIgnored=&quot;false&quot;%&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Title&lt;/title&gt; &lt;/head&gt; &lt;body&gt; ${errorMsg} &lt;/body&gt; &lt;/html&gt; success.jsp &lt;%-- Created by IntelliJ IDEA. User: Hun Date: 2019/11/5 Time: 22:41 To change this template use File | Settings | File Templates. --%&gt; &lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Title&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h3&gt;测试成功&lt;/h3&gt; &lt;%--这个代表执行的Java脚本--%&gt; &lt;% System.out.println(&quot;success.jsp成功执行了... &quot;); %&gt; &lt;/body&gt; &lt;/html&gt; web.xml &lt;!DOCTYPE web-app PUBLIC &quot;-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN&quot; &quot;http://java.sun.com/dtd/web-app_2_3.dtd&quot; &gt; &lt;web-app&gt; &lt;display-name&gt;Archetype Created Web Application&lt;/display-name&gt; &lt;servlet&gt; &lt;servlet-name&gt;dispatcherServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:springmvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;dispatcherServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;!--配置解决中文乱码的过滤器--&gt; &lt;filter&gt; &lt;filter-name&gt;characterEncodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;characterEncodingFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt; &lt;/web-app&gt; index.jsp &lt;%-- Created by IntelliJ IDEA. User: Administrator Date: 2018/5/5 Time: 22:08 To change this template use File | Settings | File Templates. --%&gt; &lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Title&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h3&gt;拦截器&lt;/h3&gt; &lt;a href=&quot;user/testInterceptor&quot; &gt;拦截器&lt;/a&gt; &lt;/body&gt; &lt;/html&gt; pom.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;cn.itcast&lt;/groupId&gt; &lt;artifactId&gt;springmvc_day02_04_interceptor&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;war&lt;/packaging&gt; &lt;name&gt;springmvc_day02_04_interceptor Maven Webapp&lt;/name&gt; &lt;!-- FIXME change it to the project&apos;s website --&gt; &lt;url&gt;http://www.example.com&lt;/url&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;spring.version&gt;5.0.2.RELEASE&lt;/spring.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-web&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;servlet-api&lt;/artifactId&gt; &lt;version&gt;2.5&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet.jsp&lt;/groupId&gt; &lt;artifactId&gt;jsp-api&lt;/artifactId&gt; &lt;version&gt;2.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;finalName&gt;springmvc_day02_04_interceptor&lt;/finalName&gt; &lt;pluginManagement&gt;&lt;!-- lock down plugins versions to avoid using Maven defaults (may be moved to parent pom) --&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-clean-plugin&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-resources-plugin&lt;/artifactId&gt; &lt;version&gt;3.0.2&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.7.0&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt; &lt;version&gt;2.20.1&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-war-plugin&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-install-plugin&lt;/artifactId&gt; &lt;version&gt;2.5.2&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-deploy-plugin&lt;/artifactId&gt; &lt;version&gt;2.8.2&lt;/version&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt; &lt;/build&gt; &lt;/project&gt;]]></content>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC]]></title>
    <url>%2F2019%2F10%2F24%2FSpringMVC%2F</url>
    <content type="text"><![CDATA[构建SpringMVC工程： 首先解决工程添加木块过慢的问题，在添加工程的时候，添加如下的属性： 然后因为开始工程目录不全，要添加一些目录： 需要把HelloController的类交给容器来处理，然后在这个方法上再添加一个注解@ResquestMapping。请求映射注解，也就是一发请求，这个方法就执行了，映射到前台的请求。需要添加一个请求的路径。path=/“hello”。然后服务器一旦启动，点击超链接，让点击超链接跳转到success.jsp界面即可。 下图是springmvc.xml文件配置文件头的讲解： 首先配置里面的开启注解扫描。然后加了这个之后，我们还要让SpringMVC加载这个Springmvc.xml这个配置文件。那么这是怎么加载的那？我们在WEB-INF文件里面的web.xml文件里面也就是前端控制器里面去加载。 前端控制器代码如下： &lt;!DOCTYPE web-app PUBLIC &quot;-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN&quot; &quot;http://java.sun.com/dtd/web-app_2_3.dtd&quot; &gt; &lt;web-app&gt; &lt;display-name&gt;Archetype Created Web Application&lt;/display-name&gt; &lt;!--配置前端控制器--&gt; &lt;servlet&gt; &lt;servlet-name&gt;dispatcherServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:springmvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;dispatcherServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;!--配置解决中文乱码的过滤器--&gt; &lt;filter&gt; &lt;filter-name&gt;characterEncodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;characterEncodingFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt; &lt;/web-app&gt; 解释一下上面的代码通过org.springframework.web.servlet.DispatcherServlet这个类里面配置classpath:springmvc.xml属性就可以帮我们加载该配置文件 下面是简单的入门案例的图解： 前端发来请求/hello然后通过前端控制器映射到处理器映射器，然后controller里面的任何方法都会通过处理器适配器，然后在执行相对应的方法。就到了第5步执行，返回的是一个ModelAndView也就是返回的是哪个success.jsp的适配器页面 但是发现为什么只是在springmvc.xml文件中配置了前端控制器呢，因为在配置了springmvc的注解开启之后就默认配置了另外的着一些适配器映射器之类的。 @RequestMapping用来和前端的请求实现配合。映射过来的。建立映射关联的。可以放到方法上，也可以放到当前类上 RequestMapping的属性： value和Path属性都是一样的，如果只有一个属性，那么也可以不写这个path和value这个 例如： Springmvc02的视频由于未保存。尚且不理他。等有时间在去更新。 首先是讲一下RequestParam注解：先来一段例子： anno.jsp类： &lt;%-- Created by IntelliJ IDEA. User: Hun Date: 2019/11/3 Time: 16:55 To change this template use File | Settings | File Templates. --%&gt; &lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Title&lt;/title&gt; &lt;/head&gt; &lt;a href=&quot;anno/testRequestParam?username=haha&quot;&gt;RequestParam&lt;/a&gt; &lt;body&gt; &lt;/body&gt; &lt;/html&gt; paramController类： package cn.itcast.controller; import cn.itcast.domain.Account; import cn.itcast.domain.User; import org.springframework.stereotype.Controller; import org.springframework.web.bind.annotation.RequestMapping; /** * @Description TODO * @Author TT Hun * @Data 2019/11/2 20:34 */ @Controller @RequestMapping(&quot;/param&quot;) public class ParamController { @RequestMapping(&quot;/hello&quot;) public String testParam(String username, String password) { System.out.println(&quot;执行了。。。&quot;); System.out.println(&quot;用户名:&quot; + username); System.out.println(&quot;密码:&quot; + password); return &quot;success&quot;; } /** * 把返回的数据封装到一个javabean的类当中 * * @return */ @RequestMapping(&quot;/saveAccount&quot;) public String saveAccount(Account account) { System.out.println(&quot;执行了。。。&quot;); System.out.println(account); return &quot;success&quot;; } @RequestMapping(&quot;/saveUser&quot;) public String saveUser(User user){ System.out.println(&quot;执行了。。。&quot;); System.out.println(user); return &quot;success&quot;; } } 一旦上面的username换成了name就不能识别了 所以在Controller类的方法里加上这么一个参数，就可以将name对应的值传递给String username了 但是以后在jsp中就必须是name=”哈哈”。了，一旦更改成别的就不行了。 RequestBody注解这个就不能写超链接，只能写表单通过post的方式提交 package cn.itcast.controller; import org.springframework.stereotype.Controller; import org.springframework.web.bind.annotation.RequestBody; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RequestParam; /** * @Description 常用的注解 * @Author TT Hun * @Data 2019/11/3 16:59 */ @Controller @RequestMapping(&quot;/anno&quot;) public class AnnoController { @RequestMapping(&quot;/testRequestParam&quot;) public String testRequestParam(String username){ System.out.println(&quot;执行了。。。&quot;); System.out.println(username); return &quot;success&quot;; } @RequestMapping(&quot;/testRequestBody&quot;) public String testRequestBody(@RequestBody String body){ System.out.println(&quot;执行了。。。&quot;); System.out.println(body); return &quot;success&quot;; } } &lt;%-- Created by IntelliJ IDEA. User: Hun Date: 2019/11/2 Time: 20:26 To change this template use File | Settings | File Templates. --%&gt; &lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Title&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;%--自定义类型转换器--%&gt; &lt;form action=&quot;param/testRequestBody&quot; method=&quot;post&quot;&gt; 用户姓名：&lt;input type=&quot;text&quot; name=&quot;username&quot; /&gt;&lt;br/&gt; 用户年龄：&lt;input type=&quot;text&quot; name=&quot;age&quot; /&gt;&lt;br/&gt; 用户生日：&lt;input type=&quot;text&quot; name=&quot;date&quot; /&gt;&lt;br/&gt; &lt;input type=&quot;submit&quot; value=&quot;提交&quot; /&gt; &lt;/form&gt; &lt;/form&gt; &lt;/body&gt; &lt;/html&gt; PathVariable注解 Restful风格编程：原来在Controller里写很多方法，在path里面写具体的路径，也就是一个方法有一个请求路径，如果是restful风格可以三个方法请求路径都是同一个。可以给不同的方法给固定的请求方式，也就是说可以根据不同的请求方式来执行。那么问题来了，查询一般都是get请求。同一个方法findall和findbyId都有可能 执行怎么办呢，可以通过占位符path=”user/{id}”通过一个id的占位符来区别。 PathVariable也就是取到的占位符的值。 代码如下： package cn.itcast.controller; import org.springframework.stereotype.Controller; import org.springframework.web.bind.annotation.PathVariable; import org.springframework.web.bind.annotation.RequestBody; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RequestParam; /** * @Description 常用的注解 * @Author TT Hun * @Data 2019/11/3 16:59 */ @Controller @RequestMapping(&quot;/anno&quot;) public class AnnoController { /** * PathVariable注解示例 * @return */ @RequestMapping(&quot;/testPathVariable/{sid}&quot;) public String testPathVariable(@PathVariable(name=&quot;sid&quot;)String id ){ System.out.println(&quot;执行了。。。&quot;); System.out.println(id); return &quot;success&quot;; } } &lt;%-- Created by IntelliJ IDEA. User: Hun Date: 2019/11/3 Time: 16:55 To change this template use File | Settings | File Templates. --%&gt; &lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Title&lt;/title&gt; &lt;/head&gt; &lt;a href=&quot;/anno/testPathVariable/10&quot;&gt;testPathVariable&lt;/a&gt; &lt;body&gt; &lt;/body&gt; &lt;/html&gt; 基于HiddenHttpMethodFliter的过滤器 了解一下即可，因为webservie有API可以配置请求方式，并且浏览器中可以装插件来配置请求方式 RequestHeader注解一般用处不大 package cn.itcast.controller; import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.*; /** @Description 常用的注解 @Author TT Hun @Data 2019/11/3 16:59*/@Controller@RequestMapping(“/anno”)public class AnnoController { /** 获取请求头信息的值 @param header @return*/@RequestMapping(“/testRequestHeader”)public String testRequestHeader(@RequestHeader(value=”Accept”) String header){ System.out.println(“执行了。。。”); System.out.println(header); return “success”;}} &lt;%-- Created by IntelliJ IDEA. User: Hun Date: 2019/11/3 Time: 16:55 To change this template use File | Settings | File Templates. --%&gt; &lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Title&lt;/title&gt; &lt;/head&gt; &lt;a href=&quot;/anno/testRequestHeader&quot;&gt;testRequestHeader&lt;/a&gt; &lt;body&gt; &lt;/body&gt; &lt;/html&gt; Cookie Value注解 package cn.itcast.controller; import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.*; /** @Description 常用的注解 @Author TT Hun @Data 2019/11/3 16:59*/@Controller@RequestMapping(“/anno”)public class AnnoController { /** 测试CookieValue注解 @param cookieValue @return*/@RequestMapping(“/testCookieValue”)public String testCookieValue(@CookieValue(value=”JSESSIONID”) String cookieValue){ System.out.println(“执行了。。。”); System.out.println(cookieValue); return “success”;} } &lt;%-- Created by IntelliJ IDEA. User: Hun Date: 2019/11/3 Time: 16:55 To change this template use File | Settings | File Templates. --%&gt; &lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Title&lt;/title&gt; &lt;/head&gt; &lt;a href=&quot;/anno/testCookieValue&quot;&gt;testCookieValue&lt;/a&gt; &lt;body&gt; &lt;/body&gt; &lt;/html&gt; ModelAttribute注解 &lt;%-- Created by IntelliJ IDEA. User: Hun Date: 2019/11/3 Time: 16:55 To change this template use File | Settings | File Templates. --%&gt; &lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Title&lt;/title&gt; &lt;/head&gt; &lt;form action=&quot;anno/testModelAttribute&quot; method=&quot;post&quot;&gt; 用户姓名：&lt;input type=&quot;text&quot; name=&quot;username&quot; /&gt;&lt;br/&gt; 用户年龄：&lt;input type=&quot;text&quot; name=&quot;age&quot; /&gt;&lt;br/&gt; &lt;input type=&quot;submit&quot; value=&quot;提交&quot; /&gt; &lt;/form&gt; &lt;br&gt; &lt;body&gt; &lt;/body&gt; &lt;/html&gt; package cn.itcast.controller; import org.springframework.stereotype.Controller; import org.springframework.web.bind.annotation.*; /** * @Description 常用的注解 * @Author TT Hun * @Data 2019/11/3 16:59 */ @Controller @RequestMapping(&quot;/anno&quot;) public class AnnoController { /** * testModelAttribute测试 * @return */ @RequestMapping(&quot;/testModelAttribute&quot;) public String testModelAttribute(){ System.out.println(&quot;testModelAttribute执行了。。。&quot;); return &quot;success&quot;; } @ModelAttribute public void showUser(){ System.out.println(&quot;showUser方法执行了...&quot;); } } 然后打印结果： 当提交的缺少一个数据，但是想让这个数据从数据库中查询或者缓存中查询。下面是有返回值的情况： package cn.itcast.controller; import cn.itcast.domain.User; import org.springframework.stereotype.Controller; import org.springframework.web.bind.annotation.*; import java.util.Date; /** * @Description 常用的注解 * @Author TT Hun * @Data 2019/11/3 16:59 */ @Controller @RequestMapping(&quot;/anno&quot;) public class AnnoController { /** * testModelAttribute测试 * @return */ @RequestMapping(&quot;/testModelAttribute&quot;) public String testModelAttribute(User user){ System.out.println(&quot;testModelAttribute执行了。。。&quot;); System.out.println(user); return &quot;success&quot;; } /** * 测试ModelAttribute注解的有返回值的情况， */ @ModelAttribute public User showUser(String uname ,Integer age){ System.out.println(&quot;showUser方法执行了...&quot;); // 通过用户名查询数据库（模拟） User user = new User(); user.setUname(uname); user.setAge(age); user.setDate(new Date()); return user; } } &lt;%-- Created by IntelliJ IDEA. User: Hun Date: 2019/11/3 Time: 16:55 To change this template use File | Settings | File Templates. --%&gt; &lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Title&lt;/title&gt; &lt;/head&gt; &lt;form action=&quot;anno/testModelAttribute&quot; method=&quot;post&quot;&gt; 用户姓名：&lt;input type=&quot;text&quot; name=&quot;uname&quot; /&gt;&lt;br/&gt; 用户年龄：&lt;input type=&quot;text&quot; name=&quot;age&quot; /&gt;&lt;br/&gt; &lt;input type=&quot;submit&quot; value=&quot;提交&quot; /&gt; &lt;/form&gt; &lt;br&gt; &lt;body&gt; &lt;/body&gt; &lt;/html&gt; 打印结果： 下面是没有返回值的写法，可以写入一个Map集合 package cn.itcast.controller; import cn.itcast.domain.User; import org.springframework.stereotype.Controller; import org.springframework.web.bind.annotation.*; import java.util.Date; import java.util.Map; /** * @Description 常用的注解 * @Author TT Hun * @Data 2019/11/3 16:59 */ @Controller @RequestMapping(&quot;/anno&quot;) public class AnnoController { /** * 测试CookieValue注解 * @param cookieValue * @return */ @RequestMapping(&quot;/testCookieValue&quot;) public String testCookieValue(@CookieValue(value=&quot;JSESSIONID&quot;) String cookieValue){ System.out.println(&quot;执行了。。。&quot;); System.out.println(cookieValue); return &quot;success&quot;; } /** * 测试没有返回值的ModelAttribute注解 */ @ModelAttribute public void showUser(String uname , Integer age, Map&lt;String,User&gt; map){ System.out.println(&quot;showUser方法执行了...&quot;); // 通过用户名查询数据库（模拟） User user = new User(); user.setUname(uname); user.setAge(age); user.setDate(new Date()); // 查完数据之后存入一个Map集合当中 map.put(&quot;abc&quot;,user); } } &lt;%-- Created by IntelliJ IDEA. User: Hun Date: 2019/11/3 Time: 16:55 To change this template use File | Settings | File Templates. --%&gt; &lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Title&lt;/title&gt; &lt;/head&gt; &lt;form action=&quot;anno/testModelAttribute&quot; method=&quot;post&quot;&gt; 用户姓名：&lt;input type=&quot;text&quot; name=&quot;uname&quot; /&gt;&lt;br/&gt; 用户年龄：&lt;input type=&quot;text&quot; name=&quot;age&quot; /&gt;&lt;br/&gt; &lt;input type=&quot;submit&quot; value=&quot;提交&quot; /&gt; &lt;/form&gt; &lt;br&gt; &lt;body&gt; &lt;/body&gt; &lt;/html&gt; SessionAttribute注解 这个是重点看 用于在request域里面添加内容，存储到域当中去，并且可以在以后jsp的界面中显示出来。 package cn.itcast.controller; import cn.itcast.domain.User; import org.springframework.stereotype.Controller; import org.springframework.ui.Model; import org.springframework.web.bind.annotation.*; import javax.servlet.http.HttpServletRequest; import java.util.Date; import java.util.Map; /** * @Description 常用的注解 * @Author TT Hun * @Data 2019/11/3 16:59 */ @Controller @RequestMapping(&quot;/anno&quot;) public class AnnoController { /** * SessionAttributes的注解 * @return */ @RequestMapping(&quot;/testSessionAttributes&quot;) public String testSessionAttributes(Model model){ System.out.println(&quot;testSessionAttributes方法执行了...&quot;); // 向requestSession域里面存值，然后在success.jsp里面取出来,底层会存储到request域对象当中去 model.addAttribute(&quot;msg&quot;,&quot;美美&quot;); return &quot;success&quot;; } } &lt;%-- Created by IntelliJ IDEA. User: Hun Date: 2019/11/3 Time: 16:55 To change this template use File | Settings | File Templates. --%&gt; &lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Title&lt;/title&gt; &lt;/head&gt; &lt;a href=&quot;/anno/testSessionAttributes&quot;&gt;testSessionAttributes&lt;/a&gt; &lt;br&gt; &lt;body&gt; &lt;/body&gt; &lt;/html&gt; &lt;%-- Created by IntelliJ IDEA. User: Hun Date: 2019/10/24 Time: 22:35 To change this template use File | Settings | File Templates. --%&gt; &lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; isELIgnored=&quot;false&quot; %&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Title&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h3&gt;入门成功了&lt;/h3&gt; &lt;%-- 表示request域的集合--%&gt; ${ requestScope } &lt;/body&gt; &lt;/html&gt; SessionAttributes这个注解智能作用在类上，相当于存到Session这个域当中去，然后使用model.addAttribute相当于把这个信息存储到requestSession这个域当中去。在这个里面添加这个这个 然后就会显示： 操作Session的内容进行增加删除查看 package cn.itcast.controller; import cn.itcast.domain.User; import org.springframework.stereotype.Controller; import org.springframework.ui.Model; import org.springframework.ui.ModelMap; import org.springframework.web.bind.annotation.*; import org.springframework.web.bind.support.SessionStatus; import javax.servlet.http.HttpServletRequest; import java.util.Date; import java.util.Map; /** * @Description 常用的注解 * @Author TT Hun * @Data 2019/11/3 16:59 */ @Controller @RequestMapping(&quot;/anno&quot;) @SessionAttributes(value={&quot;msg&quot;})//把msg=“美美&quot;在存入到Session域里面一份 public class AnnoController { /** * 从Session域里面添加值 * @return */ @RequestMapping(&quot;/testSessionAttributes&quot;) public String testSessionAttributes(Model model){ System.out.println(&quot;testSessionAttributes方法执行了...&quot;); // 向requestSession域里面存值，然后在success.jsp里面取出来,底层会存储到request域对象当中去 model.addAttribute(&quot;msg&quot;,&quot;美美&quot;); return &quot;success&quot;; } /** * 从Session域里面取值 * @return */ @RequestMapping(&quot;/getSessionAttributes&quot;) public String getSessionAttributes(ModelMap modelMap){ System.out.println(&quot;testSessionAttributes方法执行了...&quot;); // 从Session域里面取值。 String msg = (String)modelMap.get(&quot;msg&quot;); System.out.println(msg); return &quot;success&quot;; } /** * 从Session域里面删除值 * @return */ @RequestMapping(&quot;/deleteSessionAttributes&quot;) public String deleteSessionAttributes(SessionStatus status){ System.out.println(&quot;testSessionAttributes方法执行了...&quot;); // 把Session里面的东西清除掉。 status.setComplete(); return &quot;success&quot;; } } &lt;%-- Created by IntelliJ IDEA. User: Hun Date: 2019/11/3 Time: 16:55 To change this template use File | Settings | File Templates. --%&gt; &lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Title&lt;/title&gt; &lt;/head&gt; &lt;a href=&quot;/anno/testSessionAttributes&quot;&gt;testSessionAttributes&lt;/a&gt; &lt;br&gt; &lt;a href=&quot;/anno/getSessionAttributes&quot;&gt;getSessionAttributes&lt;/a&gt; &lt;br&gt; &lt;a href=&quot;/anno/deleteSessionAttributes&quot;&gt;deleteSessionAttributes&lt;/a&gt; &lt;br&gt; &lt;body&gt; &lt;/body&gt; &lt;/html&gt; &lt;%-- Created by IntelliJ IDEA. User: Hun Date: 2019/10/24 Time: 22:35 To change this template use File | Settings | File Templates. --%&gt; &lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; isELIgnored=&quot;false&quot; %&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Title&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h3&gt;入门成功了&lt;/h3&gt; &lt;%-- 表示request域的集合--%&gt; ${ requestScope.msg } ${sessionScope} &lt;/body&gt; &lt;/html&gt;]]></content>
  </entry>
  <entry>
    <title><![CDATA[Spring第四天_基于纯注解事务控制+回顾部分注解例子]]></title>
    <url>%2F2019%2F10%2F24%2FSpring%E7%AC%AC%E5%9B%9B%E5%A4%A9-%E5%9F%BA%E4%BA%8E%E7%BA%AF%E6%B3%A8%E8%A7%A3%E4%BA%8B%E5%8A%A1%E6%8E%A7%E5%88%B6-%E5%9B%9E%E9%A1%BE%E9%83%A8%E5%88%86%E6%B3%A8%E8%A7%A3%E4%BE%8B%E5%AD%90%2F</url>
    <content type="text"><![CDATA[package com.itheima.dao; import com.itheima.domain.Account; /** * 账户的持久层接口 */ public interface IAccountDao { /** * 根据Id查询账户 * @param accountId * @return */ Account findAccountById(Integer accountId); /** * 根据名称查询账户 * @param accountName * @return */ Account findAccountByName(String accountName); /** * 更新账户 * @param account */ void updateAccount(Account account); } package com.itheima.dao.impl; import com.itheima.dao.IAccountDao; import com.itheima.domain.Account; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.jdbc.core.BeanPropertyRowMapper; import org.springframework.jdbc.core.JdbcTemplate; import org.springframework.stereotype.Repository; import java.util.List; /** * 账户的持久层实现类 */ @Repository(&quot;accountDao&quot;) public class AccountDaoImpl implements IAccountDao { @Autowired private JdbcTemplate jdbcTemplate; @Override public Account findAccountById(Integer accountId) { List&lt;Account&gt; accounts = jdbcTemplate.query(&quot;select * from account where id = ?&quot;,new BeanPropertyRowMapper&lt;Account&gt;(Account.class),accountId); return accounts.isEmpty()?null:accounts.get(0); } @Override public Account findAccountByName(String accountName) { List&lt;Account&gt; accounts = jdbcTemplate.query(&quot;select * from account where name = ?&quot;,new BeanPropertyRowMapper&lt;Account&gt;(Account.class),accountName); if(accounts.isEmpty()){ return null; } if(accounts.size()&gt;1){ throw new RuntimeException(&quot;结果集不唯一&quot;); } return accounts.get(0); } @Override public void updateAccount(Account account) { jdbcTemplate.update(&quot;update account set name=?,money=? where id=?&quot;,account.getName(),account.getMoney(),account.getId()); } } package com.itheima.domain; import java.io.Serializable; /** * 账户的实体类 */ public class Account implements Serializable { private Integer id; private String name; private Float money; public Integer getId() { return id; } public void setId(Integer id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public Float getMoney() { return money; } public void setMoney(Float money) { this.money = money; } @Override public String toString() { return &quot;Account{&quot; + &quot;id=&quot; + id + &quot;, name=&apos;&quot; + name + &apos;\&apos;&apos; + &quot;, money=&quot; + money + &apos;}&apos;; } } package com.itheima.service; import com.itheima.domain.Account; /** * 账户的业务层接口 */ public interface IAccountService { /** * 根据id查询账户信息 * @param accountId * @return */ Account findAccountById(Integer accountId); /** * 转账 * @param sourceName 转成账户名称 * @param targetName 转入账户名称 * @param money 转账金额 */ void transfer(String sourceName, String targetName, Float money); } package com.itheima.service.impl; import com.itheima.dao.IAccountDao; import com.itheima.domain.Account; import com.itheima.service.IAccountService; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.stereotype.Service; import org.springframework.transaction.annotation.Propagation; import org.springframework.transaction.annotation.Transactional; /** * 账户的业务层实现类 * * 事务控制应该都是在业务层 */ @Service(&quot;accountService&quot;) @Transactional(propagation= Propagation.SUPPORTS,readOnly=true)//只读型事务的配置 public class AccountServiceImpl implements IAccountService{ @Autowired private IAccountDao accountDao; @Override public Account findAccountById(Integer accountId) { return accountDao.findAccountById(accountId); } //需要的是读写型事务配置 @Transactional(propagation= Propagation.REQUIRED,readOnly=false) @Override public void transfer(String sourceName, String targetName, Float money) { System.out.println(&quot;transfer....&quot;); //2.1根据名称查询转出账户 Account source = accountDao.findAccountByName(sourceName); //2.2根据名称查询转入账户 Account target = accountDao.findAccountByName(targetName); //2.3转出账户减钱 source.setMoney(source.getMoney()-money); //2.4转入账户加钱 target.setMoney(target.getMoney()+money); //2.5更新转出账户 accountDao.updateAccount(source); // int i=1/0; //2.6更新转入账户 accountDao.updateAccount(target); } } 重点看下面Config文件夹下面几个类： package config; import org.springframework.beans.factory.annotation.Value; import org.springframework.context.annotation.Bean; import org.springframework.jdbc.core.JdbcTemplate; import org.springframework.jdbc.datasource.DriverManagerDataSource; import javax.sql.DataSource; /** * 和连接数据库相关的配置类 */ public class JdbcConfig { @Value(&quot;${jdbc.driver}&quot;) private String driver; @Value(&quot;${jdbc.url}&quot;) private String url; @Value(&quot;${jdbc.username}&quot;) private String username; @Value(&quot;${jdbc.password}&quot;) private String password; /** * 创建JdbcTemplate * @param dataSource * @return */ @Bean(name=&quot;jdbcTemplate&quot;) public JdbcTemplate createJdbcTemplate(DataSource dataSource){ return new JdbcTemplate(dataSource); } /** * 创建数据源对象 * @return */ @Bean(name=&quot;dataSource&quot;) public DataSource createDataSource(){ DriverManagerDataSource ds = new DriverManagerDataSource(); ds.setDriverClassName(driver); ds.setUrl(url); ds.setUsername(username); ds.setPassword(password); return ds; } } package config; import org.springframework.context.annotation.ComponentScan; import org.springframework.context.annotation.Configuration; import org.springframework.context.annotation.Import; import org.springframework.context.annotation.PropertySource; import org.springframework.transaction.annotation.EnableTransactionManagement; /** * spring的配置类，相当于bean.xml */ @Configuration @ComponentScan(&quot;com.itheima&quot;) @Import({JdbcConfig.class,TransactionConfig.class}) @PropertySource(&quot;jdbcConfig.properties&quot;) @EnableTransactionManagement public class SpringConfiguration { } package config; import org.springframework.context.annotation.Bean; import org.springframework.jdbc.datasource.DataSourceTransactionManager; import org.springframework.transaction.PlatformTransactionManager; import javax.sql.DataSource; /** * 和事务相关的配置类 */ public class TransactionConfig { /** * 用于创建事务管理器对象 * @param dataSource * @return */ @Bean(name=&quot;transactionManager&quot;) public PlatformTransactionManager createTransactionManager(DataSource dataSource){ return new DataSourceTransactionManager(dataSource); } } resource包下properties文件： jdbc.driver=com.mysql.jdbc.Driver jdbc.url=jdbc:mysql://localhost:3306/eesy jdbc.username=root jdbc.password=1234 package com.itheima.test; import com.itheima.service.IAccountService; import config.SpringConfiguration; import org.junit.Test; import org.junit.runner.RunWith; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.test.context.ContextConfiguration; import org.springframework.test.context.junit4.SpringJUnit4ClassRunner; /** * 使用Junit单元测试：测试我们的配置 */ @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(classes= SpringConfiguration.class) public class AccountServiceTest { @Autowired private IAccountService as; @Test public void testTransfer(){ as.transfer(&quot;aaa&quot;,&quot;bbb&quot;,100f); } }]]></content>
  </entry>
  <entry>
    <title><![CDATA[Spring第四天_关于事务的控制通过XML和纯注解]]></title>
    <url>%2F2019%2F10%2F23%2FSpring%E7%AC%AC%E5%9B%9B%E5%A4%A9-%E5%85%B3%E4%BA%8E%E4%BA%8B%E5%8A%A1%E7%9A%84%E6%8E%A7%E5%88%B6%E9%80%9A%E8%BF%87XML%E5%92%8C%E7%BA%AF%E6%B3%A8%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Spring控制的一组关于事务的 Spring基于XML的声明式事务控制（）前面的处理还是按照以前银行例子的配置，这个地方重点看bean配置文件的配置。 package com.itheima.dao; import com.itheima.domain.Account; /** * 账户的持久层接口 */ public interface IAccountDao { /** * 根据Id查询账户 * @param accountId * @return */ Account findAccountById(Integer accountId); /** * 根据名称查询账户 * @param accountName * @return */ Account findAccountByName(String accountName); /** * 更新账户 * @param account */ void updateAccount(Account account); } package com.itheima.dao.impl; import com.itheima.dao.IAccountDao; import com.itheima.domain.Account; import org.springframework.jdbc.core.BeanPropertyRowMapper; import org.springframework.jdbc.core.support.JdbcDaoSupport; import java.util.List; /** * 账户的持久层实现类 */ public class AccountDaoImpl extends JdbcDaoSupport implements IAccountDao { @Override public Account findAccountById(Integer accountId) { List&lt;Account&gt; accounts = super.getJdbcTemplate().query(&quot;select * from account where id = ?&quot;,new BeanPropertyRowMapper&lt;Account&gt;(Account.class),accountId); return accounts.isEmpty()?null:accounts.get(0); } @Override public Account findAccountByName(String accountName) { List&lt;Account&gt; accounts = super.getJdbcTemplate().query(&quot;select * from account where name = ?&quot;,new BeanPropertyRowMapper&lt;Account&gt;(Account.class),accountName); if(accounts.isEmpty()){ return null; } if(accounts.size()&gt;1){ throw new RuntimeException(&quot;结果集不唯一&quot;); } return accounts.get(0); } @Override public void updateAccount(Account account) { super.getJdbcTemplate().update(&quot;update account set name=?,money=? where id=?&quot;,account.getName(),account.getMoney(),account.getId()); } } package com.itheima.domain; import java.io.Serializable; /** * 账户的实体类 */ public class Account implements Serializable { private Integer id; private String name; private Float money; public Integer getId() { return id; } public void setId(Integer id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public Float getMoney() { return money; } public void setMoney(Float money) { this.money = money; } @Override public String toString() { return &quot;Account{&quot; + &quot;id=&quot; + id + &quot;, name=&apos;&quot; + name + &apos;\&apos;&apos; + &quot;, money=&quot; + money + &apos;}&apos;; } } package com.itheima.service; import com.itheima.domain.Account; /** * 账户的业务层接口 */ public interface IAccountService { /** * 根据id查询账户信息 * @param accountId * @return */ Account findAccountById(Integer accountId); /** * 转账 * @param sourceName 转成账户名称 * @param targetName 转入账户名称 * @param money 转账金额 */ void transfer(String sourceName, String targetName, Float money); } package com.itheima.service.impl; import com.itheima.dao.IAccountDao; import com.itheima.domain.Account; import com.itheima.service.IAccountService; /** * 账户的业务层实现类 * * 事务控制应该都是在业务层 */ public class AccountServiceImpl implements IAccountService{ private IAccountDao accountDao; public void setAccountDao(IAccountDao accountDao) { this.accountDao = accountDao; } @Override public Account findAccountById(Integer accountId) { return accountDao.findAccountById(accountId); } @Override public void transfer(String sourceName, String targetName, Float money) { System.out.println(&quot;transfer....&quot;); //2.1根据名称查询转出账户 Account source = accountDao.findAccountByName(sourceName); //2.2根据名称查询转入账户 Account target = accountDao.findAccountByName(targetName); //2.3转出账户减钱 source.setMoney(source.getMoney()-money); //2.4转入账户加钱 target.setMoney(target.getMoney()+money); //2.5更新转出账户 accountDao.updateAccount(source); int i=1/0; //2.6更新转入账户 accountDao.updateAccount(target); } } 下面重点看：&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xsi:schemaLocation=&quot; http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd&quot;&gt; &lt;!-- 配置业务层--&gt; &lt;bean id=&quot;accountService&quot; class=&quot;com.itheima.service.impl.AccountServiceImpl&quot;&gt; &lt;property name=&quot;accountDao&quot; ref=&quot;accountDao&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 配置账户的持久层--&gt; &lt;bean id=&quot;accountDao&quot; class=&quot;com.itheima.dao.impl.AccountDaoImpl&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 配置数据源--&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;org.springframework.jdbc.datasource.DriverManagerDataSource&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;com.mysql.jdbc.Driver&quot;&gt;&lt;/property&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/eesy&quot;&gt;&lt;/property&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot;&gt;&lt;/property&gt; &lt;property name=&quot;password&quot; value=&quot;1234&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- spring中基于XML的声明式事务控制配置步骤 1、配置事务管理器 2、配置事务的通知 此时我们需要导入事务的约束 tx名称空间和约束，同时也需要aop的 使用tx:advice标签配置事务通知 属性： id：给事务通知起一个唯一标识 transaction-manager：给事务通知提供一个事务管理器引用 3、配置AOP中的通用切入点表达式 4、建立事务通知和切入点表达式的对应关系 5、配置事务的属性 是在事务的通知tx:advice标签的内部 --&gt; &lt;!-- 配置事务管理器 --&gt; &lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 配置事务的通知--&gt; &lt;tx:advice id=&quot;txAdvice&quot; transaction-manager=&quot;transactionManager&quot;&gt; &lt;!-- 配置事务的属性 isolation：用于指定事务的隔离级别。默认值是DEFAULT，表示使用数据库的默认隔离级别。 propagation：用于指定事务的传播行为。默认值是REQUIRED，表示一定会有事务，增删改的选择。查询方法可以选择SUPPORTS。 read-only：用于指定事务是否只读。只有查询方法才能设置为true。默认值是false，表示读写。 timeout：用于指定事务的超时时间，默认值是-1，表示永不超时。如果指定了数值，以秒为单位。 rollback-for：用于指定一个异常，当产生该异常时，事务回滚，产生其他异常时，事务不回滚。没有默认值。表示任何异常都回滚。 no-rollback-for：用于指定一个异常，当产生该异常时，事务不回滚，产生其他异常时事务回滚。没有默认值。表示任何异常都回滚。 --&gt; &lt;tx:attributes&gt; &lt;tx:method name=&quot;*&quot; propagation=&quot;REQUIRED&quot; read-only=&quot;false&quot;/&gt; &lt;tx:method name=&quot;find*&quot; propagation=&quot;SUPPORTS&quot; read-only=&quot;true&quot;&gt;&lt;/tx:method&gt; &lt;/tx:attributes&gt; &lt;/tx:advice&gt; &lt;!-- 配置aop--&gt; &lt;aop:config&gt; &lt;!-- 配置切入点表达式--&gt; &lt;aop:pointcut id=&quot;pt1&quot; expression=&quot;execution(* com.itheima.service.impl.*.*(..))&quot;&gt;&lt;/aop:pointcut&gt; &lt;!--建立切入点表达式和事务通知的对应关系 --&gt; &lt;aop:advisor advice-ref=&quot;txAdvice&quot; pointcut-ref=&quot;pt1&quot;&gt;&lt;/aop:advisor&gt; &lt;/aop:config&gt; &lt;/beans&gt; package com.itheima.test; import com.itheima.service.IAccountService; import org.junit.Test; import org.junit.runner.RunWith; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.test.context.ContextConfiguration; import org.springframework.test.context.junit4.SpringJUnit4ClassRunner; /** * 使用Junit单元测试：测试我们的配置 */ @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations = &quot;classpath:bean.xml&quot;) public class AccountServiceTest { @Autowired private IAccountService as; @Test public void testTransfer(){ as.transfer(&quot;aaa&quot;,&quot;bbb&quot;,100f); } } 这个地方重点看的配置就是bean里面的配置： 两种DAO的编写方式：（回顾一下DAO的实现方式） package com.itheima.dao; import com.itheima.domain.Account; /** * 账户的持久层接口 */ public interface IAccountDao { /** * 根据Id查询账户 * @param accountId * @return */ Account findAccountById(Integer accountId); /** * 根据名称查询账户 * @param accountName * @return */ Account findAccountByName(String accountName); /** * 更新账户 * @param account */ void updateAccount(Account account); } package com.itheima.dao.impl; import com.itheima.dao.IAccountDao; import com.itheima.domain.Account; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.jdbc.core.BeanPropertyRowMapper; import org.springframework.jdbc.core.JdbcTemplate; import org.springframework.stereotype.Repository; import java.util.List; /** * 账户的持久层实现类 */ @Repository(&quot;accountDao&quot;) public class AccountDaoImpl implements IAccountDao { @Autowired private JdbcTemplate jdbcTemplate; @Override public Account findAccountById(Integer accountId) { List&lt;Account&gt; accounts = jdbcTemplate.query(&quot;select * from account where id = ?&quot;,new BeanPropertyRowMapper&lt;Account&gt;(Account.class),accountId); return accounts.isEmpty()?null:accounts.get(0); } @Override public Account findAccountByName(String accountName) { List&lt;Account&gt; accounts = jdbcTemplate.query(&quot;select * from account where name = ?&quot;,new BeanPropertyRowMapper&lt;Account&gt;(Account.class),accountName); if(accounts.isEmpty()){ return null; } if(accounts.size()&gt;1){ throw new RuntimeException(&quot;结果集不唯一&quot;); } return accounts.get(0); } @Override public void updateAccount(Account account) { jdbcTemplate.update(&quot;update account set name=?,money=? where id=?&quot;,account.getName(),account.getMoney(),account.getId()); } } package com.itheima.domain; import java.io.Serializable; /** * 账户的实体类 */ public class Account implements Serializable { private Integer id; private String name; private Float money; public Integer getId() { return id; } public void setId(Integer id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public Float getMoney() { return money; } public void setMoney(Float money) { this.money = money; } @Override public String toString() { return &quot;Account{&quot; + &quot;id=&quot; + id + &quot;, name=&apos;&quot; + name + &apos;\&apos;&apos; + &quot;, money=&quot; + money + &apos;}&apos;; } } package com.itheima.service; import com.itheima.domain.Account; /** * 账户的业务层接口 */ public interface IAccountService { /** * 根据id查询账户信息 * @param accountId * @return */ Account findAccountById(Integer accountId); /** * 转账 * @param sourceName 转成账户名称 * @param targetName 转入账户名称 * @param money 转账金额 */ void transfer(String sourceName, String targetName, Float money); } package com.itheima.service.impl; import com.itheima.dao.IAccountDao; import com.itheima.domain.Account; import com.itheima.service.IAccountService; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.stereotype.Service; import org.springframework.transaction.annotation.Propagation; import org.springframework.transaction.annotation.Transactional; /** * 账户的业务层实现类 * * 事务控制应该都是在业务层 */ @Service(&quot;accountService&quot;) @Transactional(propagation= Propagation.SUPPORTS,readOnly=true)//只读型事务的配置 public class AccountServiceImpl implements IAccountService{ @Autowired private IAccountDao accountDao; @Override public Account findAccountById(Integer accountId) { return accountDao.findAccountById(accountId); } //需要的是读写型事务配置 @Transactional(propagation= Propagation.REQUIRED,readOnly=false) @Override public void transfer(String sourceName, String targetName, Float money) { System.out.println(&quot;transfer....&quot;); //2.1根据名称查询转出账户 Account source = accountDao.findAccountByName(sourceName); //2.2根据名称查询转入账户 Account target = accountDao.findAccountByName(targetName); //2.3转出账户减钱 source.setMoney(source.getMoney()-money); //2.4转入账户加钱 target.setMoney(target.getMoney()+money); //2.5更新转出账户 accountDao.updateAccount(source); int i=1/0; //2.6更新转入账户 accountDao.updateAccount(target); } } 重点的bean配置文件： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot; http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;!-- 配置spring创建容器时要扫描的包--&gt; &lt;context:component-scan base-package=&quot;com.itheima&quot;&gt;&lt;/context:component-scan&gt; &lt;!-- 配置JdbcTemplate--&gt; &lt;bean id=&quot;jdbcTemplate&quot; class=&quot;org.springframework.jdbc.core.JdbcTemplate&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 配置数据源--&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;org.springframework.jdbc.datasource.DriverManagerDataSource&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;com.mysql.jdbc.Driver&quot;&gt;&lt;/property&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/eesy&quot;&gt;&lt;/property&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot;&gt;&lt;/property&gt; &lt;property name=&quot;password&quot; value=&quot;1234&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- spring中基于注解 的声明式事务控制配置步骤 1、配置事务管理器 2、开启spring对注解事务的支持 3、在需要事务支持的地方使用@Transactional注解 --&gt; &lt;!-- 配置事务管理器 --&gt; &lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 开启spring对注解事务的支持--&gt; &lt;tx:annotation-driven transaction-manager=&quot;transactionManager&quot;&gt;&lt;/tx:annotation-driven&gt; &lt;/beans&gt; package com.itheima.test; import com.itheima.service.IAccountService; import org.junit.Test; import org.junit.runner.RunWith; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.test.context.ContextConfiguration; import org.springframework.test.context.junit4.SpringJUnit4ClassRunner; /** * 使用Junit单元测试：测试我们的配置 */ @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations = &quot;classpath:bean.xml&quot;) public class AccountServiceTest { @Autowired private IAccountService as; @Test public void testTransfer(){ as.transfer(&quot;aaa&quot;,&quot;bbb&quot;,100f); } }]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>事务控制</tag>
        <tag>XML控制事务</tag>
        <tag>纯注解控制事务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring第四天_Spring中事务控制-作业例子]]></title>
    <url>%2F2019%2F10%2F08%2FSpring%E7%AC%AC%E5%9B%9B%E5%A4%A9_Spring%E4%B8%AD%E4%BA%8B%E5%8A%A1%E6%8E%A7%E5%88%B6-%E4%BD%9C%E4%B8%9A%E4%BE%8B%E5%AD%90%2F</url>
    <content type="text"><![CDATA[这节主要讲解了从银行转账的一个基于XML配置文件的，一个是基于注解配置的两个案例。2个都是比较完整的代码，没有测试过，可能因为数据库8.0问题导致连接不上，但是代码应该没有问题。可以作为例子深入研究一下注解和XML文件配置。 首先，由注解开发的银行转账案例完整版作业： package com.itheima.dao; import com.itheima.domain.Account; import java.util.List; /** * 账户的持久层接口 */ public interface IAccountDao { /** * 查询所有 * @return */ List&lt;Account&gt; findAllAccount(); /** * 查询一个 * @return */ Account findAccountById(Integer accountId); /** * 保存 * @param account */ void saveAccount(Account account); /** * 更新 * @param account */ void updateAccount(Account account); /** * 删除 * @param acccountId */ void deleteAccount(Integer acccountId); /** * 根据名称查账户 * @param accountName 用户名称 * @return 如果有唯一结果就返回，如果没有结果就返回Null,结果集超过一个返回异常 */ Account findAccountByName(String accountName); } package com.itheima.dao.impl; import com.itheima.dao.IAccountDao; import com.itheima.domain.Account; import com.itheima.utils.ConnectionUtils; import org.apache.commons.dbutils.QueryRunner; import org.apache.commons.dbutils.handlers.BeanHandler; import org.apache.commons.dbutils.handlers.BeanListHandler; import java.util.List; /** * 账户的持久层实现类 */ public class AccountDaoImpl implements IAccountDao { private QueryRunner runner; private ConnectionUtils connectionUtils; public void setRunner(QueryRunner runner) { this.runner = runner; } public void setConnectionUtils(ConnectionUtils connectionUtils) { this.connectionUtils = connectionUtils; } public List&lt;Account&gt; findAllAccount() { try{ return runner.query(connectionUtils.getThreadConnection(),&quot;select * from account&quot;,new BeanListHandler&lt;Account&gt;(Account.class)); }catch (Exception e) { throw new RuntimeException(e); } } public Account findAccountById(Integer accountId) { try{ return runner.query(connectionUtils.getThreadConnection(),&quot;select * from account where id = ? &quot;,new BeanHandler&lt;Account&gt;(Account.class),accountId); }catch (Exception e) { throw new RuntimeException(e); } } public void saveAccount(Account account) { try{ runner.update(connectionUtils.getThreadConnection(),&quot;insert into account(name,money)values(?,?)&quot;,account.getName(),account.getMoney()); }catch (Exception e) { throw new RuntimeException(e); } } public void updateAccount(Account account) { try{ runner.update(connectionUtils.getThreadConnection(),&quot;update account set name=?,money=? where id=?&quot;,account.getName(),account.getMoney(),account.getId()); }catch (Exception e) { throw new RuntimeException(e); } } public void deleteAccount(Integer accountId) { try{ runner.update(connectionUtils.getThreadConnection(),&quot;delete from account where id=?&quot;,accountId); }catch (Exception e) { throw new RuntimeException(e); } } public Account findAccountByName(String accountName) { try{ List&lt;Account&gt; accounts = runner.query(connectionUtils.getThreadConnection(),&quot;select * from account where name = ? &quot;,new BeanListHandler&lt;Account&gt;(Account.class),accountName); if(accounts == null || accounts.size() == 0){ return null; } if(accounts.size() &gt; 1){ throw new RuntimeException(&quot;结果集不唯一，数据有问题&quot;); } return accounts.get(0); }catch (Exception e) { throw new RuntimeException(e); } } } package com.itheima.domain; import java.io.Serializable; /** * 账户的实体类 */ public class Account implements Serializable { private Integer id; private String name; private Float money; public Integer getId() { return id; } public void setId(Integer id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public Float getMoney() { return money; } public void setMoney(Float money) { this.money = money; } @Override public String toString() { return &quot;Account{&quot; + &quot;id=&quot; + id + &quot;, name=&apos;&quot; + name + &apos;\&apos;&apos; + &quot;, money=&quot; + money + &apos;}&apos;; } } package com.itheima.service; import com.itheima.domain.Account; import java.util.List; /** * 账户的业务层接口 */ public interface IAccountService { /** * 查询所有 * @return */ List&lt;Account&gt; findAllAccount(); /** * 查询一个 * @return */ Account findAccountById(Integer accountId); /** * 保存 * @param account */ void saveAccount(Account account); /** * 更新 * @param account */ void updateAccount(Account account); /** * 删除 * @param acccountId */ void deleteAccount(Integer acccountId); /** * * 转账方法 * @param sourceName 转出账户名称 * @param targetName 转入账户名称 * @param money 转账金额 */ void transfer(String sourceName, String targetName, Float money); } package com.itheima.service.impl; import com.itheima.dao.IAccountDao; import com.itheima.domain.Account; import com.itheima.service.IAccountService; import com.itheima.utils.TransactionManager; import java.util.List; /** * 账户的业务层实现类 * * 事务控制应该都是在业务层 */ public class AccountServiceImpl implements IAccountService{ private IAccountDao accountDao; private TransactionManager txManager; public void setTxManager(TransactionManager txManager) { this.txManager = txManager; } public void setAccountDao(IAccountDao accountDao) { this.accountDao = accountDao; } @Override public List&lt;Account&gt; findAllAccount() { try { //1.开启事务 txManager.beginTransaction(); //2.执行操作 List&lt;Account&gt; accounts = accountDao.findAllAccount(); //3.提交事务 txManager.commit(); //4.返回结果 return accounts; }catch (Exception e){ //5.回滚操作 txManager.rollback(); throw new RuntimeException(e); }finally { //6.释放连接 txManager.release(); } } @Override public Account findAccountById(Integer accountId) { try { //1.开启事务 txManager.beginTransaction(); //2.执行操作 Account account = accountDao.findAccountById(accountId); //3.提交事务 txManager.commit(); //4.返回结果 return account; }catch (Exception e){ //5.回滚操作 txManager.rollback(); throw new RuntimeException(e); }finally { //6.释放连接 txManager.release(); } } @Override public void saveAccount(Account account) { try { //1.开启事务 txManager.beginTransaction(); //2.执行操作 accountDao.saveAccount(account); //3.提交事务 txManager.commit(); }catch (Exception e){ //4.回滚操作 txManager.rollback(); }finally { //5.释放连接 txManager.release(); } } @Override public void updateAccount(Account account) { try { //1.开启事务 txManager.beginTransaction(); //2.执行操作 accountDao.updateAccount(account); //3.提交事务 txManager.commit(); }catch (Exception e){ //4.回滚操作 txManager.rollback(); }finally { //5.释放连接 txManager.release(); } } @Override public void deleteAccount(Integer acccountId) { try { //1.开启事务 txManager.beginTransaction(); //2.执行操作 accountDao.deleteAccount(acccountId); //3.提交事务 txManager.commit(); }catch (Exception e){ //4.回滚操作 txManager.rollback(); }finally { //5.释放连接 txManager.release(); } } @Override public void transfer(String sourceName, String targetName, Float money) { try { //1.开启事务 txManager.beginTransaction(); //2.执行操作 //2.1根据名称查询转出账户 Account source = accountDao.findAccountByName(sourceName); //2.2根据名称查询转入账户 Account target = accountDao.findAccountByName(targetName); //2.3转出账户减钱 source.setMoney(source.getMoney()-money); //2.4转入账户加钱 target.setMoney(target.getMoney()+money); //2.5更新转出账户 accountDao.updateAccount(source); int i=1/0; //2.6更新转入账户 accountDao.updateAccount(target); //3.提交事务 txManager.commit(); }catch (Exception e){ //4.回滚操作 txManager.rollback(); e.printStackTrace(); }finally { //5.释放连接 txManager.release(); } } } package com.itheima.utils; import javax.sql.DataSource; import java.sql.Connection; /** * @Description 连接的工具类，用于从数据源中获取一个连接，并且实现和线程的绑定 * @Author TT Hun * @Data 2019/10/3 11:22 */ public class ConnectionUtils { private ThreadLocal&lt;Connection&gt; tl = new ThreadLocal&lt;Connection&gt;(); private DataSource dataSource; public void setDataSource(DataSource dataSource) { this.dataSource = dataSource; } /** * 获取当前线程上的链接 * * @return */ public Connection getThreadConnection() { // 1、先从ThreadLocal上获取 Connection conn = tl.get(); // 2、判断当前线程上是否有链接 try { if (conn == null) { // 3、从数据源中获取一个连接，并且存入ThreadLocalz中 conn = dataSource.getConnection(); tl.set(conn); } // 4、返回当前线程上的连接 return conn; } catch (Exception e) { throw new RuntimeException(&quot;报错了&quot;); } } /** * 链接和线程解绑 */ public void removeConnection(){ tl.remove(); } } package com.itheima.utils; /** * @Description 和事务管理相关的工具类，包含了开启事务，提交事务，回滚事务和释放连接的方法 * @Author TT Hun * @Data 2019/10/3 11:35 */ public class TransactionManager { private ConnectionUtils connectionUtils; public void setConnectionUtils(ConnectionUtils connectionUtils) { this.connectionUtils = connectionUtils; } /** * 开始事务 */ public void beginTransaction() { try { connectionUtils.getThreadConnection().setAutoCommit(false); } catch (Exception e) { e.printStackTrace(); } } /** * 提交事务 */ public void commit() { try { connectionUtils.getThreadConnection().commit(); } catch (Exception e) { e.printStackTrace(); } } /** * 回滚事务 */ public void rollback() { try { connectionUtils.getThreadConnection().rollback(); } catch (Exception e) { e.printStackTrace(); } } /** * 释放连接事务 */ public void release() { try { // 使用服务器也有一个线程池，当tomcat启动时候，会初始化一大堆线程放到一个容器中 // 每次访问都是线程池中拿出来一个线程给我们使用，调用close方法并不是真正的关闭而是将线程还给线程池 connectionUtils.getThreadConnection().close(); // 所以想要连接和线程解绑，通过这个方法。 connectionUtils.removeConnection(); } catch (Exception e) { e.printStackTrace(); } } } &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd&quot;&gt; &lt;!-- 配置Service --&gt; &lt;bean id=&quot;accountService&quot; class=&quot;com.itheima.service.impl.AccountServiceImpl&quot;&gt; &lt;!-- 注入dao --&gt; &lt;property name=&quot;accountDao&quot; ref=&quot;accountDao&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!--配置Dao对象--&gt; &lt;bean id=&quot;accountDao&quot; class=&quot;com.itheima.dao.impl.AccountDaoImpl&quot;&gt; &lt;!-- 注入QueryRunner --&gt; &lt;property name=&quot;runner&quot; ref=&quot;runner&quot;&gt;&lt;/property&gt; &lt;!-- 注入ConnectionUtils--&gt; &lt;property name=&quot;connectionUtils&quot; ref =&quot;connectionUtils&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!--配置QueryRunner--&gt; &lt;bean id=&quot;runner&quot; class=&quot;org.apache.commons.dbutils.QueryRunner&quot; scope=&quot;prototype&quot;&gt; &lt;/bean&gt; &lt;!-- 配置数据源 --&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;com.mchange.v2.c3p0.ComboPooledDataSource&quot;&gt; &lt;!--连接数据库的必备信息--&gt; &lt;property name=&quot;driverClass&quot; value=&quot;com.mysql.cj.jdbc.Driver&quot;&gt;&lt;/property&gt; &lt;property name=&quot;jdbcUrl&quot; value=&quot;jdbc:mysql://localhost:3306/eesy?characterEncoding=utf8&amp;amp;serverTimezone=UTC&amp;amp;useSSL=false&quot;&gt;&lt;/property&gt; &lt;property name=&quot;user&quot; value=&quot;root&quot;&gt;&lt;/property&gt; &lt;property name=&quot;password&quot; value=&quot;root&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 配置Connection的工具类 ConnectionUtils--&gt; &lt;bean id =&quot;connectionUtils&quot; class =&quot;com.itheima.utils.ConnectionUtils&quot;&gt; &lt;!-- 注入数据源--&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 配置事务管理器--&gt; &lt;bean id=&quot;txManager&quot; class=&quot;com.itheima.utils.TransactionManager&quot;&gt; &lt;!-- 注入ConnectionUtils--&gt; &lt;property name=&quot;connectionUtils&quot; ref=&quot;connectionUtils&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 配置AOP--&gt; &lt;aop:config&gt; &lt;!-- 配置通用的切入点表达式--&gt; &lt;aop:pointcut id=&quot;pt1&quot; expression=&quot;execution(* com.itheima.service.impl.*.*(..)))&quot;&gt;&lt;/aop:pointcut&gt; &lt;aop:aspect id=&quot;txAdvice&quot; ref=&quot;txManager&quot;&gt; &lt;!-- 配置前置通知，开启事务--&gt; &lt;aop:before method=&quot;beginTransaction&quot; pointcut-ref=&quot;pt1&quot;&gt;&lt;/aop:before&gt; &lt;!-- 配置后置通知，提交事务--&gt; &lt;aop:after-returning method=&quot;commit&quot; pointcut-ref=&quot;pt1&quot;&gt;&lt;/aop:after-returning&gt; &lt;!-- 配置异常通知，回滚事务--&gt; &lt;aop:after-throwing method=&quot;rollback&quot; pointcut-ref=&quot;pt1&quot;&gt;&lt;/aop:after-throwing&gt; &lt;!-- 配置最终通知，释放连接--&gt; &lt;aop:after method=&quot;release&quot; pointcut-ref=&quot;pt1&quot;&gt;&lt;/aop:after&gt; &lt;/aop:aspect&gt; &lt;/aop:config&gt; &lt;/beans&gt; package com.itheima.test; import com.itheima.domain.Account; import com.itheima.service.IAccountService; import org.junit.Test; import org.junit.runner.RunWith; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.beans.factory.annotation.Qualifier; import org.springframework.test.context.ContextConfiguration; import org.springframework.test.context.junit4.SpringJUnit4ClassRunner; import java.util.List; /** * 使用Junit单元测试：测试我们的配置 */ @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations = &quot;classpath:bean.xml&quot;) public class AccountServiceTest { @Autowired private IAccountService as; @Test public void testTransfer(){ as.transfer(&quot;aaa&quot;,&quot;bbb&quot;,100f); } } 下面是基于注解的银行转行范例： package com.itheima.dao; import com.itheima.domain.Account; import java.util.List; /** * 账户的持久层接口 */ public interface IAccountDao { /** * 查询所有 * @return */ List&lt;Account&gt; findAllAccount(); /** * 查询一个 * @return */ Account findAccountById(Integer accountId); /** * 保存 * @param account */ void saveAccount(Account account); /** * 更新 * @param account */ void updateAccount(Account account); /** * 删除 * @param acccountId */ void deleteAccount(Integer acccountId); /** * 根据名称查账户 * @param accountName 用户名称 * @return 如果有唯一结果就返回，如果没有结果就返回Null,结果集超过一个返回异常 */ Account findAccountByName(String accountName); } package com.itheima.dao.impl; import com.itheima.dao.IAccountDao; import com.itheima.domain.Account; import com.itheima.utils.ConnectionUtils; import org.apache.commons.dbutils.QueryRunner; import org.apache.commons.dbutils.handlers.BeanHandler; import org.apache.commons.dbutils.handlers.BeanListHandler; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.stereotype.Repository; import java.util.List; /** * 账户的持久层实现类 */ @Repository(&quot;accountDao&quot;) public class AccountDaoImpl implements IAccountDao { @Autowired private QueryRunner runner; @Autowired private ConnectionUtils connectionUtils; public List&lt;Account&gt; findAllAccount() { try{ return runner.query(connectionUtils.getThreadConnection(),&quot;select * from account&quot;,new BeanListHandler&lt;Account&gt;(Account.class)); }catch (Exception e) { throw new RuntimeException(e); } } public Account findAccountById(Integer accountId) { try{ return runner.query(connectionUtils.getThreadConnection(),&quot;select * from account where id = ? &quot;,new BeanHandler&lt;Account&gt;(Account.class),accountId); }catch (Exception e) { throw new RuntimeException(e); } } public void saveAccount(Account account) { try{ runner.update(connectionUtils.getThreadConnection(),&quot;insert into account(name,money)values(?,?)&quot;,account.getName(),account.getMoney()); }catch (Exception e) { throw new RuntimeException(e); } } public void updateAccount(Account account) { try{ runner.update(connectionUtils.getThreadConnection(),&quot;update account set name=?,money=? where id=?&quot;,account.getName(),account.getMoney(),account.getId()); }catch (Exception e) { throw new RuntimeException(e); } } public void deleteAccount(Integer accountId) { try{ runner.update(connectionUtils.getThreadConnection(),&quot;delete from account where id=?&quot;,accountId); }catch (Exception e) { throw new RuntimeException(e); } } public Account findAccountByName(String accountName) { try{ List&lt;Account&gt; accounts = runner.query(connectionUtils.getThreadConnection(),&quot;select * from account where name = ? &quot;,new BeanListHandler&lt;Account&gt;(Account.class),accountName); if(accounts == null || accounts.size() == 0){ return null; } if(accounts.size() &gt; 1){ throw new RuntimeException(&quot;结果集不唯一，数据有问题&quot;); } return accounts.get(0); }catch (Exception e) { throw new RuntimeException(e); } } } package com.itheima.domain; import java.io.Serializable; /** * 账户的实体类 */ public class Account implements Serializable { private Integer id; private String name; private Float money; public Integer getId() { return id; } public void setId(Integer id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public Float getMoney() { return money; } public void setMoney(Float money) { this.money = money; } @Override public String toString() { return &quot;Account{&quot; + &quot;id=&quot; + id + &quot;, name=&apos;&quot; + name + &apos;\&apos;&apos; + &quot;, money=&quot; + money + &apos;}&apos;; } } package com.itheima.service; import com.itheima.domain.Account; import java.util.List; /** * 账户的业务层接口 */ public interface IAccountService { /** * 查询所有 * @return */ List&lt;Account&gt; findAllAccount(); /** * 查询一个 * @return */ Account findAccountById(Integer accountId); /** * 保存 * @param account */ void saveAccount(Account account); /** * 更新 * @param account */ void updateAccount(Account account); /** * 删除 * @param acccountId */ void deleteAccount(Integer acccountId); /** * * 转账方法 * @param sourceName 转出账户名称 * @param targetName 转入账户名称 * @param money 转账金额 */ void transfer(String sourceName, String targetName, Float money); } package com.itheima.service.impl; import com.itheima.dao.IAccountDao; import com.itheima.domain.Account; import com.itheima.service.IAccountService; import com.itheima.utils.TransactionManager; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.stereotype.Service; import java.util.List; /** * 账户的业务层实现类 * * 事务控制应该都是在业务层 */ @Service(&quot;accountService&quot;) public class AccountServiceImpl implements IAccountService{ @Autowired//dao自动按照类型注入 private IAccountDao accountDao; @Autowired private TransactionManager txManager; public List&lt;Account&gt; findAllAccount() { try { //1.开启事务 txManager.beginTransaction(); //2.执行操作 List&lt;Account&gt; accounts = accountDao.findAllAccount(); //3.提交事务 txManager.commit(); //4.返回结果 return accounts; }catch (Exception e){ //5.回滚操作 txManager.rollback(); throw new RuntimeException(e); }finally { //6.释放连接 txManager.release(); } } @Override public Account findAccountById(Integer accountId) { try { //1.开启事务 txManager.beginTransaction(); //2.执行操作 Account account = accountDao.findAccountById(accountId); //3.提交事务 txManager.commit(); //4.返回结果 return account; }catch (Exception e){ //5.回滚操作 txManager.rollback(); throw new RuntimeException(e); }finally { //6.释放连接 txManager.release(); } } @Override public void saveAccount(Account account) { try { //1.开启事务 txManager.beginTransaction(); //2.执行操作 accountDao.saveAccount(account); //3.提交事务 txManager.commit(); }catch (Exception e){ //4.回滚操作 txManager.rollback(); }finally { //5.释放连接 txManager.release(); } } @Override public void updateAccount(Account account) { try { //1.开启事务 txManager.beginTransaction(); //2.执行操作 accountDao.updateAccount(account); //3.提交事务 txManager.commit(); }catch (Exception e){ //4.回滚操作 txManager.rollback(); }finally { //5.释放连接 txManager.release(); } } @Override public void deleteAccount(Integer acccountId) { try { //1.开启事务 txManager.beginTransaction(); //2.执行操作 accountDao.deleteAccount(acccountId); //3.提交事务 txManager.commit(); }catch (Exception e){ //4.回滚操作 txManager.rollback(); }finally { //5.释放连接 txManager.release(); } } @Override public void transfer(String sourceName, String targetName, Float money) { try { //1.开启事务 txManager.beginTransaction(); //2.执行操作 //2.1根据名称查询转出账户 Account source = accountDao.findAccountByName(sourceName); //2.2根据名称查询转入账户 Account target = accountDao.findAccountByName(targetName); //2.3转出账户减钱 source.setMoney(source.getMoney()-money); //2.4转入账户加钱 target.setMoney(target.getMoney()+money); //2.5更新转出账户 accountDao.updateAccount(source); // int i=1/0; //2.6更新转入账户 accountDao.updateAccount(target); //3.提交事务 txManager.commit(); }catch (Exception e){ //4.回滚操作 txManager.rollback(); e.printStackTrace(); }finally { //5.释放连接 txManager.release(); } } } package com.itheima.utils; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.stereotype.Component; import javax.sql.DataSource; import java.sql.Connection; /** * @Description 连接的工具类，用于从数据源中获取一个连接，并且实现和线程的绑定 * @Author TT Hun * @Data 2019/10/3 11:22 */ @Component(&quot;connectionUtils&quot;) public class ConnectionUtils { private ThreadLocal&lt;Connection&gt; tl = new ThreadLocal&lt;Connection&gt;(); @Autowired private DataSource dataSource; public void setDataSource(DataSource dataSource) { this.dataSource = dataSource; } /** * 获取当前线程上的链接 * * @return */ public Connection getThreadConnection() { // 1、先从ThreadLocal上获取 Connection conn = tl.get(); // 2、判断当前线程上是否有链接 try { if (conn == null) { // 3、从数据源中获取一个连接，并且存入ThreadLocalz中 conn = dataSource.getConnection(); tl.set(conn); } // 4、返回当前线程上的连接 return conn; } catch (Exception e) { throw new RuntimeException(&quot;报错了&quot;); } } /** * 链接和线程解绑 */ public void removeConnection() { tl.remove(); } } package com.itheima.utils; import org.aspectj.lang.ProceedingJoinPoint; import org.aspectj.lang.annotation.*; import org.junit.AfterClass; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.stereotype.Component; import javax.management.relation.RoleUnresolved; /** * @Description 和事务管理相关的工具类，包含了开启事务，提交事务，回滚事务和释放连接的方法 * @Author TT Hun * @Data 2019/10/3 11:35 */ @Component(&quot;txManager&quot;) @Aspect//配置切面，表示当前类是一个切面类 public class TransactionManager { @Autowired private ConnectionUtils connectionUtils; @Pointcut(&quot;execution(* com.itheima.service.impl.*.*(..))&quot;)//切入点表达式 private void pt1(){} /** * 开始事务 */ // @Before(&quot;pt1()&quot;)不写这个因为配置注解开发会导致首先出发before然后出发release最后出发commit public void beginTransaction() { try { connectionUtils.getThreadConnection().setAutoCommit(false); } catch (Exception e) { e.printStackTrace(); } } /** * 提交事务 */ // @AfterReturning(&quot;pt1()&quot;)不写这个因为配置注解开发会导致首先出发before然后出发release最后出发commit public void commit() { try { connectionUtils.getThreadConnection().commit(); } catch (Exception e) { e.printStackTrace(); } } /** * 回滚事务 */ // @AfterReturning(&quot;pt1()&quot;)不写这个因为配置注解开发会导致首先出发before然后出发release最后出发commit public void rollback() { try { connectionUtils.getThreadConnection().rollback(); } catch (Exception e) { e.printStackTrace(); } } /** * 释放连接事务 */ // @After(&quot;pt1()&quot;)不写这个因为配置注解开发会导致首先出发before然后出发release最后出发commit public void release() { try { // 使用服务器也有一个线程池，当tomcat启动时候，会初始化一大堆线程放到一个容器中 // 每次访问都是线程池中拿出来一个线程给我们使用，调用close方法并不是真正的关闭而是将线程还给线程池 connectionUtils.getThreadConnection().close(); // 所以想要连接和线程解绑，通过这个方法。 connectionUtils.removeConnection(); } catch (Exception e) { e.printStackTrace(); } } @Around(&quot;pt1()&quot;) public Object aroundAdvice(ProceedingJoinPoint pjp){ Object rtValue = null; try{ // 1、获取参数 Object[] args = pjp.getArgs(); // 2、开启事务 this.beginTransaction(); // 3、执行方法 rtValue = pjp.proceed(args); // 4、提交事务 this.commit(); // 5、返回结果 return rtValue; }catch(Throwable e){ // 回滚事务 this.rollback(); throw new RuntimeException(e); } finally{ // 释放资源 this.release(); } } } &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;!-- 配置Spring创建容器时需要扫描的包--&gt; &lt;context:component-scan base-package=&quot;com.itheima&quot;&gt;&lt;/context:component-scan&gt; &lt;!--配置QueryRunner--&gt; &lt;bean id=&quot;runner&quot; class=&quot;org.apache.commons.dbutils.QueryRunner&quot; scope=&quot;prototype&quot;&gt; &lt;/bean&gt; &lt;!-- 配置数据源 --&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;com.mchange.v2.c3p0.ComboPooledDataSource&quot;&gt; &lt;!--连接数据库的必备信息--&gt; &lt;property name=&quot;driverClass&quot; value=&quot;com.mysql.cj.jdbc.Driver&quot;&gt;&lt;/property&gt; &lt;property name=&quot;jdbcUrl&quot; value=&quot;jdbc:mysql://localhost:3306/eesy?characterEncoding=utf8&amp;amp;serverTimezone=UTC&amp;amp;useSSL=false&quot;&gt;&lt;/property&gt; &lt;property name=&quot;user&quot; value=&quot;root&quot;&gt;&lt;/property&gt; &lt;property name=&quot;password&quot; value=&quot;root&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!--开启Spring对注解AOP的支持--&gt; &lt;aop:aspectj-autoproxy&gt;&lt;/aop:aspectj-autoproxy&gt; &lt;/beans&gt; package com.itheima.test; import com.itheima.domain.Account; import com.itheima.service.IAccountService; import org.junit.Test; import org.junit.runner.RunWith; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.beans.factory.annotation.Qualifier; import org.springframework.test.context.ContextConfiguration; import org.springframework.test.context.junit4.SpringJUnit4ClassRunner; import java.util.List; /** * 使用Junit单元测试：测试我们的配置 */ @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations = &quot;classpath:bean.xml&quot;) public class AccountServiceTest { @Autowired private IAccountService as; @Test public void testTransfer(){ as.transfer(&quot;aaa&quot;,&quot;bbb&quot;,100f); } } 在注解配置银行案例的情况下，有一个小问题就是Spring容器自己配置事务的时候会有一个BUG，就是他会首先执行1、connect 2、release3、提交。所以通过配置环绕注解来解决这个问题，环绕注解也就是注解都是自己配置的。]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
        <tag>事务控制</tag>
        <tag>作业例子</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring第四天_JdbcTemplate的基本使用]]></title>
    <url>%2F2019%2F10%2F05%2FSpring%E7%AC%AC%E5%9B%9B%E5%A4%A9_JdbcTemplate%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[类似于dbutils 下面是代码，我们可以通过IOC和AOP动态代理来简化这些代码。使代码更加灵活 小tips： 在使用query找到哪个是需要的方法：比如下面的这段，要找的是使用query带哪个参数的方法，先看有的返回void的有的返回List的，也就是Spring的Template是靠不同的query返回不同类型的东西，而dbUtils的query方法是靠ResultSetHandler的返回值来决定返回的内容的， 下面这个方法有这么多重载的方法，如何去定位： 1、我们有什么：我们有SQL语句，语句的参数， 2、我们要什么：我们要一个返回的List集合 T类型和void类型直接就排除，然后第一个参数不是SQL语句的直接排除，然后RowMapper没有参数直接排除，一般参数就是可变参数和数组 所以最后剩下如下下图所示： dbUtils的和Spring的对比： 然后讲解一下JdbcTemplate的基本用法： 首先是bean.xml文件： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:util=&quot;http://www.springframework.org/schema/util&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util.xsd&quot;&gt; &lt;!-- 配置jdbctemplate--&gt; &lt;bean id=&quot;jdbcTemplate&quot; class=&quot;org.springframework.jdbc.core.JdbcTemplate&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 配置数据源--&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;org.springframework.jdbc.datasource.DriverManagerDataSource&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;com.mysql.jdbc.Driver&quot;&gt;&lt;/property&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306_eesy&quot;&gt; &lt;/property&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot;&gt;&lt;/property&gt; &lt;property name=&quot;password&quot; value=&quot;root&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;/beans&gt; 然后是定义的domain文件： package com.itheima.domain; import java.io.Serializable; /** * @Description 账户的实体类 * @Author TT Hun * @Data 2019/10/5 11:28 */ public class Account implements Serializable { private Integer id; private String name; private Float money; public Integer getId() { return id; } public void setId(Integer id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public Float getMoney() { return money; } public void setMoney(Float money) { this.money = money; } @Override public String toString() { return &quot;Account{&quot; + &quot;id=&quot; + id + &quot;, name=&apos;&quot; + name + &apos;\&apos;&apos; + &quot;, money=&quot; + money + &apos;}&apos;; } } 然后是几个JdbcTemplate的Demo package com.itheima.jdbctemplate; import org.springframework.jdbc.core.JdbcTemplate; import org.springframework.jdbc.datasource.DriverManagerDataSource; /** * @Description JDBCTemplate最基本用法 * @Author TT Hun * @Data 2019/10/5 11:30 */ public class JdbcTemplateDemo1 { public static void main(String[] args) { // 准备数据源，Spring的内置数据源不通过Spring容器，来设置内置数据源。 DriverManagerDataSource ds = new DriverManagerDataSource(); ds.setDriverClassName(&quot;com.mysql.jdbc.Driver&quot;); ds.setUrl(&quot;jdbc:mysql://localhost:3306/eesy&quot;); ds.setUsername(&quot;root&quot;); ds.setPassword(&quot;root&quot;); // 1、创建JDBCTemplate对象 JdbcTemplate jt = new JdbcTemplate(); // 给jt设置数据源 jt.setDataSource(ds); // 2、执行操作 jt.execute(&quot;insert into account(name,money) values(&apos;ccc&apos;,1000f)&quot;); } } package com.itheima.jdbctemplate; import org.springframework.context.ApplicationContext; import org.springframework.context.support.ClassPathXmlApplicationContext; import org.springframework.jdbc.core.JdbcTemplate; import org.springframework.jdbc.datasource.DriverManagerDataSource; /** * @Description JDBCTemplate最基本用法，通过Spring容器IOC，来获取对象。 * @Author TT Hun * @Data 2019/10/5 11:30 */ public class JdbcTemplateDemo2 { public static void main(String[] args) { // 1、获取容器 ApplicationContext ac = new ClassPathXmlApplicationContext(&quot;bean.xml&quot;); // 2、获取对象 JdbcTemplate jt = ac.getBean(&quot;jdbcTemplate&quot;, JdbcTemplate.class); // 3、添加操作 jt.execute(&quot;insert into account(name,money) values(&apos;ddd&apos;,222f)&quot;); } } package com.itheima.jdbctemplate; import com.itheima.domain.Account; import org.springframework.context.ApplicationContext; import org.springframework.context.support.ClassPathXmlApplicationContext; import org.springframework.jdbc.core.BeanPropertyRowMapper; import org.springframework.jdbc.core.JdbcTemplate; import org.springframework.jdbc.core.RowMapper; import java.sql.ResultSet; import java.sql.SQLException; import java.util.List; /** * @Description JDBCTemplate的CRUD用法 * @Author TT Hun * @Data 2019/10/5 11:30 */ public class JdbcTemplateDemo3 { public static void main(String[] args) { // 1、获取容器 ApplicationContext ac = new ClassPathXmlApplicationContext(&quot;bean.xml&quot;); // 2、获取对象 JdbcTemplate jt = ac.getBean(&quot;jdbcTemplate&quot;, JdbcTemplate.class); // 3、执行操作 // 保存操作 jt.update(&quot;insert into account(name,money)values(?,?)&quot;,&quot;eee&quot;,3333f); // 更新操作 jt.update(&quot;update acocunt set name = ?,money = ?,where id=?&quot;,&quot;test&quot;,4567,7); // 删除操作 jt.update(&quot;delete from account where id=?&quot;,8); // 查询所有 // List&lt;Account&gt; accounts = jt.query(&quot;select * from account where money &gt;?&quot;, new AccountRowMapper() , 1000f); // 下面这种方法不使用自己定义的结果集，而使用Spring自带的结果集即可。 List&lt;Account&gt; accounts = jt.query(&quot;select * from account where money&gt;?&quot;, new BeanPropertyRowMapper&lt;Account&gt;(Account.class) , 1000f); for (Account account:accounts){ System.out.println(account); } // 查询一个 List&lt;Account&gt; account = jt.query(&quot;select * from account where id=?&quot;, new BeanPropertyRowMapper&lt;Account&gt;(Account.class) , 1); System.out.println(accounts.isEmpty()? &quot;没有内容&quot;:accounts.get(0)); // 查询返回一行一列(使用聚合函数但是不加groupby字句) List&lt;Long&gt; count = jt.queryForList(&quot;select count(*) from account where money &gt; ? &quot;, Long.class, 100000f); System.out.println(count); } } /** * 定义Account的封装策略的 */ class AccountRowMapper implements RowMapper&lt;Account&gt; { /** * 把结果集中的数据封装到Account中然后由Spring把每个Account加到集合中 * @param rs * @param rowNum * @return * @throws SQLException */ public Account mapRow(ResultSet rs, int rowNum) throws SQLException { Account account = new Account(); account.setId(rs.getInt(&quot;id&quot;)); account.setName(rs.getString(&quot;name&quot;)); account.setMoney(rs.getFloat(&quot;money&quot;)); return account; } } 测试类使用Dao层的操作： import com.itheima.dao.IAccountDao; import com.itheima.domain.Account; import org.springframework.context.ApplicationContext; import org.springframework.context.support.ClassPathXmlApplicationContext; import org.springframework.jdbc.core.JdbcTemplate; /** * @Description JDBCTemplate最基本用法，通过Spring容器IOC，来获取对象。 * @Author TT Hun * @Data 2019/10/5 11:30 */ public class test { public static void main(String[] args) { // 1、获取容器 ApplicationContext ac = new ClassPathXmlApplicationContext(&quot;bean.xml&quot;); // 2、获取对象 // JdbcTemplate accountDao = ac.getBean(&quot;jdbcTemplate&quot;, JdbcTemplate.class); // 3、添加操作 // jt.execute(&quot;insert into account(name,money) values(&apos;ddd&apos;,222f)&quot;); IAccountDao ad = ac.getBean(&quot;accountDao&quot;, IAccountDao.class); Account account = ad.findAccountById(1); System.out.println(account); } } 和上面测试类一起的 package com.itheima.dao.impl; import com.itheima.dao.IAccountDao; import com.itheima.domain.Account; import org.springframework.jdbc.core.BeanPropertyRowMapper; import org.springframework.jdbc.core.JdbcTemplate; import java.util.List; /** * @Description 账户的持久层实现类 * @Author TT Hun * @Data 2019/10/8 21:05 */ public class AccountDaoImpl implements IAccountDao { private JdbcTemplate jdbcTemplate; public void setJdbcTemplate(JdbcTemplate jdbcTemplate) { this.jdbcTemplate = jdbcTemplate; } public Account findAccountById(Integer accountId) { List&lt;Account&gt; accounts = jdbcTemplate.query(&quot;select * from account where id = ? &quot;,new BeanPropertyRowMapper&lt;Account&gt;(Account.class),accountId); return accounts.isEmpty()? null:accounts.get(0); } public Account findAccountByName(String accountName) { List&lt;Account&gt; accounts = jdbcTemplate.query(&quot;select * from account where name = ? &quot;,new BeanPropertyRowMapper&lt;Account&gt;(Account.class),accountName); if(accounts.isEmpty()){ return null; } if(accounts.size()&gt;1){ throw new RuntimeException(&quot;结果集不唯一&quot;); } return accounts.get(0); } public void updateAccount(Account account) { jdbcTemplate.update(&quot;update account set name=?,money = ? where id = ?&quot;,account.getName(),account.getMoney(),account.getId()); } } 通常情况下，不仅仅有一个AccountDaoImpl，还会有有一些其他的impl，比如用户的。。之类的。所以需要把重复代码抽取出来也就是让他们继承一个父类，父类中写好了重复代码即可： 也就是通过继承的方式实现重复代码的抽取，一种是注入dataSource一种是注入JdbcTemplate 除了字节写继承父类的方式解决这个问题，还可以通过Spring自带的一个类来解决，直接继承jdbcDaoSupport类即可。 也就是说根本不需要自己写这个父类，直接继承Spring自带的父类即可。 所以，现在有2种写法，一种是继承父类，一种是成员变量，区别是：继承的方法适合用xml文件配置的方式来写，而写成员变量适合用注解的方式来开发；而继承父类因为父类在jar包里面，所以不能注入。]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
        <tag>JdbcTemplate</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring第三天天_面向切片编程AOP]]></title>
    <url>%2F2019%2F10%2F04%2FSpring%E7%AC%AC%E4%B8%89%E5%A4%A9-%E9%9D%A2%E5%90%91%E5%88%87%E7%89%87%E7%BC%96%E7%A8%8BAOP%2F</url>
    <content type="text"><![CDATA[下图：开始配置serviceimpl到容器中，在配置Logger类，然后配置AOP，将logger类配置到AOP中，配置通知类型，建立通知方法和切入点关联，就是用printLog方法执行在AccountServiceimpl.saveAccount()方法前面。也就是每次执行那个saveAccount方法时候，都要通过prinLog方法，也就是使用动态代理完成 package com.itheima.service; /** * @Description 账户的业务层接口 * @Author TT Hun * @Data 2019/10/4 17:31 */ public interface IAccountService { /** * 模拟保存账户 */ void saveAccount(); /** * 模拟更新账户 */ void updateAccount(int i); /** * 模拟删除账户 * @return */ int deleteAccount(); } package com.itheima.service.impl; import com.itheima.service.IAccountService; /** * @Description TODO * @Author TT Hun * @Data 2019/10/4 17:35 */ public class accountServiceImpl implements IAccountService { public void saveAccount() { System.out.println(&quot;执行了保存&quot; ); } public void updateAccount(int i) { System.out.println(&quot;执行了更新&quot; + i ); } public int deleteAccount() { System.out.println(&quot;执行了删除&quot;); return 0; } } package com.itheima.utils; /** * @Description 用于记录日志的工具类，提供了公共的代码 * @Author TT Hun * @Data 2019/10/4 17:37 */ public class Logger { /** * 用于打印日志，计划让其在切入点方法之前执行（切入点方法就是我们的业务层昂法） */ public void printLog(){ System.out.println(&quot;Logger类中的Printlog方法开始记录日志了&quot;); } } bean.xml文件配置： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd&quot;&gt; &lt;!-- 配置Springioc把service对象配置进来--&gt; &lt;bean id=&quot;accountService&quot; class=&quot;com.itheima.service.impl.accountServiceImpl&quot;&gt;&lt;/bean&gt; &lt;!-- spring中基于xml的AOP配置步骤--&gt; &lt;!-- 1、把通知的bean也交给Spring来管理--&gt; &lt;!-- 2、使用aopconfig标签表明开始AOP的配置 3、使用aop:aspect的标签表明配置切面 id属性：是给切面提供一个唯一标识 ref属性：指定通知类bean的id 4、在aop:aspect标签的内部使用对应的标签来配置通知的类型 我们现在示例是让printlog方法在切入点方法之前执行，所以是前置通知 使用aop:before标签表示配置前置通知 method属性用于指定logger类中哪个方法是前置通知 pointcut属性：用于指定切入点表达式，该表达式的含义指的是对业务层中那些方法进行增强 切入点表达式的写法： 关键字：execution(表达式) 表达式写法： 访问修饰符 返回值 包名.包名.包名......类名.方法名(参数列表) 标准表达式写法： public void com.itheima.service.impl.AccountServiceImpl.saveAccount() 返回值可以使用通配符表示任意返回值 全通配切入点表达式写法、返回值是*然后所有的也是*： * *..*.*(..) 包名可以使用..表示当前包及其子包。表示所有包下面只要有这个方法的 * *..AccountServiceImpl.saveAccount() 类名和方法名都可以使用*来实现通配 * *..*.*() 参数列表，可以直接写数据类型： 基本类型直接写名称 int 引用类型写包名.类名的方式 java.lang.String 可以使用通配符表示任意类型，但是必须有参数 * 实际开发中切入点表达式额通常写法： 切到业务层实现类下的所有方法 * com.itheima.service.impl.*.*(..) --&gt; &lt;!-- 配置logger类--&gt; &lt;bean id=&quot;logger&quot; class=&quot;com.itheima.utils.Logger&quot;&gt;&lt;/bean&gt; &lt;!-- 配置Aop--&gt; &lt;aop:config&gt; &lt;!-- 配置切面--&gt; &lt;aop:aspect id=&quot;logAdvice&quot; ref=&quot;logger&quot;&gt; &lt;!-- 配置通知的类型且建立通知方法和切入点方法的关联--&gt; &lt;aop:before method=&quot;printLog&quot; pointcut=&quot;execution(* com.itheima.service.impl.*.*(..))&quot;&gt;&lt;/aop:before&gt; &lt;/aop:aspect&gt; &lt;/aop:config&gt; &lt;/beans&gt; 下面是前置通知、后置通知、异常通知、最终通知在AOP里面的配置 环绕通知的配置： 在xml文件里面吧前置、后置、异常、最终都注释掉。然后在Logger类中通过代码控制，实现动态代理，解决前置，后置，异常，最终的问题。也就是除了通过配置xml文件中配置实现这些，也可以靠自己编码实现 1、实现bean.xml文件只保留环绕的切面配置： 2、在Logger里面aroundPringLog()方法写前置后置等。 package com.itheima.utils; import org.aspectj.lang.ProceedingJoinPoint; /** * @Description 用于记录日志的工具类，提供了公共的代码 * @Author TT Hun * @Data 2019/10/4 17:37 */ public class Logger { /** * 后置通知 */ public void afterReturningPrintLog() { System.out.println(&quot;后置通知Logger类中的afterReturningPrintLog方法开始记录日志了&quot;); } /** * 前置通知 */ public void beforePrintLog() { System.out.println(&quot;前置通知Logger类中的Printlog方法开始记录日志了&quot;); } /** * 异常通知 */ public void afterThrowingPrintLog() { System.out.println(&quot;异常通知Logger类中的afterThrowingPrintLog方法开始记录日志了&quot;); } /** * 最终通知 */ public void afterPrintLog() { System.out.println(&quot;最终通知Logger类中的afterPrintLog方法开始记录日志了&quot;); } /** * 环绕通知 * 问题：当我们配置了环绕通知之后切入点方法没有执行，而通知方法执行了。 * 分析：通过对比动态代理中的环绕通知代码，发现动态代理中的环绕通知有明确的切入点方法调用，而我们的代码中没有。 * 解决： * Spring框架为我们提供了一个接口：ProceedingJoinPoint。该接口有一个方法叫proceed(),此方法就相当于明确调用切入点方法。 * 该接口可以作为环绕通知的方法参数，在程序执行时候，spring框架会为我们提供该接口的实现类供我们使用。 * * Spring中的环绕通知： * 他是spring框架为我们提供的一种方式：可以在代码中手动控制增强方法何时执行的方式。 * * * 也就是除了配置在bean.xml文件中，实现动态代理，解决前置，后置，异常，最终的问题 * 也可以通过代码控制的方式。 */ public Object aroundPringLog(ProceedingJoinPoint pjp){ Object rtValue = null; try { System.out.println(&quot;Logger类中的afterPrintLog方法开始记录日志了。。。前置&quot;); rtValue = pjp.getArgs();//得到方法执行所需要的参数 System.out.println(&quot;Logger类中的afterPrintLog方法开始记录日志了。。。后置&quot;); pjp.proceed();//明确调用业务层方法，也叫切入点方法 return rtValue; } catch (Throwable t) { System.out.println(&quot;Logger类中的afterPrintLog方法开始记录日志了。。。异常&quot;); throw new RuntimeException(); } finally { System.out.println(&quot;Logger类中的afterPrintLog方法开始记录日志了。。。最终&quot;); } } } Spring基于注解的AOP配置首先使用注解的AOP配置会有一个问题：就是这几个通知的顺序问题。有可能顺序错乱，这个是Spring自己的问题： 但是使用环绕通知是没有这个问题的： 所以如果使用注解AOP的话建议使用环绕模式的： 完整代码如下： package com.itheima.service.impl; import com.itheima.service.IAccountService; import org.springframework.stereotype.Service; /** * @Description TODO * @Author TT Hun * @Data 2019/10/4 17:35 */ @Service(&quot;accountService&quot;) public class accountServiceImpl implements IAccountService { public void saveAccount() { System.out.println(&quot;执行了保存&quot; ); } public void updateAccount(int i) { System.out.println(&quot;执行了更新&quot; + i ); } public int deleteAccount() { System.out.println(&quot;执行了删除&quot;); return 0; } } package com.itheima.service; /** * @Description 账户的业务层接口 * @Author TT Hun * @Data 2019/10/4 17:31 */ public interface IAccountService { /** * 模拟保存账户 */ void saveAccount(); /** * 模拟更新账户 */ void updateAccount(int i); /** * 模拟删除账户 * @return */ int deleteAccount(); } package com.itheima.utils; import org.aspectj.lang.ProceedingJoinPoint; import org.aspectj.lang.annotation.*; import org.springframework.stereotype.Component; /** * @Description 用于记录日志的工具类，提供了公共的代码 * @Author TT Hun * @Data 2019/10/4 17:37 */ @Component(&quot;Logger&quot;) @Aspect//表示当前类是一个切面类 public class Logger { @Pointcut(&quot;execution(* com.itheima.service.impl.*.*())&quot;) private void pt1(){} /** * 后置通知 */ @AfterReturning(&quot;pt1()&quot;) public void afterReturningPrintLog() { System.out.println(&quot;后置通知Logger类中的afterReturningPrintLog方法开始记录日志了&quot;); } /** * 前置通知 */ @Before(&quot;pt1()&quot;) public void beforePrintLog() { System.out.println(&quot;前置通知Logger类中的Printlog方法开始记录日志了&quot;); } /** * 异常通知 */ @AfterThrowing(&quot;pt1()&quot;) public void afterThrowingPrintLog() { System.out.println(&quot;异常通知Logger类中的afterThrowingPrintLog方法开始记录日志了&quot;); } /** * 最终通知 */ @After(&quot;pt1()&quot;) public void afterPrintLog() { System.out.println(&quot;最终通知Logger类中的afterPrintLog方法开始记录日志了&quot;); } /** * 环绕通知 * 问题：当我们配置了环绕通知之后切入点方法没有执行，而通知方法执行了。 * 分析：通过对比动态代理中的环绕通知代码，发现动态代理中的环绕通知有明确的切入点方法调用，而我们的代码中没有。 * 解决： * Spring框架为我们提供了一个接口：ProceedingJoinPoint。该接口有一个方法叫proceed(),此方法就相当于明确调用切入点方法。 * 该接口可以作为环绕通知的方法参数，在程序执行时候，spring框架会为我们提供该接口的实现类供我们使用。 * * Spring中的环绕通知： * 他是spring框架为我们提供的一种方式：可以在代码中手动控制增强方法何时执行的方式。 * * * 也就是除了配置在bean.xml文件中，实现动态代理，解决前置，后置，异常，最终的问题 * 也可以通过代码控制的方式。 */ // @Around(&quot;pt1()&quot;) public Object aroundPringLog(ProceedingJoinPoint pjp){ Object rtValue = null; try { System.out.println(&quot;Logger类中的afterPrintLog方法开始记录日志了。。。前置&quot;); rtValue = pjp.getArgs();//得到方法执行所需要的参数 System.out.println(&quot;Logger类中的afterPrintLog方法开始记录日志了。。。后置&quot;); pjp.proceed();//明确调用业务层方法，也叫切入点方法 return rtValue; } catch (Throwable t) { System.out.println(&quot;Logger类中的afterPrintLog方法开始记录日志了。。。异常&quot;); throw new RuntimeException(); } finally { System.out.println(&quot;Logger类中的afterPrintLog方法开始记录日志了。。。最终&quot;); } } } &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;!-- 配置spring创建容器时要扫描的包--&gt; &lt;context:component-scan base-package=&quot;com.itheima&quot;&gt;&lt;/context:component-scan&gt; &lt;!-- 配置spring开启注解AOP的支持 --&gt; &lt;aop:aspectj-autoproxy&gt;&lt;/aop:aspectj-autoproxy&gt; &lt;/beans&gt; package com.itheima; import com.itheima.service.IAccountService; import javafx.application.Application; import org.springframework.context.ApplicationContext; import org.springframework.context.support.ClassPathXmlApplicationContext; /** * @Description 测试AOP的配置 * @Author TT Hun * @Data 2019/10/4 18:58 */ public class test { public static void main(String[] args) { // 获取容器 ApplicationContext ac = new ClassPathXmlApplicationContext(&quot;bean.xml&quot;); // 2、获取对象 IAccountService as = (IAccountService)ac.getBean(&quot;accountService&quot;); // 3、执行方法 as.saveAccount(); } } 总结内容：1、从银行转行入手由于加了事务，导致非常麻烦，重复代码非常多，首先： 第一步是将重复的代码，事务的解决抽取出来， 然后：是将代码通过动态代理加入到源代码中，动态代理的好处是可以不改变源码的基础上，对已有的代码进行增强。 然后讲解了两种动态代理方式：一种基于子类的动态代理，一种基于接口的动态代理。 再后，通过Springaop实现动态代理。这里面有一些术语需要消化。]]></content>
      <categories>
        <category>SpringAOP</category>
      </categories>
      <tags>
        <tag>SpringAOP</tag>
        <tag>面向切片编程</tag>
        <tag>Spring第三天</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring第三天_银行转账案例]]></title>
    <url>%2F2019%2F10%2F02%2FSpring%E7%AC%AC%E4%B8%89%E5%A4%A9-%E9%93%B6%E8%A1%8C%E8%BD%AC%E8%B4%A6%E6%A1%88%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[数据库配置问题：在bean.xml文件里面写配置的时候，中间要加分号图1是正确的图2是错误的 package com.itheima.dao; import com.itheima.domain.Account; import java.util.List; /** * 账户的持久层接口 */ public interface IAccountDao { /** * 查询所有 * @return */ List&lt;Account&gt; findAllAccount(); /** * 查询一个 * @return */ Account findAccountById(Integer accountId); /** * 保存 * @param account */ void saveAccount(Account account); /** * 更新 * @param account */ void updateAccount(Account account); /** * 删除 * @param acccountId */ void deleteAccount(Integer acccountId); /** * 根据名称查账户 * @param accountName 用户名称 * @return 如果有唯一结果就返回，如果没有结果就返回Null,结果集超过一个返回异常 */ Account findAccountByName(String accountName); } package com.itheima.dao.impl; import com.itheima.dao.IAccountDao; import com.itheima.domain.Account; import org.apache.commons.dbutils.QueryRunner; import org.apache.commons.dbutils.handlers.BeanHandler; import org.apache.commons.dbutils.handlers.BeanListHandler; import org.junit.runner.Runner; import java.util.List; /** * 账户的持久层实现类 */ public class AccountDaoImpl implements IAccountDao { private QueryRunner runner; public void setRunner(QueryRunner runner) { this.runner = runner; } public List&lt;Account&gt; findAllAccount() { try { return runner.query(&quot;select * from account&quot;, new BeanListHandler&lt;Account&gt;(Account.class)); } catch (Exception e) { throw new RuntimeException(e); } } public Account findAccountById(Integer accountId) { try { return runner.query(&quot;select * from account where id = ? &quot;, new BeanHandler&lt;Account&gt;(Account.class), accountId); } catch (Exception e) { throw new RuntimeException(e); } } public void saveAccount(Account account) { try { runner.update(&quot;insert into account(name,money)values(?,?)&quot;, account.getName(), account.getMoney()); } catch (Exception e) { throw new RuntimeException(e); } } public void updateAccount(Account account) { try { runner.update(&quot;update account set name=?,money=? where id=?&quot;, account.getName(), account.getMoney(), account.getId()); } catch (Exception e) { throw new RuntimeException(e); } } public void deleteAccount(Integer accountId) { try { runner.update(&quot;delete from account where id=?&quot;, accountId); } catch (Exception e) { throw new RuntimeException(e); } } public Account findAccountByName(String accountName) { try { List&lt;Account&gt; accounts = runner.query(&quot;select * from account where name= ? &quot;, new BeanListHandler&lt;Account&gt;(Account.class), accountName); if(accounts == null || accounts.size()==0){ return null; } if(accounts.size()&gt;1){ throw new RuntimeException(&quot;结果不一致&quot;); } return accounts.get(0); } catch (Exception e) { throw new RuntimeException(e); } } } package com.itheima.service; import com.itheima.domain.Account; import java.util.List; /** * 账户的业务层接口 */ public interface IAccountService { /** * 查询所有 * @return */ List&lt;Account&gt; findAllAccount(); /** * 查询一个 * @return */ Account findAccountById(Integer accountId); /** * 保存 * @param account */ void saveAccount(Account account); /** * 更新 * @param account */ void updateAccount(Account account); /** * 删除 * @param acccountId */ void deleteAccount(Integer acccountId); /** * * 转账方法 * @param sourceName 转出账户名称 * @param targetName 转入账户名称 * @param money 转账金额 */ void transfer(String sourceName,String targetName,Float money); } package com.itheima.service.impl; import com.itheima.dao.IAccountDao; import com.itheima.domain.Account; import com.itheima.service.IAccountService; import java.util.List; /** * 账户的业务层实现类 */ public class AccountServiceImpl implements IAccountService{ private IAccountDao accountDao; public void setAccountDao(IAccountDao accountDao) { this.accountDao = accountDao; } public List&lt;Account&gt; findAllAccount() { return accountDao.findAllAccount(); } public Account findAccountById(Integer accountId) { return accountDao.findAccountById(accountId); } public void saveAccount(Account account) { accountDao.saveAccount(account); } public void updateAccount(Account account) { accountDao.updateAccount(account); } public void deleteAccount(Integer acccountId) { accountDao.deleteAccount(acccountId); } public void transfer(String sourceName, String targetName, Float money) { // 1、根据名称查询转出账户 Account source = accountDao.findAccountByName(sourceName); // 2、根据名称查询转入账户 Account target = accountDao.findAccountByName(targetName); // 3、转出账户减钱 source.setMoney(source.getMoney()- money); // 4、转入账户加钱 target.setMoney(target.getMoney()+money); // 5、更新转出账户 accountDao.updateAccount(source); // 6、更新转入账户 accountDao.updateAccount(target); } } bean.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;!-- 配置Service --&gt; &lt;bean id=&quot;accountService&quot; class=&quot;com.itheima.service.impl.AccountServiceImpl&quot;&gt; &lt;!-- 注入dao --&gt; &lt;property name=&quot;accountDao&quot; ref=&quot;accountDao&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!--配置Dao对象--&gt; &lt;bean id=&quot;accountDao&quot; class=&quot;com.itheima.dao.impl.AccountDaoImpl&quot;&gt; &lt;!-- 注入QueryRunner --&gt; &lt;property name=&quot;runner&quot; ref=&quot;runner&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!--配置QueryRunner--&gt; &lt;bean id=&quot;runner&quot; class=&quot;org.apache.commons.dbutils.QueryRunner&quot; scope=&quot;prototype&quot;&gt; &lt;!--注入数据源--&gt; &lt;constructor-arg name=&quot;ds&quot; ref=&quot;dataSource&quot;&gt;&lt;/constructor-arg&gt; &lt;/bean&gt; &lt;!-- 配置数据源 --&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;com.mchange.v2.c3p0.ComboPooledDataSource&quot;&gt; &lt;!--连接数据库的必备信息--&gt; &lt;property name=&quot;driverClass&quot; value=&quot;com.mysql.cj.jdbc.Driver&quot;&gt;&lt;/property&gt; &lt;property name=&quot;jdbcUrl&quot; value=&quot;jdbc:mysql://localhost:3306/eesy?characterEncoding=utf8;useSSL=false;serverTimezone=UTCrewriteBatchedStatements=true&quot;&gt;&lt;/property&gt; &lt;property name=&quot;user&quot; value=&quot;root&quot;&gt;&lt;/property&gt; &lt;property name=&quot;password&quot; value=&quot;root&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;/beans&gt; package com.itheima.test; import com.itheima.domain.Account; import com.itheima.service.IAccountService; import org.junit.Test; import org.junit.runner.RunWith; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.test.context.ContextConfiguration; import org.springframework.test.context.junit4.SpringJUnit4ClassRunner; import java.util.List; /** * 使用Junit单元测试：测试我们的配置 */ @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations = &quot;classpath:bean.xml&quot;) public class AccountServiceTest { @Autowired private IAccountService as; @Test public void testTransfer(){ as.transfer(&quot;aaa&quot;,&quot;bbb&quot;,100f); } } 但是如果说在serviceimpl里面加入了一个报错的语句，会导致钱转出了， 但是没有转入如下图所示： 怎么解决这个事务的问题？ 基本上就是每次都是创建一个新的连接，每次都没有任何关系，所以我们要让他们都在一个连接之内，要发生就都发生，要不发生就都不发生。需要对代码进行一定调整。 事务控制应该都在业务层 改造代码：修改Service层的代码和bean.xml的注入文件： package com.itheima.service.impl; import com.itheima.dao.IAccountDao; import com.itheima.domain.Account; import com.itheima.service.IAccountService; import com.itheima.utils.TransactionManager; import java.util.List; /** * 账户的业务层实现类 */ public class AccountServiceImpl implements IAccountService { private IAccountDao accountDao; private TransactionManager txManger; public void setTxManger(TransactionManager txManger) { this.txManger = txManger; } public void setAccountDao(IAccountDao accountDao) { this.accountDao = accountDao; } public List&lt;Account&gt; findAllAccount() { try { // 1、开启事务， txManger.beginTransaction(); // 2、执行操作 List&lt;Account&gt; accounts = accountDao.findAllAccount(); // 3、提交事务 txManger.commit(); // 4、返回结果 return accounts; } catch (Exception e) { // 5、回滚操作 txManger.rollback(); } finally { // 6、释放资源 txManger.release(); } return null; } public Account findAccountById(Integer accountId) { try { // 1、开启事务， txManger.beginTransaction(); // 2、执行操作 Account account = accountDao.findAccountById(accountId); // 3、提交事务 txManger.commit(); // 4、返回结果 return account; } catch (Exception e) { // 5、回滚操作 txManger.rollback(); throw new RuntimeException(); } finally { // 6、释放资源 txManger.release(); } } public void saveAccount(Account account) { try { // 1、开启事务， txManger.beginTransaction(); // 2、执行操作 accountDao.saveAccount(account); // 3、提交事务 txManger.commit(); // 4、返回结果 } catch (Exception e) { // 5、回滚操作 txManger.rollback(); } finally { // 6、释放资源 txManger.release(); } } public void updateAccount(Account account) { try { // 1、开启事务， txManger.beginTransaction(); // 2、执行操作 accountDao.updateAccount(account); // 3、提交事务 txManger.commit(); // 4、返回结果 } catch (Exception e) { // 5、回滚操作 txManger.rollback(); } finally { // 6、释放资源 txManger.release(); } } public void deleteAccount(Integer acccountId) { try { // 1、开启事务， txManger.beginTransaction(); // 2、执行操作 accountDao.deleteAccount(acccountId); // 3、提交事务 txManger.commit(); // 4、返回结果 } catch (Exception e) { // 5、回滚操作 txManger.rollback(); } finally { // 6、释放资源 txManger.release(); } } public void transfer(String sourceName, String targetName, Float money) { try { // 1、开启事务， txManger.beginTransaction(); // 2、执行操作 // b 1、根据名称查询转出账户 Account source = accountDao.findAccountByName(sourceName); // 2.1、根据名称查询转入账户 Account target = accountDao.findAccountByName(targetName); // 2.2、转出账户减钱 source.setMoney(source.getMoney()- money); // 2.3、转入账户加钱 target.setMoney(target.getMoney()+money); // 2.4、更新转出账户 accountDao.updateAccount(source); int i = 1/0; // 2.5、更新转入账户 accountDao.updateAccount(target); // 3、提交事务 txManger.commit(); // 4、返回结果 } catch (Exception e) { // 5、回滚操作 txManger.rollback(); } finally { // 6、释放资源 txManger.release(); } } } bean.xml文件： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;!-- 配置Service --&gt; &lt;bean id=&quot;accountService&quot; class=&quot;com.itheima.service.impl.AccountServiceImpl&quot;&gt; &lt;!-- 注入dao --&gt; &lt;property name=&quot;accountDao&quot; ref=&quot;accountDao&quot;&gt;&lt;/property&gt; &lt;!--注入事务管理器--&gt; &lt;property name=&quot;txManger&quot; ref=&quot;txManager&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!--配置Dao对象--&gt; &lt;bean id=&quot;accountDao&quot; class=&quot;com.itheima.dao.impl.AccountDaoImpl&quot;&gt; &lt;!-- 注入QueryRunner --&gt; &lt;property name=&quot;runner&quot; ref=&quot;runner&quot;&gt;&lt;/property&gt; &lt;!-- 注入ConnectionUtils--&gt; &lt;property name=&quot;connectionUtils&quot; ref =&quot;connectionUtils&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!--配置QueryRunner--&gt; &lt;bean id=&quot;runner&quot; class=&quot;org.apache.commons.dbutils.QueryRunner&quot; scope=&quot;prototype&quot;&gt; &lt;/bean&gt; &lt;!-- 配置数据源 --&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;com.mchange.v2.c3p0.ComboPooledDataSource&quot;&gt; &lt;!--连接数据库的必备信息--&gt; &lt;property name=&quot;driverClass&quot; value=&quot;com.mysql.jdbc.Driver&quot;&gt;&lt;/property&gt; &lt;property name=&quot;jdbcUrl&quot; value=&quot;jdbc:mysql://localhost:3306/eesy?localhost:3306/eesy?characterEncoding=utf-8&amp;amp;serverTimezone=UTC&amp;amp;useSSL=false&quot;&gt;&lt;/property&gt; &lt;property name=&quot;user&quot; value=&quot;root&quot;&gt;&lt;/property&gt; &lt;property name=&quot;password&quot; value=&quot;root&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 配置Connection的工具类 ConnectionUtils--&gt; &lt;bean id =&quot;connectionUtils&quot; class =&quot;com.itheima.utils.ConnectionUtils&quot;&gt; &lt;!-- 注入数据源--&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 配置事务管理器--&gt; &lt;bean id=&quot;txManager&quot; class=&quot;com.itheima.utils.TransactionManager&quot;&gt; &lt;!-- 注入ConnectionUtils--&gt; &lt;property name=&quot;connectionUtils&quot; ref=&quot;connectionUtils&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;/beans&gt; 添加Connectionutils类： package com.itheima.utils; import javax.sql.DataSource; import java.sql.Connection; /** * @Description 连接的工具类，用于从数据源中获取一个连接，并且实现和线程的绑定 * @Author TT Hun * @Data 2019/10/3 11:22 */ public class ConnectionUtils { private ThreadLocal&lt;Connection&gt; tl = new ThreadLocal&lt;Connection&gt;(); private DataSource dataSource; public void setDataSource(DataSource dataSource) { this.dataSource = dataSource; } /** * 获取当前线程上的链接 * * @return */ public Connection getThreadConnection() { // 1、先从ThreadLocal上获取 Connection conn = tl.get(); // 2、判断当前线程上是否有链接 try { if (conn == null) { // 3、从数据源中获取一个连接，并且存入ThreadLocalz中 conn = dataSource.getConnection(); tl.set(conn); } // 4、返回当前线程上的连接 return conn; } catch (Exception e) { throw new RuntimeException(&quot;报错了&quot;); } } /** * 链接和线程解绑 */ public void removeConnection(){ tl.remove(); } } 添加TransactionManager类： package com.itheima.utils; /** * @Description 和事务管理相关的工具类，包含了开启事务，提交事务，回滚事务和释放连接的方法 * @Author TT Hun * @Data 2019/10/3 11:35 */ public class TransactionManager { private ConnectionUtils connectionUtils; public void setConnectionUtils(ConnectionUtils connectionUtils) { this.connectionUtils = connectionUtils; } /** * 开始事务 */ public void beginTransaction() { try { connectionUtils.getThreadConnection().setAutoCommit(false); } catch (Exception e) { e.printStackTrace(); } } /** * 提交事务 */ public void commit() { try { connectionUtils.getThreadConnection().commit(); } catch (Exception e) { e.printStackTrace(); } } /** * 回滚事务 */ public void rollback() { try { connectionUtils.getThreadConnection().rollback(); } catch (Exception e) { e.printStackTrace(); } } /** * 释放连接事务 */ public void release() { try { // 使用服务器也有一个线程池，当tomcat启动时候，会初始化一大堆线程放到一个容器中 // 每次访问都是线程池中拿出来一个线程给我们使用，调用close方法并不是真正的关闭而是将线程还给线程池 connectionUtils.getThreadConnection().close(); // 所以想要连接和线程解绑，通过这个方法。 connectionUtils.removeConnection(); } catch (Exception e) { e.printStackTrace(); } } } 再使用test类测试： package com.itheima.test; import com.itheima.domain.Account; import com.itheima.service.IAccountService; import org.junit.Test; import org.junit.runner.RunWith; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.test.context.ContextConfiguration; import org.springframework.test.context.junit4.SpringJUnit4ClassRunner; import java.util.List; /** * 使用Junit单元测试：测试我们的配置 */ @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations = &quot;classpath:bean.xml&quot;) public class AccountServiceTest { @Autowired private IAccountService as; @Test public void testTransfer(){ as.transfer(&quot;aaa&quot;,&quot;bbb&quot;,100f); } } 基于接口的动态代理回顾/** * @Description TODO * @Author TT Hun * @Data 2019/10/3 17:51 */ public interface IProducer { public void saleProduct(float money) ; public void afterService(float money); } /** * @Description TODO * @Author TT Hun * @Data 2019/10/3 17:48 */ public class Producer implements IProducer{ /** * 销售 * * @param money */ public void saleProduct(float money) { System.out.println(&quot;销售产品，并且拿到钱&quot; + money); } /** * 售后 * * @param money */ public void afterService(float money) { System.out.println(&quot;提供售后服务并拿到钱&quot; + money); } } import java.lang.reflect.InvocationHandler; import java.lang.reflect.Method; import java.lang.reflect.Proxy; import java.util.Properties; /** * @Description 模拟一个消费者 * @Author TT Hun * @Data 2019/10/3 17:52 */ public class Client { public static void main(String[] args) { final Producer producer = new Producer(); /** * 动态代理： * 特点：字节码随用随创建，随用随加载 * 作用：不修改源码的基础上对方法进行增强 * 分类：基于接口的动态代理 * 基于子类的动态代理 * * 基于接口的动态代理：涉及的类：Proxy * 提供者：官方 * * 如何创建爱代理对象：使用Porxy类中的newProxyInstance方法 * 创建代理对象的要求：被代理类最少实现一个接口，如果没有则不能使用 * * newProxyInstance方法的参数; * Classloader：用于加载代理对象字节码的。写的是被代理对象使用相同的类加载器。代理谁就写谁的classloader固定写法 * Class[]：是字节码数组，让代理对象和被代理对象有相同的方法。固定写法 * InvocationHandler：用于提供增强的代码。他是让我们写如何代理。我们一般都是写一个该接口的实现类，通常都是匿名内 * 部类，但是不必须的。此接口的实现类都是谁用谁写， * */ IProducer proxyProducer = (IProducer) Proxy.newProxyInstance(producer.getClass().getClassLoader(), producer.getClass().getInterfaces(), new InvocationHandler() { /** * 作用：执行被代理对象的任何接口方法都会经过该方法，该方法就会有拦截的功能 * 方法参数的含义 * @param proxy ：代理对象的引用 * @param method： 表示当前执行的方法 * @param args ：当前执行方法所需要的参数 * @return 和被代理对象有相同的返回值 * @throws Throwable * 内部类访问外部对象时候要求外部对象是final的 */ public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { // 提供增强的代码 Object returnValue = null; // 1、获取方法执行的参数 Float money = (Float)args[0]; // 2、判断当前方法是不是销售方法 if(&quot;saleProduct&quot;.equals(method.getName())){ returnValue = method.invoke(producer,money*0.8f); } return returnValue; } }); proxyProducer.saleProduct(10000f); } } 上述有一个问题：如果类不实现任何接口的时候是不能使用的 基于子类的动态代理要求有第三方jar包的支持 cglib包 package com.itheima.cglib; /** * @Description TODO * @Author TT Hun * @Data 2019/10/3 17:48 */ public class Producer { /** * 销售 * * @param money */ public void saleProduct(float money) { System.out.println(&quot;销售产品，并且拿到钱&quot; + money); } /** * 售后 * * @param money */ public void afterService(float money) { System.out.println(&quot;提供售后服务并拿到钱&quot; + money); } } package com.itheima.cglib; import net.sf.cglib.proxy.Enhancer; import net.sf.cglib.proxy.MethodInterceptor; import net.sf.cglib.proxy.MethodProxy; import java.lang.reflect.InvocationHandler; import java.lang.reflect.Method; import java.lang.reflect.Proxy; /** * @Description 模拟一个消费者 * @Author TT Hun * @Data 2019/10/3 17:52 */ public class Client { public static void main(String[] args) { final Producer producer = new Producer(); /** * 动态代理： * 特点：字节码随用随创建，随用随加载 * 作用：不修改源码的基础上对方法进行增强 * 分类：基于接口的动态代理 * 基于子类的动态代理 * * 基于子类的动态代理：涉及的类：Enhancer * 提供者：第三方cglib库 * * 如何创建爱代理对象：使用Enhancer类中的create方法 * 创建代理对象的要求：被代理不能是最终类 * * create方法的参数; * class：字节码 * 用于指定被代理对象的字节码。 * callback:用于提供增强的代码 * 他是让我们写如何代理，我们一般都是写改接口的子接口实现类：MethodInterceptor * */ Producer cglibProducer = (Producer) Enhancer.create(producer.getClass(), new MethodInterceptor() { // 执行被代理对象任何方法都会经过该方法 /** * * @param o * @param method * @param args * 以上三个参数和基于接口的对象代理Invoke的代理参数是一样的 * @param methodProxy：当前执行方法的代理对象 * @return * @throws Throwable */ public Object intercept(Object o, Method method, Object[] args, MethodProxy methodProxy) throws Throwable { // 提供增强的代码 Object returnValue = null; // 1、获取方法执行的参数 Float money = (Float) args[0]; // 2、判断当前方法是不是销售方法 if (&quot;saleProduct&quot;.equals(method.getName())) { returnValue = method.invoke(producer, money * 0.8f); } return returnValue; } }); cglibProducer.saleProduct(12000f); } } 动态代理的作用： 1、连接池那个地方就是close不能关闭连接，只能将线程放到线程池，可以通过动态代理解决对connection的close()方法进行增强。把它家回到池里面去。 2、解决全栈中文乱码的问题，对其中的3个方法进行增强。 下面一段代码是通过动态代理但是不通过AOP解决事务的问题、解决重复代码，解决了方法之间的依赖，但是导致配置文件bean.xml里面的配置比较繁琐。以下是标准的使用动态代理控制事务解决银行问题的代码，是标准代码，但是我自己由于数据库问题没有运行成功： package com.itheima.dao.impl; import com.itheima.dao.IAccountDao; import com.itheima.domain.Account; import com.itheima.utils.ConnectionUtils; import org.apache.commons.dbutils.QueryRunner; import org.apache.commons.dbutils.handlers.BeanHandler; import org.apache.commons.dbutils.handlers.BeanListHandler; import java.util.List; /** * 账户的持久层实现类 */ public class AccountDaoImpl implements IAccountDao { private QueryRunner runner; private ConnectionUtils connectionUtils; public void setRunner(QueryRunner runner) { this.runner = runner; } public void setConnectionUtils(ConnectionUtils connectionUtils) { this.connectionUtils = connectionUtils; } public List&lt;Account&gt; findAllAccount() { try{ return runner.query(connectionUtils.getThreadConnection(),&quot;select * from account&quot;,new BeanListHandler&lt;Account&gt;(Account.class)); }catch (Exception e) { throw new RuntimeException(e); } } public Account findAccountById(Integer accountId) { try{ return runner.query(connectionUtils.getThreadConnection(),&quot;select * from account where id = ? &quot;,new BeanHandler&lt;Account&gt;(Account.class),accountId); }catch (Exception e) { throw new RuntimeException(e); } } public void saveAccount(Account account) { try{ runner.update(connectionUtils.getThreadConnection(),&quot;insert into account(name,money)values(?,?)&quot;,account.getName(),account.getMoney()); }catch (Exception e) { throw new RuntimeException(e); } } public void updateAccount(Account account) { try{ runner.update(connectionUtils.getThreadConnection(),&quot;update account set name=?,money=? where id=?&quot;,account.getName(),account.getMoney(),account.getId()); }catch (Exception e) { throw new RuntimeException(e); } } public void deleteAccount(Integer accountId) { try{ runner.update(connectionUtils.getThreadConnection(),&quot;delete from account where id=?&quot;,accountId); }catch (Exception e) { throw new RuntimeException(e); } } public Account findAccountByName(String accountName) { try{ List&lt;Account&gt; accounts = runner.query(connectionUtils.getThreadConnection(),&quot;select * from account where name = ? &quot;,new BeanListHandler&lt;Account&gt;(Account.class),accountName); if(accounts == null || accounts.size() == 0){ return null; } if(accounts.size() &gt; 1){ throw new RuntimeException(&quot;结果集不唯一，数据有问题&quot;); } return accounts.get(0); }catch (Exception e) { throw new RuntimeException(e); } } } package com.itheima.dao; import com.itheima.domain.Account; import java.util.List; /** * 账户的持久层接口 */ public interface IAccountDao { /** * 查询所有 * @return */ List&lt;Account&gt; findAllAccount(); /** * 查询一个 * @return */ Account findAccountById(Integer accountId); /** * 保存 * @param account */ void saveAccount(Account account); /** * 更新 * @param account */ void updateAccount(Account account); /** * 删除 * @param acccountId */ void deleteAccount(Integer acccountId); /** * 根据名称查询账户 * @param accountName * @return 如果有唯一的一个结果就返回，如果没有结果就返回null * 如果结果集超过一个就抛异常 */ Account findAccountByName(String accountName); } package com.itheima.domain; import java.io.Serializable; /** * 账户的实体类 */ public class Account implements Serializable { private Integer id; private String name; private Float money; public Integer getId() { return id; } public void setId(Integer id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public Float getMoney() { return money; } public void setMoney(Float money) { this.money = money; } @Override public String toString() { return &quot;Account{&quot; + &quot;id=&quot; + id + &quot;, name=&apos;&quot; + name + &apos;\&apos;&apos; + &quot;, money=&quot; + money + &apos;}&apos;; } } package com.itheima.service.impl; import com.itheima.dao.IAccountDao; import com.itheima.domain.Account; import com.itheima.service.IAccountService; import java.util.List; /** * 账户的业务层实现类 * * 事务控制应该都是在业务层 */ public class AccountServiceImpl implements IAccountService{ private IAccountDao accountDao; public void setAccountDao(IAccountDao accountDao) { this.accountDao = accountDao; } @Override public List&lt;Account&gt; findAllAccount() { return accountDao.findAllAccount(); } @Override public Account findAccountById(Integer accountId) { return accountDao.findAccountById(accountId); } @Override public void saveAccount(Account account) { accountDao.saveAccount(account); } @Override public void updateAccount(Account account) { accountDao.updateAccount(account); } @Override public void deleteAccount(Integer acccountId) { accountDao.deleteAccount(acccountId); } @Override public void transfer(String sourceName, String targetName, Float money) { System.out.println(&quot;transfer....&quot;); //2.1根据名称查询转出账户 Account source = accountDao.findAccountByName(sourceName); //2.2根据名称查询转入账户 Account target = accountDao.findAccountByName(targetName); //2.3转出账户减钱 source.setMoney(source.getMoney()-money); //2.4转入账户加钱 target.setMoney(target.getMoney()+money); //2.5更新转出账户 accountDao.updateAccount(source); // int i=1/0; //2.6更新转入账户 accountDao.updateAccount(target); } } package com.itheima.service; import com.itheima.domain.Account; import java.util.List; /** * 账户的业务层接口 */ public interface IAccountService { /** * 查询所有 * @return */ List&lt;Account&gt; findAllAccount(); /** * 查询一个 * @return */ Account findAccountById(Integer accountId); /** * 保存 * @param account */ void saveAccount(Account account); /** * 更新 * @param account */ void updateAccount(Account account); /** * 删除 * @param acccountId */ void deleteAccount(Integer acccountId); /** * 转账 * @param sourceName 转出账户名称 * @param targetName 转入账户名称 * @param money 转账金额 */ void transfer(String sourceName,String targetName,Float money); //void test();//它只是连接点，但不是切入点，因为没有被增强 } package com.itheima.utils; import javax.sql.DataSource; import java.sql.Connection; /** * 连接的工具类，它用于从数据源中获取一个连接，并且实现和线程的绑定 */ public class ConnectionUtils { private ThreadLocal&lt;Connection&gt; tl = new ThreadLocal&lt;Connection&gt;(); private DataSource dataSource; public void setDataSource(DataSource dataSource) { this.dataSource = dataSource; } /** * 获取当前线程上的连接 * @return */ public Connection getThreadConnection() { try{ //1.先从ThreadLocal上获取 Connection conn = tl.get(); //2.判断当前线程上是否有连接 if (conn == null) { //3.从数据源中获取一个连接，并且存入ThreadLocal中 conn = dataSource.getConnection(); tl.set(conn); } //4.返回当前线程上的连接 return conn; }catch (Exception e){ throw new RuntimeException(e); } } /** * 把连接和线程解绑 */ public void removeConnection(){ tl.remove(); } } package com.itheima.utils; /** * 和事务管理相关的工具类，它包含了，开启事务，提交事务，回滚事务和释放连接 */ public class TransactionManager { private ConnectionUtils connectionUtils; public void setConnectionUtils(ConnectionUtils connectionUtils) { this.connectionUtils = connectionUtils; } /** * 开启事务 */ public void beginTransaction(){ try { connectionUtils.getThreadConnection().setAutoCommit(false); }catch (Exception e){ e.printStackTrace(); } } /** * 提交事务 */ public void commit(){ try { connectionUtils.getThreadConnection().commit(); }catch (Exception e){ e.printStackTrace(); } } /** * 回滚事务 */ public void rollback(){ try { connectionUtils.getThreadConnection().rollback(); }catch (Exception e){ e.printStackTrace(); } } /** * 释放连接 */ public void release(){ try { connectionUtils.getThreadConnection().close();//还回连接池中 connectionUtils.removeConnection(); }catch (Exception e){ e.printStackTrace(); } } } package com.itheima.factory; import com.itheima.service.IAccountService; import com.itheima.utils.TransactionManager; import java.lang.reflect.InvocationHandler; import java.lang.reflect.Method; import java.lang.reflect.Proxy; /** * 用于创建Service的代理对象的工厂 */ public class BeanFactory { private IAccountService accountService; private TransactionManager txManager; public void setTxManager(TransactionManager txManager) { this.txManager = txManager; } public final void setAccountService(IAccountService accountService) { this.accountService = accountService; } /** * 获取Service代理对象 * @return */ public IAccountService getAccountService() { return (IAccountService)Proxy.newProxyInstance(accountService.getClass().getClassLoader(), accountService.getClass().getInterfaces(), new InvocationHandler() { /** * 添加事务的支持 * * @param proxy * @param method * @param args * @return * @throws Throwable */ public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { // if(&quot;test&quot;.equals(method.getName())){ // return method.invoke(accountService,args); // } Object rtValue = null; try { //1.开启事务 txManager.beginTransaction(); //2.执行操作 rtValue = method.invoke(accountService, args); //3.提交事务 txManager.commit(); //4.返回结果 return rtValue; } catch (Exception e) { //5.回滚操作 txManager.rollback(); throw new RuntimeException(e); } finally { //6.释放连接 txManager.release(); } } }); } } bean.xml文件 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;!--配置代理的service--&gt; &lt;bean id=&quot;proxyAccountService&quot; factory-bean=&quot;beanFactory&quot; factory-method=&quot;getAccountService&quot;&gt;&lt;/bean&gt; &lt;!--配置beanfactory--&gt; &lt;bean id=&quot;beanFactory&quot; class=&quot;com.itheima.factory.BeanFactory&quot;&gt; &lt;!-- 注入service --&gt; &lt;property name=&quot;accountService&quot; ref=&quot;accountService&quot;&gt;&lt;/property&gt; &lt;!-- 注入事务管理器 --&gt; &lt;property name=&quot;txManager&quot; ref=&quot;txManager&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 配置Service --&gt; &lt;bean id=&quot;accountService&quot; class=&quot;com.itheima.service.impl.AccountServiceImpl&quot;&gt; &lt;!-- 注入dao --&gt; &lt;property name=&quot;accountDao&quot; ref=&quot;accountDao&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!--配置Dao对象--&gt; &lt;bean id=&quot;accountDao&quot; class=&quot;com.itheima.dao.impl.AccountDaoImpl&quot;&gt; &lt;!-- 注入QueryRunner --&gt; &lt;property name=&quot;runner&quot; ref=&quot;runner&quot;&gt;&lt;/property&gt; &lt;!-- 注入ConnectionUtils --&gt; &lt;property name=&quot;connectionUtils&quot; ref=&quot;connectionUtils&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!--配置QueryRunner--&gt; &lt;bean id=&quot;runner&quot; class=&quot;org.apache.commons.dbutils.QueryRunner&quot; scope=&quot;prototype&quot;&gt;&lt;/bean&gt; &lt;!-- 配置数据源 --&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;com.mchange.v2.c3p0.ComboPooledDataSource&quot;&gt; &lt;!--连接数据库的必备信息--&gt; &lt;property name=&quot;driverClass&quot; value=&quot;com.mysql.jdbc.Driver&quot;&gt;&lt;/property&gt; &lt;property name=&quot;jdbcUrl&quot; value=&quot;jdbc:mysql://localhost:3306/eesy?characterEncoding=utf8&amp;amp;serverTimezone=UTC&amp;amp;useSSL=false&quot;&gt;&lt;/property&gt; &lt;property name=&quot;user&quot; value=&quot;root&quot;&gt;&lt;/property&gt; &lt;property name=&quot;password&quot; value=&quot;root&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 配置Connection的工具类 ConnectionUtils --&gt; &lt;bean id=&quot;connectionUtils&quot; class=&quot;com.itheima.utils.ConnectionUtils&quot;&gt; &lt;!-- 注入数据源--&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 配置事务管理器--&gt; &lt;bean id=&quot;txManager&quot; class=&quot;com.itheima.utils.TransactionManager&quot;&gt; &lt;!-- 注入ConnectionUtils --&gt; &lt;property name=&quot;connectionUtils&quot; ref=&quot;connectionUtils&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;/beans&gt; package com.itheima.test; import com.itheima.service.IAccountService; import org.junit.Test; import org.junit.runner.RunWith; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.beans.factory.annotation.Qualifier; import org.springframework.test.context.ContextConfiguration; import org.springframework.test.context.junit4.SpringJUnit4ClassRunner; /** * 使用Junit单元测试：测试我们的配置 */ @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations = &quot;classpath:bean.xml&quot;) public class AccountServiceTest { @Autowired @Qualifier(&quot;proxyAccountService&quot;) private IAccountService as; @Test public void testTransfer(){ as.transfer(&quot;aaa&quot;,&quot;bbb&quot;,100f); } } 总结上述的代码；utils.ConnectionUtils：获取当前线程上的连接，通过注入数据源、实例化ThreadLocal，通过ThreadLocal.get方法获取线程上的连接。utils.TransactionManager:里面注入ConnectionUtils，然后写开启事务，提交事务，回滚事务，释放连接的方法BeanFactory：注入Serviceimpl和TransactionManager。然后获取Service代理对象，通过getAccountService方法，并且在匿名内部类中添加事务。然后在bean.xml文件里面写好要添加到各个地方的注入，然后配置代理的Service对象 &lt;bean id = &quot;proxyAccountService&quot; factory-bean=&quot;beanFactory&quot; factory-method=&quot;getAccountService&quot;&gt; &lt;/bean&gt; 最后在test测试类中，添加注解@Qulifier因为在bean.xml文件中有2个都实现了IAccountService接口，所以要添加Qulifier注解]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
        <tag>银行转账案例</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring第二天03Spring新注解]]></title>
    <url>%2F2019%2F09%2F29%2FSpring%E7%AC%AC%E4%BA%8C%E5%A4%A903Spring%E6%96%B0%E6%B3%A8%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[这两个有什么区别：区别就是在XML文件里配置的东西会在Spring以Kv的形式存在 package com.itheima.dao; import com.itheima.domain.Account; import org.apache.commons.dbutils.QueryRunner; import org.apache.commons.dbutils.handlers.BeanHandler; import org.apache.commons.dbutils.handlers.BeanListHandler; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.stereotype.Repository; import java.util.List; /** * @Description 账户的持久层实现类 * @Author TT Hun * @Data 2019/9/27 15:42 */ @Repository(&quot;accountDao&quot;) public class AccountDaoImpl implements IAccountDao { @Autowired private QueryRunner runner; public void setRunner(QueryRunner runner) { this.runner = runner; } public List&lt;Account&gt; findAllAccount() { try { return runner.query(&quot;select * from account&quot;, new BeanListHandler&lt;Account&gt;(Account.class)); } catch (Exception e) { throw new RuntimeException(e); } } public Account findAccountById(Integer accountId) { try { return runner.query(&quot;select * from account where id = ?&quot;, new BeanHandler&lt;Account&gt;(Account.class),accountId); } catch (Exception e) { throw new RuntimeException(e); } } public void saveAccount(Account account) { try { runner.update(&quot;insert into account(name,money)values(?,?)&quot;, account.getName(),account.getMoney()); } catch (Exception e) { throw new RuntimeException(e); } } public void updateAccount(Account account) { try { runner.update(&quot;update account set name =?,money = ? where id =?&quot;, account.getName(),account.getMoney(),account.getID()); } catch (Exception e) { throw new RuntimeException(e); } } public void deleteAccount(Integer accountId) { try { runner.update(&quot;delete from account where id = ?&quot;, accountId); } catch (Exception e) { throw new RuntimeException(e); } } } package com.itheima.dao; import com.itheima.domain.Account; import java.util.List; /** * 账户的持久层接口 */ public interface IAccountDao { /** * 查询所有 * @return */ List&lt;Account&gt; findAllAccount(); /** * 查询一个 * @return */ Account findAccountById(Integer accountId); /** * 保存 * @param account */ void saveAccount(Account account); /** * 更新 * @param account */ void updateAccount(Account account); /** * 删除 * @param acccountId */ void deleteAccount(Integer acccountId); } package com.itheima.domain; /** * @Description TODO * @Author TT Hun * @Data 2019/9/27 15:01 */ import lombok.Data; import lombok.Getter; import lombok.Setter; import java.io.Serializable; @Setter @Getter @Data public class Account implements Serializable { private Integer ID; private String name; private Float money; } package com.itheima.service.impl; import com.itheima.dao.IAccountDao; import com.itheima.domain.Account; import com.itheima.service.IAccountService; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.stereotype.Service; import java.util.List; /** * 账户的业务层实现类 */ @Service(&quot;accountService&quot;) public class AccountServiceImpl implements IAccountService { @Autowired private IAccountDao accountDao; public List&lt;Account&gt; findAllAccount() { return accountDao.findAllAccount(); } public Account findAccountById(Integer accountId) { return accountDao.findAccountById(accountId); } public void saveAccount(Account account) { accountDao.saveAccount(account); } public void updateAccount(Account account) { accountDao.updateAccount(account); } public void deleteAccount(Integer acccountId) { accountDao.deleteAccount(acccountId); } } package com.itheima.service; import com.itheima.domain.Account; import java.util.List; /** * 账户的业务层接口 */ public interface IAccountService { /** * 查询所有 * @return */ List&lt;Account&gt; findAllAccount(); /** * 查询一个 * @return */ Account findAccountById(Integer accountId); /** * 保存 * @param account */ void saveAccount(Account account); /** * 更新 * @param account */ void updateAccount(Account account); /** * 删除 * @param acccountId */ void deleteAccount(Integer acccountId); } package config; import com.mchange.v2.c3p0.ComboPooledDataSource; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import javax.sql.DataSource; /** * @Description 和Spring连接数据库相关的配置类 * @Author TT Hun * @Data 2019/9/29 22:28 */ @Configuration public class JdbcConfig { /** * 创建数据源对象 * @return */ @Bean(&quot;dataSource&quot;) public DataSource createDataSource() { try { ComboPooledDataSource ds = new ComboPooledDataSource(); ds.setJdbcUrl(&quot;jdbc:mysql://localhost:3306/eesy?useUnicode=true&amp;characterEncoding=utf-8&amp;allowMultiQueries=true&amp;useSSL=false&amp;serverTimezone=UTC&quot;); ds.setDriverClass(&quot;com.mysql.jdbc.Driver&quot;); ds.setUser(&quot;root&quot;); ds.setPassword(&quot;root&quot;); return ds; } catch (Exception e) { throw new RuntimeException(e); } } } package config; import com.mchange.v2.c3p0.ComboPooledDataSource; import org.apache.commons.dbutils.QueryRunner; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.ComponentScan; import org.springframework.context.annotation.Configuration; import org.springframework.context.annotation.Scope; import org.springframework.stereotype.Component; import javax.sql.DataSource; /** * @Description 该类是一个配置类，他的作用和XML是一样的 * Spring中的新注解 * Configuration:指定当前类是一个配置类 * 细节：当配置类作为AnnotationConfigApplicationContext.class对象创建的参数的时候，该注解可以不写。 * 如果有2个config的java类配置。 * 1、ApplicationContext ac = new AnnotationConfigApplicationContext(SpringConfiguration.class，JdbcConfig.class) * 2、在扫描注解里面+2个路径，@ComponentScan({&quot;com.itheima&quot;,&quot;config&quot;})并且加上@Configuration。 * 但是如果是1的方式的话，2个字节码就是同等地位了，如果是SpringConfiguration作为主要的配置，JdbcConfig作为次要配置怎么办： * * 解决上面问题新注解：Import注解： * 作用：用于导入其他配置类 * * * ComponentScan：用于通过注解指定spring在创建容器时要扫描的包 * 属性：value:他和basePackages的作用是一样的都是用于指定创建容器时候要扫描的包 * 我们使用此注解就等同于在xml中配置了： * &lt;context:component-scan base-package=&quot;com.itheima&quot;&gt;&lt;/context:component-scan&gt; * * * Bean注解： * 作用：用于把当前方法的返回值作为bean对象存入springioc的容器中。 * 属性：用于指定bean的id。当不写时默认值id是当前方法的名称value就是返回值对象new QueryRunner * 也可以在注解后面括号加上（name=&quot;runner&quot;)写上k就是runner了。加上之后和xml就一样了。 * * 细节： * 当我们使用注解配置方法时候，如果方法有参数，Spring框架回去容器中查找有没有可以用的bean对象 * 查找的方式和autowired注解是一样的， 如果有同一个类型的就注入。 * * @Author TT Hun * @Data 2019/9/29 19:51 */ //@Configuration @ComponentScan({&quot;com.itheima&quot;,&quot;config&quot;}) public class SpringConfiguration { /** * 用于创建一个QueryRunner对象 * @param dataSource * @return */ @Bean(name = &quot;runner&quot;) // 这个地方的细节如果没有scope的注解就是意味着单例模式，在创建QuerryRunner语句的时候可能出问题。 @Scope(&quot;prototype&quot;) public QueryRunner createQueryRunner(DataSource dataSource) { return new QueryRunner(dataSource); } } &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;/beans&gt; Import注解package config; import com.mchange.v2.c3p0.ComboPooledDataSource; import org.apache.commons.dbutils.QueryRunner; import org.springframework.context.annotation.*; import org.springframework.stereotype.Component; import javax.sql.DataSource; /** * @Description 该类是一个配置类，他的作用和XML是一样的 * Spring中的新注解 * Configuration:指定当前类是一个配置类 * 细节：当配置类作为AnnotationConfigApplicationContext.class对象创建的参数的时候，该注解可以不写。 * 如果有2个config的java类配置。 * 1、ApplicationContext ac = new AnnotationConfigApplicationContext(SpringConfiguration.class，JdbcConfig.class) * 2、在扫描注解里面+2个路径，@ComponentScan({&quot;com.itheima&quot;,&quot;config&quot;})并且加上@Configuration。 * 但是如果是1的方式的话，2个字节码就是同等地位了，并列状态。如果是SpringConfiguration作为主要的配置，JdbcConfig作为次要配置怎么办： * * 解决上面问题新注解：Import注解： * 作用：用于导入其他配置类 * 属性：是个value，用于指定其他配置类的字节码。 * 但我们使用import注解之后，有Import注解的类就是主配置类。而导入的都是子配置类 * * ComponentScan：用于通过注解指定spring在创建容器时要扫描的包 * 属性：value:他和basePackages的作用是一样的都是用于指定创建容器时候要扫描的包 * 我们使用此注解就等同于在xml中配置了： * &lt;context:component-scan base-package=&quot;com.itheima&quot;&gt;&lt;/context:component-scan&gt; * * * 解决上面问题新注解：Import注解： * 作用：用于导入其他配置类 * Import就是导入一个主要的配置类，然后其他的副的配置类 * * * Bean注解： * 作用：用于把当前方法的返回值作为bean对象存入springioc的容器中。 * 属性：用于指定bean的id。当不写时默认值id是当前方法的名称value就是返回值对象new QueryRunner * 也可以在注解后面括号加上（name=&quot;runner&quot;)写上k就是runner了。加上之后和xml就一样了。 * * 细节： * 当我们使用注解配置方法时候，如果方法有参数，Spring框架回去容器中查找有没有可以用的bean对象 * 查找的方式和autowired注解是一样的， 如果有同一个类型的就注入。 * * @Author TT Hun * @Data 2019/9/29 19:51 */ //@Configuration @ComponentScan({&quot;com.itheima&quot;,&quot;config&quot;}) @Import(JdbcConfig.class) public class SpringConfiguration { /** * 用于创建一个QueryRunner对象 * @param dataSource * @return */ @Bean(name = &quot;runner&quot;) // 这个地方的细节如果没有scope的注解就是意味着单例模式，在创建QuerryRunner语句的时候可能出问题。 @Scope(&quot;prototype&quot;) public QueryRunner createQueryRunner(DataSource dataSource) { return new QueryRunner(dataSource); } } 但是这一行的加载配置还是要有的： Class AccountServiceTest ApplicationContext ac = new AnnotationConfigApplicationContext(SpringConfiguration.class); 但是上面写的仍然有问题，问题在于 这里面的数据库配置仍然是写死的，所以仍然需要改造。。 下面是改造办法： 改造办法就是将JdbcConfig里的配置写到JdbcConfiguration里面去，然后通过@Value注解将里面的配置加载到JdbcConfig里面去。。 也就是： jdbcConfig.properties配置文件 jdbc.driver=com.mysql.jdbc.Driver jdbc.url=jdbc:mysql://localhost:3306/eesy?characterEncoding=utf8&amp;useSSL=false&amp;serverTimezone=UTC&amp;rewriteBatchedStatements=true jdbc.username=root jdbc.password=root package config; import com.mchange.v2.c3p0.ComboPooledDataSource; import org.springframework.beans.factory.annotation.Value; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import javax.sql.DataSource; /** * @Description 和Spring连接数据库相关的配置类 * @Author TT Hun * @Data 2019/9/29 22:28 */ @Configuration public class JdbcConfig { @Value(&quot;${jdbc.driver}&quot;) private String driver; @Value(&quot;${jdbc.url}&quot;) private String url; @Value(&quot;${jdbc.username}&quot;) private String username; @Value(&quot;${jdbc.password}&quot;) private String password; /** * 创建数据源对象 * @return */ @Bean(&quot;dataSource&quot;) public DataSource createDataSource() { try { ComboPooledDataSource ds = new ComboPooledDataSource(); ds.setDriverClass(driver); ds.setJdbcUrl(url); ds.setUser(username); ds.setPassword(password); return ds; } catch (Exception e) { throw new RuntimeException(e); } } } 这块有一个问题就是在连接数据库的时候需要使用的pom依赖： &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql- connector-java 8.0.17 然后连接的时候写的配置需要特殊加上一些东西： jdbc.driver=com.mysql.jdbc.Driver jdbc.url=jdbc:mysql://localhost:3306/eesy?characterEncoding=utf8&amp;useSSL=false&amp;serverTimezone=UTC&amp;rewriteBatchedStatements=true jdbc.username=root jdbc.password=root Qualifier注解的另一种用法、 Spring整合Junit解决的问题：]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
        <tag>新注解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring第二天02基于XML和使用注解的IOC案例]]></title>
    <url>%2F2019%2F09%2F29%2FSpring%E7%AC%AC%E4%BA%8C%E5%A4%A902%E5%9F%BA%E4%BA%8EXML%E5%92%8C%E4%BD%BF%E7%94%A8%E6%B3%A8%E8%A7%A3%E7%9A%84IOC%E6%A1%88%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[基于XML的IOC的案例：package com.itheima.dao; import com.itheima.domain.Account; import org.apache.commons.dbutils.QueryRunner; import org.apache.commons.dbutils.handlers.BeanHandler; import org.apache.commons.dbutils.handlers.BeanListHandler; import java.util.List; /** * @Description 账户的持久层实现类 * @Author TT Hun * @Data 2019/9/27 15:42 */ public class AccountDaoImpl implements IAccountDao { private QueryRunner runner; public void setRunner(QueryRunner runner) { this.runner = runner; } public List&lt;Account&gt; findAllAccount() { try { return runner.query(&quot;select * from account&quot;, new BeanListHandler&lt;Account&gt;(Account.class)); } catch (Exception e) { throw new RuntimeException(e); } } public Account findAccountById(Integer accountId) { try { return runner.query(&quot;select * from account where id = ?&quot;, new BeanHandler&lt;Account&gt;(Account.class),accountId); } catch (Exception e) { throw new RuntimeException(e); } } public void saveAccount(Account account) { try { runner.update(&quot;insert into account(name,money)values(?,?)&quot;, account.getName(),account.getMoney()); } catch (Exception e) { throw new RuntimeException(e); } } public void updateAccount(Account account) { try { runner.update(&quot;update account set name =?,money = ? where id =?&quot;, account.getName(),account.getMoney(),account.getID()); } catch (Exception e) { throw new RuntimeException(e); } } public void deleteAccount(Integer accountId) { try { runner.update(&quot;delete from account where id = ?&quot;, accountId); } catch (Exception e) { throw new RuntimeException(e); } } } package com.itheima.dao; import com.itheima.domain.Account; import java.util.List; /** * 账户的持久层接口 */ public interface IAccountDao { /** * 查询所有 * @return */ List&lt;Account&gt; findAllAccount(); /** * 查询一个 * @return */ Account findAccountById(Integer accountId); /** * 保存 * @param account */ void saveAccount(Account account); /** * 更新 * @param account */ void updateAccount(Account account); /** * 删除 * @param acccountId */ void deleteAccount(Integer acccountId); } package com.itheima.service; import com.itheima.domain.Account; import java.util.List; /** * 账户的业务层接口 */ public interface IAccountService { /** * 查询所有 * @return */ List&lt;Account&gt; findAllAccount(); /** * 查询一个 * @return */ Account findAccountById(Integer accountId); /** * 保存 * @param account */ void saveAccount(Account account); /** * 更新 * @param account */ void updateAccount(Account account); /** * 删除 * @param acccountId */ void deleteAccount(Integer acccountId); } package com.itheima.service.impl; import com.itheima.dao.IAccountDao; import com.itheima.domain.Account; import com.itheima.service.IAccountService; import java.util.List; /** * 账户的业务层实现类 */ public class AccountServiceImpl implements IAccountService{ private IAccountDao accountDao; public void setAccountDao(IAccountDao accountDao) { this.accountDao = accountDao; } public List&lt;Account&gt; findAllAccount() { return accountDao.findAllAccount(); } public Account findAccountById(Integer accountId) { return accountDao.findAccountById(accountId); } public void saveAccount(Account account) { accountDao.saveAccount(account); } public void updateAccount(Account account) { accountDao.updateAccount(account); } public void deleteAccount(Integer acccountId) { accountDao.deleteAccount(acccountId); } } package com.itheima.domain; /** * @Description TODO * @Author TT Hun * @Data 2019/9/27 15:01 */ import lombok.Data; import lombok.Getter; import lombok.Setter; import java.io.Serializable; @Setter @Getter @Data public class Account implements Serializable { private Integer ID; private String name; private Float money; } &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:util=&quot;http://www.springframework.org/schema/util&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util.xsd&quot;&gt; &lt;!-- 配对业务层对象，知识点：如何创建bean对象 --&gt; &lt;bean id=&quot;accountService&quot; class=&quot;com.itheima.service.impl.AccountServiceImpl&quot;&gt; &lt;!-- 注入dao，知识点：如何注入数据、通过set方法注入数据、注入的类型：其他bean类型 这个地方由于使用了accountDao对象，所以--&gt; &lt;property name=&quot;accountDao&quot; ref=&quot;accountDao&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 里面需要注入DAO对象，配置DAO对象--&gt; &lt;bean id=&quot;accountDao&quot; class=&quot;com.itheima.dao.AccountDaoImpl&quot;&gt; &lt;!-- 注入queryrunner丨由于accountDaoimpl里面也是在使用了QueryRunner对象所以注入queryrunner对象--&gt; &lt;property name=&quot;runner&quot; ref=&quot;runner&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!--配置queryrunner对象丨这个地方的细节：runner对象是一个单例对象单例对象会导致多线程在往数据库插入 的时候会导致一个插入还没插入完就又开始了，相互干扰。所以加上一个scope属性，配置多例模式，--&gt; &lt;bean id=&quot;runner&quot; class=&quot;org.apache.commons.dbutils.QueryRunner&quot; scope=&quot;prototype&quot;&gt; &lt;!-- 注入数据源、知识点：通过构造函数注入数据--&gt; &lt;constructor-arg name=&quot;ds&quot; ref=&quot; dataSource&quot;&gt;&lt;/constructor-arg&gt; &lt;/bean&gt; &lt;!-- 配置数据源、知识点：注入的类型：基本类型的注入--&gt; &lt;bean id=&quot; dataSource&quot; class =&quot;com.mchange.v2.c3p0.ComboPooledDataSource&quot;&gt; &lt;!-- 连接数据库的必备信息--&gt; &lt;property name=&quot;driverClass&quot; value=&quot;com.mysql.jdbc.Driver&quot;&gt;&lt;/property&gt; &lt;property name=&quot;jdbcUrl&quot; value=&quot;jdbc:mysql://localhost:3306/eesy?characterEncoding=utf-8&amp;amp;serverTimezone=UTC&amp;amp;useSSL=false&quot;&gt;&lt;/property&gt; &lt;property name=&quot;user&quot; value=&quot;root&quot;&gt;&lt;/property&gt; &lt;property name=&quot;password&quot; value=&quot;root&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!--总结：这个配置文件就是配置service对象和dao对象，但是由于service对象里面使用了dao对象，所以要 把dao对象注入到serviceImpl里面，由于dao对象使用了dbutil的对象，所以还要注入这个对象和，然后在设置 dbutil反射对象和dbutil需要注入的基本类型的数据--&gt; &lt;/beans&gt; package com.itheima.test; import com.itheima.domain.Account; import com.itheima.service.IAccountService; import org.junit.Test; import org.springframework.context.ApplicationContext; import org.springframework.context.support.ClassPathXmlApplicationContext; import java.util.List; /** * @Description TODO * @Author TT Hun * @Data 2019/9/29 14:33 */ public class AccountServiceTest { @Test public void testFindAll() { // 获取容器 ApplicationContext ac = new ClassPathXmlApplicationContext(&quot;bean.xml&quot;); // 得到业务层对象 IAccountService as = ac.getBean(&quot;accountService&quot;, IAccountService.class); // 执行方法 List&lt;Account&gt; accounts = as.findAllAccount(); for (Account account : accounts) { System.out.println(account); } } @Test public void testFindOne() { // 获取容器 ApplicationContext ac = new ClassPathXmlApplicationContext(&quot;bean.xml&quot;); // 得到业务层对象 IAccountService as = ac.getBean(&quot;accountService&quot;, IAccountService.class); // 执行方法 Account account = as.findAccountById(1); System.out.println(account); } @Test public void testSave() { Account account = new Account(); account.setName(&quot;test&quot;); account.setMoney(12345f); // 获取容器 ApplicationContext ac = new ClassPathXmlApplicationContext(&quot;bean.xml&quot;); // 得到业务层对象 IAccountService as = ac.getBean(&quot;accountService&quot;, IAccountService.class); // 执行方法 as.saveAccount(account); } @Test public void testUpdate() { // 获取容器 ApplicationContext ac = new ClassPathXmlApplicationContext(&quot;bean.xml&quot;); // 得到业务层对象 IAccountService as = ac.getBean(&quot;accountService&quot;, IAccountService.class); // 执行方法 Account account = new Account(); account.setMoney(23456f); as.updateAccount(account); } @Test public void testDelete() { // 获取容器 ApplicationContext ac = new ClassPathXmlApplicationContext(&quot;bean.xml&quot;); // 得到业务层对象 IAccountService as = ac.getBean(&quot;accountService&quot;, IAccountService.class); // 执行方法 as.deleteAccount(4); } } 基于注解的IOC的案例：package com.itheima.dao; import com.itheima.domain.Account; import java.util.List; /** * 账户的持久层接口 */ public interface IAccountDao { /** * 查询所有 * @return */ List&lt;Account&gt; findAllAccount(); /** * 查询一个 * @return */ Account findAccountById(Integer accountId); /** * 保存 * @param account */ void saveAccount(Account account); /** * 更新 * @param account */ void updateAccount(Account account); /** * 删除 * @param acccountId */ void deleteAccount(Integer acccountId); } package com.itheima.dao; import com.itheima.domain.Account; import org.apache.commons.dbutils.QueryRunner; import org.apache.commons.dbutils.handlers.BeanHandler; import org.apache.commons.dbutils.handlers.BeanListHandler; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.stereotype.Repository; import java.util.List; /** * @Description 账户的持久层实现类 * @Author TT Hun * @Data 2019/9/27 15:42 */ @Repository(&quot;accountDao&quot;) public class AccountDaoImpl implements IAccountDao { @Autowired private QueryRunner runner; public void setRunner(QueryRunner runner) { this.runner = runner; } public List&lt;Account&gt; findAllAccount() { try { return runner.query(&quot;select * from account&quot;, new BeanListHandler&lt;Account&gt;(Account.class)); } catch (Exception e) { throw new RuntimeException(e); } } public Account findAccountById(Integer accountId) { try { return runner.query(&quot;select * from account where id = ?&quot;, new BeanHandler&lt;Account&gt;(Account.class),accountId); } catch (Exception e) { throw new RuntimeException(e); } } public void saveAccount(Account account) { try { runner.update(&quot;insert into account(name,money)values(?,?)&quot;, account.getName(),account.getMoney()); } catch (Exception e) { throw new RuntimeException(e); } } public void updateAccount(Account account) { try { runner.update(&quot;update account set name =?,money = ? where id =?&quot;, account.getName(),account.getMoney(),account.getID()); } catch (Exception e) { throw new RuntimeException(e); } } public void deleteAccount(Integer accountId) { try { runner.update(&quot;delete from account where id = ?&quot;, accountId); } catch (Exception e) { throw new RuntimeException(e); } } } package com.itheima.service; import com.itheima.domain.Account; import java.util.List; /** * 账户的业务层接口 */ public interface IAccountService { /** * 查询所有 * @return */ List&lt;Account&gt; findAllAccount(); /** * 查询一个 * @return */ Account findAccountById(Integer accountId); /** * 保存 * @param account */ void saveAccount(Account account); /** * 更新 * @param account */ void updateAccount(Account account); /** * 删除 * @param acccountId */ void deleteAccount(Integer acccountId); } package com.itheima.service.impl; import com.itheima.dao.IAccountDao; import com.itheima.domain.Account; import com.itheima.service.IAccountService; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.stereotype.Service; import java.util.List; /** * 账户的业务层实现类 */ @Service(&quot;accountService&quot;) public class AccountServiceImpl implements IAccountService { @Autowired private IAccountDao accountDao; public List&lt;Account&gt; findAllAccount() { return accountDao.findAllAccount(); } public Account findAccountById(Integer accountId) { return accountDao.findAccountById(accountId); } public void saveAccount(Account account) { accountDao.saveAccount(account); } public void updateAccount(Account account) { accountDao.updateAccount(account); } public void deleteAccount(Integer acccountId) { accountDao.deleteAccount(acccountId); } } &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;!-- 使用注解开发就需要告知Spring在使用注解的时候需要扫描的包--&gt; &lt;context:component-scan base-package=&quot;com.itheima&quot;&gt;&lt;/context:component-scan&gt; &lt;!--配置QueryRunner--&gt; &lt;bean id=&quot;runner&quot; class=&quot;org.apache.commons.dbutils.QueryRunner&quot; scope=&quot;prototype&quot;&gt; &lt;!--注入数据源--&gt; &lt;constructor-arg name=&quot;ds&quot; ref=&quot;dataSource&quot;&gt;&lt;/constructor-arg&gt; &lt;/bean&gt; &lt;!-- 配置数据源 --&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;com.mchange.v2.c3p0.ComboPooledDataSource&quot;&gt; &lt;!--连接数据库的必备信息--&gt; &lt;property name=&quot;driverClass&quot; value=&quot;com.mysql.jdbc.Driver&quot;&gt;&lt;/property&gt; &lt;property name=&quot;jdbcUrl&quot; value=&quot;jdbc:mysql://localhost:3306/eesy?characterEncoding=utf-8&amp;amp;serverTimezone=UTC&amp;amp;useSSL=false&quot;&gt;&lt;/property&gt; &lt;property name=&quot;user&quot; value=&quot;root&quot;&gt;&lt;/property&gt; &lt;property name=&quot;password&quot; value=&quot;root&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;/beans&gt; package com.itheima.test; import com.itheima.domain.Account; import com.itheima.service.IAccountService; import org.junit.Test; import org.springframework.context.ApplicationContext; import org.springframework.context.support.ClassPathXmlApplicationContext; import java.util.List; /** * @Description TODO * @Author TT Hun * @Data 2019/9/29 14:33 */ public class AccountServiceTest { @Test public void testFindAll() { // 获取容器 ApplicationContext ac = new ClassPathXmlApplicationContext(&quot;bean.xml&quot;); // 得到业务层对象 IAccountService as = ac.getBean(&quot;accountService&quot;, IAccountService.class); // 执行方法 List&lt;Account&gt; accounts = as.findAllAccount(); for (Account account : accounts) { System.out.println(account); } } @Test public void testFindOne() { // 获取容器 ApplicationContext ac = new ClassPathXmlApplicationContext(&quot;bean.xml&quot;); // 得到业务层对象 IAccountService as = ac.getBean(&quot;accountService&quot;, IAccountService.class); // 执行方法 Account account = as.findAccountById(1); System.out.println(account); } @Test public void testSave() { Account account = new Account(); account.setName(&quot;test&quot;); account.setMoney(12345f); // 获取容器 ApplicationContext ac = new ClassPathXmlApplicationContext(&quot;bean.xml&quot;); // 得到业务层对象 IAccountService as = ac.getBean(&quot;accountService&quot;, IAccountService.class); // 执行方法 as.saveAccount(account); } @Test public void testUpdate() { // 获取容器 ApplicationContext ac = new ClassPathXmlApplicationContext(&quot;bean.xml&quot;); // 得到业务层对象 IAccountService as = ac.getBean(&quot;accountService&quot;, IAccountService.class); // 执行方法 Account account = new Account(); account.setMoney(23456f); as.updateAccount(account); } @Test public void testDelete() { // 获取容器 ApplicationContext ac = new ClassPathXmlApplicationContext(&quot;bean.xml&quot;); // 得到业务层对象 IAccountService as = ac.getBean(&quot;accountService&quot;, IAccountService.class); // 执行方法 as.deleteAccount(4); } }]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>SpringIOC</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F09%2F25%2FSpring%E7%AC%94%E8%AE%B0%2FSpring%E4%B8%ADbean%E7%BB%86%E8%8A%82%E4%B8%89%E7%A7%8D%E5%88%9B%E5%BB%BA%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[Spring bean的三种创建细节和生命周期首先如果一个类的默认构造函数是空值构造函数，然后在写一个带参构造函数，那么这个默认构造函数就被覆盖了。如下： public class AccountServiceImpl implements IAccountService { public AccountServiceImpl(String name) { System.out.println(&quot;对象创建了&quot;); } } 这种就是被覆盖了的构造函数 第一种方式：使用默认构造函数创建。这种情况是使用自己写好的类 在spring的配置文件中使用bean标签，配以id和class属性之后，且没有其他属性和标签时。 采用的就是默认构造函数创建bean对象，此时如果类中没有默认构造函数，则对象无法创建。 &lt;bean id=&quot;accountService&quot; class=&quot;com.itheima.service.impl.AccountServiceImpl&quot;&gt;&lt;/bean&gt;&gt; 在实际开发中有可能使用别人定义好的类，那么如何调用在Jar包里面的类呢？第二种方式： 使用普通工厂中的方法创建对象（使用某个类中的方法创建对象，并存入spring容器 //bean.xml文件里面的配置 &lt;bean id=&quot;instanceFactory&quot; class=&quot;com.itheima.factory.InstanceFactory&quot;&gt;&lt;/bean&gt; &lt;bean id=&quot;accountService&quot; factory-bean=&quot;instanceFactory&quot; factory method=&quot;getAccountService&quot;&gt;&lt;/bean&gt; /** * jar包中的类，模拟一个工厂类（该类可能是存在于jar包中的，我们无法通过修改源码的方式来提供默认构造函数） */ public class InstanceFactory { public IAccountService getAccountService(){ return new AccountServiceImpl(); } } 第二种方式的要求工厂类instanceFactory，类名配好，然后下面那一行，有一个getAccountService方法和一个instanceFactory类。 &lt;!--spring对bean的管理细节 1.创建bean的三种方式 2.bean对象的作用范围 3.bean对象的生命周期 --&gt; &lt;!--创建Bean的三种方式 --&gt; &lt;!-- 第一种方式：使用默认构造函数创建。 在spring的配置文件中使用bean标签，配以id和class属性之后，且没有其他属性和标签时。 采用的就是默认构造函数创建bean对象，此时如果类中没有默认构造函数，则对象无法创建。 &lt;bean id=&quot;accountService&quot; class=&quot;com.itheima.service.impl.AccountServiceImpl&quot;&gt;&lt;/bean&gt; --&gt; &lt;!-- 第二种方式： 使用普通工厂中的方法创建对象（使用某个类中的方法创建对象，并存入spring容器） &lt;bean id=&quot;instanceFactory&quot; class=&quot;com.itheima.factory.InstanceFactory&quot;&gt;&lt;/bean&gt; &lt;bean id=&quot;accountService&quot; factory-bean=&quot;instanceFactory&quot; factory-method=&quot;getAccountService&quot;&gt;&lt;/bean&gt; --&gt; &lt;!-- 第三种方式：使用工厂中的静态方法创建对象（使用某个类中的静态方法创建对象，并存入spring容器) &lt;bean id=&quot;accountService&quot; class=&quot;com.itheima.factory.StaticFactory&quot; factory-method=&quot;getAccountService&quot;&gt;&lt;/bean&gt; --&gt; &lt;!-- bean的作用范围调整 bean标签的scope属性： 作用：用于指定bean的作用范围 取值： 常用的就是单例的和多例的 singleton：单例的（默认值） prototype：多例的 request：作用于web应用的请求范围 session：作用于web应用的会话范围 global-session：作用于集群环境的会话范围（全局会话范围），当不是集群环境时，它就是session &lt;bean id=&quot;accountService&quot; class=&quot;com.itheima.service.impl.AccountServiceImpl&quot; scope=&quot;prototype&quot;&gt;&lt;/bean&gt; --&gt; &lt;!-- bean对象的生命周期 单例对象 出生：当容器创建时对象出生 活着：只要容器还在，对象一直活着 死亡：容器销毁，对象消亡 总结：单例对象的生命周期和容器相同 多例对象 出生：当我们使用对象时spring框架为我们创建 活着：对象只要是在使用过程中就一直活着。 死亡：当对象长时间不用，且没有别的对象引用时，由Java的垃圾回收器回收 --&gt; &lt;bean id=&quot;accountService&quot; class=&quot;com.itheima.service.impl.AccountServiceImpl&quot; scope=&quot;prototype&quot; init-method=&quot;init&quot; destroy-method=&quot;destroy&quot;&gt;&lt;/bean&gt; &lt;/beans&gt; Spring的依赖注入1、使用构造函数注入&lt;!--spring中的依赖注入--&gt; &lt;!-- 依赖注入：--&gt; &lt;!-- Dependency Injection--&gt; &lt;!-- IOC的作用：降低程序之间的依赖关系，但是没有消除。--&gt; &lt;!-- 依赖关系的管理：--&gt; &lt;!-- 以后都交给了spring来维护，--&gt; &lt;!-- 在当前类中需要用到其他的对象，由spring为我们提供，我们只需要在配置文件中说明--&gt; &lt;!-- 依赖关系的维护：就称之为依赖注入，--&gt; &lt;!-- 能注入的数据有三种：--&gt; &lt;!-- 基本类型和String、其他的bean类型（在配置文件中或者注解配置过的bean)、复杂类型|集合类型--&gt; &lt;!-- 注入的方式有三种：--&gt; &lt;!-- 第一种：使用构造函数提供--&gt; &lt;!-- 第二种：使用set方法提供--&gt; &lt;!-- 第三种：使用注解提供（明天内容）--&gt; 第一种方式： package com.itheima.service.impl; import com.itheima.service.IAccountService; import java.util.Date; /** * 账户的业务层实现类 */ public class AccountServiceImpl implements IAccountService { //如果是经常变化的数据，并不适合注入的方式，这里演示的3种类型，不考虑他们代表的意义，假设他们不经常变化适合注入 private String name; private Integer age; private Date birthday; public AccountServiceImpl(String name, Integer age ,Date birthday){ this.name=name; this.age=age; this.birthday=birthday; } public void saveAccount(){ System.out.println(&quot;service中的saveAccount方法执行了。。。&quot;); } } // 但是像下面这样子注入的情况下，将test注入到上面实现类的构造函数里面，使用的是tpye方式，但是一旦构造函数中有2个以上的同类型，就难以判断是哪一个，这个是因为每个只有一种类型，所以可以解决。 &lt;bean id=&quot;accountService&quot; class=&quot;com.itheima.service.impl.AccountServiceImpl&quot;&gt; &lt;constructor-arg type=&quot;java.lang.String&quot; value=&quot;test&quot;&gt;&lt;/constructor-arg&gt; &lt;/bean&gt; &lt;!--构造函数注入： 使用的标签:constructor-arg 标签出现的位置：bean标签的内部 标签中的属性 type：用于指定要注入的数据的数据类型，该数据类型也是构造函数中某个或某些参数的类型 index：用于指定要注入的数据给构造函数中指定索引位置的参数赋值。索引的位置是从0开始 name：用于指定给构造函数中指定名称的参数赋值 常用的 =============以上三个用于指定给构造函数中哪个参数赋值=============================== value：用于提供基本类型和String类型的数据 ref：用于指定其他的bean类型数据。它指的就是在spring的Ioc核心容器中出现过的bean对象 优势： 在获取bean对象时，注入数据是必须的操作，否则对象无法创建成功。 弊端： 改变了bean对象的实例化方式，使我们在创建对象时，如果用不到这些数据，也必须提供。 --&gt; &lt;bean id=&quot;accountService&quot; class=&quot;com.itheima.service.impl.AccountServiceImpl&quot;&gt; &lt;constructor-arg name=&quot;name&quot; value=&quot;泰斯特&quot;&gt;&lt;/constructor-arg&gt; &lt;constructor-arg name=&quot;age&quot; value=&quot;18&quot;&gt;&lt;/constructor-arg&gt; &lt;constructor-arg name=&quot;birthday&quot; ref=&quot;now&quot;&gt;&lt;/constructor-arg&gt; &lt;/bean&gt; &lt;!-- 配置一个日期对象 --&gt; &lt;bean id=&quot;now&quot; class=&quot;java.util.Date&quot;&gt;&lt;/bean&gt; 2、使用set方法注入&lt;!-- set方法注入 更常用的方式 涉及的标签：property 出现的位置：bean标签的内部 标签的属性 name：用于指定注入时所调用的set方法名称 value：用于提供基本类型和String类型的数据 ref：用于指定其他的bean类型数据。它指的就是在spring的Ioc核心容器中出现过的bean对象 优势： 创建对象时没有明确的限制，可以直接使用默认构造函数 弊端： 对象是有可能set方法没有执行。（解释；有可能在调用AccountServiceImpl2这个类的时候已经用完了，所以set不了，每太明白。） --&gt; package com.itheima.service.impl; import com.itheima.service.IAccountService; import java.util.Date; /** * 账户的业务层实现类 */ public class AccountServiceImpl2 implements IAccountService { //如果是经常变化的数据，并不适合注入的方式，这里演示的3种类型，不考虑他们代表的意义，假设他们不经常变化适合注入 private String name; private Integer age; private Date birthday; public void setName(String name) { this.name = name; } public void setAge(Integer age) { this.age = age; } public void setBirthday(Date birthday) { this.birthday = birthday; } public void saveAccount() { System.out.println(&quot;service中的saveAccount方法执行了。。。&quot;); } } &lt;bean id=&quot;accountService2&quot; class=&quot;com.itheima.service.impl.AccountServiceImpl2&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;TEST&quot; &gt;&lt;/property&gt; &lt;property name=&quot;age&quot; value=&quot;21&quot;&gt;&lt;/property&gt; &lt;property name=&quot;birthday&quot; ref=&quot;now&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 配置一个日期对象 --&gt; &lt;bean id=&quot;now&quot; class=&quot;java.util.Date&quot;&gt;&lt;/bean&gt; 复杂类型的注入package com.itheima.service.impl; import com.itheima.service.IAccountService; import java.util.Arrays; import java.util.List; import java.util.Properties; import java.util.Set; import java.util.Map; /** * 账户的业务层实现类 */ public class AccountServiceImpl3 implements IAccountService { // 数组、List集合、Set集合、Map集合、 private String[] myStrs; private List&lt;String&gt; myList; private Set&lt;String&gt; mySet; private Map&lt;String,String&gt; myMap; private Properties myProps; public void setMyStrs(String[] myStrs) { this.myStrs = myStrs; } public void setMyList(List&lt;String&gt; myList) { this.myList = myList; } public void setMySet(Set&lt;String&gt; mySet) { this.mySet = mySet; } public void setMyMap(Map&lt;String, String&gt; myMap) { this.myMap = myMap; } public void setMyProps(Properties myProps) { this.myProps = myProps; } public void saveAccount(){ System.out.println(Arrays.toString(myStrs)); System.out.println(myList); System.out.println(mySet); System.out.println(myMap); System.out.println(myProps); } } &lt;!-- 复杂类型的注入/集合类型的注入 用于给List结构集合注入的标签： list array set 用于个Map结构集合注入的标签: map props 结构相同，标签可以互换 --&gt; &lt;bean id=&quot;accountService3&quot; class=&quot;com.itheima.service.impl.AccountServiceImpl3&quot;&gt; &lt;property name=&quot;myStrs&quot;&gt; &lt;set&gt; &lt;value&gt;AAA&lt;/value&gt; &lt;value&gt;BBB&lt;/value&gt; &lt;value&gt;CCC&lt;/value&gt; &lt;/set&gt; &lt;/property&gt; &lt;property name=&quot;myList&quot;&gt; &lt;array&gt; &lt;value&gt;AAA&lt;/value&gt; &lt;value&gt;BBB&lt;/value&gt; &lt;value&gt;CCC&lt;/value&gt; &lt;/array&gt; &lt;/property&gt; &lt;property name=&quot;mySet&quot;&gt; &lt;list&gt; &lt;value&gt;AAA&lt;/value&gt; &lt;value&gt;BBB&lt;/value&gt; &lt;value&gt;CCC&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;property name=&quot;myMap&quot;&gt; &lt;props&gt; &lt;prop key=&quot;testC&quot;&gt;ccc&lt;/prop&gt; &lt;prop key=&quot;testD&quot;&gt;ddd&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;property name=&quot;myProps&quot;&gt; &lt;map&gt; &lt;entry key=&quot;testA&quot; value=&quot;aaa&quot;&gt;&lt;/entry&gt; &lt;entry key=&quot;testB&quot;&gt; &lt;value&gt;BBB&lt;/value&gt; &lt;/entry&gt; &lt;/map&gt; &lt;/property&gt; &lt;/bean&gt;]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F09%2F21%2FVUE%E7%AC%AC%E4%B8%80%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[1.Vue概述2.Vue快速入门3.Vue语法：插值表达式事件绑定数据显示逻辑判断和循环输出4.Vue生命周期 8个声明周期的执行点 4个基本 4个特殊5.axios的ajax异步请求 他和jquery的ajax比较相似6.综合案例 实现用户的查询列表和更新操作]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F09%2F18%2FSpring%E7%AC%94%E8%AE%B0%2FSpring%E7%AC%AC%E4%B8%8003.Spring%E7%9A%84%20IOC%20%E5%92%8C%20DI%2F</url>
    <content type="text"><![CDATA[应用APP直接跟资源联系他们之间是必然联系，笑出不掉，很难应用独立，或者资源独立 这种通过APP联系工厂获得资源也就是所说的IOC控制反转。 在]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F09%2F17%2FSpring%E7%AC%94%E8%AE%B0%2FSpring%E7%AC%AC%E4%B8%80%E5%A4%A9%E7%A8%8B%E5%BA%8F%E9%97%B4%E8%80%A6%E5%90%88%2F</url>
    <content type="text"><![CDATA[一般不定义service层和dao层的类属性成员，因为一旦是单例对象的话就很容易每次调用的时候都修改掉他。对象创建多次，执行效率没有单例对象高。工厂模式的解耦的升级版，我的理解是，在service层和controller层每次都需要一个下一层的对象，那么这个时候怎么办呢，就需要实例化一个对象，但是由于一直实例化对象不好，所以要通过写xml文件或者properties文件来写上service层的位置和dao层的位置，每次使用的时候就读取这个文件，然后通过getclassloader的getResourceAsStream来读取里面的文件，把这个存入到InputStream里面，在通过props.load(in)来加载这个放到props里面，在映射这个对象的时候，就是在工厂类里面来做这个事情的，然后通过while循环拿到props里面的每一个Key，然后在通过getProperty方法获取到Key对应的Propertiy，这个就是类全限定名路径，然后在通过Class.forName(beanPath).newInstance()来通过反射创建一个类。下面是一个工厂类。 import java.io.InputStream; import java.util.Enumeration; import java.util.HashMap; import java.util.Map; import java.util.Properties; /** * @Description 一个创建bean对象的工厂 * Bean在英语中是：有可重用组件的意思。 * java bena不等于实体类。javabean的范围大于实体类。 * JavaBean:用Java语言编写的可重用组件 * &lt;p&gt; * 他就是创建我们的service和dao对象的 * 1、需要一个配置文件来配置我们的service和dao * 配置内容：唯一标志=全限定类名，(keyvalue) * 2、通过读取配置文件中配置的内容，反射创建对象 * &lt;p&gt; * 配置文件可以是xml也可以是Properties； * @Author TT Hun * @Data 2019/9/16 22:19 */ public class BeanFactory { // 读取properties文件。 // 定义一个properties对象 private static Properties props; //定义一个map，用于存放我们要创建的对象，我们把它称之为容器，是因为如果一个对象长时间不用的话垃圾回收机制就会把他回收 private static Map&lt;String, Object&gt; beans; // 使用静态代码块为Properties 对象赋值 static { try { // 实例化对象 props = new Properties(); // 不要采用如下的方式来New这一个对象，因为里面一旦写了src地址在编译期就没了，然后如果是写的绝对地址C盘，D盘的话也没了所以 // 不用以下的方式 // InputStream in = new InputStream(); // 获取Proerties文件的流对象 // 所以这个地方我们使用类加载器来获取，里面的resource的路径会成为根路径下的一个文件。 InputStream in = BeanFactory.class.getClassLoader().getResourceAsStream(&quot;bean.properties&quot;); props.load(in); // 实例化容器 beans = new HashMap&lt;String, Object&gt;(); // 取出来配置文件中所有的Key，keys返回的是一个枚举类型 Enumeration keys = props.keys(); // 遍历枚举 while (keys.hasMoreElements()) { // 取出来每个key String key = keys.nextElement().toString(); // 根据Key获取value String beanPath = props.getProperty(key); // 反射创建对象 Object value = Class.forName(beanPath).newInstance(); // 把Key和value存入容器之中 beans.put(key, value); } } catch (Exception e) { // 这个地方一旦读取Properties失败后面的也就都失败了，所以这里面就可以抛出一个初始化异常，这个异常本质上也是一个error throw new ExceptionInInitializerError(&quot;初始化properties失败程序不能执行&quot;); } } /*根据benad的名称获取对象，此时已经是单例模式了*/ public static Object getBean(String beanName) { return beans.get(beanName); } // // /*根据bena的名称获取bean的对象*/ // public static Object getBean(String beanName) { // Object bean = null; // try { // String beanPath = props.getProperty(beanName); // bean = Class.forName(beanPath).newInstance(); // } catch (Exception e) { // e.printStackTrace(); // } // return bean; // } }]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F09%2F17%2FSpring%E7%AC%94%E8%AE%B0%2FSpring%E7%AC%AC%E4%B8%80%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[一般不定义service层和dao层的类属性成员，因为一旦是单例对象的话就很容易每次调用的时候都修改掉他。对象创建多次，执行效率没有单例对象高。 工厂模式的解耦的升级版，我的理解是，在service层和controller层每次都需要一个下一层的对象，那么这个时候怎么办呢，就需要实例化一个对象，但是由于一直实例化对象不好，所以要通过写xml文件或者properties文件来写上service层的位置和dao层的位置，每次使用的时候就读取这个文件，然后通过getclassloader的getResourceAsStream来读取里面的文件，把这个存入到InputStream里面，在通过props.load(in)来加载这个放到props里面，在映射这个对象的时候，就是在工厂类里面来做这个事情的，然后通过while循环拿到props里面的每一个Key，然后在通过getProperty方法获取到Key对应的Propertiy，这个就是类全限定名路径，然后在通过Class.forName(beanPath).newInstance()来通过反射创建一个类。下面是一个工厂类。 import java.io.InputStream; import java.util.Enumeration; import java.util.HashMap; import java.util.Map; import java.util.Properties; /** * @Description 一个创建bean对象的工厂 * Bean在英语中是：有可重用组件的意思。 * java bena不等于实体类。javabean的范围大于实体类。 * JavaBean:用Java语言编写的可重用组件 * &lt;p&gt; * 他就是创建我们的service和dao对象的 * 1、需要一个配置文件来配置我们的service和dao * 配置内容：唯一标志=全限定类名，(keyvalue) * 2、通过读取配置文件中配置的内容，反射创建对象 * &lt;p&gt; * 配置文件可以是xml也可以是Properties； * @Author TT Hun * @Data 2019/9/16 22:19 */ public class BeanFactory { // 读取properties文件。 // 定义一个properties对象 private static Properties props; //定义一个map，用于存放我们要创建的对象，我们把它称之为容器，是因为如果一个对象长时间不用的话垃圾回收机制就会把他回收 private static Map&lt;String, Object&gt; beans; // 使用静态代码块为Properties 对象赋值 static { try { // 实例化对象 props = new Properties(); // 不要采用如下的方式来New这一个对象，因为里面一旦写了src地址在编译期就没了，然后如果是写的绝对地址C盘，D盘的话也没了所以 // 不用以下的方式 // InputStream in = new InputStream(); // 获取Proerties文件的流对象 // 所以这个地方我们使用类加载器来获取，里面的resource的路径会成为根路径下的一个文件。 InputStream in = BeanFactory.class.getClassLoader().getResourceAsStream(&quot;bean.properties&quot;); props.load(in); // 实例化容器 beans = new HashMap&lt;String, Object&gt;(); // 取出来配置文件中所有的Key，keys返回的是一个枚举类型 Enumeration keys = props.keys(); // 遍历枚举 while (keys.hasMoreElements()) { // 取出来每个key String key = keys.nextElement().toString(); // 根据Key获取value String beanPath = props.getProperty(key); // 反射创建对象 Object value = Class.forName(beanPath).newInstance(); // 把Key和value存入容器之中 beans.put(key, value); } } catch (Exception e) { // 这个地方一旦读取Properties失败后面的也就都失败了，所以这里面就可以抛出一个初始化异常，这个异常本质上也是一个error throw new ExceptionInInitializerError(&quot;初始化properties失败程序不能执行&quot;); } } /*根据benad的名称获取对象，此时已经是单例模式了*/ public static Object getBean(String beanName) { return beans.get(beanName); } // // /*根据bena的名称获取bean的对象*/ // public static Object getBean(String beanName) { // Object bean = null; // try { // String beanPath = props.getProperty(beanName); // bean = Class.forName(beanPath).newInstance(); // } catch (Exception e) { // e.printStackTrace(); // } // return bean; // } }]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F09%2F12%2FSpark%E7%AC%AC%E4%BA%8C%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[Spark通用性。 Spark模块Spark Core //核心库 Spark SQL //SQL Spark Streaming //准实时计算。 Spark MLlib //机器学习库 Spark graph //图计算 Spark集群运行1.local //本地模式 2.standalone //独立模式 3.yarn //yarn模式 4.mesos //mesos start-all.shstart-master.sh //RPC端口 7077 start-slave.sh spark://s201:7077 webuihttp://s201:8080 添加针对scala文件的编译插件在IDEA的settings里面没有设置自动编译的情况下，需要记入scala编译插件，所以打包不含scala的类。 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.it18zhang&lt;/groupId&gt; &lt;artifactId&gt;SparkDemo1&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;build&gt; &lt;sourceDirectory&gt;src/main/java&lt;/sourceDirectory&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt; &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt; &lt;version&gt;3.2.2&lt;/version&gt; &lt;configuration&gt; &lt;recompileMode&gt;incremental&lt;/recompileMode&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;goal&gt;testCompile&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;version&gt;2.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; C:\Users\Administrator.m2\repository\netC:\Users\Administrator.m2\repository\net\alchim31\maven... SparkContext:Spark集群的连接。主要入口点。 SparkConf = new (); conf.setApp(&quot;&quot;) conf.setMaster(&quot;local&quot;) ; sc = new SparkContext(conf); //RDD : Resilient distributed dataset,弹性分布式数据集。 val rdd1 = sc.textFile(&quot;d:/scala/test.txt&quot;); val rdd2 = rdd1.flatMap(line=&gt;line.split(&quot; &quot;)); val rdd3 = rdd2.map(word=&gt;(word,1)); val rdd4 = rdd3.reduceByKey(_ + _) ; val list = rdd4.collect() list.foreach(e=&gt;println(e)); // sc.textFile(&quot;d:/scala&quot;).flatMap(_.split(&quot; &quot;)).map((_1)).reduceByKey(_ + _).collect().foreach(println) spark基于hadoop的mr，扩展MR模型高效使用MR模型，内存型集群计算，提高app处理速度。 spark特点速度:在内存中存储中间结果。 支持多种语言. 内置了80+的算子. 高级分析:MR，SQL/ Streamming /mllib / graph spark模块 core //通用执行引擎，提供内存计算和对外部数据集的引用。 SQL //构建在core之上，引入新的抽象SchemaRDD，提供了结构化和半结构化支持。 Streaming //小批量计算，用的是RDD. MLlib //机器学习库。core在。 Graphx //图计算。 RDD:是spark的基本数据结构，是不可变数据集。RDD中的数据集进行逻辑分区，每个分区可以单独在集群节点 进行计算。可以包含任何java,scala，python和自定义类型。 RDD是只读的记录分区集合。RDD具有容错机制。 创建RDD方式，一、并行化一个现有集合。 hadoop 花费90%时间用户rw。、但是也不一定，如果是一次ｍｒ可能就不进入到磁盘里面，如果多次ｍｒ肯定进入到磁盘里面了。要等到数据都写入才可以，收到磁盘Ｉｏ影响严重 内存处理计算。在job间进行数据共享。内存的IO速率高于网络和disk的10 ~ 100之间。 内部包含5个主要属性 ----------------------- 1.分区列表 2.针对每个split的计算函数。 3.对其他rdd的依赖列表 4.可选，如果是KeyValueRDD的话，可以带分区类。 5.可选，首选块位置列表(hdfs block location); //默认并发度下面从下往上面看是sc.textFile的源码过程 这边确定一下，这个textFile里面的defaultMinPartitions指的是分区个数。 然后在代码里面conf.setMaster(&quot;local[2]&quot;)这个里面的2指的才是线程个数才是并发度。这个线程就类似于我们完全分布下的节点 local.backend.defaultParallelism() = scheduler.conf.getInt(&quot;spark.default.parallelism&quot;, totalCores) taskScheduler.defaultParallelism = backend.defaultParallelism() sc.defaultParallelism =...; taskScheduler.defaultParallelism defaultMinPartitions = math.min(defaultParallelism, 2) sc.textFile(path,defaultMinPartitions) //1,2 先说一下mapreduce里面的Map和reduce可以和spark里面的Map和reduce相结合看一下： Map过程：并行读取文本，对读取的单词进行map操作，每个词都以&lt;key,value&gt;形式生成。 一个有三行文本的文件进行MapReduce操作。 读取第一行Hello World Bye World ，分割单词形成Map。 &lt;Hello,1&gt; &lt;World,1&gt; &lt;Bye,1&gt; &lt;World,1&gt; 读取第二行Hello Hadoop Bye Hadoop ，分割单词形成Map。 &lt;Hello,1&gt; &lt;Hadoop,1&gt; &lt;Bye,1&gt; &lt;Hadoop,1&gt; 读取第三行Bye Hadoop Hello Hadoop，分割单词形成Map。 &lt;Bye,1&gt; &lt;Hadoop,1&gt; &lt;Hello,1&gt; &lt;Hadoop,1&gt; Reduce操作是对map的结果进行排序，合并，最后得出词频。我的理解： 经过进一步处理(combiner),将形成的Map根据相同的key组合成value数组。 &lt;Bye,1,1,1&gt; &lt;Hadoop,1,1,1,1&gt; &lt;Hello,1,1,1&gt; &lt;World,1,1&gt; 循环执行Reduce(K,V[])，分别统计每个单词出现的次数。 &lt;Bye,3&gt; &lt;Hadoop,4&gt; &lt;Hello,3&gt; &lt;World,2&gt; RDD变换返回指向新rdd的指针，在rdd之间创建依赖关系。每个rdd都有计算函数和指向父RDD的指针。 map() //对每个元素进行变换，应用变换函数 //(T)=&gt;V filter() //过滤器,(T)=&gt;Boolean flatMap() //压扁,T =&gt; TraversableOnce[U] mapPartitions() //对每个分区进行应用变换，输入的Iterator,返回新的迭代器，可以对分区进行函数处理。 //Iterator&lt;T&gt; =&gt; Iterator&lt;U&gt; mapPartitionsWithIndex(func) //同上，(Int, Iterator&lt;T&gt;) =&gt; Iterator&lt;U&gt; sample(withReplacement, fraction, seed) //采样返回采样的RDD子集。 //withReplacement 元素是否可以多次采样. //fraction : 期望采样数量.[0,1] union() //类似于mysql union操作。 //select * from persons where id &lt; 10 //union select * from id persons where id &gt; 29 ; intersection //交集,提取两个rdd中都含有的元素。 distinct([numTasks])) //去重,去除重复的元素。 groupByKey() //(K,V) =&gt; (K,Iterable&lt;V&gt;) reduceByKey(*) //按key聚合。 aggregateByKey(zeroValue)(seqOp, combOp, [numTasks]) //按照key进行聚合 key:String U:Int = 0 sortByKey //排序 join(otherDataset, [numTasks]) //连接,(K,V).join(K,W) =&gt;(K,(V,W)) cogroup //协分组 //(K,V).cogroup(K,W) =&gt;(K,(Iterable&lt;V&gt;,Iterable&lt;!-- &lt;W&gt; --&gt;)) cartesian(otherDataset) //笛卡尔积,RR[T] RDD[U] =&gt; RDD[(T,U)] pipe //将rdd的元素传递给脚本或者命令，执行结果返回形成新的RDD coalesce(numPartitions) //减少分区 repartition //可增可减 repartitionAndSortWithinPartitions(partitioner) //再分区并在分区内进行排序 RDD Actioncollect() //收集rdd元素形成数组. count() //统计rdd元素的个数 reduce() //聚合,返回一个值。 first //取出第一个元素take(1) take // takeSample (withReplacement,num, [seed]) takeOrdered(n, [ordering]) saveAsTextFile(path) //保存到文件 saveAsSequenceFile(path) //保存成序列文件 saveAsObjectFile(path) (Java and Scala) countByKey() //按照key,统计每个key下value的个数. spark集成hadoop ha1.复制core-site.xml + hdfs-site.xml到spark/conf目录下 2.分发文件到spark所有work节点 3.启动spark集群 4.启动spark-shell,连接spark集群上 $&gt;spark-shell --master spark://s201:7077 $scala&gt;sc.textFile(&quot;hdfs://mycluster/user/centos/test.txt&quot;).collect();]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F09%2F11%2FMybatis%E7%AC%AC%E4%BA%8C%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F09%2F10%2FMybatis%E7%AC%AC%E4%B8%80%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[mybatis框架共四天第一天：mybatis入门 mybatis的概述 mybatis的环境搭建 mybatis入门案例 自定义mybatis框架（主要的目的是为了让大家了解mybatis中执行细节）第二天：mybatis基本使用 mybatis的单表crud操作 mybatis的参数和返回值 mybatis的dao编写 mybatis配置的细节 几个标签的使用第三天：mybatis的深入和多表 mybatis的连接池 mybatis的事务控制及设计的方法 mybatis的多表查询 一对多（多对一） 多对多第四天：mybatis的缓存和注解开发 mybatis中的加载时机（查询的时机） mybatis中的一级缓存和二级缓存 mybatis的注解开发 单表CRUD 多表查询 1、什么是框架？ 它是我们软件开发中的一套解决方案，不同的框架解决的是不同的问题。 使用框架的好处： 框架封装了很多的细节，使开发者可以使用极简的方式实现功能。大大提高开发效率。2、三层架构 表现层： 是用于展示数据的 业务层： 是处理业务需求 持久层： 是和数据库交互的3、持久层技术解决方案 JDBC技术： Connection PreparedStatement ResultSet Spring的JdbcTemplate： Spring中对jdbc的简单封装 Apache的DBUtils： 它和Spring的JdbcTemplate很像，也是对Jdbc的简单封装 以上这些都不是框架 JDBC是规范 Spring的JdbcTemplate和Apache的DBUtils都只是工具类 4、mybatis的概述 mybatis是一个持久层框架，用java编写的。 它封装了jdbc操作的很多细节，使开发者只需要关注sql语句本身，而无需关注注册驱动，创建连接等繁杂过程 它使用了ORM思想实现了结果集的封装。 ORM： Object Relational Mappging 对象关系映射 简单的说： 就是把数据库表和实体类及实体类的属性对应起来 让我们可以操作实体类就实现操作数据库表。 user User id userId user_name userName 今天我们需要做到 实体类中的属性和数据库表的字段名称保持一致。 user User id id user_name user_name 5、mybatis的入门 mybatis的环境搭建 第一步：创建maven工程并导入坐标 第二步：创建实体类和dao的接口 第三步：创建Mybatis的主配置文件 SqlMapConifg.xml 第四步：创建映射配置文件 IUserDao.xml 环境搭建的注意事项： 第一个：创建IUserDao.xml 和 IUserDao.java时名称是为了和我们之前的知识保持一致。 在Mybatis中它把持久层的操作接口名称和映射文件也叫做：Mapper 所以：IUserDao 和 IUserMapper是一样的 第二个：在idea中创建目录的时候，它和包是不一样的 包在创建时：com.itheima.dao它是三级结构 目录在创建时：com.itheima.dao是一级目录 第三个：mybatis的映射配置文件位置必须和dao接口的包结构相同 第四个：映射配置文件的mapper标签namespace属性的取值必须是dao接口的全限定类名 第五个：映射配置文件的操作配置（select），id属性的取值必须是dao接口的方法名 当我们遵从了第三，四，五点之后，我们在开发中就无须再写dao的实现类。 mybatis的入门案例 第一步：读取配置文件 第二步：创建SqlSessionFactory工厂 第三步：创建SqlSession 第四步：创建Dao接口的代理对象 第五步：执行dao中的方法 第六步：释放资源 注意事项： 不要忘记在映射配置中告知mybatis要封装到哪个实体类中 配置的方式：指定实体类的全限定类名 mybatis基于注解的入门案例： 把IUserDao.xml移除，在dao接口的方法上使用@Select注解，并且指定SQL语句 同时需要在SqlMapConfig.xml中的mapper配置时，使用class属性指定dao接口的全限定类名。 明确： 我们在实际开发中，都是越简便越好，所以都是采用不写dao实现类的方式。 不管使用XML还是注解配置。 但是Mybatis它是支持写dao实现类的。 6、自定义Mybatis的分析： mybatis在使用代理dao的方式实现增删改查时做什么事呢？ 只有两件事： 第一：创建代理对象 第二：在代理对象中调用selectList 自定义mybatis能通过入门案例看到类 class Resources class SqlSessionFactoryBuilder interface SqlSessionFactory interface SqlSession]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F09%2F07%2FSpark%E7%AC%AC%E4%B8%80%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[并行集群计算。 并行计算。 并发并发执行。 SparkLightning-fast cluster computing。 快如闪电的集群计算。 大规模快速通用的计算引擎。 速度: 比hadoop 100x,磁盘计算快10x 使用: java / Scala /R /python 提供80+算子(操作符)，容易构建并行应用。 通用: 组合SQL ，流计算 + 复杂分析。 运行： Hadoop, Mesos, standalone, or in the cloud,local. Spark模块Spark core //核心模块 Spark SQL //SQL Spark Streaming //流计算 Spark MLlib //机器学习 Spark graph //图计算 DAG //direct acycle graph,有向无环图。 安装Spark1.下载spark-2.1.0-bin-hadoop2.7.tgz .. 2.解压 .. 3.环境变量 [/etc/profile] SPARK_HOME=/soft/spark PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin [source] $&gt;source /etc/profile 4.验证spark $&gt;cd /soft/spark $&gt;./spark-shell 5.webui http://s201:4040/ 体验spark0.sc SparkContext，Spark程序的入口点，封装了整个spark运行环境的信息。 1.进入spark-shell $&gt;spark-shell $scala&gt;sc API[SparkContext] Spark程序的入口点，封装了整个spark运行环境的信息。 [RDD] resilient distributed dataset,弹性分布式数据集。等价于集合。 spark实现word count//加载文本文件,以换行符方式切割文本.Array(hello world2,hello world2 ,...) val rdd1 = sc.textFile(&quot;/home/centos/test.txt&quot;); //单词统计1 $scala&gt;val rdd1 = sc.textFile(&quot;/home/centos/test.txt&quot;) $scala&gt;val rdd2 = rdd1.flatMap(line=&gt;line.split(&quot; &quot;)) $scala&gt;val rdd3 = rdd2.map(word = &gt; (word,1)) $scala&gt;val rdd4 = rdd3.reduceByKey(_ + _) $scala&gt;rdd4.collect //单词统计2 sc.textFile(&quot;/home/centos/test.txt&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_ + _).collect //统计所有含有wor字样到单词个数。filter //过滤单词 sc.textFile(&quot;/home/centos/test.txt&quot;).flatMap(_.split(&quot; &quot;)).filter(_.contains(&quot;wor&quot;)).map((_,1)).reduceByKey(_ + _).collect [API] SparkContext: Spark功能的主要入口点。代表到Spark集群的连接，可以创建RDD、累加器和广播变量. 每个JVM只能激活一个SparkContext对象，在创建sc之前需要stop掉active的sc。 SparkConf: spark配置对象，设置Spark应用各种参数，kv形式。 编写scala程序，引入spark类库，完成wordcount1.创建Scala模块,并添加pom.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.it18zhang&lt;/groupId&gt; &lt;artifactId&gt;SparkDemo1&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;version&gt;2.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; 2.编写scala文件 import org.apache.spark.{SparkConf, SparkContext} /** * Created by Administrator on 2017/4/20. */ object WordCountDemo { def main(args: Array[String]): Unit = { //创建Spark配置对象 val conf = new SparkConf(); conf.setAppName(&quot;WordCountSpark&quot;) //设置master属性 conf.setMaster(&quot;local&quot;) ; //通过conf创建sc val sc = new SparkContext(conf); //加载文本文件 val rdd1 = sc.textFile(&quot;d:/scala/test.txt&quot;); //压扁 val rdd2 = rdd1.flatMap(line =&gt; line.split(&quot; &quot;)) ; //映射w =&gt; (w,1) val rdd3 = rdd2.map((_,1)) val rdd4 = rdd3.reduceByKey(_ + _) val r = rdd4.collect() r.foreach(println) } } java版单词统计import org.apache.spark.SparkConf; import org.apache.spark.SparkContext; import org.apache.spark.api.java.JavaPairRDD; import org.apache.spark.api.java.JavaRDD; import org.apache.spark.api.java.JavaSparkContext; import org.apache.spark.api.java.function.FlatMapFunction; import org.apache.spark.api.java.function.Function2; import org.apache.spark.api.java.function.PairFunction; import scala.Tuple2; import java.util.ArrayList; import java.util.Iterator; import java.util.List; /** * java版 */ public class WordCountJava2 { public static void main(String[] args) { //创建SparkConf对象 SparkConf conf = new SparkConf(); conf.setAppName(&quot;WordCountJava2&quot;); conf.setMaster(&quot;local&quot;); //创建java sc JavaSparkContext sc = new JavaSparkContext(conf); //加载文本文件 JavaRDD&lt;String&gt; rdd1 = sc.textFile(&quot;d:/scala//test.txt&quot;); //压扁 JavaRDD&lt;String&gt; rdd2 = rdd1.flatMap(new FlatMapFunction&lt;String, String&gt;() { public Iterator&lt;String&gt; call(String s) throws Exception { List&lt;String&gt; list = new ArrayList&lt;String&gt;(); String[] arr = s.split(&quot; &quot;); for(String ss :arr){ list.add(ss); } return list.iterator(); } }); //映射,word -&gt; (word,1) JavaPairRDD&lt;String,Integer&gt; rdd3 = rdd2.mapToPair(new PairFunction&lt;String, String, Integer&gt;() { public Tuple2&lt;String, Integer&gt; call(String s) throws Exception { return new Tuple2&lt;String, Integer&gt;(s,1); } }); //reduce化简 JavaPairRDD&lt;String,Integer&gt; rdd4 = rdd3.reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() { public Integer call(Integer v1, Integer v2) throws Exception { return v1 + v2; } }); // List&lt;Tuple2&lt;String,Integer&gt;&gt; list = rdd4.collect(); for(Tuple2&lt;String, Integer&gt; t : list){ System.out.println(t._1() + &quot; : &quot; + t._2()); } } } Spark2.1.0最新版是基于Scala2.11.8版本，因此安装scala2.11.8版本， 否则如果基于2.12.0版本编译会出现找不到包的问题。1.卸载原来的scala. 2.重新安装scala2.11.8版本 3.配置idea的全局库 project settings -&gt; global library -&gt; 删除原来的scala sdk project settings -&gt; global library -&gt; 添加sdk -&gt; browser -&gt; 定位scala安装目录 -&gt;选中scala-compiler.jar + scala-library.jar + scala-reflect.jar 4.在模块中添加scala sdk 2.11.8版本 5.重新编译项目 -&gt; 导入jar -&gt;丢到集群运行。 提交作业到spark集群运行1.导出jar包 2.spark-submit提交命令运行job //Scala版本 $&gt;spark-submit --master local --name MyWordCount --class com.it18zhang.spark.scala.WordCountScala SparkDemo1-1.0-SNAPSHOT.jar /home/centos/test.txt //java版 $&gt;spark-submit --master local --name MyWordCount --class com.it18zhang.spark.java.WordCountJava SparkDemo1-1.0-SNAPSHOT.jar /home/centos/test.txt Spark集群模式1.local nothing! spark-shell --master local; //默认 2.standalone 独立模式。 a)复制spark安装目录到其他几个主机 b)配置其他主机的所有环境变量 [/etc/profile] SPARK_HOME PATH c)配置master节点的slaves [/soft/spark/conf/slaves]文件中添加下面的 s202 s203 s204 d)启动spark集群，和hadoop一样，但是要指定目录下即可。这个是独立模式和没有关系 /soft/spark/sbin/start-all.sh e)查看进程 $&gt;xcall.jps jps master //s201 worker //s202 worker //s203 worker //s204 e)webui，然后验证webui，8080端口。本地模式是4040端口 http://s201:8080/ 提交作业jar到完全分布式spark集群1.需要启动hadoop集群(只需要hdfs，是standalone模式，不需要yarn调度) $&gt;start-dfs.sh 2.put文件到hdfs. 3.运行spark-submit $&gt;spark-submit --master spark://s201:7077 --name MyWordCount --class com.it18zhang.spark.scala.WordCountScala SparkDemo1-1.0-SNAPSHOT.jar hdfs://s201:8020/user/centos/test.txt 脚本分析[start-all.sh] sbin/spark-config.sh sbin/spark-master.sh //启动master进程 sbin/spark-slaves.sh //启动worker进程 [start-master.sh] sbin/spark-config.sh org.apache.spark.deploy.master.Master spark-daemon.sh start org.apache.spark.deploy.master.Master --host --port --webui-port ... [spark-slaves.sh] sbin/spark-config.sh slaves.sh //主要循环这个文件conf/slaves [slaves.sh] for conf/slaves{ ssh host start-slave.sh ... } [start-slave.sh] 先找到这个work类 CLASS=”org.apache.spark.deploy.worker.Worker” sbin/spark-config.sh 走这个配置 for (( .. )) ; do start_instance $(( 1 + $i )) “$@” done $&gt;cd /soft/spark/sbin $&gt;./stop-all.sh //停掉整个spark集群. $&gt;./start-master.sh //停掉整个spark集群. $&gt;./start-master.sh //启动master节点 $&gt;./start-slaves.sh //启动所有worker节点 从s204里面启动某一个slave ./start-slave.sh spark://s201:7077 对上面几个命令了解可以通过 start-master.sh --help来查询，其他类似]]></content>
  </entry>
  <entry>
    <title><![CDATA[编程题]]></title>
    <url>%2F2019%2F03%2F04%2F%E7%BC%96%E7%A8%8B%E9%A2%98%2F</url>
    <content type="text"><![CDATA[public class Problem3 { public static void main1(String[] args) { int[] arr = new int[]{2, 3, 1, 0, 2, 5, 3}; int[] ints = new int[10]; boolean duplicate = duplicate(arr, 6, ints); System.out.println(ints[0]); System.out.println(); } public static boolean duplicate(int numbers[], int length, int[] duplication) { if (numbers == null || length &lt;= 0) return false; for (int a : numbers) { if (a &lt; 0 || a &gt;= length) return false; } int temp; for (int i = 0; i &lt; length; i++) { while (numbers[i] != i) { if (numbers[numbers[i]] == numbers[i]) { duplication[0] = numbers[i]; return true; } temp = numbers[i]; numbers[i] = numbers[temp]; numbers[temp] = temp; } } return false; } //另一种方法 /** * 避免使用辅助空间 */ public int getDuplication(int[] arr) { for(int i = 0;i &lt; arr.length;i++) { if(arr[i] &lt; 0 || arr[i] &gt;= arr.length) throw new IllegalArgumentException(&quot;输入参数不合法&quot;); } int start = 0; int end = arr.length-1; int flag = 0; int middle = 0; while(end &gt;= start) { if(flag == 0) middle = (end + start)/2; int count = countRange(arr,start,middle); if(end == start) { if(count &gt; 1) return start; else break; } if(count &gt; (middle-start+1))//说明(start,middle)这个区间有重复的数 { end = middle; flag = 0; }else if(count == (middle-start+1))//不能判断(start,middle)这个区间有重复的数 { middle = middle - 1; if(middle &lt; start)//说明(start,middle)这个区间没有重复的数 { start = (start+end)/2 + 1; flag = 0; }else flag = 1; }else //说明(middle+1,end)这个区间有重复的数 { start = middle + 1; flag = 0; } } return -1; } private int countRange(int[] arr, int start, int end) { int count = 0; for(int i = 0;i &lt; arr.length;i++) { if(arr[i] &gt;= start &amp;&amp; arr[i] &lt;= end) ++count; } return count; } public static void main(String[] args) { Problem3 test = new Problem3(); int[] arr = {0,3,5,4,1,2,6,7}; int value = test.getDuplication(arr); } }]]></content>
  </entry>
  <entry>
    <title><![CDATA[剑指offer第45题]]></title>
    <url>%2F2019%2F01%2F28%2F%E5%89%91%E6%8C%87offer%E7%AC%AC45%E9%A2%98%2F</url>
    <content type="text"><![CDATA[关于compare排序的讲解：https://blog.csdn.net/lx_nhs/article/details/78871295]]></content>
  </entry>
  <entry>
    <title><![CDATA[实战大数据电信项目面试总结]]></title>
    <url>%2F2019%2F01%2F15%2F%E5%AE%9E%E6%88%98%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%94%B5%E4%BF%A1%E9%A1%B9%E7%9B%AE%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[4.Hadoop以及HBase的HA集群配置与实战。 hadoop的使用QJM的高可用架构配置讲解，ResourceManager的高可用架构配置讲解。 zookeeper的工作原理以及配置、实操演练，hbase与Hadoop HA集成注意事项以及客户端 API编程细节处理。 hadoop HDFS HA高可用配置也就是实现2个namenode一个active一个standby，利用zk实现自动容灾配置hdfs-site和core-site.xml指定zk地址。 自动容灾引入两个组件，zk quarum + zk容灾控制器(ZKFC)。 运行NN的主机还要运行ZKFC进程，主要负责: a.健康监控 b.session管理 c.选举 resourcemanager自动容灾配置对yarn-site.xml进行配置配置自动容灾并且将zk地址配制进去。 部署zk集群1、配置zk配置文件 [/soft/zk/conf/zoo.cfg] … dataDir=/home/centos/zookeeper server.1=s201:2888:3888 server.2=s202:2888:3888 server.3=s203:2888:3888 2、在每台主机的/home/centos/zookeeper中添加myid,内容分别是1,2,3 配置HBase和Hbase高可用安装配置1、安装后配置环境后在修改配置文件[hbase/conf/hbase-env.sh]，添加hbase在hdfs存放路径、zk地址、zk本地目录、使用完全分布式true。 2、配置regionservers [hbase/conf/regionservers] s202 s203 s204 高可用直接在2台机器上执行命令： hbase-daemon.sh start master 配置Kafka集群1、配置kafka [kafka/config/server.properties] … broker.id=202 … listeners=PLAINTEXT://:9092 … log.dirs=/home/centos/kafka/logs … zookeeper.connect=s201:2181,s202:2181,s203:2181 2、分发server.properties，同时修改每个文件的broker.id 配置Flume配置flume将其配置文件修改为source为某个文件夹，然后sink为kafka集群 创建hbase名字空间+表1.创建名字空间和表。表是：名字空间+列族。 $&gt;hbase shell //进入hbase shell $hbase&gt;create_namespace &apos;ns1&apos; //创建空间 $hbase&gt;create &apos;ns1:calllogs&apos; , &apos;f1&apos; //创建表 $hbase&gt;truncate &apos;ns1:calllogs&apos; //重建表 创建kafka消费者，订阅calllog主题1.设计rowkey 业务数据: caller , callee , date , duration 分区号 号码 时间,标记 (对方号码),时长 regionNo, caller,date , flag,callee,duration caller(11) + 通话时间(201701) = (后四位) + 201701 = % 100 对日志信息进行整理，删除多余的部分如/:之类的解析日志数据将日志数据截取串，实例化put对象，插入Hbase中去。 打成jar包放到classpath之中，将kafka消费者打成Jar包，并且，用maven在windows将其的依赖都下载下来放到classpath当中。 将这个文件夹都放置到Linux下，然后写一个执行这个jar包的脚本： java -cp CallLogConsumerModule.jar;./lib/activation-1.1.jar;./lib/apacheds-i18n-2.0.0-M15.jar;./lib/apacheds-kerberos-codec-2.0.0-M15.jar;./lib/api-asn1-api-1.0.0-M20.jar;./lib/api-util-1.0.0-M20.jar;./lib/avro-1.7.4.jar;./lib/commons-beanutils-1.7.0.jar;./lib/commons-beanutils-core-1.8.0.jar;./lib/commons-cli-1.2.jar;./lib/commons-codec-1.9.jar;./lib/commons-collections-3.2.2.jar;./lib/commons-compress-1.4.1.jar;./lib/commons-configuration-1.6.jar;./lib/commons-digester-1.8.jar;./lib/commons-el-1.0.jar;./lib/commons-httpclient-3.1.jar;./lib/commons-io-2.4.jar;./lib/commons-lang-2.6.jar;./lib/commons-logging-1.2.jar;./lib/commons-math3-3.1.1.jar;./lib/commons-net-3.1.jar;./lib/findbugs-annotations-1.3.9-1.jar;./lib/guava-12.0.1.jar;./lib/hadoop-annotations-2.5.1.jar;./lib/hadoop-auth-2.5.1.jar;./lib/hadoop-common-2.5.1.jar;./lib/hadoop-mapreduce-client-core-2.5.1.jar;./lib/hadoop-yarn-api-2.5.1.jar;./lib/hadoop-yarn-common-2.5.1.jar;./lib/hamcrest-core-1.3.jar;./lib/hbase-annotations-1.2.4.jar;./lib/hbase-client-1.2.4.jar;./lib/hbase-common-1.2.4.jar;./lib/hbase-protocol-1.2.4.jar;./lib/htrace-core-3.1.0-incubating.jar;./lib/httpclient-4.2.5.jar;./lib/httpcore-4.2.4.jar;./lib/jackson-core-asl-1.9.13.jar;./lib/jackson-mapper-asl-1.9.13.jar;./lib/jaxb-api-2.2.2.jar;./lib/jcodings-1.0.8.jar;./lib/jdk.tools-1.6.jar;./lib/jetty-util-6.1.26.jar;./lib/jline-0.9.94.jar;./lib/joni-2.1.2.jar;./lib/jopt-simple-4.9.jar;./lib/jsch-0.1.42.jar;./lib/jsr305-1.3.9.jar;./lib/junit-4.12.jar;./lib/kafka-clients-0.10.0.1.jar;./lib/kafka_2.11-0.10.0.1.jar;./lib/log4j-1.2.15.jar;./lib/lz4-1.3.0.jar;./lib/mail-1.4.jar;./lib/metrics-core-2.2.0.jar;./lib/netty-3.7.0.Final.jar;./lib/netty-all-4.0.23.Final.jar;./lib/paranamer-2.3.jar;./lib/protobuf-java-2.5.0.jar;./lib/scala-library-2.11.8.jar;./lib/scala-parser-combinators_2.11-1.0.4.jar;./lib/slf4j-api-1.6.1.jar;./lib/slf4j-log4j12-1.7.21.jar;./lib/snappy-java-1.1.2.6.jar;./lib/stax-api-1.0-2.jar;./lib/xmlenc-0.52.jar;./lib/xz-1.0.jar;./lib/zkclient-0.8.jar;./lib/zookeeper-3.4.6.jar com.it18zhang.calllog.consumer.HbaseConsumer 执行kafak消费者 业务场景一：输入电话号码查询所有该用户所有通话记录（利用Hbase查询）通过在service中建立和hbase的连接，通过使用scan对象，将查询的内容填充到实体类中，将实体类添加到list当中去，然后返回list，在controller层调用service层的findall方法。返回给前段，前段通过jsp界面，将返回的数据填充到表格中去。 业务场景二：根据主叫和开始时间年月，结束时间年月来查询该用户的通话记录包括主叫和被叫(利用hbase查询)这块就有点内容了，首先hbase的rowkey的实际根据实际的业务需要把rowkey设置成reginonumber,callerid calltime(精确到分) flag calleeid duration.为什么设置成这样根据rowkey的设计原则，吧尽可能多的内容设计到rowkey里面，可以直接查询的到。最常用的放到前面，可以通过callerid和time直接查询的到。设计成不同的regionnumber将不同的号码进行类似于分桶可以切割放到不同的服务器里面。 在查询rowkey的时候考虑到根据时间段查询的时候，由于rowkey的设计是哈希号，主叫，时间，被叫，所以哈希号是个根据主叫+时间（年月）来设计的，这块根据主叫+年月的设计是因为要是淡出根据手机号来哈希的话大多数都是138值类的会造成热点问题。所以根据时间+手机号来设计，然后搜易在根据时间来查找的时候，要考虑起止时间是否是同年月，不同年月，如果是同年月的话就是说在同一个哈希号之间，开始时间就是开始时间，结束时间是day+1，前包后不包，如果是不同年月的话就在不同的区号之间，比如是2017.3.11-2017.5.9就要搜索2017.3.11-2017.4,2017.4-2017.5,2017.5-2017,5,9 要取到callerid+年月作为hash的值而不能取到时，分作为哈希的值，因为取到时，分的话就每个都是不一样的哈希的值，查询的时候就要根据时分来查询而不能根据年月来查询。因为每次查询的时候要指定callerid和年月。如果是根据callerid+时分来计算的话，每次存储的时候即便是同一天同一个月也不会分散的同一个哈希区域，而是分散到不同的区域，难以查询到，如果是这样只能通过每一分钟每一分钟的查询，没法查询了。 由于在输入数据的时候只是插入主叫的数据，在查询通话详单的时候要查询该号码即是主叫，又是被叫的情况下，在rowkey是rno,callerid,calltime,calleeid,calleeid,duration的情况下要查询位置在后面的calleeid的情况下，几乎要全表扫描。所以这块我们在每次put进来数据的时候我们使用协处理器，每次添加主叫的时候，在添加一个被叫。这个被叫的Rowkey是哈希+被叫+时间+1+主叫+时长。然后在put这个被叫的时候区域号也就是rno还是根据主叫+时间片来哈希的。就可以让被叫信息和主叫信息在同一个哈希区域内，然后插入flag=1的rowkey，然后他的value值就是原来主叫表的rowkey的各个值。这样的话避免了将之前主叫表的冗余的value在重复插入一遍。也就是一个二级索引的思想，在协处理器里面重写Postput方法和postScannerNext()方法，postput方法的作用就是在插入一条主叫记录的同时，在插入一条被叫记录，而postgetOp()方法的作用是查询被叫返回主叫信息。在查询的时候用的是scan的API然后查询value值，所以在查询被叫的时候让他返回主叫信息，（这边视频中这边考虑的是如果得到rowkey还需要解析，就算了这样说的）， 业务场景三：输入手机号和开始年月，结束年月，查询用户通话记录的主叫被叫实名（利用hiveSQL）业务场景是吧人员信息放到一个简单的关系型数据库中，也就是人员信息在公安部的信息中，然后实现一个和关系型数据库的交互查询 。 查询电话号码最近通话记录通过完成hive到hbase表的映射，实现对最近通话信息的查询，从现有手段hbase查询，首先rowkey没法确定。rowkey是手机号+年份+月份。因为每个月份的哈希code都不一样，所以只能一个月一个月查。很麻烦。但是我们可以通过hive里面的max()聚集函数查询，借助于mr，也就是用Hive查询。通过hive操纵hbase里面的表，创建一个外部表映射到hbase上去通过Hive的聚集函数通过max min count等聚集函数来查询。 由于hive操作是通过hive和beeline来操作。hive客户端只能本地使用并且不能并发，所以使用hiveserver2服务通过beeline.sh这个脚本，㑨10000，走的是thrift服务器。而ssm是通过ssm里面的service通过jdbc和hiveserver2交互，hiveserver2找到hive外部表并且操作这个是走的jdbc可以实现远程访问。 那么怎样在hive里面查询最近通话详单呢：（最近一条记录）$hive&gt;select * from ext_calllogs_in_hbase where id like ‘%xxxx%’ order by callTime desc limit 1 ; 业务场景四：查询用户各个月份的通话次数并且以echart柱状图展示（利用HiveSQL）hive中查询某个人一年的通话记录按月份进行分组： select count(*) , substr(calltime,1,6) from ext_calllogs_in_hbase where caller = ‘15032293356’ and substr(calltime,1,4) == ‘2017’ group by substr(calltime,1,6) ;]]></content>
  </entry>
  <entry>
    <title><![CDATA[实战大数据电信项目（四）]]></title>
    <url>%2F2019%2F01%2F11%2F%E5%AE%9E%E6%88%98%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%94%B5%E4%BF%A1%E9%A1%B9%E7%9B%AE%EF%BC%88%E5%9B%9B%EF%BC%89%2F</url>
    <content type="text"><![CDATA[hive统计某个人的通话次数：数据在kafka最多是7天时间。hive中查询某个人一年的通话记录按月份进行分组：select count(*) , substr(calltime,1,6) from ext_calllogs_in_hbase where caller = ‘15032293356’ and substr(calltime,1,4) == ‘2017’ group by substr(calltime,1,6) ; 要写hiveservice，在里面添加 1.HiveCallLogService.java /** * 查询指定人员指定年份中各个月份的通话次数 */ public List&lt;CallLogStat&gt; statCallLogsCount(String caller, String year){ List&lt;CallLogStat&gt; list = new ArrayList&lt;CallLogStat&gt;() ; try { Connection conn = DriverManager.getConnection(url); Statement st = conn.createStatement(); String sql = &quot;select count(*) ,substr(calltime,1,6) from ext_calllogs_in_hbase &quot; + &quot;where caller = &apos;&quot; + caller+&quot;&apos; and substr(calltime,1,4) == &apos;&quot; + year + &quot;&apos; group by substr(calltime,1,6);&quot;; ResultSet rs = st.executeQuery(sql); CallLog log = null; while (rs.next()) { CallLogStat logSt = new CallLogStat(); logSt.setCount(rs.getInt(1)); logSt.setYearMonth(rs.getString(2)); list.add(logSt); } rs.close(); return list; } catch (Exception e) { e.printStackTrace(); } return null; } 2.CallLogController.java /** * 统计指定人员，指定月份的通话次数 */ @RequestMapping(&quot;/callLog/toStatCallLog&quot;) public String toStatCallLog(){ return &quot;callLog/statCallLog&quot; ; } /** * 统计指定人员，指定月份的通话次数 */ @RequestMapping(&quot;/callLog/statCallLog&quot;) public String statCallLog(Model m ,@RequestParam(&quot;caller&quot;) String caller ,@RequestParam(&quot;year&quot;) String year){ List&lt;CallLogStat&gt; list = hcs.statCallLogsCount(caller, year); m.addAttribute(&quot;stat&quot; , list) ; return &quot;callLog/statCallLog&quot; ; } 3.statCallLog.jsp &lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt; &lt;%@ taglib uri=&quot;http://java.sun.com/jsp/jstl/core&quot; prefix=&quot;c&quot; %&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;通话记录统计结果&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;../css/my.css&quot;&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;../js/jquery-3.2.0.min.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot;&gt; //定义函数 function refreshTable(){ $(&quot;#t1 tbody&quot;).empty(); $.getJSON(&quot;/callLog/json/findAll&quot;, function (data) { $.each(data, function (i, obj) { var str = &quot;&lt;tr&gt;&lt;td&gt;&quot; + obj.caller + &quot;&lt;/td&gt;&quot;; str = str + &quot;&lt;td&gt; &quot; + obj.callerName + &quot;&lt;/td&gt;&quot;; str = str + &quot;&lt;td&gt; &quot; + obj.callee + &quot;&lt;/td&gt;&quot;; str = str + &quot;&lt;td&gt; &quot; + obj.calleeName + &quot;&lt;/td&gt;&quot;; str = str + &quot;&lt;td&gt;&lt;/td&gt;&quot;; str = str + &quot;&lt;td&gt; &quot; + obj.callTime + &quot;&lt;/td&gt;&quot;; str = str + &quot;&lt;td&gt; &quot; + obj.callDuration + &quot;&lt;/td&gt;&quot;; str = str + &quot;&lt;/tr&gt;&quot;; $(&quot;#t1 tbody&quot;).append(str); }); }); } $(function(){ setInterval(refreshTable, 2000); }) &lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;form action=&apos;&lt;c:url value=&quot;/callLog/statCallLog&quot; /&gt;&apos; method=&quot;post&quot;&gt; 电话号码 : &lt;input type=&quot;text&quot; name=&quot;caller&quot;&gt;&lt;br&gt; 年 份: &lt;input type=&quot;text&quot; name=&quot;year&quot;&gt;&lt;br&gt; &lt;input type=&quot;submit&quot; name=&quot;查询&quot;&gt; &lt;/form&gt; &lt;br&gt; &lt;table id=&quot;t1&quot; border=&quot;1px&quot; class=&quot;t-1&quot; style=&quot;width: 800px&quot;&gt; &lt;thead&gt; &lt;tr&gt; &lt;td&gt;月份&lt;/td&gt; &lt;td&gt;次数&lt;/td&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;c:forEach items=&quot;${stat}&quot; var=&quot;s&quot;&gt; &lt;tr&gt; &lt;td&gt;&lt;c:out value=&quot;${s.yearMonth}&quot;/&gt;&lt;/td&gt; &lt;td&gt;&lt;c:out value=&quot;${s.count}&quot;/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/c:forEach&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;/body&gt; &lt;/html&gt; 做了一个xcall.sh和xkill脚本[xkill.sh] #!/bin/bash pids=`jps | grep $1 | awk &apos;{print $1}&apos;` for pid in $pids ; do kill -9 $pid done [xcall.sh] #!/bin/bash params=$@ i=201 for (( i=201 ; i &lt;= 206 ; i = $i + 1 )) ; do tput setaf 2 echo ============= s$i ============= tput setaf 7 ssh -4 s$i &quot;source /etc/profile ; $params&quot; done //开启kafka集群[/usr/local/bin/xkafka-cluster-start.sh] #!/bin/bash servsers=&quot;s202 s203 s204&quot; for s in $servers ; do ssh $s &quot;source /etc/profile ; kafka-server-start.sh -daemon /soft/kakfa/config/server.properties&quot; done //启动zk集群[/usr/local/bin/xzk-cluster-start.sh] #!/bin/bash servers=&quot;s201 s202 s203&quot; for s in $servers ; do ssh $s &quot;source /etc/profile ; zkServer.sh start&quot; done //xconsumer-start.sh[/usr/local/bin/xconsumer-start.sh] #!/bin/bash cd /home/centos/KafkaHbaseConsumer run.sh &amp; //s201:xflume-calllog-start.sh[/usr/local/bin/xconsumer-start.sh] #!/bin/bash cd /soft/flume/conf flume-ng agent -f calllog.conf -n a1 &amp; 查询所有用户的各个月份的通话次数 使用echart实现数据可视化 业务场景：根据电话号码，年份实现每个月的电话次数以echar可视化的图标展示通过在service连接hive服务器，执行Hivesql查询查询到的内容，通过controller层调用返回的内容传到前台。前台jsp界面继承echart和c标签库展示后台查询的内容 在集群中安装ganglia监控集群CPU内存进程监控fulme kafka gendata数据生成进程 ganglia集群监控. 不仅能够监控单个主机的资源情况，还可以对集群整个资源进行统计。 gmond //在每个节点收集资源数据的。 gmetad //接受每个节点发送资源数据 gweb //webui,展示数据web程序，和gmetad通信。 安装ganglia1.ganglia-gmond 所有节点。 $&gt;sudo yum install -y ganglia-gmond 2.ganglia-gmetad s201 $&gt;sudo yum install -y ganglia-gmetad 3.ganglia-gweb [s201] a)安装依赖 $&gt;sudo yum install -y httpd php b)下载ganglia-web-3.5.12.tar.gz程序 wget http://ncu.dl.sourceforge.net/project/ganglia/ganglia-web/3.5.12/ganglia-web-3.5.12.tar.gz c)tar开文件 d)修改Makefile文件，执行编译命令sudo make install ... e)启动服务 [s201] $&gt;sudo service httpd start $&gt;sudo service gmetad start $&gt;sudo service gmond start [s202] $&gt;sudo service gmond start]]></content>
  </entry>
  <entry>
    <title><![CDATA[实战大数据电信项目（三）]]></title>
    <url>%2F2019%2F01%2F11%2F%E5%AE%9E%E6%88%98%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%94%B5%E4%BF%A1%E9%A1%B9%E7%9B%AE%EF%BC%88%E4%B8%89%EF%BC%89%2F</url>
    <content type="text"><![CDATA[前段请求和hive交互，在hive之中创建外部表，映射到Hbase当中一般hive在远端通过jdbc方式来交互，要想走jdbc协议hive还需要打开hiveserver2，其实就是启动thrift服务器，端口是10000.而ssm是在controller里面，交互service在SSM里面得有一个service。service通过jdbc协议和hive server2交互，转换hive语句，在通过hbase交互。 为什么要用到Hive映射到hbase表： 要想查询用户最近通话信息 ##，从现有手段hbase查询，首先rowkey没法确定。rowkey是手机号+年份+月份。因为每个月份的哈希code都不一样，所以只能一个月一个月查。很麻烦。但是我们可以通过hive里面的max()聚集函数查询，借助于mr，也就是用Hive查询。通过hive操纵hbase里面的表，创建一个外部表映射到hbase上去通过Hive的聚集函数通过max min count等聚集函数来查询。 由于hive操作是通过hive和beeline来操作。hive客户端只能本地使用并且不能并发，所以使用hiveserver2服务通过beeline.sh这个脚本，㑨10000，走的是thrift服务器。而ssm是通过ssm里面的service通过jdbc和hiveserver2交互，hiveserver2找到hive外部表并且操作这个是走的jdbc可以实现远程访问。 那么怎样在hive里面查询最近通话详单呢：（最近一条记录）$hive&gt;select * from ext_calllogs_in_hbase where id like ‘%xxxx%’ order by callTime desc limit 1 ; 1.SSm中创建service做一个hive的聚集表的查询，查询最近的通话记录进行Mr查询。加一个pom的hive驱动 @Service(“hiveCallLogService”) public class HiveCallLogService { //hiveserver2连接串 private static String url = &quot;jdbc:hive2://s201:10000/&quot; ; //驱动程序类 private static String driverClass = &quot;org.apache.hive.jdbc.HiveDriver&quot; ; static{ try { Class.forName(driverClass); } catch (Exception e) { e.printStackTrace(); } } /** * 查询最近的通话记录,使用hive进行mr查询. */ public CallLog findLatestCallLog(String phoneNum){ try { Connection conn = DriverManager.getConnection(url); Statement st = conn.createStatement(); String sql = &quot;select * from ext_calllogs_in_hbase where id like &apos;%&quot;+ phoneNum+&quot;%&apos; order by callTime desc limit 1&quot; ; ResultSet rs = st.executeQuery(sql); CallLog log = null ; if(rs.next()){ log = new CallLog(); log.setCaller(rs.getString(&quot;caller&quot;)); log.setCallee(rs.getString(&quot;caller&quot;)); log.setCallTime(rs.getString(&quot;callTime&quot;)); log.setCallDuration(rs.getString(&quot;callDuration&quot;)); } rs.close(); return log ; } catch (Exception e) { e.printStackTrace(); } return null ; } } 在controller层添加内容： /** * 查询最近通话记录 */ @RequestMapping(value = &quot;/callLog/findLatestCallLog&quot;,method = RequestMethod.POST) public String findLatestCallLog(Model m , @RequestParam(&quot;caller&quot;) String caller){ CallLog log = hcs.findLatestCallLog(caller); if(log != null){ m.addAttribute(&quot;log&quot;, log); } return &quot;callLog/latestCallLog&quot; ; } /** * 查询最近通话记录 */ @RequestMapping(value = &quot;/callLog/toFindLatestCallLog&quot;) public String toFindLatestCallLog(){ return &quot;callLog/findLatestCallLog&quot; ; } } jsp界面编写： &lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt; &lt;%@ taglib uri=&quot;http://java.sun.com/jsp/jstl/core&quot; prefix=&quot;c&quot; %&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;通话记录&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;../css/my.css&quot;&gt; &lt;/head&gt; &lt;body&gt; &lt;c:if test=&quot;${log == null}&quot;&gt; 无记录！ &lt;/c:if&gt; &lt;c:if test=&quot;${log != null}&quot;&gt; &lt;table id=&quot;t1&quot; border=&quot;1px&quot; class=&quot;t-1&quot; style=&quot;width: 800px&quot;&gt; &lt;tr&gt; &lt;td&gt;电话1&lt;/td&gt; &lt;td&gt;&lt;c:out value=&quot;${log.caller}&quot; /&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;电话2&lt;/td&gt; &lt;td&gt;&lt;c:out value=&quot;${log.callee}&quot;/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;时间&lt;/td&gt; &lt;td&gt;&lt;c:out value=&quot;${log.callTime}&quot;/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;时长&lt;/td&gt; &lt;td&gt;&lt;c:out value=&quot;${log.callDuration}&quot;/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/c:if&gt; &lt;/body&gt; &lt;/html&gt; 在中间有一个报错问题：解决办法是更新继承的tomcat倒9。0版本。然后hive的依赖百年城1.2.1 然后做一个增加人员信息的内容：然后完成一个人员信息查询在界面显示的内容业务场景是吧人员信息放到一个简单的关系型数据库中，也就是人员信息在公安部的信息中，然后实现一个和关系型数据库的交互查询 1.建表 create table persons(id ...) ; 2.domain public class Person { private Integer id ; private String name ; private String phone ; ... } 3.dao 4.service 5. //在查询之中集成MYSQL的查询到的内容，整理一下下就是写mapper的SQL语句，在DAO层getSqlSession().selectOne(“persons.selectNameByPhone”,phone)通过这句话来返回，然后service层调用这个方法，然后在到personserviceimpl中调用这个方法，然后在calllogseriviceimpl中调用这个方法返回内容填充到实体类里面，controller层调用这些方法然后在jsp文件里面进行一个内容的填充。—————— MR运行参数配置，关闭物理内存和虚拟内存对容器的限制默认限制是开启的，最多分配给容器8G的物理内存，虚拟内存是物理内存的2.1倍。 [yarn-site.xml] &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;8192&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt; &lt;value&gt;2.1&lt;/value&gt; &lt;/property&gt; 0310然后实现一个生成带名字的数据源的代码。然后要做一个局部刷新的fastjson。0311实现一个通过服务器返回给客户端json格式的内容，通过集成jQuery实现ajax访问，动态刷新通话记录。 启动顺序，先启动kafka，启动kafka消费者，启动flume，启动ssm，启动日志生成程序。.flume-ng agent -f calllog.conf -n a1在s201中kafka-server-start.sh -daemon /soft/kafka/config/server.properties启动3个启动kafka消费者：kafka-console-consumer.sh –zookeeper s202:2181 –topic calllog开启hadoop集群和hbase集群 真机使用：10个kafka3台zk我们是1，2，3是zk。3台zk里面分配好myid。配置好zoo.cfg里面配置好3台主机的Ip地址kafka配置配置kafka/config/server.properties 。kafka依赖于zk在kafka的配置文件里面配置zk的ip单独安装flume配置配置文件。配置source channel sink一边生成日志一边搜集。 协处理器:批处理。1.类似于触发器。 完成被叫日志的写入过程。 2.重写postPut()/postGetOp()/postScannNext(); put / get / scann 直接返回主叫。 按时间段查询通话记录hashcode //确定分区。100 用户最近的通话信息hbase:rowkey max()聚集函数。 mr:hiveMapReduce. 用户最近的通话信息1.启动hadoop的yarn集群 [s201] $&gt;start-yarn.sh [s206] $&gt;yarn-daemon.sh start resourcemanager [验证] http://s201:8088/ 2.初始化hive $&gt;cd /soft/hive/bin $&gt;./schemaTool -dbType mysql -initSchema $&gt;hive //进入hive的shell $hive&gt;create database mydb ; $hive&gt;use mydb ; $hive&gt;create external table ext_calllogs_in_hbase(id string, caller string,callTime string,callee string,callDuration string) STORED BY &apos;org.apache.hadoop.hive.hbase.HBaseStorageHandler&apos; WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,f1:caller,f1:callTime,f1:callee,f1:callDuration&quot;) TBLPROPERTIES (&quot;hbase.table.name&quot; = &quot;ns1:calllogs&quot;); $hive&gt;select * from ext_calllogs_in_hbase where id like &apos;%xxxx%&apos; order by callTime desc limit 1 ; $hive&gt;select * from ext_calllogs_in_hbase where callTime = (select max(tt.callTime) from ext_calllogs_in_hbase tt where tt.id like &apos;%xxx%&apos;); 3.ssm中创建service，查询hive表中数据。 a.增加依赖 [pom.xml] &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt; &lt;version&gt;1.2.1&lt;/version&gt; &lt;/dependency&gt; b.编写类 package com.it18zhang.ssm.hive; import com.it18zhang.ssm.domain.CallLog; import java.sql.Connection; import java.sql.DriverManager; /** * Created by Administrator on 2017/4/14. */ public class HiveCallLogService { //hiveserver2连接串 private static String url = &quot;jdbc:hive2://s201:10000/mydb&quot; ; //驱动程序类 private static String driverClass = &quot;org.apache.hive.jdbc.HiveDriver&quot; ; static{ try { Class.forName(driverClass); } catch (Exception e) { e.printStackTrace(); } } /** * 查询最近的通话记录,使用hive进行mr查询. */ public CallLog findLatestCallLog(){ try { Connection conn = DriverManager.getConnection(url); System.out.println(conn); } catch (Exception e) { e.printStackTrace(); } return null ; } } c.启动hiveserver2服务器 $&gt;hive/bin/hiveserver2 &amp; d.验证hiveserver2端口 $&gt;netstat -anop | grep 10000 4.测试类 package com.it18zhang.ssm.hive; import com.it18zhang.ssm.domain.CallLog; import org.apache.hadoop.hbase.client.Result; import java.sql.Connection; import java.sql.DriverManager; import java.sql.ResultSet; import java.sql.Statement; /** * */ public class HiveCallLogService { //hiveserver2连接串 private static String url = &quot;jdbc:hive2://s201:10000/&quot; ; //驱动程序类 private static String driverClass = &quot;org.apache.hive.jdbc.HiveDriver&quot; ; static{ try { Class.forName(driverClass); } catch (Exception e) { e.printStackTrace(); } } /** * 查询最近的通话记录,使用hive进行mr查询. */ public CallLog findLatestCallLog(){ try { Connection conn = DriverManager.getConnection(url); Statement st = conn.createStatement(); ResultSet rs = st.executeQuery(&quot;select * from ext_calllogs_in_hbase&quot;); while(rs.next()){ String id = rs.getString(&quot;id&quot;); String caller = rs.getString(&quot;caller&quot;); String callee = rs.getString(&quot;callee&quot;); String callTime = rs.getString(&quot;callTime&quot;); String callDuration = rs.getString(&quot;callDuration&quot;); System.out.println(id + &quot; : &quot; + caller); } rs.close(); System.out.println(conn); } catch (Exception e) { e.printStackTrace(); } return null ; } } 5.注意事项 SSM集成hive-jdbc访问hive的hiveserver2时，需要如下处理: 5.1)使用hive-jdbc-1.2.1的依赖版本 &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt; &lt;version&gt;1.2.1&lt;/version&gt; &lt;/dependency&gt; 5.5)需要集成apache-tomcat-9.0.0.M19版本，否则报编译器错误。 添加人员信息1.建表 create table persons(id ...) ; 2.domain public class Person { private Integer id ; private String name ; private String phone ; ... } 3.dao 4.service 5. MR运行参数配置，关闭物理内存和虚拟内存对容器的限制默认限制是开启的，最多分配给容器8G的物理内存，虚拟内存是物理内存的2.1倍。 [yarn-site.xml] &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;8192&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt; &lt;value&gt;2.1&lt;/value&gt; &lt;/property&gt; 实现局部实时刷新通话记录的功能1.引入pom.xml &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.24&lt;/version&gt; &lt;/dependency&gt; 2.编写Controller，增加方法 @RequestMapping(&quot;/callLog/json/findAll&quot;) public String findAllJson(HttpServletResponse response) { List&lt;CallLog&gt; list = cs.findAll(); String json = JSON.toJSONString(list); //内容类型 response.setContentType(&quot;application/json&quot;); try { OutputStream out = response.getOutputStream(); out.write(json.getBytes()); out.flush(); out.close(); } catch (IOException e) { e.printStackTrace(); } return null; } 1.启动顺序 a)1.zookeeper b)2.hadoop c)3.hbase d)4.kakfa e)5.HbaseConsumer f)6.flume g)7.web程序 h)8.数据生成程序.2.3.4.5.]]></content>
  </entry>
  <entry>
    <title><![CDATA[实战大数据电信项目（二）]]></title>
    <url>%2F2019%2F01%2F11%2F%E5%AE%9E%E6%88%98%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%94%B5%E4%BF%A1%E9%A1%B9%E7%9B%AE%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[按照时间段查询通话记录： 1.按照时间段查询通话记录，设置startRow+endRow。2.web部分设置一个电话号码输入起始时间输入结束时间输入，然后在SSMweb层controller设置一个查询使用hbaseapi。重点在月份值取出来哈希构造rowkey。 设置一个表单的查询的jsp界面，action是动态嵌套打印的用url标识库，提交给/callLog/finCallLog通过Post方式，返回输入值起始时间和结束时间到controller层接受参数， 传值到controller层之后，controller层拿到了参数，拿到starttime callerid endtie。 对起始日期和结束日期进行一个calendar.getinstance()，日历这个可以计算天数月份数，有多少东西都可以算出来。 编程：在得到的起始时间和结束时间对2个时间点做操作：得到一个结束的endpoint=开始的前6个数字是年和月+最后2个数字是天+1。 如果是同年月，起始点就是起始串，结束点是天+1的endpoint。 public static List getCallLogRanges(String startStr ,String endStr){ try{ SimpleDateFormat sdfYMD = new SimpleDateFormat(“yyyyMMdd”); SimpleDateFormat sdfYM = new SimpleDateFormat(“yyyyMM”); DecimalFormat df00 = new DecimalFormat(“00”); // List&lt;CallLogRange&gt; list = new ArrayList&lt;CallLogRange&gt;(); //字符串时间 String startPrefix = startStr.substring(0, 6); String endPrefix = endStr.substring(0, 6); int endDay = Integer.parseInt(endStr.substring(6, 8)); //结束点 String endPoint = endPrefix + df00.format(endDay + 1); //日历对象 Calendar c = Calendar.getInstance(); //同年月 if (startPrefix.equals(endPrefix)) { CallLogRange range = new CallLogRange(); range.setStartPoint(startStr); //设置起始点 range.setEndPoint(endPoint); //设置结束点 list.add(range); } else { //1.起始月 CallLogRange range = new CallLogRange(); range.setStartPoint(startStr); //设置日历的时间对象 c.setTime(sdfYMD.parse(startStr)); c.add(Calendar.MONTH, 1); range.setEndPoint(sdfYM.format(c.getTime())); list.add(range); //是否是最后一月 while (true) { //到了结束月份 if (endStr.startsWith(sdfYM.format(c.getTime()))) { range = new CallLogRange(); range.setStartPoint(sdfYM.format(c.getTime())); range.setEndPoint(endPoint); list.add(range); break; } else { range = new CallLogRange(); //起始时间 range.setStartPoint(sdfYM.format(c.getTime())); //增加月份 c.add(Calendar.MONTH, 1); range.setEndPoint(sdfYM.format(c.getTime())); list.add(range); } } } return list ; } catch(Exception e){ e.printStackTrace(); } return null ; } /** * 对时间进行格式化 */ public static String formatDate(String timeStr){ try { return sdfFriend.format(sdf.parse(timeStr)); } catch (Exception e) { e.printStackTrace(); } return null ; } } 上图是工具类时间的代码。 下面代码是service层的具体实现内容： /** * 按照范围查询通话记录 */ public List&lt;CallLog&gt; findCallogs(String call , List&lt;CallLogRange&gt; ranges){ List&lt;CallLog&gt; logs = new ArrayList&lt;CallLog&gt;(); try { for(CallLogRange range : ranges){ Scan scan = new Scan(); //设置扫描起始行 scan.setStartRow(Bytes.toBytes(CallLogUtil.getStartRowkey(call, range.getStartPoint(),100))); //设置扫描结束行 scan.setStopRow(Bytes.toBytes(CallLogUtil.getStopRowkey(call, range.getStartPoint(), range.getEndPoint(),100))); ResultScanner rs = table.getScanner(scan); Iterator&lt;Result&gt; it = rs.iterator(); byte[] f = Bytes.toBytes(&quot;f1&quot;); byte[] caller = Bytes.toBytes(&quot;caller&quot;); byte[] callee = Bytes.toBytes(&quot;callee&quot;); byte[] callTime = Bytes.toBytes(&quot;callTime&quot;); byte[] callDuration = Bytes.toBytes(&quot;callDuration&quot;); CallLog log = null; while (it.hasNext()) { log = new CallLog(); Result r = it.next(); //rowkey String rowkey = Bytes.toString(r.getRow()); String flag = rowkey.split(&quot;,&quot;)[3] ; log.setFlag(flag.equals(&quot;0&quot;)?true:false); //caller log.setCaller(Bytes.toString(r.getValue(f, caller))); //callee log.setCallee(Bytes.toString(r.getValue(f, callee))); //callTime log.setCallTime(Bytes.toString(r.getValue(f, callTime))); //callDuration log.setCallDuration(Bytes.toString(r.getValue(f, callDuration))); logs.add(log); } } return logs; } catch (Exception e) { e.printStackTrace(); } return null; } } 在查询的controller层里面写调用工具类里面对时间进行格式化。格式化之后的List集合作为参数传入到service层的findCallogs（）里面查询到hbase内容集合logs，将logs传入Model.addAttribute里面（向模型中传入数据也就是让前台可以调用）。下面是controller层的内容： /** * 进入查询通话记录的页面,form */ @RequestMapping(&quot;/callLog/toFindCallLogPage&quot;) public String toFindCallLogPage(){ return &quot;callLog/findCallLog&quot; ; } @RequestMapping(value = &quot;/callLog/findCallLog&quot;,method = RequestMethod.POST) public String findCallLog(Model m , @RequestParam(&quot;caller&quot;) String caller, @RequestParam(&quot;startTime&quot;) String startTime, @RequestParam(&quot;endTime&quot;) String endTime){ List&lt;CallLogRange&gt; list = CallLogUtil.getCallLogRanges(startTime, endTime); List&lt;CallLog&gt; logs = cs.findCallogs(caller,list); m.addAttribute(&quot;callLogs&quot;, logs); return &quot;callLog/callLogList&quot; ; } } 上述就完成了主叫查询功能。但是查询的只有主叫功能，没有被叫功能，我们在编写一个协处理器处理被叫查询功能： /** * Put后处理 */ public void postPut(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Put put, WALEdit edit, Durability durability) throws IOException { super.postPut(e, put, edit, durability); // String tableName0 = TableName.valueOf(CALL_LOG_TABLE_NAME).getNameAsString(); //得到当前的TableName对象 String tableName1 = e.getEnvironment().getRegion().getRegionInfo().getTable().getNameAsString(); //判断是否是ns1:calllogs表 if (!tableName0.equals(tableName1)) { return; } //得到主叫的rowkey, String rowkey = Bytes.toString(put.getRow()); //如果被叫就放行 String[] arr = rowkey.split(&quot;,&quot;); if (arr[3].equals(&quot;1&quot;)) { return; } //hashcode,caller,time,flag,callee,duration String caller = arr[1] ; //主叫 String callTime = arr[2] ; //通话时间 String callee = arr[4] ; //被叫 String callDuration = arr[5] ; //通话时长 //被叫hashcode String hashcode = CallLogUtil.getHashcode(callee,callTime,100); //被叫rowkey String calleeRowKey = hashcode + &quot;,&quot; + callee + &quot;,&quot; + callTime + &quot;,1,&quot; + caller + &quot;,&quot; + callDuration; Put newPut = new Put(Bytes.toBytes(calleeRowKey)); newPut.addColumn(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(REF_ROW_ID), Bytes.toBytes(rowkey)); TableName tn = TableName.valueOf(CALL_LOG_TABLE_NAME); Table t = e.getEnvironment().getTable(tn); t.put(newPut); } 也就是创建协处理器，在postput的方法下（这个方法本来是用于在添加一条主叫信息后在添加一条被叫信息）重写之后，在添加主叫信息后，在添加一条被叫信息，该被叫信息的value值插入到f2列族，refrowid列。然后value值就是 原来主叫的rowkey,这个也就是二级索引的思想，避免了原来的主叫的value重复存储，减少了冗余。所以是重写postput方法。 在没有重写postgetOp()方法的时候返回的是这种情况： 发现里面是被叫的时候没有电话1和电话2为什么呢？因为在检索时候的是这样子写的：检索填充的值是通过getvalue方法，而通过协处理器填进去的是一个被叫信息是控制，所以get不到value信息。 但是查询的时候，还是不能查询到主叫，所以重写检索方法,在协处理器中重写 postscannerNext（）方法。完成被叫查询返回主叫的rowkey值。 /** * */ public boolean postScannerNext(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, InternalScanner s, List&lt;Result&gt; results, int limit, boolean hasMore) throws IOException { boolean b = super.postScannerNext(e, s, results, limit, hasMore); //新集合 List&lt;Result&gt; newList = new ArrayList&lt;Result&gt;(); //获得表名 String tableName = e.getEnvironment().getRegion().getRegionInfo().getTable().getNameAsString(); //判断表名是否是ns1:calllogs if (tableName.equals(CALL_LOG_TABLE_NAME)) { Table tt = e.getEnvironment().getTable(TableName.valueOf(CALL_LOG_TABLE_NAME)); for(Result r : results){ //rowkey String rowkey = Bytes.toString(r.getRow()); String flag = rowkey.split(&quot;,&quot;)[3] ; //主叫 if(flag.equals(&quot;0&quot;)){ newList.add(r) ; } //被叫 else{ //取出主叫号码 byte[] refrowkey = r.getValue(Bytes.toBytes(&quot;f2&quot;),Bytes.toBytes(REF_ROW_ID)) ; Get newGet = new Get(refrowkey); newList.add(tt.get(newGet)); } } results.clear(); results.addAll(newList); } return b ; } } 1.hbase交互 Scan : 设置startRow + endRow rowkey : hashcode , + callerid , + callTime, 0 , callee , duration. 2.3. 编写CallLogController.java/** * 进入查询通话记录的页面,form */ @RequestMapping(&quot;/callLog/toFindCallLogPage&quot;) public String toFindCallLogPage(){ return &quot;callLog/findCallLog&quot; ; } @RequestMapping(value = &quot;/callLog/findCallLog&quot;,method = RequestMethod.POST) public String findCallLog(@RequestParam(&quot;caller&quot;) String caller, @RequestParam(&quot;startTime&quot;) String startTime, @RequestParam(&quot;endTime&quot;) String endTime){ Calendar startCalendar = Calendar.getInstance(); Calendar endCalendar = Calendar.getInstance(); return &quot;callLog/callLogList&quot; ; } 编写jsp&lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt; &lt;%@ taglib uri=&quot;http://java.sun.com/jsp/jstl/core&quot; prefix=&quot;c&quot; %&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;查询通话记录&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;../css/my.css&quot;&gt; &lt;/head&gt; &lt;body&gt; &lt;form action=&apos;&lt;c:url value=&quot;/callLog/findCallLog&quot; /&gt;&apos; method=&quot;post&quot;&gt; &lt;table&gt; &lt;tr&gt; &lt;td&gt;电话号码 :&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;text&quot; name=&quot;caller&quot;&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;起始时间 :&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;text&quot; name=&quot;startTime&quot;&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;结束时间:&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;text&quot; name=&quot;endTime&quot;&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td colspan=&quot;2&quot;&gt; &lt;input type=&quot;submit&quot; value=&quot;查询&quot;/&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/form&gt; &lt;/body&gt; &lt;/html&gt; 实现时间段查询1.提取时间范围 [CallLogUtil.java] /** * 起始时间 */ public static String getStartRowkey(String caller, String startTime, int partitions){ String hashcode = getHashcode(caller, startTime,partitions); return hashcode + &quot;,&quot; + caller + &quot;,&quot; + startTime ; } /** * 结束时间 */ public static String getStopRowkey(String caller, String startTime,String endTime, int partitions){ String hashcode = getHashcode(caller, startTime,partitions); return hashcode + &quot;,&quot; + caller + &quot;,&quot; + endTime ; } /** * 计算查询时间范围 */ public static List&lt;CallLogRange&gt; getCallLogRanges(String startStr ,String endStr){ try{ SimpleDateFormat sdfYMD = new SimpleDateFormat(&quot;yyyyMMdd&quot;); SimpleDateFormat sdfYM = new SimpleDateFormat(&quot;yyyyMM&quot;); DecimalFormat df00 = new DecimalFormat(&quot;00&quot;); // List&lt;CallLogRange&gt; list = new ArrayList&lt;CallLogRange&gt;(); //字符串时间 String startPrefix = startStr.substring(0, 6); String endPrefix = endStr.substring(0, 6); int endDay = Integer.parseInt(endStr.substring(6, 8)); //结束点 String endPoint = endPrefix + df00.format(endDay + 1); //日历对象 Calendar c = Calendar.getInstance(); //同年月 if (startPrefix.equals(endPrefix)) { CallLogRange range = new CallLogRange(); range.setStartPoint(startStr); //设置起始点 range.setEndPoint(endPoint); //设置结束点 list.add(range); } else { //1.起始月 CallLogRange range = new CallLogRange(); range.setStartPoint(startStr); //设置日历的时间对象 c.setTime(sdfYMD.parse(startStr)); c.add(Calendar.MONTH, 1); range.setEndPoint(sdfYM.format(c.getTime())); list.add(range); //是否是最后一月 while (true) { //到了结束月份 if (endStr.startsWith(sdfYM.format(c.getTime()))) { range = new CallLogRange(); range.setStartPoint(sdfYM.format(c.getTime())); range.setEndPoint(endPoint); list.add(range); break; } else { range = new CallLogRange(); //起始时间 range.setStartPoint(sdfYM.format(c.getTime())); //增加月份 c.add(Calendar.MONTH, 1); range.setEndPoint(sdfYM.format(c.getTime())); list.add(range); } } } return list ; } catch(Exception e){ e.printStackTrace(); } return null ; } 2.编写service. 3. 4. 实现hbase的协处理器0.说明 HBaseConsumer put的数据都是主叫，被叫数据在Coprossor中完成。 1.创建协处理器 package com.it18zhang.calllog.coprossor; import org.apache.hadoop.hbase.TableName; import org.apache.hadoop.hbase.client.Durability; import org.apache.hadoop.hbase.client.Put; import org.apache.hadoop.hbase.client.Table; import org.apache.hadoop.hbase.coprocessor.BaseRegionObserver; import org.apache.hadoop.hbase.coprocessor.ObserverContext; import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment; import org.apache.hadoop.hbase.regionserver.wal.WALEdit; import org.apache.hadoop.hbase.util.Bytes; import java.io.IOException; /** * 协处理器, */ public class CallLogRegionObserver extends BaseRegionObserver { /** * Put后处理 */ public void postPut(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Put put, WALEdit edit, Durability durability) throws IOException { super.postPut(e, put, edit, durability); // String tableName0 = TableName.valueOf(&quot;ns1:calllogs&quot;).getNameAsString(); //得到当前的TableName对象 String tableName1 = e.getEnvironment().getRegion().getRegionInfo().getTable().getNameAsString(); //判断是否是ns1:calllogs表 if (!tableName0.equals(tableName1)) { return; } //得到主叫的rowkey, String rowkey = Bytes.toString(put.getRow()); //如果被叫就放行 String[] arr = rowkey.split(&quot;,&quot;); if (arr[3].equals(&quot;1&quot;)) { return; } //hashcode,caller,time,flag,callee,duration String caller = arr[1] ; //主叫 String callTime = arr[2] ; //通话时间 String callee = arr[4] ; //被叫 String callDuration = arr[5] ; //通话时长 //被叫hashcode String hashcode = CallLogUtil.getHashcode(callee,callTime,100); //被叫rowkey String calleeRowKey = hashcode + &quot;,&quot; + callee + &quot;,&quot; + callTime + &quot;,1,&quot; + caller + &quot;,&quot; + callDuration; Put newPut = new Put(Bytes.toBytes(calleeRowKey)); newPut.addColumn(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;refrowid&quot;), Bytes.toBytes(rowkey)); TableName tn = TableName.valueOf(&quot;ns1:calllogs&quot;); Table t = e.getEnvironment().getTable(tn); t.put(newPut); } } 2.注册协处理器 a)导出jar包,分到集群. ... b)修改hbase配置文件并分发. [hbase-site.xml] &lt;property&gt; &lt;name&gt;hbase.coprocessor.region.classes&lt;/name&gt; &lt;value&gt;com.it18zhang.calllog.coprossor.CallLogRegionObserver&lt;/value&gt; &lt;/property&gt; c)停止hbase集群 d)重新启动 ... e)进入hbase shell,重建ns1:calllogs $hbase&gt;create &apos;ns1:calllogs&apos; , &apos;f1&apos;,&apos;f2&apos; 主叫rowkey: 12,1234,xxx,0,5678,60,.,.,.,., 被叫rowkey: f2:refrowkey 98,5678,xxxx,1,1234,60 --&gt; 12,1234,xxx,0,5678,60,. 重写RegionObserver的postGetOp方法,完成被叫查询时，直接返回主叫的记录1.重写 [CallLogRegionObserver.java] package com.it18zhang.calllog.coprossor; import org.apache.hadoop.hbase.Cell; import org.apache.hadoop.hbase.CellUtil; import org.apache.hadoop.hbase.TableName; import org.apache.hadoop.hbase.client.*; import org.apache.hadoop.hbase.coprocessor.BaseRegionObserver; import org.apache.hadoop.hbase.coprocessor.ObserverContext; import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment; import org.apache.hadoop.hbase.regionserver.InternalScanner; import org.apache.hadoop.hbase.regionserver.wal.WALEdit; import org.apache.hadoop.hbase.util.Bytes; import java.io.IOException; import java.util.ArrayList; import java.util.List; /** * 协处理器, */ public class CallLogRegionObserver extends BaseRegionObserver { //被叫引用id private static final String REF_ROW_ID = &quot;refrowid&quot; ; //通话记录表名 private static final String CALL_LOG_TABLE_NAME = &quot;ns1:calllogs&quot; ; /** * Put后处理 */ public void postPut(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Put put, WALEdit edit, Durability durability) throws IOException { super.postPut(e, put, edit, durability); // String tableName0 = TableName.valueOf(CALL_LOG_TABLE_NAME).getNameAsString(); //得到当前的TableName对象 String tableName1 = e.getEnvironment().getRegion().getRegionInfo().getTable().getNameAsString(); //判断是否是ns1:calllogs表 if (!tableName0.equals(tableName1)) { return; } //得到主叫的rowkey, String rowkey = Bytes.toString(put.getRow()); //如果被叫就放行 String[] arr = rowkey.split(&quot;,&quot;); if (arr[3].equals(&quot;1&quot;)) { return; } //hashcode,caller,time,flag,callee,duration String caller = arr[1] ; //主叫 String callTime = arr[2] ; //通话时间 String callee = arr[4] ; //被叫 String callDuration = arr[5] ; //通话时长 //被叫hashcode String hashcode = CallLogUtil.getHashcode(callee,callTime,100); //被叫rowkey String calleeRowKey = hashcode + &quot;,&quot; + callee + &quot;,&quot; + callTime + &quot;,1,&quot; + caller + &quot;,&quot; + callDuration; Put newPut = new Put(Bytes.toBytes(calleeRowKey)); newPut.addColumn(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(REF_ROW_ID), Bytes.toBytes(rowkey)); TableName tn = TableName.valueOf(CALL_LOG_TABLE_NAME); Table t = e.getEnvironment().getTable(tn); t.put(newPut); } /** * 重写方法，完成被叫查询，返回主叫结果。 */ public void postGetOp(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Get get, List&lt;Cell&gt; results) throws IOException { //获得表名 String tableName = e.getEnvironment().getRegion().getRegionInfo().getTable().getNameAsString(); //判断表名是否是ns1:calllogs if(!tableName.equals(CALL_LOG_TABLE_NAME)){ super.preGetOp(e, get, results); } else{ //得到rowkey String rowkey = Bytes.toString(get.getRow()); // String[] arr = rowkey.split(&quot;,&quot;); //主叫 if(arr[3].equals(&quot;0&quot;)){ super.postGetOp(e, get, results); } //被叫 else{ //得到主叫方的rowkey String refrowid = Bytes.toString(CellUtil.cloneValue(results.get(0))); // Table tt = e.getEnvironment().getTable(TableName.valueOf(CALL_LOG_TABLE_NAME)); Get g = new Get(Bytes.toBytes(refrowid)); Result r = tt.get(g); List&lt;Cell&gt; newList = r.listCells(); results.clear(); results.addAll(newList); } } } /** * */ public boolean postScannerNext(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, InternalScanner s, List&lt;Result&gt; results, int limit, boolean hasMore) throws IOException { boolean b = super.postScannerNext(e, s, results, limit, hasMore); //新集合 List&lt;Result&gt; newList = new ArrayList&lt;Result&gt;(); //获得表名 String tableName = e.getEnvironment().getRegion().getRegionInfo().getTable().getNameAsString(); //判断表名是否是ns1:calllogs if (tableName.equals(CALL_LOG_TABLE_NAME)) { Table tt = e.getEnvironment().getTable(TableName.valueOf(CALL_LOG_TABLE_NAME)); for(Result r : results){ //rowkey String rowkey = Bytes.toString(r.getRow()); String flag = rowkey.split(&quot;,&quot;)[3] ; //主叫 if(flag.equals(&quot;0&quot;)){ newList.add(r) ; } //被叫 else{ //取出主叫号码 byte[] refrowkey = r.getValue(Bytes.toBytes(&quot;f2&quot;),Bytes.toBytes(REF_ROW_ID)) ; Get newGet = new Get(refrowkey); newList.add(tt.get(newGet)); } } results.clear(); results.addAll(newList); } return b ; } } 1&apos;.修改jsp页面 [callLogList.jsp] &lt;c:if test=&quot;${log.caller == param.caller}&quot;&gt;主叫&lt;/c:if&gt; &lt;c:if test=&quot;${log.caller != param.caller}&quot;&gt;被叫&lt;/c:if&gt; 2.部署jar包 3.测试 ... 4.]]></content>
  </entry>
  <entry>
    <title><![CDATA[实战大数据电信项目（一）]]></title>
    <url>%2F2019%2F01%2F11%2F%E5%AE%9E%E6%88%98%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%94%B5%E4%BF%A1%E9%A1%B9%E7%9B%AE%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[0.rowkey的设计因为通过rowkey查询，吧尽可能多的数据放到rowkey里面去。 考虑的问题： 从通话记录整个来考虑，记录量比较大。让记录分散，吧主叫被叫编写进去时间编进去，就是一个rowkey的构成，如果用主叫被叫时间做rowkey的话。 每个服务器承载的数据量。hbase没经过10G就切割成2个区域，分成2个区域服务器，一条记录是1k10G是多少行可以算出来，可以设计出来区间。要知道rowkey的范围 。 如果数据都在一台服务器吧请求都发送给一台服务器，压力很大。 对于服务器来说，每个服务器应该多少数据量的问题。怎么衡量？ 对于单张表是10G就切割。切割出来的区域分成2个区域服务器上去了是10G的数据量。可以算一下一条记录是1K那么10G的数据量是多少行，根据这个法则设定空间，要知道rowkey的范围。加入1个区域存放10G要是有10个区域存放10G是100G。存放就是10G、1k=1亿数据量。 rowkey是排序的，所以排序最好是等长度的，因为如果从1-10000情况下8是大于10000的。 可以通过预切割避免切割风暴。 139。。。。137.。。。切割成区域，如果定了10个区域，要把所有的数据分散到10个区域里面去就是00-09，所以在编写rowkey里面，139，137的都有很多，所以在前面加一个前缀，也就是说盐析。 rowkey设计的长度原则。一般是定长，越短越好，不超过1个字节：因为目前都是64位系统，内存是8字节对其，控制在16字节8字节的整数倍利用了操作系统的最佳特性。 建议用rowkey的高位进行散列处理，由程序随机生成，低位放时间字段，这样提高数据均衡分布在每个regionServer来实现负载均衡。 如果不进行散列，如果用时间片编写开头。起始的Rowkey的弊端就会集中在一个regionserver上，光是交换机写入就已经爆掉hbase了。因为数据量太大了。 以电话号码编写开头的话，所有数据都在 rowkey设计原则由于rowkey是字典排序的所以要将常用的放到一个region但是又不能量太大。 热点： 总结来说就是在整个数据rowkey前面加个2位数的随机数也就是通过盐析的方式避免热点问题。2位数的随机数是通过一个hash算法实现0-100散列。分布在0-100的不通的region里面。协处理器相当于一个区域观察期，类似于zookeeper的观察者。也就是插入了一条数据他就观察的到通过该preput 和postput重写这些方法来实现在插入前和插入后的动作，和插入的时候类似，通过哈希算法实现前面的随机数。完成新的rowkey的重组。这个地方根据业务场景的需要，每次查询的时候主叫和被叫都要查询出来，所以他的设计是将两个电话的主叫和被叫同时分布在一个区域里面。使得查询的效率更加高效分布更加合理。通过在协处理器哈希算法的时候通过被叫号码+时间片来哈希，主动插入的时候通过主叫号码+时间片来哈希。就可以实现主叫和被叫一个号码的都在同一个区域里面。 在查询几月到几月的通话详单的时候，通过设置scan.startKey和scan.stopRow查询到开始日期和结束日期。打印是通过resultscanner 如果查询某一个号码作为被叫的信息，我们可以在设计一张表calleelog被叫记录日志。rowkey设计：呼叫时间吧calltime和calleeid作为rowkey的设计什么时候被叫的但是他的value就是之前设计的callerlog表的rowkey的值，然后通过被叫表的value，这个value就是主叫表的rowkey通过这个也就是通过被叫表查询到了主叫表。这个场景还有2中可能，一种是设计2张表，然后查询，要是不设计2张表的话就是通过上文说过的通过设计一个被叫表指向主叫表的rowkey来完成这个业务场景。 主叫表查询的rowkey设计：因为主要查询的时候都是通过主叫电话号码和时间来查询通过清单的，所以rowkey设计为hash,callerid,time,0,calleeid，然后value里面放一些其他诸如基站，手机信息等等其他无关紧要的信息，吧主要要查询的内容放到rowkey里面可以增加效率，直接查询rowkey就可以查询到所需要的内容。 被叫表rowkey设计：设计一个另外一个表。calleeid,callertime里面的value放主叫表的rowkey即可。是为了避免第二张表大量冗余。 1.按照时间段查询通话记录 1.hbase交互 Scan : 设置startRow + endRow rowkey : hashcode , + callerid , + callTime, 0 , callee , duration. 2.3. 编写CallLogController.java/** * 进入查询通话记录的页面,form */ @RequestMapping(&quot;/callLog/toFindCallLogPage&quot;) public String toFindCallLogPage(){ return &quot;callLog/findCallLog&quot; ; } @RequestMapping(value = &quot;/callLog/findCallLog&quot;,method = RequestMethod.POST) public String findCallLog(@RequestParam(&quot;caller&quot;) String caller, @RequestParam(&quot;startTime&quot;) String startTime, @RequestParam(&quot;endTime&quot;) String endTime){ Calendar startCalendar = Calendar.getInstance(); Calendar endCalendar = Calendar.getInstance(); return &quot;callLog/callLogList&quot; ; } 编写jsp界面&lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt; &lt;%@ taglib uri=&quot;http://java.sun.com/jsp/jstl/core&quot; prefix=&quot;c&quot; %&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;查询通话记录&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;../css/my.css&quot;&gt; &lt;/head&gt; &lt;body&gt; &lt;form action=&apos;&lt;c:url value=&quot;/callLog/findCallLog&quot; /&gt;&apos; method=&quot;post&quot;&gt; &lt;table&gt; &lt;tr&gt; &lt;td&gt;电话号码 :&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;text&quot; name=&quot;caller&quot;&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;起始时间 :&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;text&quot; name=&quot;startTime&quot;&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;结束时间:&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;text&quot; name=&quot;endTime&quot;&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td colspan=&quot;2&quot;&gt; &lt;input type=&quot;submit&quot; value=&quot;查询&quot;/&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/form&gt; &lt;/body&gt; &lt;/html&gt; 实现时间段查询1.提取时间范围 [CallLogUtil.java] /** * 起始时间 */ public static String getStartRowkey(String caller, String startTime, int partitions){ String hashcode = getHashcode(caller, startTime,partitions); return hashcode + &quot;,&quot; + caller + &quot;,&quot; + startTime ; } /** * 结束时间 */ public static String getStopRowkey(String caller, String startTime,String endTime, int partitions){ String hashcode = getHashcode(caller, startTime,partitions); return hashcode + &quot;,&quot; + caller + &quot;,&quot; + endTime ; } /** * 计算查询时间范围 */ public static List&lt;CallLogRange&gt; getCallLogRanges(String startStr ,String endStr){ try{ SimpleDateFormat sdfYMD = new SimpleDateFormat(&quot;yyyyMMdd&quot;); SimpleDateFormat sdfYM = new SimpleDateFormat(&quot;yyyyMM&quot;); DecimalFormat df00 = new DecimalFormat(&quot;00&quot;); // List&lt;CallLogRange&gt; list = new ArrayList&lt;CallLogRange&gt;(); //字符串时间 String startPrefix = startStr.substring(0, 6); String endPrefix = endStr.substring(0, 6); int endDay = Integer.parseInt(endStr.substring(6, 8)); //结束点 String endPoint = endPrefix + df00.format(endDay + 1); //日历对象 Calendar c = Calendar.getInstance(); //同年月 if (startPrefix.equals(endPrefix)) { CallLogRange range = new CallLogRange(); range.setStartPoint(startStr); //设置起始点 range.setEndPoint(endPoint); //设置结束点 list.add(range); } else { //1.起始月 CallLogRange range = new CallLogRange(); range.setStartPoint(startStr); //设置日历的时间对象 c.setTime(sdfYMD.parse(startStr)); c.add(Calendar.MONTH, 1); range.setEndPoint(sdfYM.format(c.getTime())); list.add(range); //是否是最后一月 while (true) { //到了结束月份 if (endStr.startsWith(sdfYM.format(c.getTime()))) { range = new CallLogRange(); range.setStartPoint(sdfYM.format(c.getTime())); range.setEndPoint(endPoint); list.add(range); break; } else { range = new CallLogRange(); //起始时间 range.setStartPoint(sdfYM.format(c.getTime())); //增加月份 c.add(Calendar.MONTH, 1); range.setEndPoint(sdfYM.format(c.getTime())); list.add(range); } } } return list ; } catch(Exception e){ e.printStackTrace(); } return null ; } 2.编写service. 3. 4. 实现hbase的协处理器0.说明 HBaseConsumer put的数据都是主叫，被叫数据在Coprossor中完成。 1.创建协处理器 package com.it18zhang.calllog.coprossor; import org.apache.hadoop.hbase.TableName; import org.apache.hadoop.hbase.client.Durability; import org.apache.hadoop.hbase.client.Put; import org.apache.hadoop.hbase.client.Table; import org.apache.hadoop.hbase.coprocessor.BaseRegionObserver; import org.apache.hadoop.hbase.coprocessor.ObserverContext; import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment; import org.apache.hadoop.hbase.regionserver.wal.WALEdit; import org.apache.hadoop.hbase.util.Bytes; import java.io.IOException; /** * 协处理器, */ public class CallLogRegionObserver extends BaseRegionObserver { /** * Put后处理 */ public void postPut(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Put put, WALEdit edit, Durability durability) throws IOException { super.postPut(e, put, edit, durability); // String tableName0 = TableName.valueOf(&quot;ns1:calllogs&quot;).getNameAsString(); //得到当前的TableName对象 String tableName1 = e.getEnvironment().getRegion().getRegionInfo().getTable().getNameAsString(); //判断是否是ns1:calllogs表 if (!tableName0.equals(tableName1)) { return; } //得到主叫的rowkey, String rowkey = Bytes.toString(put.getRow()); //如果被叫就放行 String[] arr = rowkey.split(&quot;,&quot;); if (arr[3].equals(&quot;1&quot;)) { return; } //hashcode,caller,time,flag,callee,duration String caller = arr[1] ; //主叫 String callTime = arr[2] ; //通话时间 String callee = arr[4] ; //被叫 String callDuration = arr[5] ; //通话时长 //被叫hashcode String hashcode = CallLogUtil.getHashcode(callee,callTime,100); //被叫rowkey String calleeRowKey = hashcode + &quot;,&quot; + callee + &quot;,&quot; + callTime + &quot;,1,&quot; + caller + &quot;,&quot; + callDuration; Put newPut = new Put(Bytes.toBytes(calleeRowKey)); newPut.addColumn(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;refrowid&quot;), Bytes.toBytes(rowkey)); TableName tn = TableName.valueOf(&quot;ns1:calllogs&quot;); Table t = e.getEnvironment().getTable(tn); t.put(newPut); } } 2.注册协处理器 a)导出jar包,分到集群. ... b)修改hbase配置文件并分发. [hbase-site.xml] &lt;property&gt; &lt;name&gt;hbase.coprocessor.region.classes&lt;/name&gt; &lt;value&gt;com.it18zhang.calllog.coprossor.CallLogRegionObserver&lt;/value&gt; &lt;/property&gt; c)停止hbase集群 d)重新启动 ... e)进入hbase shell,重建ns1:calllogs $hbase&gt;create &apos;ns1:calllogs&apos; , &apos;f1&apos;,&apos;f2&apos; 主叫rowkey: 12,1234,xxx,0,5678,60,.,.,.,., 被叫rowkey: f2:refrowkey 98,5678,xxxx,1,1234,60 --&gt; 12,1234,xxx,0,5678,60,. 重写RegionObserver的postGetOp方法,完成被叫查询时，直接返回主叫的记录1.重写 [CallLogRegionObserver.java] package com.it18zhang.calllog.coprossor; import org.apache.hadoop.hbase.Cell; import org.apache.hadoop.hbase.CellUtil; import org.apache.hadoop.hbase.TableName; import org.apache.hadoop.hbase.client.*; import org.apache.hadoop.hbase.coprocessor.BaseRegionObserver; import org.apache.hadoop.hbase.coprocessor.ObserverContext; import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment; import org.apache.hadoop.hbase.regionserver.InternalScanner; import org.apache.hadoop.hbase.regionserver.wal.WALEdit; import org.apache.hadoop.hbase.util.Bytes; import java.io.IOException; import java.util.ArrayList; import java.util.List; /** * 协处理器, */ public class CallLogRegionObserver extends BaseRegionObserver { //被叫引用id private static final String REF_ROW_ID = &quot;refrowid&quot; ; //通话记录表名 private static final String CALL_LOG_TABLE_NAME = &quot;ns1:calllogs&quot; ; /** * Put后处理 */ public void postPut(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Put put, WALEdit edit, Durability durability) throws IOException { super.postPut(e, put, edit, durability); // String tableName0 = TableName.valueOf(CALL_LOG_TABLE_NAME).getNameAsString(); //得到当前的TableName对象 String tableName1 = e.getEnvironment().getRegion().getRegionInfo().getTable().getNameAsString(); //判断是否是ns1:calllogs表 if (!tableName0.equals(tableName1)) { return; } //得到主叫的rowkey, String rowkey = Bytes.toString(put.getRow()); //如果被叫就放行 String[] arr = rowkey.split(&quot;,&quot;); if (arr[3].equals(&quot;1&quot;)) { return; } //hashcode,caller,time,flag,callee,duration String caller = arr[1] ; //主叫 String callTime = arr[2] ; //通话时间 String callee = arr[4] ; //被叫 String callDuration = arr[5] ; //通话时长 //被叫hashcode String hashcode = CallLogUtil.getHashcode(callee,callTime,100); //被叫rowkey String calleeRowKey = hashcode + &quot;,&quot; + callee + &quot;,&quot; + callTime + &quot;,1,&quot; + caller + &quot;,&quot; + callDuration; Put newPut = new Put(Bytes.toBytes(calleeRowKey)); newPut.addColumn(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(REF_ROW_ID), Bytes.toBytes(rowkey)); TableName tn = TableName.valueOf(CALL_LOG_TABLE_NAME); Table t = e.getEnvironment().getTable(tn); t.put(newPut); } /** * 重写方法，完成被叫查询，返回主叫结果。 */ public void postGetOp(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Get get, List&lt;Cell&gt; results) throws IOException { //获得表名 String tableName = e.getEnvironment().getRegion().getRegionInfo().getTable().getNameAsString(); //判断表名是否是ns1:calllogs if(!tableName.equals(CALL_LOG_TABLE_NAME)){ super.preGetOp(e, get, results); } else{ //得到rowkey String rowkey = Bytes.toString(get.getRow()); // String[] arr = rowkey.split(&quot;,&quot;); //主叫 if(arr[3].equals(&quot;0&quot;)){ super.postGetOp(e, get, results); } //被叫 else{ //得到主叫方的rowkey String refrowid = Bytes.toString(CellUtil.cloneValue(results.get(0))); // Table tt = e.getEnvironment().getTable(TableName.valueOf(CALL_LOG_TABLE_NAME)); Get g = new Get(Bytes.toBytes(refrowid)); Result r = tt.get(g); List&lt;Cell&gt; newList = r.listCells(); results.clear(); results.addAll(newList); } } } /** * */ public boolean postScannerNext(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, InternalScanner s, List&lt;Result&gt; results, int limit, boolean hasMore) throws IOException { boolean b = super.postScannerNext(e, s, results, limit, hasMore); //新集合 List&lt;Result&gt; newList = new ArrayList&lt;Result&gt;(); //获得表名 String tableName = e.getEnvironment().getRegion().getRegionInfo().getTable().getNameAsString(); //判断表名是否是ns1:calllogs if (tableName.equals(CALL_LOG_TABLE_NAME)) { Table tt = e.getEnvironment().getTable(TableName.valueOf(CALL_LOG_TABLE_NAME)); for(Result r : results){ //rowkey String rowkey = Bytes.toString(r.getRow()); String flag = rowkey.split(&quot;,&quot;)[3] ; //主叫 if(flag.equals(&quot;0&quot;)){ newList.add(r) ; } //被叫 else{ //取出主叫号码 byte[] refrowkey = r.getValue(Bytes.toBytes(&quot;f2&quot;),Bytes.toBytes(REF_ROW_ID)) ; Get newGet = new Get(refrowkey); newList.add(tt.get(newGet)); } } results.clear(); results.addAll(newList); } return b ; } } 1&apos;.修改jsp页面 [callLogList.jsp] &lt;c:if test=&quot;${log.caller == param.caller}&quot;&gt;主叫&lt;/c:if&gt; &lt;c:if test=&quot;${log.caller != param.caller}&quot;&gt;被叫&lt;/c:if&gt; 2.部署jar包 3.测试 ... 4.]]></content>
  </entry>
  <entry>
    <title><![CDATA[Spring第二天_01Spring常用注解]]></title>
    <url>%2F2019%2F01%2F11%2FSpring%E7%AC%94%E8%AE%B0%2FSpring%E7%AC%AC%E4%BA%8C%E5%A4%A9_01Spring%E5%B8%B8%E7%94%A8%E6%B3%A8%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[前言：上一章节讲解的是就是通过工厂类获取bean实例，通过获取xml文件里面的配置，value是serviceimpl对应一个Id，然后获取一个serviceimpl的实例，或者获取一个daoimpl的实例，就不用通过new来创建了，本章讲解的是通过Spring容器，注解的方式达到上面的目的，直接注入实例或者注入某一成员类型的值。 Spring基于注解的IOC以及IOC的案例1、spring中ioc的常用注解2、案例使用xml方式和注解方式实现单表的CRUD操作持久层技术选择：dbutils 3、改造基于注解的ioc案例，使用纯注解的方式实现spring的一些新注解使用 4、spring和Junit整合曾经XML的配置， &lt;bean id=&quot;accountService&quot; class=&quot;com.itheima.service.impl.AccountServiceImpl&quot; &lt;scope =&quot;&quot; init-method=&quot;&quot; destory-method=&quot;&quot;&gt; &lt;propertiy name = &quot;&quot; value=&quot;&quot; | ref=&quot;&quot;&gt;&lt;/propertiy&gt; &lt;/bean&gt; 1、用于创建对象的注解： @Component 他们的作用就和在XML配置文件中编写一个标签实现的功能一样的 2、用于注入数据的注解： 他们的作用就和在XML配置文件中写一个prepertiy标签的作用是一样的 3、用于改变范围的注解 他们的作用就和在bean标签中使用scope属性实现的功能是一样的 4、和生命周期相关的 他们的作用就和在bean标签中使用Init-method和destory标签是一样的 package com.itheima.service.impl; import com.itheima.dao.IAccountDao; import com.itheima.dao.impl.AccountDaoImpl; import com.itheima.service.IAccountService; import org.springframework.stereotype.Component; /** * @Description 账户的业务层实现类 * @Author TT Hun * @Data 2019/9/16 22:09 * * 1、用于创建对象的注解 * 他们的作用就和在XML配置文件中编写一个&lt;bean&gt;&lt;/bean&gt;标签实现的功能一样的 */ @Component(value=&quot;accountService&quot;) public class AccountServiceImpl implements IAccountService { public AccountServiceImpl(){ System.out.println(&quot;对象被创建了&quot;); } public void saveAccount() { accountDao.saveAccount(); } } &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;!-- 告知Spring在创建容器的时候需要扫描的包，配置所需要的标签不是在beans这个约束中，而是一个名称为context名称空间和约束中--&gt; &lt;context:component-scan base-package=&quot;com.itheima&quot;&gt;&lt;/context:component-scan&gt; &lt;/beans&gt; package com.itheima.ui; import com.itheima.dao.IAccountDao; import com.itheima.service.IAccountService; import com.itheima.service.impl.AccountServiceImpl; import org.springframework.beans.factory.BeanFactory; import org.springframework.beans.factory.config.AutowireCapableBeanFactory; import org.springframework.beans.factory.xml.XmlBeanFactory; import org.springframework.context.ApplicationContext; import org.springframework.context.support.ClassPathXmlApplicationContext; import org.springframework.context.support.FileSystemXmlApplicationContext; import org.springframework.core.io.ClassPathResource; import org.springframework.core.io.Resource; import sun.security.tools.keytool.Resources; /** * @Description 用这个模拟一个表现层用于调用业务层，实际上应该是一个servlet * @Author TT Hun * @Data 2019/9/16 22:15 */ public class Client { public static void main(String[] args) { ApplicationContext ac = new ClassPathXmlApplicationContext(&quot;bean.xml&quot;); // 2、根据ID获取bean对象 IAccountService as = (IAccountService) ac.getBean(&quot;accountService&quot;); System.out.println(as); } } 由Component衍生出来的三个注解：/** 1、用于创建对象的注解 * 他们的作用就和在XML配置文件中编写一个&lt;bean&gt;&lt;/bean&gt;标签实现的功能一样的 * Component;作用，用于把当前对象注入Srping容器中 * 属性：value:用于指定的bean的id。当我们不写的时候，它默认值是当前类名，而且首字母小写 * Controller:用在表现层 * Service:业务层 * Respository：用于持久层 * 以上是和Component一样的作用，他们是Spring框架提供的为了明确三层使用的注解，是我们对三层更加清晰 * * 2、用于注入数据的注解 * 他们的作用就和在XML配置文件中写一个prepertiy标签的作用是一样的 * 3、用于改变范围的注解 * 他们的作用就和在bean标签中使用scope属性实现的功能是一样的 * 4、和生命周期相关的 * 他们的作用就和在bean标签中使用Init-method和destory标签是一样的 **/ 下面这段例子会报错因为没有注入数据，导致空指针异常。 package com.itheima.service.impl; import com.itheima.dao.IAccountDao; import com.itheima.dao.impl.AccountDaoImpl; import com.itheima.service.IAccountService; import org.springframework.stereotype.Component; /** * @Description 账户的业务层实现类 * @Author TT Hun * @Data 2019/9/16 22:09 * 曾经XML的配置， * &lt;bean id=&quot;accountService&quot; class=&quot;com.itheima.service.impl.AccountServiceImpl&quot; * &lt;scope =&quot;&quot; init-method=&quot;&quot; destory-method=&quot;&quot;&gt; * &lt;propertiy name = &quot;&quot; value=&quot;&quot; | ref=&quot;&quot;&gt;&lt;/propertiy&gt; * &lt;/bean&gt; * * * * 1、用于创建对象的注解 * 他们的作用就和在XML配置文件中编写一个&lt;bean&gt;&lt;/bean&gt;标签实现的功能一样的 * Component;作用，用于把当前对象注入Srping容器中 * 属性：value:用于指定的bean的id。当我们不写的时候，它默认值是当前类名，而且首字母小写 * Controller:用在表现层 * Service:业务层 * Respository：用于持久层 * 以上是和Component一样的作用，他们是Spring框架提供的为了明确三层使用的注解，是我们对三层更加清晰 * * 2、用于注入数据的注解 * 他们的作用就和在XML配置文件中写一个prepertiy标签的作用是一样的 * 3、用于改变范围的注解 * 他们的作用就和在bean标签中使用scope属性实现的功能是一样的 * 4、和生命周期相关的 * 他们的作用就和在bean标签中使用Init-method和destory标签是一样的 */ @Component(value=&quot;accountService&quot;) public class AccountServiceImpl implements IAccountService { private IAccountDao accountDao ; public void saveAccount() { accountDao.saveAccount(); } } package com.itheima.dao.impl; import com.itheima.dao.IAccountDao; import org.springframework.stereotype.Repository; /** * @Description 账户的持久层实现类 * @Author TT Hun * @Data 2019/9/16 22:13 */ @Repository(&quot;accountDao&quot;) public class AccountDaoImpl implements IAccountDao { public void saveAccount() { System.out.println(&quot;保存了账户&quot;); } } package com.itheima.ui; import com.itheima.dao.IAccountDao; import com.itheima.service.IAccountService; import com.itheima.service.impl.AccountServiceImpl; import org.springframework.beans.factory.BeanFactory; import org.springframework.beans.factory.config.AutowireCapableBeanFactory; import org.springframework.beans.factory.xml.XmlBeanFactory; import org.springframework.context.ApplicationContext; import org.springframework.context.support.ClassPathXmlApplicationContext; import org.springframework.context.support.FileSystemXmlApplicationContext; import org.springframework.core.io.ClassPathResource; import org.springframework.core.io.Resource; import sun.security.tools.keytool.Resources; /** * @Description 用这个模拟一个表现层用于调用业务层，实际上应该是一个servlet * @Author TT Hun * @Data 2019/9/16 22:15 */ public class Client { public static void main(String[] args) { ApplicationContext ac = new ClassPathXmlApplicationContext(&quot;bean.xml&quot;); // 2、根据ID获取Service的实例 IAccountService as = (IAccountService) ac.getBean(&quot;accountService&quot;); // 下面是获取dao对象的实例， IAccountDao adao = ac.getBean(&quot;accountDao&quot;,IAccountDao.class); // 这里只显示调用了对象，没有使用其中的方法，因为其中的Serviceimpl里面隐入了Dao类型的变量，因为还没有实例化，所以他是null，会出现空指针异常， as.saveAccount(); } } 所以要在上面这一段代码的数据类型serviceimpl里面的成员变量上面写上@Autowired 下面这一段讲解当出现相同的类的时候，Autowired注解怎么解决： Spring容器本质上是Map，key-value格式的 如果在注入数据的时候有2个DAO类型的，那么首先，他会先匹配数据类型一致的，他会发现2个。 然后在根据变量名称找对应的Spring容器里面的key写的，哪一个是相同的名字。 Qualifier:作用：在按照类中注入的基础之上再按照名称注入。他在给类成员注入时不能单独使用。但是在方法范数注入的时候可以。在给类成员注入的时候，需要和antuwired联合使用，在给方法注入时候可以单独使用属性：value用于指定注入bean的id。 Resource：作用：直接按照bean的id注入。可以独立使用属性：name:用于指定bean的Id Autowired注解、Qualifier注解、Resource注解只能注入其他类型的bean数据类型，而基本类型和String类型无法实现上述实现。另外，集合类型的注入智能通过XML来实现。 基本类型和String类型的注入用Value注解作用：用于注入基本类型和String类型的数据 属性：value用于指定数据的值，他可以使用Spring中的SPEl（也就是Srpring的el表达式）Spel的写法：${表达式} 用于改变作用范围的： 他们的作用就和在bean标签中使用scope属性实现的功能是一样的。 scope: 作用：用于指定bean的作用范围， 属性：value:用于指定范围的取值。常用取值：singleton 和 prototype 和生命周期相关： 他们的作用和在bean标签中使用init-method和destroy-method的作用是一样的。 PreDestory 用于指定销毁方法 PostConstruct 作用用于指定初始化方法 小结： 1、多例模式，关闭Spring不会销毁对象， 2、想要实现销毁方法的实现，如果在serviceimpl注入初始化和销毁注解，然后写方法，然后再在main方法里面先初始化Spring容器，但是只有写了spring的close方法， 才会执行对象的销毁方法，这个地方的初始化和销毁，销毁的是spring容器创建的，比如下图；创建的是IAccountService类型的bean对象，最后要使用ac.close()方法关闭spring容器。]]></content>
  </entry>
  <entry>
    <title><![CDATA[SSM第四天]]></title>
    <url>%2F2019%2F01%2F10%2FSSM%E7%AC%AC%E5%9B%9B%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[新建SSM模块，添加完之后再加入maven支持，刷新 session数据访问1.java @RequestMapping(&quot;/toLogin&quot;) public String doLogin() { return &quot;login&quot;; } @RequestMapping(&quot;/doLogin&quot;) public String doLogin(User u ,HttpSession s){ //将数据存放到session范围。 s.setAttribute(&quot;user&quot;,u); return &quot;index&quot; ; } 2.jsp &lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt; &lt;%@ taglib uri=&quot;http://java.sun.com/jsp/jstl/core&quot; prefix=&quot;c&quot; %&gt; &lt;%@ page session=&quot;true&quot; %&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;login.jsp&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;c:out value=&quot;${sessionScope.user.name}&quot;/&gt; &lt;c:if test=&quot;${sessionScope.user != null}&quot;&gt; 欢迎 &lt;c:out value=&quot;${sessionScope.user.name}&quot;/&gt; &lt;/c:if&gt; &lt;c:if test=&quot;${sessionScope.user == null}&quot;&gt; 您尚未登录,请登录!! &lt;/c:if&gt; &lt;c:out value=&quot;${sessionScope.user.name}&quot;/&gt; &lt;form action=&apos;&lt;c:url value=&quot;/doLogin&quot; /&gt;&apos; method=&quot;post&quot;&gt; UserName : &lt;input type=&quot;text&quot; name=&quot;name&quot;&gt;&lt;br&gt; Password : &lt;input type=&quot;password&quot; name=&quot;password&quot;&gt;&lt;br&gt; &lt;input type=&quot;submit&quot;/&gt; &lt;/form&gt; &lt;/body&gt; &lt;/html&gt; 整合SSM(springmvc + spring + mybatis)1.创建模块 ssm(javaee) 2.添加maven支持 3.添加依赖。 mysql驱动 c3p0数据源 mybatis spring(tx | aop | context))包括事务AOP上下文。 mybatis-spring spring mvc &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.it18zhang&lt;/groupId&gt; &lt;artifactId&gt;ssm&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.17&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;c3p0&lt;/groupId&gt; &lt;artifactId&gt;c3p0&lt;/artifactId&gt; &lt;version&gt;0.9.1.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;3.2.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;version&gt;4.3.3.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt; &lt;version&gt;4.3.3.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;4.3.3.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt; &lt;version&gt;1.3.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt; &lt;version&gt;1.8.10&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; 4.创建包 com.it18zhang.ssm.dao.impl com.it18zhang.ssm.service.impl com.it18zhang.ssm.domain com.it18zhang.ssm.util com.it18zhang.ssm.web.controller 5.创建基本类库 com.it18zhang.ssm.domain.User com.it18zhang.ssm.domain.Order com.it18zhang.ssm.domain.Item ... 6.复制beans.xml + mybatis-cconfig.xml+UserMapper.xml + OrderMapper.xml + ItemMapper.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.3.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-4.3.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-4.3.xsd&quot; default-autowire=&quot;byType&quot;&gt; &lt;!-- 配置事务特征 --&gt; &lt;tx:advice id=&quot;txAdvice&quot; transaction-manager=&quot;txManager&quot;&gt; &lt;tx:attributes&gt; &lt;tx:method name=&quot;*&quot; propagation=&quot;REQUIRED&quot; isolation=&quot;DEFAULT&quot;/&gt; &lt;/tx:attributes&gt; &lt;/tx:advice&gt; &lt;!-- 配置事务切面 --&gt; &lt;aop:config&gt; &lt;aop:advisor advice-ref=&quot;txAdvice&quot; pointcut=&quot;execution(* *..*Service.*(..))&quot; /&gt; &lt;/aop:config&gt; &lt;!-- 扫描包 --&gt; &lt;context:component-scan base-package=&quot;com.it18zhang.ssm.dao,com.it18zhang.ssm.service&quot; /&gt; &lt;!-- 数据源 --&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;com.mchange.v2.c3p0.ComboPooledDataSource&quot;&gt; &lt;property name=&quot;driverClass&quot; value=&quot;com.mysql.jdbc.Driver&quot;/&gt; &lt;property name=&quot;jdbcUrl&quot; value=&quot;jdbc:mysql://localhost:3306/mybatis?user=root&amp;amp;password=root&quot;/&gt; &lt;property name=&quot;user&quot; value=&quot;root&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;root&quot;/&gt; &lt;property name=&quot;maxPoolSize&quot; value=&quot;10&quot;/&gt; &lt;property name=&quot;minPoolSize&quot; value=&quot;2&quot;/&gt; &lt;property name=&quot;initialPoolSize&quot; value=&quot;3&quot;/&gt; &lt;property name=&quot;acquireIncrement&quot; value=&quot;2&quot;/&gt; &lt;/bean&gt; &lt;!-- mybatis整合spring的核心类。 --&gt; &lt;bean id=&quot;sf&quot; class=&quot;org.mybatis.spring.SqlSessionFactoryBean&quot;&gt; &lt;!-- 指定数据源 --&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot; /&gt; &lt;!-- 指定mybatis配置文件 --&gt; &lt;property name=&quot;configLocation&quot; value=&quot;classpath:mybatis-config.xml&quot;/&gt; &lt;/bean&gt; &lt;!-- 数据源事务管理器 --&gt; &lt;bean id=&quot;txManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot; /&gt; &lt;/bean&gt; &lt;/beans&gt; ... 7.创建UserController.java package com.it18zhang.ssm.web.controller; import com.it18zhang.ssm.domain.User; import com.it18zhang.ssm.service.UserService; import org.springframework.stereotype.Controller; import org.springframework.ui.Model; import org.springframework.web.bind.annotation.RequestMapping; import javax.annotation.Resource; import java.util.List; /** * */ @Controller public class UserController { @Resource(name=&quot;userService&quot;) private UserService us ; /** * 查看全部user */ @RequestMapping(&quot;/user/findall&quot;) public String findAll(Model m ){ List&lt;User&gt; list = us.selectAll(); m.addAttribute(&quot;allUsers&quot;,list); return &quot;user/userList&quot; ; } } 8.编写web.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;web-app xmlns=&quot;http://xmlns.jcp.org/xml/ns/javaee&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_3_1.xsd&quot; version=&quot;3.1&quot;&gt; &lt;!-- 指定spring的配置文件beans.xml --&gt; &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath*:beans.xml&lt;/param-value&gt; &lt;/context-param&gt; &lt;!-- 确保web服务器启动时，完成spring的容器初始化 --&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt; &lt;!-- 配置分发器Servlet --&gt; &lt;servlet&gt; &lt;servlet-name&gt;dispatcher&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;dispatcher&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;/web-app&gt; 9.编写WEB-INF/dispatcher-servlet.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc-4.3.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.3.xsd&quot;&gt; &lt;!-- 配置扫描路径 --&gt; &lt;context:component-scan base-package=&quot;com.it18zhang.ssm.web.controller&quot; /&gt; &lt;!-- 使用注解驱动 --&gt; &lt;mvc:annotation-driven /&gt; &lt;!-- 内部资源视图解析器 --&gt; &lt;bean class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt; &lt;property name=&quot;prefix&quot; value=&quot;/&quot; /&gt; &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot;/&gt; &lt;/bean&gt; &lt;/beans&gt; spring mvc静态资源部分1.描述 html image css js 2.在[dispatcher-servlet.xml]添加如下元素 &lt;mvc:resources mapping=&quot;/jsssss/**&quot; location=&quot;/js/&quot;/&gt; &lt;mvc:resources mapping=&quot;/css/**&quot; location=&quot;/css/&quot;/&gt; &lt;mvc:resources mapping=&quot;/images/**&quot; location=&quot;/images/&quot;/&gt; &lt;mvc:resources mapping=&quot;/html/**&quot; location=&quot;/html/&quot;/&gt; 3.常见相关目录 /web/js /web/css /web/images /web/html 4.放置相应文件 /web/html/hello.html &lt;!DOCTYPE html&gt; &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Hello.html&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;div&gt;hello&lt;/div&gt; &lt;table&gt; &lt;tr&gt; &lt;td&gt;ID&lt;/td&gt; &lt;td&gt;name&lt;/td&gt; &lt;td&gt;age&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;tom&lt;/td&gt; &lt;td&gt;12&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;tomas&lt;/td&gt; &lt;td&gt;13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;tomasLee&lt;/td&gt; &lt;td&gt;4&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/body&gt; &lt;/html&gt; 5.启动程序访问 http://localhost:9090/html/hello.html css选择器table //标签选择 #id //id选择 .class-name //类选择 tabble td //后台选择 中文乱码1.jsp页面使用&lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; pageEncoding=&quot;UTF-8&quot; language=&quot;java&quot; %&gt;进行utf8码声明。 2.在web.xml文件中，加入filter。 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;web-app xmlns=&quot;http://xmlns.jcp.org/xml/ns/javaee&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_3_1.xsd&quot; version=&quot;3.1&quot;&gt; &lt;!-- 指定spring的配置文件beans.xml --&gt; &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath*:beans.xml&lt;/param-value&gt; &lt;/context-param&gt; &lt;!-- 确保web服务器启动时，完成spring的容器初始化 --&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt; &lt;filter&gt; &lt;filter-name&gt;characterEncodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;forceEncoding&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;characterEncodingFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt; &lt;!-- 配置分发器Servlet --&gt; &lt;servlet&gt; &lt;servlet-name&gt;dispatcher&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;dispatcher&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;/web-app&gt; 3.确认mysql使用utf8编码，可以修改mysql的编码. [mysql安装目录/my.ini] uft8 4.spring中的数据库连接地址url中显式指定编码. [beans.xml] &lt;property name=&quot;jdbcUrl&quot; value=&quot;jdbc:mysql://localhost:3306/mybatis?user=root&amp;amp;password=root&amp;amp;useUnicode=true&amp;amp;characterEncoding=utf8&quot;/&gt; unicode : 2 //两个字节 ascii : 1 //7iso8859 : 1 //8utf8 //3gbk //2 分页0.技术背景 mysql: select * from users limit 10 10 1.dao + service 2.userList.jsp 3. 4. 5.]]></content>
  </entry>
  <entry>
    <title><![CDATA[电信项目第一天]]></title>
    <url>%2F2019%2F01%2F05%2F%E5%AE%9E%E6%88%98%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%94%B5%E4%BF%A1%E9%A1%B9%E7%9B%AE%EF%BC%88%E9%9B%B6%EF%BC%89%2F</url>
    <content type="text"><![CDATA[可视化:1. 2. 3. 4. 5. package com.it18zhang.callloggen; import java.util.HashMap;import java.util.Map; /* */public class App { public static Map&lt;String,String&gt; caller = new HashMap&lt;String, String&gt;(); static{ caller.put(“15810092493”, “史玉龙”); caller.put(“18000696806”, “赵贺彪”); caller.put(“15151889601”, “张倩 “); caller.put(“13269361119”, “王世昌”); caller.put(“15032293356”, “张涛”); caller.put(“17731088562”, “张阳”); caller.put(“15338595369”, “李进全”); caller.put(“15733218050”, “杜泽文”); caller.put(“15614201525”, “任宗阳”); caller.put(“15778423030”, “梁鹏”); caller.put(“18641241020”, “郭美彤”); caller.put(“15732648446”, “刘飞飞”); caller.put(“13341109505”, “段光星”); caller.put(“13560190665”, “唐会华”); caller.put(“18301589432”, “杨力谋”); caller.put(“13520404983”, “温海英”); caller.put(“18332562075”, “朱尚宽”); caller.put(“18620192711”, “刘能宗”); } public static void main(String[] args) { genCallLog(); } public static void genCallLog(){ } 生成jar包，部署到centos执行1.使用maven生成jar文件 ... 2.部署到centos 3.执行 $&gt;mkdir /home/centos/calllog java -cp xxx.jar com.it18zhang.callloggen.App /home/centos/calllog/calllog.log 4.创建centos上的执行脚本 [calllog.sh] #!/bin/bash java -cp Calllog.jar com.it18zhang.callloggen.App /home/centos/calllog/calllog.log 5.修改权限 $&gt;chmod a+x calllog.sh 6.执行脚本 $&gt;cd ~/calllog $&gt;./calllog.sh 手机号0755-67568979 186---------- 086+0755-67568979 固话0755-67568979 0755-67568979 网络电话12358757575765656565 启动zk集群[s201 + s202 + s203]$&gt;zkServer.sh start 启动kafka集群[s202 + s203 + s204]$&gt;cd /soft/kafka/config $&gt;kafka-server-start.sh -daemon server.properties 创建kafka主题//创建主题 $&gt;kafka-topics.sh --zookeeper s202:2181 --topic calllog --create --replication-factor 3 --partitions 4 //查看主题列表 $&gt;kafka-topics.sh --zookeeper s202:2181 --list //启动控制台消费者,消费calllog主题,用于测试.测试flume有没有取得到。 消费者需要连接到zk主题需要连接到zk生产者不用连接zk $&gt;kafka-console-consumer.sh –zookeeper s201:2181 –topic calllog s201上编写flume配置文件件，实时收集calllog.log日志1.配置文件 [/soft/flume/conf/calllog.conf] a1.sources = r1 a1.sinks = k1 a1.channels = c1 a1.sources.r1.type=exec #-F 最后10行,如果从头开始收集 -c +0 -F:持续收集后续数据,否则进程停止。 a1.sources.r1.command=tail -F -c +0 /home/centos/calllog/calllog.log a1.channels.c1.type=memory a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink a1.sinks.k1.kafka.topic = calllog #三台节点，启动的时候连接座初始化 a1.sinks.k1.kafka.bootstrap.servers = s202:9092 s203:9092 s204:9092 a1.sinks.k1.kafka.flumeBatchSize = 20 a1.sinks.k1.kafka.producer.acks = 1 a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 2.启动flume收集程序 $&gt;flume-ng agent -f /soft/flume/conf/calllog.conf -n a1 &amp; 在s202主机安装flume软件... 启动hadoop hdfs集群[s201 ~ s206]角色划分:NameNode //s201,s206DataNode //s202,s203,s204,s205JournalNode //s202,s203,s204 ZK //s201,s2061.启动hadoop(完全分布式 + HA,HA是s201 + s206作为主备名称节点namenode的2个节点) 在s201上启动dfs集群. $&gt;start-dfs.sh 2.webui http://s201:50070/ 3.查看节点状态 $&gt;hdfs haadmin -getServiceState nn1 4.容灾切换 $&gt;hdfs haadmin -failover nn1 nn2 启动hbase集群1.[角色划分]对hbase做高可用 master //s201,s204 regsionServer //s202,s203,s204 2.启动hbase集群 //s201 $&gt;start-hbase.sh 3.查看进程和webui http://s201:16010 4.启动备份master节点 //s204 $&gt;hbase-daemon.sh start master 创建hbase名字空间+表1.创建名字空间 $&gt;hbase shell //进入hbase shell $hbase&gt;create_namespace &apos;ns1&apos; //创建空间 $hbase&gt;create &apos;ns1:calllogs&apos; , &apos;f1&apos; //创建表 $hbase&gt;truncate &apos;ns1:calllogs&apos; //重建表 IBM Power720(210000) 基本规格 处理器类型POWER7+ 处理器主频3.6GHz 处理器缓存每个内核256KB二级缓存，4MB三级缓存 最大处理器个数8 内存类型DDR3 标准内存容量32GB 最大内存容量128GB 4颗CPU 去IOEIBM: //ibm小型机 Oracle: //oracle数据 EMC //网络存储设备 戴尔（DELL）13900 * 100 = 140万 商品名称： 戴尔（DELL）R730服务器 机架式主机 2U 至强 E5处理器 内存类型 ECC 硬盘总容量：8T以上 8T x 100 = 800T = 副本(3) 800/3 = 250 80%(4/5) = 150T 类型： 机架服务器电源：冗余 操作系统： DOS 内存 总容量：64G及以上 硬盘转速：其他 支持CPU颗数 2颗 机箱规格 2U机架式 显存 集成显卡 硬盘类型 SAS Raid卡 缓存： 无缓存 处理器： 至强Xeon-E5 创建kafka消费者，订阅calllog主题1.设计rowkey 业务数据: caller , callee , date , duration 分区号 号码 时间,标记 (对方号码),时长 regionNo, caller,date , flag,callee,duration caller(11) + 通话时间(201701) = (后四位) + 201701 = % 100 2. 3. 4. 11 - 4 = 7 hbase(main):001:0&gt; scan ‘ns1:calllogs’ROW COLUMN+CELL 39,15032293356,20170313140201,0,18620192711,297 column=f1:callDuration, timestamp=1491896679156, value=297 39,15032293356,20170313140201,0,18620192711,297 column=f1:callTime, timestamp=1491896679156, value=20170313140201 39,15032293356,20170313140201,0,18620192711,297 column=f1:callee, timestamp=1491896679156, value=18620192711 39,15032293356,20170313140201,0,18620192711,297 column=f1:caller, timestamp=1491896679156, value=15032293356 93,18620192711,20170313140201,1,15032293356,297 column=f1:dummy, timestamp=1491896679184, value=no 使用mvn命令，下载工件的所有依赖软件包mvn -DoutputDirectory=./lib -DgroupId=com.it18zhang -DartifactId=CallLogConsumerModule -Dversion=1.0-SNAPSHOT dependency:copy-dependencies 导入kafka消费者，放置以上面的lib下，使用命令行方式运行java -cp CallLogConsumerModule.jar;./lib/activation-1.1.jar;./lib/apacheds-i18n-2.0.0-M15.jar;./lib/apacheds-kerberos-codec-2.0.0-M15.jar;./lib/api-asn1-api-1.0.0-M20.jar;./lib/api-util-1.0.0-M20.jar;./lib/avro-1.7.4.jar;./lib/commons-beanutils-1.7.0.jar;./lib/commons-beanutils-core-1.8.0.jar;./lib/commons-cli-1.2.jar;./lib/commons-codec-1.9.jar;./lib/commons-collections-3.2.2.jar;./lib/commons-compress-1.4.1.jar;./lib/commons-configuration-1.6.jar;./lib/commons-digester-1.8.jar;./lib/commons-el-1.0.jar;./lib/commons-httpclient-3.1.jar;./lib/commons-io-2.4.jar;./lib/commons-lang-2.6.jar;./lib/commons-logging-1.2.jar;./lib/commons-math3-3.1.1.jar;./lib/commons-net-3.1.jar;./lib/findbugs-annotations-1.3.9-1.jar;./lib/guava-12.0.1.jar;./lib/hadoop-annotations-2.5.1.jar;./lib/hadoop-auth-2.5.1.jar;./lib/hadoop-common-2.5.1.jar;./lib/hadoop-mapreduce-client-core-2.5.1.jar;./lib/hadoop-yarn-api-2.5.1.jar;./lib/hadoop-yarn-common-2.5.1.jar;./lib/hamcrest-core-1.3.jar;./lib/hbase-annotations-1.2.4.jar;./lib/hbase-client-1.2.4.jar;./lib/hbase-common-1.2.4.jar;./lib/hbase-protocol-1.2.4.jar;./lib/htrace-core-3.1.0-incubating.jar;./lib/httpclient-4.2.5.jar;./lib/httpcore-4.2.4.jar;./lib/jackson-core-asl-1.9.13.jar;./lib/jackson-mapper-asl-1.9.13.jar;./lib/jaxb-api-2.2.2.jar;./lib/jcodings-1.0.8.jar;./lib/jdk.tools-1.6.jar;./lib/jetty-util-6.1.26.jar;./lib/jline-0.9.94.jar;./lib/joni-2.1.2.jar;./lib/jopt-simple-4.9.jar;./lib/jsch-0.1.42.jar;./lib/jsr305-1.3.9.jar;./lib/junit-4.12.jar;./lib/kafka-clients-0.10.0.1.jar;./lib/kafka_2.11-0.10.0.1.jar;./lib/log4j-1.2.15.jar;./lib/lz4-1.3.0.jar;./lib/mail-1.4.jar;./lib/metrics-core-2.2.0.jar;./lib/netty-3.7.0.Final.jar;./lib/netty-all-4.0.23.Final.jar;./lib/paranamer-2.3.jar;./lib/protobuf-java-2.5.0.jar;./lib/scala-library-2.11.8.jar;./lib/scala-parser-combinators_2.11-1.0.4.jar;./lib/slf4j-api-1.6.1.jar;./lib/slf4j-log4j12-1.7.21.jar;./lib/snappy-java-1.1.2.6.jar;./lib/stax-api-1.0-2.jar;./lib/xmlenc-0.52.jar;./lib/xz-1.0.jar;./lib/zkclient-0.8.jar;./lib/zookeeper-3.4.6.jar com.it18zhang.calllog.consumer.HbaseConsumer 编写web程序，从hbase中提取所有进行展示，可视化1.导入ssm项目 ... 2.创建CallLog ... 3.创建CallLogService.java + CallLogServiceImpl.java ... 4. ======================================= 在windows下编写的.sh文件到了linux上变成不可用了什么原因： 因为换行符的原因，windows下的换行符和Linux下的换行符不一样。所以要在linux下编写.sh文件即可解决。 ================= kafka同一个组下只能有一个消费者。被消费一次就不能再次消费了 所以修改配置文件里面的group.id=5。这是kafka里面的具体问题了。 ==================]]></content>
  </entry>
  <entry>
    <title><![CDATA[Java视频中的面试题]]></title>
    <url>%2F2018%2F12%2F10%2FJava%E8%A7%86%E9%A2%91%E4%B8%AD%E7%9A%84%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[集合中的面试题ArrayList和LinkList的区别1、ArrayList和LinkedList可想从名字分析，它们一个是Array(动态数组)的数据结构，一个是Link(链表)的数据结构，此外，它们两个都是对List接口的实现。 前者是数组队列，相当于动态数组；后者为双向链表结构，也可当作堆栈、队列、双端队列 2、当随机访问List时（get和set操作），ArrayList比LinkedList的效率更高，因为LinkedList是线性的数据存储方式，所以需要移动指针从前往后依次查找。 3、当对数据进行增加和删除的操作时(add和remove操作)，LinkedList比ArrayList的效率更高，因为ArrayList是数组，所以在其中进行增删操作时，会对操作点之后所有数据的下标索引造成影响，需要进行数据的移动。 4、从利用效率来看，ArrayList自由性较低，因为它需要手动的设置固定大小的容量，但是它的使用比较方便，只需要创建，然后添加数据，通过调用下标进行使用；而LinkedList自由性较高，能够动态的随数据量的变化而变化，但是它不便于使用。 5、ArrayList主要控件开销在于需要在lList列表预留一定空间；而LinkList主要控件开销在于需要存储结点信息以及结点指针信息。 Java对象的创建过程从new指令(我说的是JVM的层面)开始的(具体请看图1)，JVM首先对符号引用进行解析，如果找不到对应的符号引用，那么这个类还没有被加载，因此JVM便会进行类加载过程（具体加载过程可参见我的另一篇博文）。符号引用解析完毕之后，JVM会为对象在堆中分配内存，HotSpot虚拟机实现的JAVA对象包括三个部分：对象头、实例字段和对齐填充字段，其中要注意的是，实例字段包括自身定义的和从父类继承下来的（即使父类的实例字段被子类覆盖或者被private修饰，都照样为其分配内存）。相信很多人在刚接触面向对象语言时，总把继承看成简单的“复制”，这其实是完全错误的。JAVA中的继承仅仅是类之间的一种逻辑关系（具体如何保存记录这种逻辑关系，则设计到Class文件格式的知识，具体请看我的另一篇博文），唯有创建对象时的实例字段，可以简单的看成“复制”。 为对象分配完堆内存之后，JVM会将该内存（除了对象头区域）进行零值初始化，这也就解释了为什么JAVA的属性字段无需显示初始化就可以被使用，而方法的局部变量却必须要显示初始化后才可以访问。最后，JVM会调用对象的构造函数，当然，调用顺序会一直上溯到Object类。 至此，一个对象就被创建完毕，此时，一般会有一个引用指向这个对象。在JAVA中，存在两种数据类型，一种就是诸如int、double等基本类型，另一种就是引用类型，比如类、接口、内部类、枚举类、数组类型的引用等。引用的实现方式一般有两种，具体请看图3。 接口类和抽象类抽象类在了解抽象类之前，先来了解一下抽象方法。抽象方法是一种特殊的方法：它只有声明，而没有具体的实现。抽象方法的声明格式为： abstract void fun(); 抽象方法必须用abstract关键字进行修饰。如果一个类含有抽象方法，则称这个类为抽象类，抽象类必须在类前用abstract关键字修饰。因为抽象类中含有无具体实现的方法，所以不能用抽象类创建对象。 下面要注意一个问题：在《JAVA编程思想》一书中，将抽象类定义为“包含抽象方法的类”，但是后面发现如果一个类不包含抽象方法，只是用abstract修饰的话也是抽象类。也就是说抽象类不一定必须含有抽象方法。个人觉得这个属于钻牛角尖的问题吧，因为如果一个抽象类不包含任何抽象方法，为何还要设计为抽象类？所以暂且记住这个概念吧，不必去深究为什么。 [public] abstract class ClassName { abstract void fun(); } 从这里可以看出，抽象类就是为了继承而存在的，如果你定义了一个抽象类，却不去继承它，那么等于白白创建了这个抽象类，因为你不能用它来做任何事情。对于一个父类，如果它的某个方法在父类中实现出来没有任何意义，必须根据子类的实际需求来进行不同的实现，那么就可以将这个方法声明为abstract方法，此时这个类也就成为abstract类了。 包含抽象方法的类称为抽象类，但并不意味着抽象类中只能有抽象方法，它和普通类一样，同样可以拥有成员变量和普通的成员方法。注意，抽象类和普通类的主要有三点区别： 1）抽象方法必须为public或者protected（因为如果为private，则不能被子类继承，子类便无法实现该方法），缺省情况下默认为public。 2）抽象类不能用来创建对象； 3）如果一个类继承于一个抽象类，则子类必须实现父类的抽象方法。如果子类没有实现父类的抽象方法，则必须将子类也定义为为abstract类。 在其他方面，抽象类和普通的类并没有区别。 接口接口，英文称作interface，在软件工程中，接口泛指供别人调用的方法或者函数。从这里，我们可以体会到Java语言设计者的初衷，它是对行为的抽象。在Java中，定一个接口的形式如下： [public] interface InterfaceName { } 接口中可以含有 变量和方法。但是要注意，接口中的变量会被隐式地指定为public static final变量（并且只能是public static final变量，用private修饰会报编译错误），而方法会被隐式地指定为public abstract方法且只能是public abstract方法（用其他关键字，比如private、protected、static、 final等修饰会报编译错误），并且接口中所有的方法不能有具体的实现，也就是说，接口中的方法必须都是抽象方法。从这里可以隐约看出接口和抽象类的区别，接口是一种极度抽象的类型，它比抽象类更加“抽象”，并且一般情况下不在接口中定义变量。 要让一个类遵循某组特地的接口需要使用implements关键字，具体格式如下： class ClassName implements Interface1,Interface2,[....]{ } 可以看出，允许一个类遵循多个特定的接口。如果一个非抽象类遵循了某个接口，就必须实现该接口中的所有方法。对于遵循某个接口的抽象类，可以不实现该接口中的抽象方法。 抽象类和接口的区别1.语法层面上的区别 1）抽象类可以提供成员方法的实现细节，而接口中只能存在public abstract 方法； 2）抽象类中的成员变量可以是各种类型的，而接口中的成员变量只能是public static final类型的； 3）接口中不能含有静态代码块以及静态方法，而抽象类可以有静态代码块和静态方法； 4）一个类只能继承一个抽象类，而一个类却可以实现多个接口。 2.设计层面上的区别 1）抽象类是对一种事物的抽象，即对类抽象，而接口是对行为的抽象。抽象类是对整个类整体进行抽象，包括属性、行为，但是接口却是对类局部（行为）进行抽象。举个简单的例子，飞机和鸟是不同类的事物，但是它们都有一个共性，就是都会飞。那么在设计的时候，可以将飞机设计为一个类Airplane，将鸟设计为一个类Bird，但是不能将 飞行 这个特性也设计为类，因此它只是一个行为特性，并不是对一类事物的抽象描述。此时可以将 飞行 设计为一个接口Fly，包含方法fly( )，然后Airplane和Bird分别根据自己的需要实现Fly这个接口。然后至于有不同种类的飞机，比如战斗机、民用飞机等直接继承Airplane即可，对于鸟也是类似的，不同种类的鸟直接继承Bird类即可。从这里可以看出，继承是一个 “是不是”的关系，而 接口 实现则是 “有没有”的关系。如果一个类继承了某个抽象类，则子类必定是抽象类的种类，而接口实现则是有没有、具备不具备的关系，比如鸟是否能飞（或者是否具备飞行这个特点），能飞行则可以实现这个接口，不能飞行就不实现这个接口。 2）设计层面不同，抽象类作为很多子类的父类，它是一种模板式设计。而接口是一种行为规范，它是一种辐射式设计。什么是模板式设计？最简单例子，大家都用过ppt里面的模板，如果用模板A设计了ppt B和ppt C，ppt B和ppt C公共的部分就是模板A了，如果它们的公共部分需要改动，则只需要改动模板A就可以了，不需要重新对ppt B和ppt C进行改动。而辐射式设计，比如某个电梯都装了某种报警器，一旦要更新报警器，就必须全部更新。也就是说对于抽象类，如果需要添加新的方法，可以直接在抽象类中添加具体的实现，子类可以不进行变更；而对于接口则不行，如果接口进行了变更，则所有实现这个接口的类都必须进行相应的改动。 下面看一个网上流传最广泛的例子：门和警报的例子：门都有open( )和close( )两个动作，此时我们可以定义通过抽象类和接口来定义这个抽象概念： abstract class Door { public abstract void open(); public abstract void close(); } 或者： interface Door { public abstract void open(); public abstract void close(); } 但是现在如果我们需要门具有报警alarm( )的功能，那么该如何实现？下面提供两种思路： 1）将这三个功能都放在抽象类里面，但是这样一来所有继承于这个抽象类的子类都具备了报警功能，但是有的门并不一定具备报警功能； 2）将这三个功能都放在接口里面，需要用到报警功能的类就需要实现这个接口中的open( )和close( )，也许这个类根本就不具备open( )和close( )这两个功能，比如火灾报警器。 从这里可以看出， Door的open() 、close()和alarm()根本就属于两个不同范畴内的行为，open()和close()属于门本身固有的行为特性，而alarm()属于延伸的附加行为。因此最好的解决办法是单独将报警设计为一个接口，包含alarm()行为,Door设计为单独的一个抽象类，包含open和close两种行为。再设计一个报警门继承Door类和实现Alarm接口。 interface Alram { void alarm(); } abstract class Door { void open(); void close(); } class AlarmDoor extends Door implements Alarm { void oepn() { //.... } void close() { //.... } void alarm() { //.... } } 重写和重载的区别1.重写(Override)从字面上看，重写就是 重新写一遍的意思。其实就是在子类中把父类本身有的方法重新写一遍。子类继承了父类原有的方法，但有时子类并不想原封不动的继承父类中的某个方法，所以在方法名，参数列表，返回类型(除过子类中方法的返回值是父类中方法返回值的子类时)都相同的情况下， 对方法体进行修改或重写，这就是重写。但要注意子类函数的访问修饰权限不能少于父类的。例如： public class Father { public static void main(String[] args) { // TODO Auto-generated method stub Son s = new Son(); s.sayHello(); } public void sayHello() { System.out.println(&quot;Hello&quot;); } } class Son extends Father{ @Override public void sayHello() { // TODO Auto-generated method stub System.out.println(&quot;hello by &quot;); } } 重写 总结：1.发生在父类与子类之间 2.方法名，参数列表，返回类型（除过子类中方法的返回类型是父类中返回类型的子类）必须相同 3.访问修饰符的限制一定要大于被重写方法的访问修饰符（public&gt;protected&gt;default&gt;private) 4.重写方法一定不能抛出新的检查异常或者比被重写方法申明更加宽泛的检查型异常 2.重载(Overload)在一个类中，同名的方法如果有不同的参数列表（参数类型不同、参数个数不同甚至是参数顺序不同）则视为重载。同时，重载对返回类型没有要求，可以相同也可以不同，但不能通过返回类型是否相同来判断重载。例如： public class Father { public static void main(String[] args) { // TODO Auto-generated method stub Father s = new Father(); s.sayHello(); s.sayHello(&quot;wintershii&quot;); } public void sayHello() { System.out.println(&quot;Hello&quot;); } public void sayHello(String name) { System.out.println(&quot;Hello&quot; + &quot; &quot; + name); } } 重载 总结：1.重载Overload是一个类中多态性的一种表现2.重载要求同名方法的参数列表不同(参数类型，参数个数甚至是参数顺序)3.重载的时候，返回值类型可以相同也可以不相同。无法以返回型别作为重载函数的区分标准 面试时，问：重载（Overload）和重写（Override）的区别？ （这部分吴锦峰大数据数讲解好）答：方法的重载和重写都是实现多态的方式，区别在于前者实现的是编译时的多态性，而后者实现的是运行时的多态性。 重载发生在一个类中，同名的方法如果有不同的参数列表（参数类型不同、参数个数不同或者二者都不同）则视为重载；重写发生在子类与父类之间，重写要求子类被重写方法与父类被重写方法有相同的参数列表，有兼容的返回类型，比父类被重写方法更好访问，不能比父类被重写方法声明更多的异常（里氏代换原则）。重载对返回类型没有特殊的要求，不能根据返回类型进行区分。 为什么要使用内部类？HashMap与HashSet的区别 先了解一下HashMap跟HashSet HashSet： HashSet实现了Set接口，它不允许集合中出现重复元素。当我们提到HashSet时，第一件事就是在将对象存储在 HashSet之前，要确保重写hashCode（）方法和equals（）方法，这样才能比较对象的值是否相等，确保集合中没有 储存相同的对象。如果不重写上述两个方法，那么将使用下面方法默认实现： public boolean add(Object obj)方法用在Set添加元素时，如果元素值重复时返回 “false”，如果添加成功则返回”true” HashMap： HashMap实现了Map接口，Map接口对键值对进行映射。Map中不允许出现重复的键（Key）。Map接口有两个基本的实现 TreeMap和HashMap。TreeMap保存了对象的排列次序，而HashMap不能。HashMap可以有空的键值对（Key（null）-Value（null）） HashMap是非线程安全的（非Synchronize），要想实现线程安全，那么需要调用collections类的静态方法synchronizeMap（）实现。 public Object put(Object Key,Object value)方法用来将元素添加到map中。 HashSet与HashMap的区别： 一个字节的数据大小范围为什么是-128~127一个字节是8位，最高位是符号位，最高位为0则是正数。最高位为1则是负数 如果一个数是正数，最大数则为：01111111，转为十进制为127， 如果一个数是负数，按照一般人都会觉得是11111111，转为十进制为-127， 但是：一个+0表示为：00000000，一个-0表示为：1000000，因为符号位不算在里面，所以就会有两个0，所以从一开始发明二进制的时候，就把-0规定为-128，如此二进制的补码就刚好在计算机中运作中吻合。 公式：计算一个数据类型的数据大小范围：-2^（字节数8-1）~2^(字节数8-1)-1 异或运算符异或运算符（^） 参加运算的两个数据，按二进制位进行“异或”运算。 运算规则：0^0=0； 0^1=1； 1^0=1； 1^1=0； 即：参加运算的两个对象，如果两个相应位为“异”（值不同），则该位结果为1，否则为0。 关于Treeset和Treemap:这两个方法其实是通过二叉树进行比较的，也就是说通过二叉树来存储，时间复杂度为0(logn) 二叉树是小的放在左边，打的放在右边子叶 然后，每次添加的时候都要通过一个一个比较大小来进行选择添加的位置。那么这时候，就出现一个问题，怎么比较大小 正常情况下如果泛型是Integer的时候Integer实现了Comparable方法，所以在添加的时候就可以通过自带的比较直接比较添加。但是如果泛型是其他自定义的类的时候就需要重写compare方法。 treeset和treeset就像hashmap和hashset是一个道理的。都是本质上是一样的。treeset也是通过通过treemap的键值对里面的其中一个存放present一个垃圾值来计算的 在实现Comparator的时候本来要重写2个方法一个是equlas一个是compare方法，但是由于Comparator的父类是Object已经给重写好了，所以只需要重写Compar的方法即可 public void testAddDog(){ Set&lt;Dog&gt; set = new Treeeset&lt;Dog&gt;(new DogComparator()); set.add(new Dog(&quot;大黄“)); } //需要重写的对比器 Class DogComparator implements Comparator&lt;Dog&gt;{ ...... } Class Dog{ private Sring name; publi void DOg(String name){ super; this.name =name} } 关于hashmap和hashsethashmap和hashset都采用hash方法来添加内容里面的结构是桶+链表的形式，桶类似于arraylist是内存连续的。 put方法的流程，拿到E之后先做一个判断判断e在不在这个桶里面，看下桶是不是空的，如果是空的直接放里面，如果是空的，还需要判断是不是跟桶里面的元素相等。 因为Hashset不能相等，通过三个步骤判断。1看hashcode判断判断x和y的哈希码是否相同，相同不能说明是相等的，但是不同肯定是不等的，以为哈希码可能通过同一个算法计算出同一个值，但是不同的肯定是不等的，2如果是相同的哈希码则再去判断是不是同一个对象，3如果是同一个对象，则是相同的，就不往里放了，同时判断equals方法，这地方考虑equals是否重写没重写还是比较内存地址。 这个时候equals方法和hashcode方法并没有重写， 所以说，equals方法比较的还是内存地址。hashcode默认取得是内存地址。一般重写equals方法的时候都要重写hashcode方法。 并发编程有哪些缺点1 频繁的上下文切换时间片是CPU分配给各个线程的时间，因为时间非常短，所以CPU不断通过切换线程，让我们觉得多个线程是同时执行的，时间片一般是几十毫秒。而每次切换时，需要保存当前的状态起来，以便能够进行恢复先前状态，而这个切换时非常损耗性能，过于频繁反而无法发挥出多线程编程的优势。通常减少上下文切换可以采用无锁并发编程，CAS算法，使用最少的线程和使用协程。 无锁并发编程：可以参照concurrentHashMap锁分段的思想，不同的线程处理不同段的数据，这样在多线程竞争的条件下，可以减少上下文切换的时间。CAS算法，利用Atomic下使用CAS算法来更新数据，使用了乐观锁，可以有效的减少一部分不必要的锁竞争带来的上下文切换使用最少线程：避免创建不需要的线程，比如任务很少，但是创建了很多的线程，这样会造成大量的线程都处于等待状态协程：在单线程里实现多任务的调度，并在单线程里维持多个任务间的切换 2 线程安全那么，通常可以用如下方式避免死锁的情况： 避免一个线程同时获得多个锁；避免一个线程在锁内部占有多个资源，尽量保证每个锁只占用一个资源；尝试使用定时锁，使用lock.tryLock(timeOut)，当超时等待时当前线程不会阻塞；对于数据库锁，加锁和解锁必须在一个数据库连接里，否则会出现解锁失败的情况 需要了解的概念3.1 同步VS异步同步和异步通常用来形容一次方法调用。同步方法调用一开始，调用者必须等待被调用的方法结束后，调用者后面的代码才能执行。而异步调用，指的是，调用者不用管被调用方法是否完成，都会继续执行后面的代码，当被调用的方法完成后会通知调用者。比如，在超时购物，如果一件物品没了，你得等仓库人员跟你调货，直到仓库人员跟你把货物送过来，你才能继续去收银台付款，这就类似同步调用。而异步调用了，就像网购，你在网上付款下单后，什么事就不用管了，该干嘛就干嘛去了，当货物到达后你收到通知去取就好。 3.2 并发与并行并发和并行是十分容易混淆的概念。并发指的是多个任务交替进行，而并行则是指真正意义上的“同时进行”。实际上，如果系统内只有一个CPU，而使用多线程时，那么真实系统环境下不能并行，只能通过切换时间片的方式交替进行，而成为并发执行任务。真正的并行也只能出现在拥有多个CPU的系统中。 3.3 阻塞和非阻塞阻塞和非阻塞通常用来形容多线程间的相互影响，比如一个线程占有了临界区资源，那么其他线程需要这个资源就必须进行等待该资源的释放，会导致等待的线程挂起，这种情况就是阻塞，而非阻塞就恰好相反，它强调没有一个线程可以阻塞其他线程，所有的线程都会尝试地往前运行。 3.4 临界区临界区用来表示一种公共资源或者说是共享数据，可以被多个线程使用。但是每个线程使用时，一旦临界区资源被一个线程占有，那么其他线程必须等待。 volatile讲解更详细地说是要符合以下两个规则： 线程对变量进行修改之后，要立刻回写到主内存。 线程对变量读取的时候，要从主内存中读，而不是缓存。 上面提到volatile的两条语义保证了线程间共享变量的及时可见性，但整个过程并没有保证同步 对于共享普通变量来说，约定了变量在工作内存中发生变化了之后，必须要回写到工作内存(迟早要回写但并非马上回写)，但对于volatile变量则要求工作内存中发生变化之后，必须马上回写到工作内存，而线程读取volatile变量的时候，必须马上到工作内存中去取最新值而不是读取本地工作内存的副本，此规则保证了前面所说的“当线程A对变量X进行了修改后，在线程A后面执行的其他线程能看到变量X的变动”。 大部分网上的文章对于volatile的解释都是到此为止，但我觉得还是有遗漏的，提出来探讨。工作内存可以说是主内存的一份缓存，为了避免缓存的不一致性，所以volatile需要废弃此缓存。但除了内存缓存之外，在CPU硬件级别也是有缓存的，即寄存器。假如线程A将变量X由0修改为1的时候，CPU是在其缓存内操作，没有及时回写到内存，那么JVM是无法X=1是能及时被之后执行的线程B看到的，所以我觉得JVM在处理volatile变量的时候，也同样用了硬件级别的缓存一致性原则(CPU的缓存一致性原则参见《Java的多线程机制系列：(二）缓存一致性和CAS》。 如本文开头时只有在while死循环时才体现出volatile的作用，哪怕只是加了System.out.println(1)这么一小段，普通变量也能达到volatile的效果，这是什么原因呢？原来只有在对变量读取频率很高的情况下，虚拟机才不会及时回写主内存，而当频率没有达到虚拟机认为的高频率时，普通变量和volatile是同样的处理逻辑。如在每个循环中执行System.out.println(1)加大了读取变量的时间间隔，使虚拟机认为读取频率并不那么高，所以实现了和volatile的效果(本文开头的例子只在HotSpot24上测试过，没有在JRockit之类其余版本JDK上测过)。volatile的效果在jdk1.2及之前很容易重现，但随着虚拟机的不断优化，如今的普通变量的可见性已经不是那么严重的问题了 为什么volatile不能保证原子性？原子性锁提供了两种主要特性：原子性和可见性。 原子性即一次只允许一个线程持有某个特定的锁，一次就只有一个线程能够使用共享数据。可见性是必须确保释放锁之前对共享数据做出的更改对于随后获得该锁的另一个线程是可见的 。 Volatile 变量具有 synchronized 的可见性特性，但是不具备原子特性。当一个变量定义为 volatile 之后，将具备： 1.保证此变量对所有的线程的可见性，当一个线程修改了这个变量的值，volatile 保证了新值能立即同步到主内存，其它线程每次使用前立即从主内存刷新。但普通变量做不到这点，普通变量的值在线程间传递均需要通过主内存来完成。2.禁止指令重排序优化。有volatile修饰的变量，赋值后多执行了一个“load addl $0x0, (%esp)”操作，这个操作相当于一个内存屏障（指令重排序时不能把后面的指令重排序到内存屏障之前的位置） 假如说线程A在做了i+1，但未赋值的时候，线程B就开始读取i，那么当线程A赋值i=1，并回写到主内存，而此时线程B已经不再需要i的值了，而是直接交给处理器去做+1的操作，于是当线程B执行完并回写到主内存，i的值仍然是1，而不是预期的2。也就是说，volatile缩短了普通变量在不同线程之间执行的时间差，但仍然存有漏洞，依然不能保证原子性。 什么是指令重排序？有两个层面： 在虚拟机层面为了尽可能减少内存操作速度远慢于CPU运行速度所带来的CPU空置的影响，虚拟机会按照自己的一些规则(这规则后面再叙述)将程序编写顺序打乱——即写在后面的代码在时间顺序上可能会先执行，而写在前面的代码会后执行——以尽可能充分地利用CPU。拿上面的例子来说：假如不是a=1的操作，而是a=new byte1024*1024，那么它会运行地很慢，此时CPU是等待其执行结束呢，还是先执行下面那句flag=true呢？显然，先执行flag=true可以提前使用CPU，加快整体效率，当然这样的前提是不会产生错误(什么样的错误后面再说)。虽然这里有两种情况：后面的代码先于前面的代码开始执行；前面的代码先开始执行，但当效率较慢的时候，后面的代码开始执行并先于前面的代码执行结束。不管谁先开始，总之后面的代码在一些情况下存在先结束的可能。 在硬件层面CPU会将接收到的一批指令按照其规则重排序，同样是基于CPU速度比缓存速度快的原因，和上一点的目的类似，只是硬件处理的话，每次只能在接收到的有限指令范围内重排序，而虚拟机可以在更大层面、更多指令范围内重排序。硬件的重排序机制参见《从JVM并发看CPU内存指令重排序(Memory Reordering)》 内存屏障内存屏障分为两种：Load Barrier 和 Store Barrier即读屏障和写屏障。 内存屏障有两个作用： 1.阻止屏障两侧的指令重排序；2.强制把写缓冲区/高速缓存中的脏数据等写回主内存，让缓存中相应的数据失效。 对于Load Barrier来说，在指令前插入Load Barrier，可以让高速缓存中的数据失效，强制从新从主内存加载数据；对于Store Barrier来说，在指令后插入Store Barrier，能让写入缓存中的最新数据更新写入主内存，让其他线程可见。 java的内存屏障通常所谓的四种即LoadLoad,StoreStore,LoadStore,StoreLoad实际上也是上述两种的组合，完成一系列的屏障和数据同步功能。 LoadLoad屏障：对于这样的语句Load1; LoadLoad; Load2，在Load2及后续读取操作要读取的数据被访问前，保证Load1要读取的数据被读取完毕。StoreStore屏障：对于这样的语句Store1; StoreStore; Store2，在Store2及后续写入操作执行前，保证Store1的写入操作对其它处理器可见。LoadStore屏障：对于这样的语句Load1; LoadStore; Store2，在Store2及后续写入操作被刷出前，保证Load1要读取的数据被读取完毕。StoreLoad屏障：对于这样的语句Store1; StoreLoad; Load2，在Load2及后续所有读取操作执行前，保证Store1的写入对所有处理器可见。它的开销是四种屏障中最大的。在大多数处理器的实现中，这个屏障是个万能屏障，兼具其它三种内存屏障的功能 volatile的内存屏障策略非常严格保守，非常悲观且毫无安全感的心态： 在每个volatile写操作前插入StoreStore屏障，在写操作后插入StoreLoad屏障； 在每个volatile读操作前插入LoadLoad屏障，在读操作后插入LoadStore屏障； 由于内存屏障的作用，避免了volatile变量和其它指令重排序、线程之间实现了通信，使得volatile表现出了锁的特性。]]></content>
  </entry>
  <entry>
    <title><![CDATA[剑指offer面试题Java版(一)]]></title>
    <url>%2F2018%2F12%2F01%2F%E5%89%91%E6%8C%87offer%E9%9D%A2%E8%AF%95%E9%A2%98Java%E7%89%88(%E4%B8%80)%2F</url>
    <content type="text"><![CDATA[数组基础： import java.util.Arrays; import java.util.Scanner; public class jichu { public static void main(String args[]) { int[][] array = {{1, 3, 4,}, {123, 23, 4}}; int target = 3; jichu offer = new jichu(); boolean b = offer.find(array, target); System.out.println(b); System.out.println(array[0][0]); System.out.println(array[0][1]); System.out.println(array[0][2]); System.out.println(array[1][0]); System.out.println(array[1][1]); System.out.println(array[1][2]); System.out.println(&quot;一维数组转换成String,一维数组按大小排序，以为数组取最大值最小值&quot;); offer.chaxun(); System.out.println(&quot;遍历数组的几种方式==================&quot;); offer.bianli1(array); offer.bianli2(array); offer.bianli3(array); System.out.println(&quot;实现数组交换----------------&quot;); offer.jiaohuan(array); System.out.println(&quot;打印杨辉三角=========================&quot;); offer.yanghhui(); } public void chaxun() { int[] a = new int[]{80, 23, 46, 26}; System.out.println(Arrays.toString(a)); Arrays.sort(a); System.out.println(Arrays.toString(a)); System.out.println(&quot;最小值：&quot; + a[0] + &quot;, 最大值&quot; + a[a.length - 1]); System.out.println(Arrays.binarySearch(a, 450)); } //遍历二维数组 public void bianli1(int[][] arr) { for (int i = 0; i &lt; arr.length; i++) { for (int j = 0; j &lt; arr[i].length; j++) { System.out.print(arr[i][j] + &quot; &quot;); } System.out.println(); //换行 } } public void bianli2(int[][] arr) { for (int[] a : arr) { for (int b : a) { System.out.print(b + &quot; &quot;); } System.out.println(); } } public void bianli3(int[][] arr) { System.out.println(Arrays.toString(arr[0])); for (int i = 0; i &lt; arr.length; i++) System.out.println(Arrays.toString(arr[i])); } //二维数组所有的都头尾交换 public void jiaohuan(int[][] arr) { for (int i = 0; i &lt; arr.length; i++) { for (int j = 0; j &lt; arr[i].length; j++) { System.out.print(arr[i][j] + &quot; &quot;); } System.out.println(); //换行 } for (int start = 0, end = arr.length - 1; start &lt; end; start++, end--) { int[] temp = arr[start]; arr[start] = arr[end]; arr[end] = temp; } for (int i = 0; i &lt; arr.length; i++) { for (int j = 0; j &lt; arr[i].length; j++) { System.out.print(arr[i][j] + &quot; &quot;); } System.out.println(); //换行 } } public void yanghhui() { //从控制台获取行数 Scanner s = new Scanner(System.in); int row = s.nextInt(); //根据行数定义好二维数组，由于每一行的元素个数不同，所以不定义每一行的个数 int[][] arr = new int[row][]; //遍历二维数组 for (int i = 0; i &lt; row; i++) { //初始化每一行的这个一维数组 arr[i] = new int[i + 1]; //遍历这个一维数组，添加元素 for (int j = 0; j &lt;= i; j++) { //每一列的开头和结尾元素为1，开头的时候，j=0，结尾的时候，j=i if (j == 0 || j == i) { arr[i][j] = 1; } else {//每一个元素是它上一行的元素和斜对角元素之和 arr[i][j] = arr[i - 1][j] + arr[i - 1][j - 1]; } System.out.print(arr[i][j] + &quot;\t&quot;); } System.out.println(&quot;&quot;); } } public void yanghuipra() { Scanner scanner = new Scanner(System.in); int row = scanner.nextInt(); int[][] arr = new int[row][]; for (int i = 0; i &lt; row; i++) { arr[i] = new int[i + 1]; for (int j = 0; j &lt; row; j++) { if (j == 0 || j == i) { arr[i][j] = 1; } else { arr[i][j] = arr[i - 1][j - 1] + arr[i - 1][j]; } System.out.println(arr[i][j] + &quot;\n&quot;); } } } } public static void main(String[] args) { //// write your code here int[][] A=new int[][]{{1,2},{4,5},{7,8,10,11,12},{}}; System.out.println(A.length);//4,表示数组的行数 System.out.println(A[0].length);//2，表示对应行的长度 System.out.println(A[1].length);//2 System.out.println(A[2].length);//5 } if语句基础： 第一个个算法 在一个二维数组中，每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数 public static void main(String args[]) { int[][] array = {{1, 3, 4,}, {123, 23, 4}}; int target = 3; jichu offer = new jichu(); offer.find(); } public boolean find(int[][] array, int target) { if (array == null) { return false; } int row = 0; int column = array[0].length - 1; while (row &lt; array.length &amp;&amp; column &gt;= 0) { if (array[row][column] == target) { return true; } if (array[row][column] &gt; target) { column--; } else { row++; } } return false; } 第二个算法：将一个字符串中的空格替换成“%20”。]]></content>
      <categories>
        <category>面试</category>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>面试</tag>
        <tag>剑指offer</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer面试题Java版]]></title>
    <url>%2F2018%2F12%2F01%2F%E5%89%91%E6%8C%87offer%E9%9D%A2%E8%AF%95%E9%A2%98Java%E7%89%88%2F</url>
    <content type="text"></content>
      <categories>
        <category>算法</category>
        <category>面试</category>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>面试</tag>
        <tag>剑指offer</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSM第三天]]></title>
    <url>%2F2018%2F11%2F29%2FSSM%E7%AC%AC%E4%B8%89%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[部署tomcat服务器1.下载apache-tomcat-7.0.70-windows-x64.zip 2.解压文件，不要放到中文或者空格目录下。 3.进入加压目录. cmd&gt;cd {tomcat_home}/bin 4.执行命令 cmd&gt;startup.bat 5.通过浏览器访问页面，查看是否启动成功。 http://localhost:8080/ tomcat目录结构bin //执行文件 conf //配置文件目录,server.xml,修改8080. webapps //项目部署目录,项目打成war包，运行期间自行解压。 work //临时目录 logs //日志目录 在idea中开发web项目1.在idea中配置tomcat服务器 settings &gt; applications server -&gt; + --&gt; 定位到tomcat解压目录 -&gt;ok 2.创建java模块 + javaEE支持 + maven支持. 3.运行web程序 配置idea中tomcat热部署0.关闭tomcat服务器 1.run --&gt; edit configuration 2.Server选项卡--&gt; VM options部分 on &quot;Update&quot; action : update Classes and resources on Frame deactivation : update Classes and resources 3.启动服务器要选择&quot;debug&quot;模式. 在web模块中添加mvn支持这个地方controller返回的是modelandview然后在返回给分发器程序的。返回的是模型视图的逻辑名。而且还需要配置一个视图解析器。 handlemapping controller viewresolve需要配置到beans.xml里面，这个文件可以随意配置名字。 1.在pom.xml引入springmvc的依赖 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.it18zhang&lt;/groupId&gt; &lt;artifactId&gt;myspringmvc&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;4.3.3.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; 2.在web/WEB-INF/web.xml文件中配置DispatcherServlet分发器. &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;web-app xmlns=&quot;http://xmlns.jcp.org/xml/ns/javaee&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_3_1.xsd&quot; version=&quot;3.1&quot;&gt; &lt;!-- 配置分发器Servlet --&gt; &lt;servlet&gt; &lt;servlet-name&gt;dispatcher&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;dispatcher&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;/web-app&gt; 3.配置springmvc配置文件，使用注解驱动配置项(默认名称就是dispatcher-servlet.xml) [web/WEB-INF/dispatcher-servlet.xml] &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc-4.3.xsd&quot;&gt; &lt;!-- 使用注解驱动 --&gt; &lt;mvc:annotation-driven /&gt; &lt;/beans&gt; 4.编写控制器类 [HomeController.java] package com.it18zhang.springmvc.web.controller; import org.springframework.stereotype.Controller; import org.springframework.web.bind.annotation.RequestMapping; /** * HomeController */ @Controller public class HomeController { /** * 打开主页 */ @RequestMapping(&quot;/home&quot;) public String openHome(){ System.out.println(&quot;hello world&quot;); return null ; } } 5.配置dispatcher-servlet.xml文件，增加扫描路径配置。 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc-4.3.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.3.xsd&quot;&gt; &lt;!-- 配置扫描路径 --&gt; &lt;context:component-scan base-package=&quot;com.it18zhang.springmvc.web.controller&quot; /&gt; &lt;!-- 使用注解驱动 --&gt; &lt;mvc:annotation-driven /&gt; &lt;/beans&gt; 6.启动程序，访问地址 http://localhost:9090/ 7.出现类找不到的原因。 idea的web项目默认不会将依赖类库放置到web-inf/lib下，需要手动设置详情见PPT。 project structure -&gt; artifacts -&gt; myspringmvc:war exploded -&gt; 选择 output layout选项卡 -&gt; 选择右侧的available elements下myspringmvc条目的所有类库 -&gt;右键 -&gt; put into WEB-INF/lib即可。 8.运行程序。 9.配置Spring MVC是视图解析器. [web/WEB-INF/dispatcher-servlet.xml] &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc-4.3.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.3.xsd&quot;&gt; &lt;!-- 配置扫描路径 --&gt; &lt;context:component-scan base-package=&quot;com.it18zhang.springmvc.web.controller&quot; /&gt; &lt;!-- 使用注解驱动 --&gt; &lt;mvc:annotation-driven /&gt; &lt;!-- 内部资源视图解析器 --&gt; &lt;bean class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt; &lt;property name=&quot;prefix&quot; value=&quot;/&quot; /&gt; &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot;/&gt; &lt;/bean&gt; &lt;/beans&gt; 10.添加jsp页面和控制代码 [HomeController.java] package com.it18zhang.springmvc.web.controller; import org.springframework.stereotype.Controller; import org.springframework.web.bind.annotation.RequestMapping; /** * HomeController */ @Controller public class HomeController { /** * 打开主页 */ @RequestMapping(&quot;/home&quot;) public String openHome(){ System.out.println(&quot;hello world&quot;); return &quot;index&quot;; } /** * 打开主页 */ @RequestMapping(&quot;/home2&quot;) public String home2(){ System.out.println(&quot;how are you???&quot;); return &quot;index2&quot;; } } 11./web/index.jsp + /web/index2.jsp [/web/index2.jsp] &lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;index2.jsp&lt;/title&gt; &lt;/head&gt; &lt;body&gt; welcome to spring mvc !!! &lt;/body&gt; &lt;/html&gt; html:标签类型 inline //行内标签,自己不占一行,和其他标签可以在一行.&lt;br&gt; block //块标签,自己占一行。 &lt;a href=&quot;&quot;&gt;百度&lt;/a&gt; 模拟注册行为1.创建/web/reg.jsp &lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;reg.jsp&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;form action=&quot;/doReg2&quot; method=&quot;post&quot;&gt; UserName : &lt;input type=&quot;text&quot; name=&quot;username&quot;&gt;&lt;br&gt; Password : &lt;input type=&quot;password&quot; name=&quot;password&quot;&gt;&lt;br&gt; &lt;input type=&quot;submit&quot;/&gt; &lt;/form&gt; &lt;/body&gt; &lt;/html&gt; 2&apos;.引入Servlet API类库 [pom.xml] &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.it18zhang&lt;/groupId&gt; &lt;artifactId&gt;myspringmvc&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;4.3.3.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;servlet-api&lt;/artifactId&gt; &lt;version&gt;2.5&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; 2.创建RegController.java package com.it18zhang.springmvc.web.controller; import org.springframework.stereotype.Controller; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RequestParam; import javax.servlet.http.HttpServletRequest; /** * */ @Controller public class RegController { @RequestMapping(&quot;/toReg&quot;) public String toRegView(){ return &quot;reg&quot; ; } @RequestMapping(&quot;/doReg&quot;) public String doReg(@RequestParam(&quot;username&quot;) String username, @RequestParam(&quot;password&quot;) String password){ System.out.println(&quot;插入数据&quot;); System.out.println(username + &quot;,&quot; + password); return &quot;index&quot; ; } @RequestMapping(&quot;/doReg2&quot;) public String doReg(HttpServletRequest req) { System.out.println(&quot;插入数据222&quot;); String user = req.getParameter(&quot;username&quot;); System.out.println(user); return &quot;index&quot;; } } 引入jstl标签库,jee标准标签库。1.添加pom.xml依赖 &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;jstl&lt;/artifactId&gt; &lt;version&gt;1.2&lt;/version&gt; &lt;/dependency&gt; 2.修改jsp页面,声明标签库并使用标签 &lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt; &lt;%@ taglib uri=&quot;http://java.sun.com/jsp/jstl/core&quot; prefix=&quot;c&quot; %&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;reg.jsp&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;form action=&apos;&lt;c:url value=&quot;/reg.jsp&quot; /&gt;&apos; method=&quot;post&quot;&gt; UserName : &lt;input type=&quot;text&quot; name=&quot;username&quot;&gt;&lt;br&gt; Password : &lt;input type=&quot;password&quot; name=&quot;password&quot;&gt;&lt;br&gt; &lt;input type=&quot;submit&quot;/&gt; &lt;/form&gt; &lt;/body&gt; &lt;/html&gt; 3.修改上下文名称 project structure --&gt; artifacts --&gt; edit 4. 5. 模拟查询-查询一个User对象1.添加方法 class RegController{ ... /*****从请求中提取uid参数******/ @RequestMapping(&quot;/selectOne&quot;) public String selectOne(Model model , @RequestParam(&quot;uid&quot;) int uid){ System.out.println(&quot;接受到了参数 : uid = &quot; + uid); String username =&quot;tomson&quot; ; //将数据存放到model中，向jsp传递. model.addAttribute(&quot;myusername&quot;, username); return &quot;selectOne&quot; ; } } 2.创建selectOne.jsp [selectOne.jsp] &lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt; &lt;%@ taglib uri=&quot;http://java.sun.com/jsp/jstl/core&quot; prefix=&quot;c&quot; %&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;selectOne.jsp&lt;/title&gt; &lt;/head&gt; &lt;body&gt; username : &lt;c:out value=&quot;${myusername}&quot; /&gt; &lt;/body&gt; &lt;/html&gt; 3.打开浏览器输入地址; http://localhost:9090/selectOne?uid=100 模拟查询-查询全部信息1.定义User类。 public class User { private Integer id; private String name; private int age; ... //get/set } 2.在RegController中添加方法 class RegController{ ... @RequestMapping(&quot;/selectAll&quot;) public String selectAll(Model m){ List&lt;User&gt; list = new ArrayList&lt;User&gt;(); for(int i = 1 ; i &lt;= 50 ; i ++){ User u = new User(); u.setId(i); u.setName(&quot;tom&quot; + i); u.setAge(i % 20); list.add(u) ; } // m.addAttribute(&quot;allUsers&quot;,list); return &quot;userList&quot; ; } 3.创建userList.jsp &lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt; &lt;%@ taglib uri=&quot;http://java.sun.com/jsp/jstl/core&quot; prefix=&quot;c&quot; %&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;selectOne.jsp&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;table border=&quot;1px&quot;&gt; &lt;tr&gt; &lt;td&gt;ID&lt;/td&gt; &lt;td&gt;NAME&lt;/td&gt; &lt;td&gt;AGE&lt;/td&gt; &lt;/tr&gt; &lt;c:forEach items=&quot;${allUsers}&quot; var=&quot;u&quot;&gt; &lt;tr&gt; &lt;td&gt;&lt;c:out value=&quot;${u.id}&quot;/&gt;&lt;/td&gt; &lt;td&gt;&lt;c:out value=&quot;${u.name}&quot;/&gt;&lt;/td&gt; &lt;td&gt;&lt;c:out value=&quot;${u.age}&quot;/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/c:forEach&gt; &lt;/table&gt; &lt;/body&gt; &lt;/html&gt; 4.启动服务器,输入地址 http://localhost:9090/selectAll forward:请求转发，在服务器内部完成。客户端不参与，地址栏不改变。 而且只能转发到本应用的其他路径上。共享请求参数。 redirect 重定向，客户端参与，地址栏变，可以重定向到任意url地址。 不能共享变量。]]></content>
      <tags>
        <tag>SSM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java基础（关于面试）]]></title>
    <url>%2F2018%2F11%2F28%2FJava%E5%9F%BA%E7%A1%80%EF%BC%88%E5%85%B3%E4%BA%8E%E9%9D%A2%E8%AF%95%EF%BC%89%2F</url>
    <content type="text"><![CDATA[bit意为“位”或“比特”，是电子计算机中最小的数据单位，是计算机存储设备的最小单位，每一位的状态只能是0或1。 Byte意为“字节”，8个二进制位构成1个”字节(Byte)”，即1Byte=8bit,两者换算是1：8的关系，字节是计算机处理数据的基本单位，即以字节为单位解释信息。1个字节可以储存1个英文字母或者半个汉字，换句话说，1个汉字占据2个字节的存储空间。 发现 数据类型占内存的位数实际上与操作系统的位数和编译器（不同编译器支持的位数可能有所不同）都有关，具体某种数据类型占字节数得编译器根据操作系统位数两者之间进行协调好后分配内存大小。具体在使用的时候如想知道具体占内存的位数通过sizeof(int)可以得到准确的答案。 int 和byte[]之间的转换 ArrayList和LinkedList的区别-完整总结1.ArrayList是实现了基于动态数组的数据结构，每个元素在内存中存储地址是连续的；LinkedList基于链表的数据结构，每个元素内容包扩previous, next, element（其中，previous是该节点的上一个节点，next是该节点的下一个节点，element是该节点所包含的值），也是由于这一性质支持了每个元素在内存中分布存储。 2.为了使得突破动态长度数组而衍生的ArrayList初始容量为10，每次扩容会固定为之前的1.5倍，所以当你ArrayList达到一定量之后会是一种很大的浪费，并且每次扩容的过程是内部复制数组到新数组；LinkedList的每一个元素都需要消耗一定的空间 3.对于每个元素的检索，ArrayList要优于LinkedList。因为ArrayList从一定意义上来说，就是复杂的数组，所以基于数组index的 检索性能显然高于通过for循环来查找每个元素的LinkedList。 4.元素插入删除的效率对比，要视插入删除的位置来分析，各有优劣 在列表首位添加（删除）元素，LnkedList性能远远优于ArrayList,原因在于ArrayList要后移（前移）每个元素的索引和数组扩容（删除元素时则不需要扩容）。（测试的时候当然插入一次是看不出来什么的，我自己测试插入十万次，就会有数组扩容arraycopy的因素）而LinkedList则直接增加元素，修改原第一元素该节点的上一个节点即可，删除同理 在列表中间位置添加（删除）元素，总的来说位置靠前则LnkedList性能优于ArrayList，靠后则相反。出现这种情况的原因在于ArrayList性能主要损耗在后移（前移）该位置之后的元素索引，而LinkedList损耗在for循环从第一位检索该位置的元素。这个性能反转的临界点不固定，我自己测试插入十万次，在15000次左右损耗时间相比出现变化 在列表末尾位置添加（删除）元素，性能相差不大。]]></content>
      <categories>
        <category>Java基础</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>Java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka]]></title>
    <url>%2F2018%2F11%2F25%2FKafka%2F</url>
    <content type="text"><![CDATA[flume收集日志、移动、聚合框架。 基于事件。 agentsource //接收数据,生产者 //put() //NetcatSource //ExecSource,实时收集 tail -F xxx.txt //spooldir //seq //Stress //avroSource channel //暂存数据，缓冲区, //非永久性:MemoryChannel //永久性 :FileChannel,磁盘. //SpillableMemoryChannel :Mem + FileChannel.Capacity sink //输出数据,消费者 //从channel提取take()数据,write()destination. //HdfsSink //HbaseSink //avroSink 看一下一边都是存储到哪里，如果是电信的那种需要经常查询的就需要放到Hbase里面，如果是放到hdfs里面就只能是全表扫描了。hbase可以随机定位。瞬间定位。 JMSjava message service,java消息服务。 queue //只有能有一个消费者。P2P模式(点对点). //发布订阅(publish-subscribe,主题模式)， kafka分布式流处理平台。 在系统之间构建实时数据流管道。 以topic分类对记录进行存储 每个记录包含key-value+timestamp 每秒钟百万消息吞吐量。 producer //消息生产者 consumer //消息消费者 consumer group //消费者组 kafka server //broker,kafka服务器也叫broker topic //主题,副本数,分区. zookeeper //hadoop namenoade + RM HA | hbase | kafka 安装kafka0.选择s202 ~ s204三台主机安装kafka 1.准备zk 略 2.jdk 略 3.tar文件 4.环境变量 略 5.配置kafka [kafka/config/server.properties] ... broker.id=202 ... listeners=PLAINTEXT://:9092 ... log.dirs=/home/centos/kafka/logs ... zookeeper.connect=s201:2181,s202:2181,s203:2181 6.分发server.properties，同时修改每个文件的broker.id 7.启动kafka服务器 a)先启动zk b)启动kafka [s202 ~ s204] $&gt;bin/kafka-server-start.sh config/server.properties c)验证kafka服务器是否启动 $&gt;netstat -anop | grep 9092 8.创建主题 $&gt;bin/kafka-topics.sh --create --zookeeper s201:2181 --replication-factor 3 --partitions 3 --topic test 9.查看主题列表 $&gt;bin/kafka-topics.sh --list --zookeeper s201:2181 10.启动控制台生产者 $&gt;bin/kafka-console-producer.sh --broker-list s202:9092 --topic test1 11.启动控制台消费者 $&gt;bin/kafka-console-consumer.sh --bootstrap-server s202:9092 --topic test1 --from-beginning --zookeeper s202:2181 12.在生产者控制台输入hello world kafka集群在zk的配置/controller ===&gt; {&quot;version&quot;:1,&quot;brokerid&quot;:202,&quot;timestamp&quot;:&quot;1490926369148&quot; /controller_epoch ===&gt; 1 /brokers /brokers/ids //记载kfk集群每个服务器的信息 /brokers/ids/202 ===&gt; {&quot;jmx_port&quot;:-1,&quot;timestamp&quot;:&quot;1490926370304&quot;,&quot;endpoints&quot;:[&quot;PLAINTEXT://s202:9092&quot;],&quot;host&quot;:&quot;s202&quot;,&quot;version&quot;:3,&quot;port&quot;:9092} //每个节点的连接信息 /brokers/ids/203 /brokers/ids/204 //每个主题下分区数据，主题是有分区的。 /brokers/topics/test/partitions/0/state ===&gt;{&quot;controller_epoch&quot;:1,&quot;leader&quot;:203,&quot;version&quot;:1,&quot;leader_epoch&quot;:0,&quot;isr&quot;:[203,204,202]} /brokers/topics/test/partitions/1/state ===&gt;... /brokers/topics/test/partitions/2/state ===&gt;... leader是针对于主题来说的，是针对主题上的分区来说的。每个分区test主题下。controller也是kfk注册的他和broker是一个层级。说明在kfk集群里面s202是类似于Leader的身份。在 /brokers/seqid ===&gt; null /admin /admin/delete_topics/test ===&gt;标记删除的主题 /isr_change_notification /consumers/xxxx/ /config 下图是生产者没有连接zk别的都是有连接zk的 容错创建主题//2个副本5个分区，2乘以5对应了10个文件夹分配到3个节点所以s202里面有3个， repliation_factor 2 partitions 5 $&gt;kafka-topic.sh --zookeeper s202:2181 --replication_factor 3 --partitions 4 --create --topic test3 2 x 5 = 10 //是个文件夹 [s202] test2-1 // test2-2 // test2-3 // [s203] test2-0 test2-2 test2-3 test2-4 [s204] test2-0 test2-1 test2-4 重新布局分区和副本，手动再平衡$&gt;kafka-topics.sh --alter --zookeeper s202:2181 --topic test2 --replica-assignment 203:204,203:204,203:204,203:204,203:204 副本broker存放消息以消息达到顺序存放。生产和消费都是副本感知的。 支持到n-1故障。每个分区都有leader，follow. leader挂掉时，消息分区写入到本地log或者，向生产者发送消息确认回执之前，生产者向新的leader发送消息。 新leader的选举是通过isr进行，第一个注册的follower成为leader。 kafka支持副本模式[同步复制] 1.producer联系zk识别leader 2.向leader发送消息 3.leadr收到消息写入到本地log 4.follower从leader pull消息 5.follower向本地写入log 6.follower向leader发送ack消息 7.leader收到所有follower的ack消息 8.leader向producer回传ack [异步副本] 和同步复制的区别在与leader写入本地log之后， 直接向client回传ack消息，不需要等待所有follower复制完成。 通过java API实现消息生产者，发送消息package com.it18zhang.kafkademo.test; import org.junit.Test; import kafka.javaapi.producer.Producer; import kafka.producer.KeyedMessage; import kafka.producer.ProducerConfig; import java.util.HashMap; import java.util.Properties; /** * Created by Administrator on 2017/3/31. */ public class TestProducer { @Test public void testSend(){ Properties props = new Properties(); //broker列表 props.put(&quot;metadata.broker.list&quot;, &quot;s202:9092&quot;); //串行化 props.put(&quot;serializer.class&quot;, &quot;kafka.serializer.StringEncoder&quot;); // props.put(&quot;request.required.acks&quot;, &quot;1&quot;); //创建生产者配置对象 ProducerConfig config = new ProducerConfig(props); //创建生产者 Producer&lt;String, String&gt; producer = new Producer&lt;String, String&gt;(config); KeyedMessage&lt;String, String&gt; msg = new KeyedMessage&lt;String, String&gt;(&quot;test3&quot;,&quot;100&quot; ,&quot;hello world tomas100&quot;); producer.send(msg); System.out.println(&quot;send over!&quot;); } } 消息消费者/** * 消费者 */ @Test public void testConumser(){ // Properties props = new Properties(); props.put(&quot;zookeeper.connect&quot;, &quot;s202:2181&quot;); props.put(&quot;group.id&quot;, &quot;g3&quot;); props.put(&quot;zookeeper.session.timeout.ms&quot;, &quot;500&quot;); props.put(&quot;zookeeper.sync.time.ms&quot;, &quot;250&quot;); props.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;); props.put(&quot;auto.offset.reset&quot;, &quot;smallest&quot;); //创建消费者配置对象 ConsumerConfig config = new ConsumerConfig(props); // Map&lt;String, Integer&gt; map = new HashMap&lt;String, Integer&gt;(); map.put(&quot;test3&quot;, new Integer(1)); Map&lt;String, List&lt;KafkaStream&lt;byte[], byte[]&gt;&gt;&gt; msgs = Consumer.createJavaConsumerConnector(new ConsumerConfig(props)).createMessageStreams(map); List&lt;KafkaStream&lt;byte[], byte[]&gt;&gt; msgList = msgs.get(&quot;test3&quot;); for(KafkaStream&lt;byte[],byte[]&gt; stream : msgList){ ConsumerIterator&lt;byte[],byte[]&gt; it = stream.iterator(); while(it.hasNext()){ byte[] message = it.next().message(); System.out.println(new String(message)); } } } flume集成kafka1.KafkaSink [生产者] a1.sources = r1 a1.sinks = k1 a1.channels = c1 a1.sources.r1.type=netcat a1.sources.r1.bind=localhost a1.sources.r1.port=8888 a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink a1.sinks.k1.kafka.topic = test3 a1.sinks.k1.kafka.bootstrap.servers = s202:9092 a1.sinks.k1.kafka.flumeBatchSize = 20 a1.sinks.k1.kafka.producer.acks = 1 a1.channels.c1.type=memory a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 2.KafkaSource [消费者] a1.sources = r1 a1.sinks = k1 a1.channels = c1 a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource a1.sources.r1.batchSize = 5000 a1.sources.r1.batchDurationMillis = 2000 a1.sources.r1.kafka.bootstrap.servers = s202:9092 a1.sources.r1.kafka.topics = test3 a1.sources.r1.kafka.consumer.group.id = g4 a1.sinks.k1.type = logger a1.channels.c1.type=memory a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 3.Channel 生产者 + 消费者 a1.sources = r1 a1.sinks = k1 a1.channels = c1 a1.sources.r1.type = avro a1.sources.r1.bind = localhost a1.sources.r1.port = 8888 a1.sinks.k1.type = logger a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel a1.channels.c1.kafka.bootstrap.servers = s202:9092 a1.channels.c1.kafka.topic = test3 a1.channels.c1.kafka.consumer.group.id = g6 a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 flume重启后会重新读取里面的内容，那么怎么解决重复读取？要引进行序号，每一行都是一个事件，所以在sink里面之后写入redis或者hbase里面因为是高速的，下次再启动flume，虽然从头开始读，也往通道里面放，但是对于sink来讲，会判断他的行号，如果序号比我的数据存的要早，就滤过。比这个大就往里写。相当于一个拦截器]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hbase第四天]]></title>
    <url>%2F2018%2F11%2F22%2FHbase%E7%AC%AC%E5%9B%9B%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[hbase协处理器. Observer //触发器,基于事件激活的。 Endpoint //存储过程,客户端调用。 RegionObserver //system --&gt; user[加载顺序] 100 00-99 callerId - 201703 : hashcode % 100 = 00-99 01,139xxxx,138yyy,.... 热点让数据均匀分散。 create ‘ns1:calllogs’ , SPLITS=&gt;[01,02,03,,…99,] rowkey按照byte排序。 create table xxx(){ } rowkey分区编号 xx,callerId,callTime,calleeId startkey = xx,19626675332, startkey = xx,19626675333, 对通话记录表的设计：（具体在HBase的设计原则2） 对于首先创建的主叫的表是上面这张表，在查询主叫的时候只需要指定xx，主叫时间,时间片。即可。但是查询被叫的时候就不行了。几乎要全表扫描。用00,138xx,2017010101,139xxx这样查询。所以每次向这个表写记录的时候，我们都知道被叫是谁，如果知道被叫的话。我们可以在设计一张表，叫calleeLog,他的rowkey有calleid，time,callerid构成。被叫表存的value存的是主叫表rowkey里面的被叫。然后根据这个查到主叫表后面的内容。但是如果只想知道谁给你打的电话，所以在被叫表的rowkey里面加了一个callerid，如果想查询谁给你打了电话，就在被叫表rowkey里面加了一个callerid。主叫表的rowkey里面都是作为主叫出现的，被叫表里面的数据都是作为被叫出现的。主叫表的内容被叫表里没有，被叫表的内容主叫表也没有的。而且应该吧时长duration也放在里面。也就是说要把最经常使用的信息都编入rowkey里面去，能不查具体的value就尽量不查询具体的value，但是如果要查询具体的数据，什么基站，那个口啊，就是要查询主叫表里面的value的值。这个下图所示的也就是叫二次索引。在写入主叫表的时候也在往被叫表里面写入，用什么写入？也就是用协处理器来处理，怎么处理呢，就是在你写入主叫表的时候，协处理器立刻截获，然后重写里面的方法，往被叫表里面写入就可以了。所以这个就是一个电信HBase的设计原则。 通化记录1.创建表 create &apos;ns1:calllogs&apos;,&apos;f1&apos; 2.创建单元测试 @Test public void put() throws Exception { Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); TableName tname = TableName.valueOf(&quot;ns1:calllogs&quot;); Table table = conn.getTable(tname); String callerId = &quot;13845456767&quot; ; String calleeId = &quot;139898987878&quot; ; SimpleDateFormat sdf = new SimpleDateFormat(); sdf.applyPattern(&quot;yyyyMMddHHmmss&quot;); String callTime = sdf.format(new Date()); int duration = 100 ; DecimalFormat dff = new DecimalFormat(); dff.applyPattern(&quot;00000&quot;); String durStr = dff.format(duration); //区域00-99 int hash = (callerId + callTime.substring(0, 6)).hashCode(); hash = (hash &amp; Integer.MAX_VALUE) % 100 ; //hash区域号 DecimalFormat df = new DecimalFormat(); df.applyPattern(&quot;00&quot;); String regNo = df.format(hash); //拼接rowkey //xx , callerid , time , direction, calleid ,duration String rowkey = regNo + &quot;,&quot; + callerId + &quot;,&quot; + callTime + &quot;,&quot; + &quot;0,&quot; + calleeId + &quot;,&quot; + durStr ; byte[] rowid = Bytes.toBytes(rowkey); Put put = new Put(rowid); put.addColumn(Bytes.toBytes(&quot;f1&quot;),Bytes.toBytes(&quot;callerPos&quot;),Bytes.toBytes(&quot;河北&quot;)); put.addColumn(Bytes.toBytes(&quot;f1&quot;),Bytes.toBytes(&quot;calleePos&quot;),Bytes.toBytes(&quot;河南&quot;)); //执行插入 table.put(put); System.out.println(&quot;over&quot;); } 3.创建协处理器 public class CalleeLogRegionObserver extends BaseRegionObserver{ public void postPut(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Put put, WALEdit edit, Durability durability) throws IOException { super.postPut(e, put, edit, durability); // TableName callLogs = TableName.valueOf(&quot;calllogs&quot;); //得到当前的TableName对象 TableName tableName = e.getEnvironment().getRegion().getRegionInfo().getTable(); if(!callLogs.equals(tableName)){ return ; } //得到主叫的rowkey //xx , callerid , time , direction, calleid ,duration //被叫:calleid,time, String rowkey = Bytes.toString(put.getRow()); String[] arr = rowkey.split(&quot;,&quot;); String hash = Util.getRegNo(arr[4],arr[2]); //hash String newRowKey = hash + &quot;,&quot; + arr[4] + &quot;,&quot; + arr[2] + &quot;,1,&quot; + arr[1] + &quot;,&quot; + arr[5] ; Put newPut = new Put(Bytes.toBytes(newRowKey)); Table t = e.getEnvironment().getTable(tableName); t.put(newPut); } } 4.配置hbase-site.xml并分发分发jar包。 &lt;property&gt; &lt;name&gt;hbase.coprocessor.region.classes&lt;/name&gt; &lt;value&gt;com.it18zhang.hbasedemo.coprocessor.CalleeLogRegionObserver&lt;/value&gt; &lt;/property&gt; 5.启动hbase集群. BloomFilter布隆过滤器。 代码如下: phonix1.安装phonix a)下载apache-phoenix-4.10.0-HBase-1.2-bin.tar.gz b)tar c)复制xxx-server.jar到hbase的lib目录，并且分发,删除以前的phonixjar包。 d)重启hbase 2.使用phonix的命令行程序 $&gt;phonix/bin/.sqlline.py s202 //连接的是zk服务器 $phonix&gt;!tables $phonix&gt;!help //查看帮助 phoenix在创建表的时候要使用大量的协处理器，他是在建表时候不区分大小写的，而且hbase不可以识别得出来他的表，但是hbase shell里面建的表他能识别。 2.SQL Client安装 a)下载squirrel-sql-3.7.1-standard.jar 该文件是安装文件，执行的安装程序。 $&gt;jar -jar squirrel-sql-3.7.1-standard.jar $&gt;下一步... b)复制phoenix-4.10.0-HBase-1.2-client.jar到SQuerrel安装目录的lib下(c:\myprograms\squirrel)。 c)启动SQuirrel(GUI) 定位安装目录-&gt;执行squirrel-sql.bat d)打开GUI界面 d)在左侧的边栏选中&quot;Drivers&quot;选项卡， 点击 &quot;+&quot; -&gt; URL : jdbc:phoenix:192.168.231.202 Driverclass : org.apache.phoenix.jdbc.PhoenixDriver jdbc:phoenix: s202 d)测试。 3.SQLLine客户端操作 //建表 $jdbc:phoenix&gt;create table IF NOT EXISTS test.Person (IDCardNum INTEGER not null primary key, Name varchar(20),Age INTEGER); //插入数据 $jdbc:phoenix&gt;UPSERT INTO test.PERSON(IDCardNum , Name,Age) VALUES (1,&apos;tom&apos;,12); //删除数据 $jdbc:phoenix&gt;delete from test.persion where idcardnum = 1 ; //更新数据 //upsert into test.PERSON(IDCardNum , Name,Age) VALUES (1,&apos;tom&apos;,12);]]></content>
  </entry>
  <entry>
    <title><![CDATA[Flume]]></title>
    <url>%2F2018%2F11%2F21%2FFlume%2F</url>
    <content type="text"><![CDATA[hbaseNoSQL. 面向列族。 随机定位 实时读写。 分布式 可伸缩 HA zookeeper version(列族) rowkey/famil+qualifier/timestamp = value rowkey //唯一性,散列性,定长,不要太长,加盐. 二次索引 byte[] hive离线计算。 MR:MapReducehadoop : DBWritable + WritableComparable : 将hbase的表影射到hive上，使用hive的查询语句。CREATE TABLE mydb.t11(key string, name string) STORED BY &apos;org.apache.hadoop.hive.hbase.HBaseStorageHandler&apos; WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,cf:name&quot;) TBLPROPERTIES(&quot;hbase.table.name&quot; = &quot;ns1:t11&quot;); select count(*) from mydb.t11 ; flume收集、移动、聚合大量日志数据的服务。 基于流数据的架构，用于在线日志分析。 基于事件。 在生产和消费者之间启动协调作用。 提供了事务保证，确保消息一定被分发。 Source 多种 sink多种. multihop //多级跃点.可以从一个flume到另外一个flume 水平扩展: //加节点， 竖直扩展 //增加硬件。 Source接受数据，类型有多种。 Channel临时存放地，对Source中来的数据进行缓冲，直到sink消费掉。 Sink从channel提取数据存放到中央化存储(hadoop / hbase)。 实时产生的数据， Flume的优点以下是使用Flume的优点：使用Apache Flume，我们可以将数据存储到任何集中存储中（HBase，HDFS）。当传入数据的速率超过可以写入数据的速率时目的地，Flume充当数据生产者和数据生成者之间的中介集中存储并在它们之间提供稳定的数据流。Flume提供了上下文路由的功能。 FLUME - 介绍App Flume2Flume中的交易是基于渠道的，其中两个交易（一个发件人为每条消息维护一个接收器。它保证了可靠的信息交货。Flume具有可靠性，容错性，可扩展性，可管理性和可定制性。水槽的特点Flume的一些显着特征如下：Flume将来自多个Web服务器的日志数据提取到集中存储（HDFS，HBase）有效。使用Flume，我们可以立即将来自多个服务器的数据导入Hadoop。与日志文件一起，Flume还用于导入大量事件数据由Facebook和Twitter等社交网站和电子商务制作亚马逊和Flipkart等网站。Flume支持大量源和目标类型。Flume支持多跳流，扇入扇出流，上下文路由等。水槽可以水平缩放 安装flume1.下载 2.tar 3.环境变量 4.验证flume是否成功 $&gt;flume-ng version //next generation.下一代. 配置flume1.创建配置文件 [/soft/flume/conf/hello.conf] #声明三种组件 a1.sources = r1 a1.channels = c1 a1.sinks = k1 #定义source信息 a1.sources.r1.type=netcat a1.sources.r1.bind=localhost a1.sources.r1.port=8888 #定义sink信息 a1.sinks.k1.type=logger #定义channel信息 a1.channels.c1.type=memory #绑定在一起 a1.sources.r1.channels=c1 a1.sinks.k1.channel=c1 2.运行 a)启动flume agent $&gt;bin/flume-ng agent -f ../conf/helloworld.conf -n a1 -Dflume.root.logger=INFO,console b)启动nc的客户端 $&gt;nc localhost 8888 $nc&gt;hello world c)在flume的终端输出hello world. 安装nc$&gt;sudo yum install nmap-ncat.x86_64 清除仓库缓存$&gt;修改ali.repo --&gt; ali.repo.bak文件。 $&gt;sudo yum clean all $&gt;sudo yum makecache #例如阿里基本源 $&gt;sudo wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo #阿里epel源 $&gt;sudo wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo flume source1.netcat nc .. 2.exec 实时日志收集,实时收集日志。 a1.sources = r1 a1.sinks = k1 a1.channels = c1 a1.sources.r1.type=exec a1.sources.r1.command=tail -F /home/centos/test.txt a1.sinks.k1.type=logger a1.channels.c1.type=memory a1.sources.r1.channels=c1 a1.sinks.k1.channel=c1 3.批量收集 监控一个文件夹，静态文件。 收集完之后，会重命名文件成新文件。.compeleted. a)配置文件 [spooldir_r.conf] a1.sources = r1 a1.channels = c1 a1.sinks = k1 a1.sources.r1.type=spooldir a1.sources.r1.spoolDir=/home/centos/spool a1.sources.r1.fileHeader=true a1.sinks.k1.type=logger a1.channels.c1.type=memory a1.sources.r1.channels=c1 a1.sinks.k1.channel=c1 b)创建目录 $&gt;mkdir ~/spool c)启动flume $&gt;bin/flume-ng agent -f ../conf/helloworld.conf -n a1 -Dflume.root.logger=INFO,console 4.序列source [seq] a1.sources = r1 a1.channels = c1 a1.sinks = k1 a1.sources.r1.type=seq a1.sources.r1.totalEvents=1000 a1.sinks.k1.type=logger a1.channels.c1.type=memory a1.sources.r1.channels=c1 a1.sinks.k1.channel=c1 [运行] $&gt;bin/flume-ng agent -f ../conf/helloworld.conf -n a1 -Dflume.root.logger=INFO,console 5.StressSource a1.sources = stresssource-1 a1.channels = memoryChannel-1 a1.sources.stresssource-1.type = org.apache.flume.source.StressSource a1.sources.stresssource-1.size = 10240 a1.sources.stresssource-1.maxTotalEvents = 1000000 a1.sources.stresssource-1.channels = memoryChannel-1 flume sink1.hdfs a1.sources = r1 a1.channels = c1 a1.sinks = k1 a1.sources.r1.type = netcat a1.sources.r1.bind = localhost a1.sources.r1.port = 8888 a1.sinks.k1.type = hdfs a1.sinks.k1.hdfs.path = /flume/events/%y-%m-%d/%H/%M/%S a1.sinks.k1.hdfs.filePrefix = events- #是否是产生新目录,每十分钟产生一个新目录,一般控制的目录方面。round是决定是否产生新文件的，滚动是决定是否产生新文件的。 #2017-12-12 --&gt; #2017-12-12 --&gt;%H%M%S a1.sinks.k1.hdfs.round = true a1.sinks.k1.hdfs.roundValue = 10 a1.sinks.k1.hdfs.roundUnit = second a1.sinks.k1.hdfs.useLocalTimeStamp=true #是否产生新文件。 a1.sinks.k1.hdfs.rollInterval=10 a1.sinks.k1.hdfs.rollSize=10 a1.sinks.k1.hdfs.rollCount=3 a1.channels.c1.type=memory a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 2.hive 略 3.hbase a1.sources = r1 a1.channels = c1 a1.sinks = k1 a1.sources.r1.type = netcat a1.sources.r1.bind = localhost a1.sources.r1.port = 8888 a1.sinks.k1.type = hbase a1.sinks.k1.table = ns1:t12 a1.sinks.k1.columnFamily = f1 a1.sinks.k1.serializer = org.apache.flume.sink.hbase.RegexHbaseEventSerializer a1.channels.c1.type=memory a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 4.kafka 数据进入到源里面去，最终进入通道里面，有很多通道c1,c2,c3，这个取决于通道选择器，ChannelProcessor，对事件进行处理，先经过一堆拦截器也是有很多种 。拦截器在各个文件前加东西，拦截之后再回到选择器。拦截器是典型的批处理，把加到东西流水线似的加到头文件里面。不管是什么对象，通过什么source进来的，都被转换成envent对象。在拦截器这个地方是链式技术。把事件放到通道里面，每个通道放置事件都是一个事务，保证能成功， 这个图是source到通道的图 sink的图： 使用avroSource和AvroSink实现跃点agent处理1.创建配置文件 [avro_hop.conf] #a1 a1.sources = r1 a1.sinks= k1 a1.channels = c1 a1.sources.r1.type=netcat a1.sources.r1.bind=localhost a1.sources.r1.port=8888 a1.sinks.k1.type = avro a1.sinks.k1.hostname=localhost a1.sinks.k1.port=9999 a1.channels.c1.type=memory a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 #a2 a2.sources = r2 a2.sinks= k2 a2.channels = c2 a2.sources.r2.type=avro a2.sources.r2.bind=localhost a2.sources.r2.port=9999 a2.sinks.k2.type = logger a2.channels.c2.type=memory a2.sources.r2.channels = c2 a2.sinks.k2.channel = c2 2.启动a2 $&gt;flume-ng agent -f /soft/flume/conf/avro_hop.conf -n a2 -Dflume.root.logger=INFO,console 3.验证a2 $&gt;netstat -anop | grep 9999 4.启动a1 $&gt;flume-ng agent -f /soft/flume/conf/avro_hop.conf -n a1 5.验证a1 $&gt;netstat -anop | grep 8888 channel1.MemoryChannel 略 2.FileChannel a1.sources = r1a1.sinks= k1a1.channels = c1 a1.sources.r1.type=netcata1.sources.r1.bind=localhosta1.sources.r1.port=8888 a1.sinks.k1.type=logger a1.channels.c1.type = file a1.channels.c1.checkpointDir = /home/centos/flume/fc_check a1.channels.c1.dataDirs = /home/centos/flume/fc_data a1.sources.r1.channels=c1a1.sinks.k1.channel=c1 可溢出文件通道a1.channels = c1a1.channels.c1.type = SPILLABLEMEMORY #0表示禁用内存通道，等价于文件通道a1.channels.c1.memoryCapacity = 0 #0,禁用文件通道，等价内存通道。a1.channels.c1.overflowCapacity = 2000 a1.channels.c1.byteCapacity = 800000a1.channels.c1.checkpointDir = /user/centos/flume/fc_checka1.channels.c1.dataDirs = /user/centos/flume/fc_data 创建Flume模块1.添加pom.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.it18zhang&lt;/groupId&gt; &lt;artifactId&gt;FluemDemo&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flume&lt;/groupId&gt; &lt;artifactId&gt;flume-ng-core&lt;/artifactId&gt; &lt;version&gt;1.7.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt;]]></content>
      <tags>
        <tag>flume</tag>
        <tag>source</tag>
        <tag>sink</tag>
        <tag>channel</tag>
        <tag>安装flume</tag>
        <tag>avro跃点</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试理论知识]]></title>
    <url>%2F2018%2F11%2F21%2F%E9%9D%A2%E8%AF%95%E7%90%86%E8%AE%BA%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[flume：Apache Flume是一种工具/服务/数据提取机制，用于收集聚合和从各种传输大量的流数据，如日志文件，事件（等等）源集中数据存储。 Flume是一种高度可靠，分布式和可配置的工具。 它主要是为了设计的将流数据（日志数据）从各种Web服务器复制到HDFS。]]></content>
  </entry>
  <entry>
    <title><![CDATA[SSM第一天]]></title>
    <url>%2F2018%2F11%2F19%2FSSM%E7%AC%AC%E4%B8%80%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[SSM第二天三大框架的整合： 数据库层：mybatis数据持久化层：dao在交互mybatisdao:数据访问层：数据访问对象，存放简单的增删改查。每个表都有这个业务service层：业务层，和dao交互，对dao的方法进行组合调用。形成套餐springmvc展现层：做网页，做web开发的，springmvc应该和业务层交互，在springmvc里面的controller层和业务层交互，展现的东西就是jsp做web开发做网站的，做web应用的。通过web架构来实现。springmvc就是web架构里面的一个。 Mybais和数据库整合 1.在pom中添加依赖： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.it18zhang&lt;/groupId&gt; &lt;artifactId&gt;SpringmybatisDemo&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;3.4.6&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.17&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.mchange&lt;/groupId&gt; &lt;artifactId&gt;c3p0&lt;/artifactId&gt; &lt;version&gt;0.9.5.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;version&gt;4.3.3.RElEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;version&gt;4.3.3.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aop&lt;/artifactId&gt; &lt;version&gt;4.3.3.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt; &lt;version&gt;1.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; 2.创建包 com.it18zhang.springmybatis.dao com.it18zhang.springmybatis.service com.it18zhang.springmybatis.utils 3.配置beans.xml在source下 坑爹的错误： 在上图的情下，看见下面的mybtis-spring是依赖于mybatis3.1.0，而上面有一个跟他一模一样的包，所以到下面这块，下面的这个版本的包就看不出来了。所以如果有的类仅仅是在下面这个版本下才有的，需要把上面的第一个包先删除掉。因为它只显示第一个依赖的包 复杂应用1.准备数据 sql.sql 2.创建java类. [Order.java] public class Order { private Integer id ; private String orderNo ; //简历关联关系 private User user ; //get/set } [Item.java] public class Item { private Integer id; private String itemName; //订单项和订单之间的关联关系 private Order order; //get/set } 3.创建Order映射文件 [resource/OrderMapper.xml] &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt; &lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt; &lt;mapper namespace=&quot;orders&quot;&gt; &lt;insert id=&quot;insert&quot;&gt; insert into orders(orderno,uid) values(#{orderNo},#{user.id}) &lt;/insert&gt; &lt;!-- findById --&gt; &lt;select id=&quot;selectOne&quot; parameterType=&quot;int&quot; resultMap=&quot;RM_Order&quot;&gt; select o.id oid , o.orderno oorderno , o.uid uid , u.name uname , u.age uage from orders o left outer join users u on o.uid = u.id where o.id = #{id} &lt;/select&gt; &lt;!-- findAll --&gt; &lt;select id=&quot;selectAll&quot; resultMap=&quot;RM_Order&quot;&gt; select o.id oid , o.orderno oorderno , o.uid uid , u.name uname , u.age uage from orders o left outer join users u on o.uid = u.id &lt;/select&gt; &lt;!-- 自定义结果映射 --&gt; &lt;resultMap id=&quot;RM_Order&quot; type=&quot;com.it18zhang.mybatisdemo.domain.Order&quot;&gt; &lt;id property=&quot;id&quot; column=&quot;oid&quot;/&gt; &lt;result property=&quot;orderNo&quot; column=&quot;oorderno&quot;/&gt; &lt;association property=&quot;user&quot; javaType=&quot;com.it18zhang.mybatisdemo.domain.User&quot;&gt; &lt;id property=&quot;id&quot; column=&quot;uid&quot; /&gt; &lt;result property=&quot;name&quot; column=&quot;uname&quot; /&gt; &lt;result property=&quot;age&quot; column=&quot;uage&quot; /&gt; &lt;/association&gt; &lt;/resultMap&gt; &lt;/mapper&gt; 4.修改配置文件,添加映射。 [resource/mybatis-config.xml] &lt;!-- 引入映射文件 --&gt; &lt;mappers&gt; &lt;mapper resource=&quot;*Mapper.xml&quot;/&gt; &lt;/mappers&gt; 5.测试类 public class TestOrder { /** * insert */ @Test public void insert() throws Exception { String resource = &quot;mybatis-config.xml&quot;; InputStream inputStream = Resources.getResourceAsStream(resource); SqlSessionFactory sf = new SqlSessionFactoryBuilder().build(inputStream); SqlSession s = sf.openSession(); User u = new User(); u.setId(2); Order o = new Order(); o.setOrderNo(&quot;No005&quot;); o.setUser(u); s.insert(&quot;orders.insert&quot;,o); s.commit(); s.close(); } @Test public void selectOne() throws Exception { String resource = &quot;mybatis-config.xml&quot;; InputStream inputStream = Resources.getResourceAsStream(resource); SqlSessionFactory sf = new SqlSessionFactoryBuilder().build(inputStream); SqlSession s = sf.openSession(); Order order = s.selectOne(&quot;orders.selectOne&quot;,1); System.out.println(order.getOrderNo()); s.commit(); s.close(); } @Test public void selectAll() throws Exception { String resource = &quot;mybatis-config.xml&quot;; InputStream inputStream = Resources.getResourceAsStream(resource); SqlSessionFactory sf = new SqlSessionFactoryBuilder().build(inputStream); SqlSession s = sf.openSession(); List&lt;Order&gt; list = s.selectList(&quot;orders.selectAll&quot;); for(Order o : list){ System.out.println(o.getOrderNo() + &quot; : &quot; + o.getUser().getName()); } s.commit(); s.close(); } } 配置一对多1.在User中增加orders集合。 public class User { ... private List&lt;Order&gt; orders ; //get/set } 2.改造UserMapper.xml 组合多对一和一对多关联关系到一个实体(Order)中1.关系 Order(*) -&gt; (1)User Order(1) -&gt; (*)Item 2.Order.java class Order{ ... List&lt;Item&gt; items ; //get/set } 2&apos;.修改配置文件增加别名 [resources/mybatis-config.xml] &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt; &lt;!DOCTYPE configuration PUBLIC &quot;-//mybatis.org//DTD Config 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot;&gt; &lt;configuration&gt; &lt;typeAliases&gt; &lt;typeAlias type=&quot;com.it18zhang.mybatisdemo.domain.User&quot; alias=&quot;_User&quot;/&gt; &lt;typeAlias type=&quot;com.it18zhang.mybatisdemo.domain.Order&quot; alias=&quot;_Order&quot;/&gt; &lt;typeAlias type=&quot;com.it18zhang.mybatisdemo.domain.Item&quot; alias=&quot;_Item&quot;/&gt; &lt;/typeAliases&gt; &lt;environments default=&quot;development&quot;&gt; &lt;environment id=&quot;development&quot;&gt; &lt;transactionManager type=&quot;JDBC&quot;/&gt; &lt;dataSource type=&quot;POOLED&quot;&gt; &lt;property name=&quot;driver&quot; value=&quot;com.mysql.jdbc.Driver&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/mybatis&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;root&quot;/&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;!-- 引入映射文件 --&gt; &lt;mappers&gt; &lt;mapper resource=&quot;UserMapper.xml&quot;/&gt; &lt;mapper resource=&quot;OrderMapper.xml&quot;/&gt; &lt;/mappers&gt; &lt;/configuration&gt; 3.OrderMapper.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt; &lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt; &lt;mapper namespace=&quot;orders&quot;&gt; &lt;insert id=&quot;insert&quot;&gt; insert into orders(orderno,uid) values(#{orderNo},#{user.id}) &lt;/insert&gt; &lt;!-- findById --&gt; &lt;select id=&quot;selectOne&quot; parameterType=&quot;int&quot; resultMap=&quot;RM_Order&quot;&gt; select o.id oid , o.orderno oorderno , o.uid uid , u.name uname , u.age uage , i.id iid, i.itemname iitemname from orders o left outer join users u on o.uid = u.id left outer join items i on o.id = i.oid where o.id = #{id} &lt;/select&gt; &lt;!-- findAll --&gt; &lt;select id=&quot;selectAll&quot; resultMap=&quot;RM_Order&quot;&gt; select o.id oid , o.orderno oorderno , o.uid uid , u.name uname , u.age uage , i.id iid, i.itemname iitemname from orders o left outer join users u on o.uid = u.id left outer join items i on o.id = i.oid &lt;/select&gt; &lt;!-- 自定义结果映射 --&gt; &lt;resultMap id=&quot;RM_Order&quot; type=&quot;com.it18zhang.mybatisdemo.domain.Order&quot;&gt; &lt;id property=&quot;id&quot; column=&quot;oid&quot;/&gt; &lt;result property=&quot;orderNo&quot; column=&quot;oorderno&quot;/&gt; &lt;association property=&quot;user&quot; javaType=&quot;com.it18zhang.mybatisdemo.domain.User&quot;&gt; &lt;id property=&quot;id&quot; column=&quot;uid&quot; /&gt; &lt;result property=&quot;name&quot; column=&quot;uname&quot; /&gt; &lt;result property=&quot;age&quot; column=&quot;uage&quot; /&gt; &lt;/association&gt; &lt;collection property=&quot;items&quot; ofType=&quot;_Item&quot;&gt; &lt;id property=&quot;id&quot; column=&quot;iid&quot; /&gt; &lt;result property=&quot;itemName&quot; column=&quot;iitemname&quot; /&gt; &lt;/collection&gt; &lt;/resultMap&gt; &lt;/mapper&gt; 4.测试 @Test public void selectOne() throws Exception { String resource = &quot;mybatis-config.xml&quot;; InputStream inputStream = Resources.getResourceAsStream(resource); SqlSessionFactory sf = new SqlSessionFactoryBuilder().build(inputStream); SqlSession s = sf.openSession(); Order order = s.selectOne(&quot;orders.selectOne&quot;,1); System.out.println(order.getOrderNo() + order.getUser().getName()); for(Item i : order.getItems()){ System.out.println(i.getId() + &quot;:&quot; + i.getItemName()); } s.commit(); s.close(); } 改造项目1.引入Util类 package com.it18zhang.mybatisdemo.util; import org.apache.ibatis.io.Resources; import org.apache.ibatis.session.SqlSession; import org.apache.ibatis.session.SqlSessionFactory; import org.apache.ibatis.session.SqlSessionFactoryBuilder; import java.io.InputStream; /** * 工具类 */ public class Util { // private static SqlSessionFactory sf ; static{ try { String resource = &quot;mybatis-config.xml&quot;; InputStream inputStream = Resources.getResourceAsStream(resource); sf = new SqlSessionFactoryBuilder().build(inputStream); } catch (Exception e) { e.printStackTrace(); } } /** * 开启会话 */ public static SqlSession openSession(){ return sf.openSession() ; } /** * 关闭会话 */ public static void closeSession(SqlSession s){ if(s != null){ s.close(); } } /** * 关闭会话 */ public static void rollbackTx(SqlSession s) { if (s != null) { s.rollback(); } } } 2.设计模板类DaoTemplate和回调MybatisCallback接口 [DaoTemplate.java] package com.it18zhang.mybatisdemo.dao; import com.it18zhang.mybatisdemo.util.Util; import org.apache.ibatis.session.SqlSession; /** * 模板类 */ public class DaoTemplate { /** * 执行 */ public static Object execute(MybatisCallback cb){ SqlSession s = null; try { s = Util.openSession(); Object ret = cb.doInMybatis(s); s.commit(); return ret ; } catch (Exception e) { Util.rollbackTx(s); } finally { Util.closeSession(s); } return null ; } } [MybatisCallback.java] package com.it18zhang.mybatisdemo.dao; import org.apache.ibatis.session.SqlSession; /** * 回调接口 */ public interface MybatisCallback { public Object doInMybatis(SqlSession s); } 3.通过模板类+回调接口实现UserDao.java [UserDao.java] package com.it18zhang.mybatisdemo.dao; import com.it18zhang.mybatisdemo.domain.User; import com.it18zhang.mybatisdemo.util.Util; import org.apache.ibatis.session.SqlSession; import java.util.List; /** * UserDao */ public class UserDao { /** * 插入操作 */ public void insert(final User user){ DaoTemplate.execute(new MybatisCallback() { public Object doInMybatis(SqlSession s) { s.insert(&quot;users.insert&quot;,user); return null ; } }); } /** * 插入操作 */ public void update(final User user){ DaoTemplate.execute(new MybatisCallback() { public Object doInMybatis(SqlSession s) { s.update(&quot;users.update&quot;, user); return null ; } }); } public User selctOne(final Integer id){ return (User)DaoTemplate.execute(new MybatisCallback() { public Object doInMybatis(SqlSession s) { return s.selectOne(&quot;users.selectOne&quot;,id); } }); } public List&lt;User&gt; selctAll(){ return (List&lt;User&gt;)DaoTemplate.execute(new MybatisCallback() { public Object doInMybatis(SqlSession s) { return s.selectList(&quot;users.selectAll&quot;); } }); } } 4.App测试 public static void main(String[] args) { UserDao dao = new UserDao(); User u = dao.selctOne(1); System.out.println(u.getName()); } 回调接口的一个画图分析：]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F11%2F19%2FHbase%E7%AC%AC%E4%B8%89%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[title: Hbase第三天tags: nullcategories: nulldate: 2018-11-19 16:56:58description: 复习Hbase第二天随机定位+实时读写 nosql:not only sql数据库 key-value对形式的存储 key: rowkey/family+col/timstamp = value rowkey 排序,byte[] 客户端先联系zk找到元数据表hbae:meta，存放了整个数据库的表和区域服务器信息，相当于目录，类似于名称节点。当找到了之后就可以定位到区域服务器，所以hbase数据读写和HRegionServer来交互，有很多regionServer构成了一个集群。数据先进入到写前日志，写前日志用于容错，用于恢复，所以在交互的时候client先交互HRegionServer然后在网Hlog里写入数据，然后在溢出之后写入HRegion，对于HRegion来说有个内存储MemStore在内存中存储数据，用来提高速度，MemStore达到一定值溢出到磁盘，所以还有一个StoreFile存储，用来和底层交互，底层就是Hfile。通过Hfile对象来跟HDFS交互，就找到了HDFS客户端DFSClient了，这个DFSClient就是hdfs范畴了，最终数据存储到HDFS里面了。 表的切割指的是切割表或者切割区域，按照rowkey来切分，因为rowkey是有序的，相当于建立索引，通过切割可以实现负载均衡，如果所有东西都在一个点就会出现热点问题。 hbase的增删改查是： put(rowkey).addColumn(). put(Put) delete get() 更新也是put scan() merge合并。 移动区域，目的减少某一个服务器的压力。可以任意配置区域所在地，由那个区域服务器承载。 切割风暴：达到10G之后同时到达临界点，同时切割，为了避免可以让这个10G的值变大再切割，也就是不让他自动切割了。可以手动切割避免。或者预切割来处理。 hbase存储的荣誉量比较大，因为它存储的时候都是以kv的方式来存储，而key是三极坐标，rowkey，列，列族，时间戳+一个value，所以前三个值都要存放很多次。所以要求列族和列的名称和rowkey的名字不能太长。一旦过长，就会发现存储的时候被坐标占用了大量的空间，而value的很少，最好列和列族名字不要太长。 本天会涉及rowkey的设计问题。 预先切割创建表时可以预先对表进行切割。 切割线就是rowkey create ‘ns1:t2’,’f1’,SPLITES=&gt;[‘row3000’,’row6000] 预先切割创建表时，预先对表进行切割。 切割线是rowkey. $hbase&gt;create &apos;ns1:t2&apos;,&apos;f1&apos;,SPLITS=&gt;[&apos;row3000&apos;,&apos;row6000&apos;] 创建表时指定列族的版本数,该列族的所有列都具有相同数量版本$hbase&gt;create &apos;ns1:t3&apos;,{NAME=&gt;&apos;f1&apos;,VERSIONS=&gt;3} //创建表时，指定列族的版本数。 $hbase&gt;get &apos;ns1:t3&apos;,&apos;row1&apos;,{COLUMN=&gt;&apos;f1&apos;,VERSIONS=&gt;4} //检索的时候，查询多少个版本。 $hbase&gt;put &apos;ns1:t3&apos;,&apos;row1&apos;,&apos;f1:name&apos;,&apos;tom&apos; 关于查询的命令行： 关于创建表的命令： @Test public void getWithVersions() throws IOException { Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); TableName tname = TableName.valueOf(&quot;ns1:t3&quot;); Table table = conn.getTable(tname); Get get = new Get(Bytes.toBytes(&quot;row1&quot;)); //检索所有版本 get.setMaxVersions(); Result r = table.get(get); List&lt;Cell&gt; cells = r.getColumnCells(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;)); for(Cell c : cells){ String f = Bytes.toString(c.getFamily()); String col = Bytes.toString(c.getQualifier()); long ts = c.getTimestamp(); String val = Bytes.toString(c.getValue()); System.out.println(f + &quot;/&quot; + col + &quot;/&quot; + ts + &quot;=&quot; + val); } } 原生扫描(专家)1.原生扫描$hbase&gt;scan &apos;ns1:t3&apos;,{COLUMN=&gt;&apos;f1&apos;,RAW=&gt;true,VERSIONS=&gt;10} //包含标记了delete的数据 2.删除数据$hbase&gt;delete &apos;nd1:t3&apos;,&apos;row1&apos;,&apos;f1:name&apos;,148989875645 //删除数据，标记为删除. //小于该删除时间的数据都作废。 3.TTLtime to live ,存活时间。 影响所有的数据，包括没有删除的数据。 超过该时间，原生扫描也扫不到数据。 $hbase&gt;create &apos;ns1:tx&apos; , {NAME=&gt;&apos;f1&apos;,TTL=&gt;10,VERSIONS} 4.KEEP_DELETED_CELLS删除key之后，数据是否还保留。 $hbase&gt;create &apos;ns1:tx&apos; , {NAME=&gt;&apos;f1&apos;,TTL=&gt;10,VERSIONS,KEEP_DELETED_CELLS=&gt;true} 缓存和批处理 1.开启服务器端扫描器缓存 a)表层面(全局)只需要配置一个属性即可 &lt;property&gt; &lt;name&gt;hbase.client.scanner.caching&lt;/name&gt; &lt;!-- 整数最大值 --&gt; &lt;value&gt;2147483647&lt;/value&gt; &lt;source&gt;hbase-default.xml&lt;/source&gt; &lt;/property&gt; b)操作层面 //设置量 scan.setCaching(10); 2. 3. cache row nums : 1000 //632 cache row nums : 5000 //423 cache row nums : 1 //7359 扫描器缓存面向行级别的。 @Test public void getScanCache() throws IOException { Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); TableName tname = TableName.valueOf(&quot;ns1:t1&quot;); Scan scan = new Scan(); scan.setCaching(5000); Table t = conn.getTable(tname); ResultScanner rs = t.getScanner(scan); long start = System.currentTimeMillis() ; Iterator&lt;Result&gt; it = rs.iterator(); while(it.hasNext()){ Result r = it.next(); System.out.println(r.getColumnLatestCell(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;))); } System.out.println(System.currentTimeMillis() - start); } 批量扫描是面向列级别 控制每次next()服务器端返回的列的个数。 scan.setBatch(5); //每次next返回5列。 测试缓存和批处理 */ @Test public void testBatchAndCaching() throws IOException { Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); TableName tname = TableName.valueOf(&quot;ns1:t7&quot;); Scan scan = new Scan(); scan.setCaching(2); scan.setBatch(3); Table t = conn.getTable(tname); ResultScanner rs = t.getScanner(scan); Iterator&lt;Result&gt; it = rs.iterator(); while (it.hasNext()) { Result r = it.next(); System.out.println(&quot;========================================&quot;); //得到一行的所有map,key=f1,value=Map&lt;Col,Map&lt;Timestamp,value&gt;&gt; NavigableMap&lt;byte[], NavigableMap&lt;byte[], NavigableMap&lt;Long, byte[]&gt;&gt;&gt; map = r.getMap(); // for (Map.Entry&lt;byte[], NavigableMap&lt;byte[], NavigableMap&lt;Long, byte[]&gt;&gt;&gt; entry : map.entrySet()) { //得到列族 String f = Bytes.toString(entry.getKey()); Map&lt;byte[], NavigableMap&lt;Long, byte[]&gt;&gt; colDataMap = entry.getValue(); for (Map.Entry&lt;byte[], NavigableMap&lt;Long, byte[]&gt;&gt; ets : colDataMap.entrySet()) { String c = Bytes.toString(ets.getKey()); Map&lt;Long, byte[]&gt; tsValueMap = ets.getValue(); for (Map.Entry&lt;Long, byte[]&gt; e : tsValueMap.entrySet()) { Long ts = e.getKey(); String value = Bytes.toString(e.getValue()); System.out.print(f + &quot;/&quot; + c + &quot;/&quot; + ts + &quot;=&quot; + value + &quot;,&quot;); } } } System.out.println(); } } 先插入数据 代码运行结果： 上面代码和上图对应的。设置2个cach和3个batch视频上说是2行3列，但是我觉得应该是2个列族3个列的这样子去输出。然后最后这一行还剩下2个输出2个，也就是3个输出，2个输出，3个输出，2个输出，。 ======================================== f1/id/1490595148588=1,f2/addr/1490595182150=hebei,f2/age/1490595174760=12,f2/id/1490595164473=1,f2/name/1490595169589=tom, ======================================== f1/id/1490595196410=2,f1/name/1490595213090=tom2.1,f2/addr/1490595264734=tangshan,f2/age/1490595253996=13,f2/id/1490595233568=2,f2/name/1490595241891=tom2.2, ======================================== f1/age/1490595295427=14,f1/id/1490595281251=3,f1/name/1490595289587=tom3.1,f2/addr/1490595343690=beijing,f2/age/1490595336300=14,f2/id/1490595310966=3, ========================================f2/name/1490595327531=tom3.2, Filter过滤器远程服务器收到scan对象进行反序列化，恢复成scan对象进行过滤，对每个区域进行过滤，每个区域服务器有很多区域，每个区域里面有区域扫描器， RegionScanner，会使用区域过滤器。 1.RowFilter select * from ns1:t1 where rowkey &lt;= row100 /** /** * 测试RowFilter过滤器 */ @Test public void testRowFilter() throws IOException { Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); TableName tname = TableName.valueOf(&quot;ns1:t1&quot;); Scan scan = new Scan(); RowFilter rowFilter = new RowFilter(CompareFilter.CompareOp.LESS_OR_EQUAL, new BinaryComparator(Bytes.toBytes(&quot;row0100&quot;))); scan.setFilter(rowFilter); Table t = conn.getTable(tname); ResultScanner rs = t.getScanner(scan); Iterator&lt;Result&gt; it = rs.iterator(); while (it.hasNext()) { Result r = it.next(); System.out.println(Bytes.toString(r.getRow())); } } /** * 测试FamilyFilter过滤器 */ @Test public void testFamilyFilter() throws IOException { Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); TableName tname = TableName.valueOf(&quot;ns1:t7&quot;); Scan scan = new Scan(); FamilyFilter filter = new FamilyFilter(CompareFilter.CompareOp.LESS, new BinaryComparator(Bytes.toBytes(&quot;f2&quot;))); scan.setFilter(filter); Table t = conn.getTable(tname); ResultScanner rs = t.getScanner(scan); Iterator&lt;Result&gt; it = rs.iterator(); while (it.hasNext()) { Result r = it.next(); byte[] f1id = r.getValue(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;)); byte[] f2id = r.getValue(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;id&quot;)); System.out.println(f1id + &quot; : &quot; + f2id); } } /** * 测试QualifierFilter(列过滤器) */ @Test public void testColFilter() throws IOException { Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); TableName tname = TableName.valueOf(&quot;ns1:t7&quot;); Scan scan = new Scan(); QualifierFilter colfilter = new QualifierFilter(CompareFilter.CompareOp.EQUAL, new BinaryComparator(Bytes.toBytes(&quot;id&quot;))); scan.setFilter(colfilter); Table t = conn.getTable(tname); ResultScanner rs = t.getScanner(scan); Iterator&lt;Result&gt; it = rs.iterator(); while (it.hasNext()) { Result r = it.next(); byte[] f1id = r.getValue(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;)); byte[] f2id = r.getValue(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;id&quot;)); byte[] f2name = r.getValue(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;name&quot;)); System.out.println(f1id + &quot; : &quot; + f2id + &quot; : &quot; + f2name); } } /** * 测试ValueFilter(值过滤器) * 过滤value的值，含有指定的字符子串 */ @Test public void testValueFilter() throws IOException { Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); TableName tname = TableName.valueOf(&quot;ns1:t7&quot;); Scan scan = new Scan(); ValueFilter filter = new ValueFilter(CompareFilter.CompareOp.EQUAL, new SubstringComparator(&quot;to&quot;)); scan.setFilter(filter); Table t = conn.getTable(tname); ResultScanner rs = t.getScanner(scan); Iterator&lt;Result&gt; it = rs.iterator(); while (it.hasNext()) { Result r = it.next(); byte[] f1id = r.getValue(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;)); byte[] f2id = r.getValue(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;id&quot;)); byte[] f1name = r.getValue(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;)); byte[] f2name = r.getValue(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;name&quot;)); System.out.println(f1id + &quot; : &quot; + f2id + &quot; : &quot; + Bytes.toString(f1name) + &quot; : &quot; + Bytes.toString(f2name)); } } /** * 依赖列过滤器 */ @Test public void testDepFilter() throws IOException { Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); TableName tname = TableName.valueOf(&quot;ns1:t7&quot;); Scan scan = new Scan(); DependentColumnFilter filter = new DependentColumnFilter(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;addr&quot;), true, CompareFilter.CompareOp.NOT_EQUAL, new BinaryComparator(Bytes.toBytes(&quot;beijing&quot;)) ); //ValueFilter filter = new ValueFilter(CompareFilter.CompareOp.EQUAL, new SubstringComparator(&quot;to&quot;)); scan.setFilter(filter); Table t = conn.getTable(tname); ResultScanner rs = t.getScanner(scan); Iterator&lt;Result&gt; it = rs.iterator(); while (it.hasNext()) { Result r = it.next(); byte[] f1id = r.getValue(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;)); byte[] f2id = r.getValue(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;id&quot;)); byte[] f1name = r.getValue(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;)); byte[] f2name = r.getValue(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;name&quot;)); System.out.println(f1id + &quot; : &quot; + f2id + &quot; : &quot; + Bytes.toString(f1name) + &quot; : &quot; + Bytes.toString(f2name)); } } /** * 单列值value过滤， * */ @Test public void testSingleColumValueFilter() throws IOException { Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); TableName tname = TableName.valueOf(&quot;ns1:t7&quot;); Scan scan = new Scan(); SingleColumnValueFilter filter = new SingleColumnValueFilter(Bytes.toBytes(&quot;f2&quot;, Bytes.toBytes(&quot;name&quot;), CompareFilter.CompareOp.NOT_EQUAL), new BinaryComparator(Bytes.toBytes(&quot;tom2.1&quot;))); //ValueFilter filter = new ValueFilter(CompareFilter.CompareOp.EQUAL, new SubstringComparator(&quot;to&quot;)); scan.setFilter(filter); Table t = conn.getTable(tname); ResultScanner rs = t.getScanner(scan); Iterator&lt;Result&gt; it = rs.iterator(); while (it.hasNext()) { Result r = it.next(); byte[] f1id = r.getValue(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;)); byte[] f2id = r.getValue(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;id&quot;)); byte[] f1name = r.getValue(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;)); byte[] f2name = r.getValue(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;name&quot;)); System.out.println(f1id + &quot; : &quot; + f2id + &quot; : &quot; + Bytes.toString(f1name) + &quot; : &quot; + Bytes.toString(f2name)); } } 复杂查询select * from t7 where ((age &lt;= 13) and (name like &apos;%t&apos;) or (age &gt; 13) and (name like &apos;t%&apos;)) 指定列族，指定列，指定对比方式，指定值(小于用二进制比较，等于用正则表达式串对比器) 复杂查询实现方式:FilterList@Test public void testComboFilter() throws IOException { Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); TableName tname = TableName.valueOf(&quot;ns1:t7&quot;); Scan scan = new Scan(); //where ... f2:age &lt;= 13 SingleColumnValueFilter ftl = new SingleColumnValueFilter( Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;age&quot;), CompareFilter.CompareOp.LESS_OR_EQUAL, new BinaryComparator(Bytes.toBytes(&quot;13&quot;)) ); //where ... f2:name like %t SingleColumnValueFilter ftr = new SingleColumnValueFilter( Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;name&quot;), CompareFilter.CompareOp.EQUAL, new RegexStringComparator(&quot;^t&quot;)//以t开头 ); //ft FilterList ft = new FilterList(FilterList.Operator.MUST_PASS_ALL); ft.addFilter(ftl); ft.addFilter(ftr); //where ... f2:age &gt; 13 SingleColumnValueFilter fbl = new SingleColumnValueFilter( Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;age&quot;), CompareFilter.CompareOp.GREATER, new BinaryComparator(Bytes.toBytes(&quot;13&quot;)) ); //where ... f2:name like %t SingleColumnValueFilter fbr = new SingleColumnValueFilter( Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;name&quot;), CompareFilter.CompareOp.EQUAL, new RegexStringComparator(&quot;t$&quot;)//以t结尾 ); //ft FilterList fb = new FilterList(FilterList.Operator.MUST_PASS_ALL); fb.addFilter(fbl); fb.addFilter(fbr); FilterList fall = new FilterList(FilterList.Operator.MUST_PASS_ONE); fall.addFilter(ft); fall.addFilter(fb); scan.setFilter(fall); Table t = conn.getTable(tname); ResultScanner rs = t.getScanner(scan); Iterator&lt;Result&gt; it = rs.iterator(); while (it.hasNext()) { Result r = it.next(); byte[] f1id = r.getValue(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;)); byte[] f2id = r.getValue(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;id&quot;)); byte[] f1name = r.getValue(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;)); byte[] f2name = r.getValue(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;name&quot;)); System.out.println(f1id + &quot; : &quot; + f2id + &quot; : &quot; + Bytes.toString(f1name) + &quot; : &quot; + Bytes.toString(f2name)); } } 计数器$hbase&gt;incr &apos;ns1:t8&apos;,&apos;row1&apos;,&apos;f1:click&apos;,1 $hbase&gt;get_counter &apos;ns1:t8&apos;,&apos;row1&apos;,&apos;f1:click&apos; //得到计数器的值 [API编程] @Test public void testIncr() throws IOException { Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); TableName tname = TableName.valueOf(&quot;ns1:t8&quot;); Table t = conn.getTable(tname); Increment incr = new Increment(Bytes.toBytes(&quot;row1&quot;)); incr.addColumn(Bytes.toBytes(&quot;f1&quot;),Bytes.toBytes(&quot;daily&quot;),1); incr.addColumn(Bytes.toBytes(&quot;f1&quot;),Bytes.toBytes(&quot;weekly&quot;),10); incr.addColumn(Bytes.toBytes(&quot;f1&quot;),Bytes.toBytes(&quot;monthly&quot;),100); t.increment(incr); } coprocessor协处理器工作过程： 批处理的，等价于存储过程或者触发器 [Observer] 观察者,类似于触发器，基于事件。发生动作时，回调相应方法。 RegionObserver //RegionServer区域观察者 MasterObserver //Master节点。 WAlObserver // [Endpoint] 终端,类似于存储过程。 1.加载 [hbase-site.xml] &lt;property&gt; &lt;name&gt;hbase.coprocessor.region.classes&lt;/name&gt; &lt;value&gt;coprocessor.RegionObserverExample, coprocessor.AnotherCoprocessor&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.coprocessor.master.classes&lt;/name&gt; &lt;value&gt;coprocessor.MasterObserverExample&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.coprocessor.wal.classes&lt;/name&gt; &lt;value&gt;coprocessor.WALObserverExample, bar.foo.MyWALObserver&lt;/value&gt; &lt;/property&gt; 2.自定义观察者 [MyRegionObserver] package com.it18zhang.hbasedemo.coprocessor; import org.apache.hadoop.hbase.Cell; import org.apache.hadoop.hbase.CoprocessorEnvironment; import org.apache.hadoop.hbase.client.Delete; import org.apache.hadoop.hbase.client.Durability; import org.apache.hadoop.hbase.client.Get; import org.apache.hadoop.hbase.client.Put; import org.apache.hadoop.hbase.coprocessor.BaseRegionObserver; import org.apache.hadoop.hbase.coprocessor.ObserverContext; import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment; import org.apache.hadoop.hbase.regionserver.wal.WALEdit; import org.apache.hadoop.hbase.util.Bytes; import java.io.FileWriter; import java.io.IOException; import java.util.List; /** * 自定义区域观察者 */ public class MyRegionObserver extends BaseRegionObserver{ private void outInfo(String str){ try { FileWriter fw = new FileWriter(&quot;/home/centos/coprocessor.txt&quot;,true); fw.write(str + &quot;\r\n&quot;); fw.close(); } catch (Exception e) { e.printStackTrace(); } } public void start(CoprocessorEnvironment e) throws IOException { super.start(e); outInfo(&quot;MyRegionObserver.start()&quot;); } public void preOpen(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e) throws IOException { super.preOpen(e); outInfo(&quot;MyRegionObserver.preOpen()&quot;); } public void postOpen(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e) { super.postOpen(e); outInfo(&quot;MyRegionObserver.postOpen()&quot;); } @Override public void preGetOp(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Get get, List&lt;Cell&gt; results) throws IOException { super.preGetOp(e, get, results); String rowkey = Bytes.toString(get.getRow()); outInfo(&quot;MyRegionObserver.preGetOp() : rowkey = &quot; + rowkey); } public void postGetOp(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Get get, List&lt;Cell&gt; results) throws IOException { super.postGetOp(e, get, results); String rowkey = Bytes.toString(get.getRow()); outInfo(&quot;MyRegionObserver.postGetOp() : rowkey = &quot; + rowkey); } public void prePut(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Put put, WALEdit edit, Durability durability) throws IOException { super.prePut(e, put, edit, durability); String rowkey = Bytes.toString(put.getRow()); outInfo(&quot;MyRegionObserver.prePut() : rowkey = &quot; + rowkey); } @Override public void postPut(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Put put, WALEdit edit, Durability durability) throws IOException { super.postPut(e, put, edit, durability); String rowkey = Bytes.toString(put.getRow()); outInfo(&quot;MyRegionObserver.postPut() : rowkey = &quot; + rowkey); } @Override public void preDelete(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Delete delete, WALEdit edit, Durability durability) throws IOException { super.preDelete(e, delete, edit, durability); String rowkey = Bytes.toString(delete.getRow()); outInfo(&quot;MyRegionObserver.preDelete() : rowkey = &quot; + rowkey); } @Override public void postDelete(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Delete delete, WALEdit edit, Durability durability) throws IOException { super.postDelete(e, delete, edit, durability); String rowkey = Bytes.toString(delete.getRow()); outInfo(&quot;MyRegionObserver.postDelete() : rowkey = &quot; + rowkey); } } 2.注册协处理器并分发 &lt;property&gt; &lt;name&gt;hbase.coprocessor.region.classes&lt;/name&gt; &lt;value&gt;com.it18zhang.hbasedemo.coprocessor.MyRegionObserver&lt;/value&gt; &lt;/property&gt; 3.导出jar包。 4.复制jar到共享目录，分发到jar到hbase集群的hbase lib目录下. [/soft/hbase/lib]]]></content>
  </entry>
  <entry>
    <title><![CDATA[SSM第二天]]></title>
    <url>%2F2018%2F11%2F19%2FSSM%E7%AC%AC%E4%BA%8C%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[mybatis持久化技术。jdbc sql Spring 业务层框架。 管理bean的。 new Map&lt;String,Object&gt; 体验spring1.创建模块 ,添加pom.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.it18zhang&lt;/groupId&gt; &lt;artifactId&gt;springdemo&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;4.3.3.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; 2.创建java类 public class WelcomeService { private String message ; public String getMessage() { return message; } public void setMessage(String message) { this.message = message; } public void sayHello(){ System.out.println(message); } } 3.创建配置文件 [resources/beans.xml] &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;bean id=&quot;ws&quot; class=&quot;com.it18zhang.springdemo.service.WelcomeService&quot;&gt; &lt;property name=&quot;message&quot; value=&quot;hello world&quot; /&gt; &lt;/bean&gt; &lt;/beans&gt; 4.创建App [App.java] public static void main(String[] args) { //创建容器 ApplicationContext ac = new ClassPathXmlApplicationContext(&quot;beans.xml&quot;); WelcomeService ws = (WelcomeService)ac.getBean(&quot;ws&quot;); ws.sayHello(); } spring的注解方式使用0.增加pom.xml文件 &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;version&gt;4.3.3.RELEASE&lt;/version&gt; &lt;/dependency&gt; 1.UserDao增加@Repository注解. @Repository(&quot;userDao&quot;) public class UserDao{ ... } 2.Service增加 @Service注解。 @Service(&quot;ws&quot;) public class WelcomeService { ... //注入指定的dao对象 @Resource(name = &quot;userDao&quot;) public void setDao(UserDao dao) { this.dao = dao; } } 3.修改beans.xml文件，引入context空间，使用组件扫描。 [resrouces/beans.xml] &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.3.xsd&quot;&gt; &lt;context:component-scan base-package=&quot;com.it18zhang.springdemo.dao,com.it18zhang.springdemo.service&quot; /&gt; 4.测试App.java public class App { public static void main(String[] args) { //创建容器 ApplicationContext ac = new ClassPathXmlApplicationContext(&quot;beans.xml&quot;); WelcomeService ws = (WelcomeService)ac.getBean(&quot;ws&quot;); ws.sayHello(); } } spring 整合mybatis1.创建模块 pom.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.it18zhang&lt;/groupId&gt; &lt;artifactId&gt;springmybatisdemo&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.17&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;c3p0&lt;/groupId&gt; &lt;artifactId&gt;c3p0&lt;/artifactId&gt; &lt;version&gt;0.9.1.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;version&gt;4.3.3.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;version&gt;4.3.3.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aop&lt;/artifactId&gt; &lt;version&gt;4.3.3.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt; &lt;version&gt;4.3.3.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt; &lt;version&gt;1.3.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt; &lt;version&gt;1.8.10&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; 2.创建包 com.it18zhang.springmybatis.dao com.it18zhang.springmybatis.service com.it18zhang.springmybatis.util 3.配置beans.xml [resources/beans.xml] &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;!-- 数据源 --&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;com.mchange.v2.c3p0.ComboPooledDataSource&quot;&gt; &lt;property name=&quot;driverClass&quot; value=&quot;com.mysql.jdbc.Driver&quot;/&gt; &lt;property name=&quot;jdbcUrl&quot; value=&quot;jdbc:mysql://localhost:3306/mybatis&quot;/&gt; &lt;property name=&quot;user&quot; value=&quot;root&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;root&quot;/&gt; &lt;property name=&quot;maxPoolSize&quot; value=&quot;10&quot;/&gt; &lt;property name=&quot;minPoolSize&quot; value=&quot;2&quot;/&gt; &lt;property name=&quot;initialPoolSize&quot; value=&quot;3&quot;/&gt; &lt;property name=&quot;acquireIncrement&quot; value=&quot;2&quot;/&gt; &lt;/bean&gt; &lt;/beans&gt; 4.编写单元测试 @Test public void testConn() throws Exception { ApplicationContext ac = new ClassPathXmlApplicationContext(&quot;beans.xml&quot;); DataSource ds = (DataSource)ac.getBean(&quot;dataSource&quot;); System.out.println(ds.getConnection()); } 5.添加domain类 User Order Item 略 6.添加Mapper.xml映射文件 //注意：修改类的别名 resources/UserMapper.xml resources/OrderMapper.xml 7.添加mybatis-config.xml [resources/mybatis-config.xml] &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt; &lt;!DOCTYPE configuration PUBLIC &quot;-//mybatis.org//DTD Config 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot;&gt; &lt;configuration&gt; &lt;typeAliases&gt; &lt;typeAlias type=&quot;com.it18zhang.springmybatis.domain.User&quot; alias=&quot;_User&quot;/&gt; &lt;typeAlias type=&quot;com.it18zhang.springmybatis.domain.Order&quot; alias=&quot;_Order&quot;/&gt; &lt;typeAlias type=&quot;com.it18zhang.springmybatis.domain.Item&quot; alias=&quot;_Item&quot;/&gt; &lt;/typeAliases&gt; &lt;!-- 引入映射文件 --&gt; &lt;mappers&gt; &lt;mapper resource=&quot;UserMapper.xml&quot;/&gt; &lt;mapper resource=&quot;OrderMapper.xml&quot;/&gt; &lt;/mappers&gt; &lt;/configuration&gt; 8.创建Dao接口和实现类. [BaseDao.java] package com.it18zhang.springmybatis.dao; import java.util.List; /** *基本Dao接口 */ public interface BaseDao&lt;T&gt; { public void insert(T t) ; public void update(T t) ; public void delete(Integer id) ; public T selectOne(Integer id) ; public List&lt;T&gt; selectAll() ; } [UserDao.java] package com.it18zhang.springmybatis.dao; import com.it18zhang.springmybatis.domain.User; import org.mybatis.spring.support.SqlSessionDaoSupport; import java.util.List; /** */ @Repository(&quot;userDao&quot;) public class UserDao extends SqlSessionDaoSupport implements BaseDao&lt;User&gt; { public void insert(User user) { getSqlSession().insert(&quot;users.insert&quot;,user); } public void update(User user) { getSqlSession().update(&quot;users.update&quot;, user); } public void delete(Integer id ) { getSqlSession().delete(&quot;users.delete&quot;, id); } public User selectOne(Integer id) { return getSqlSession().selectOne(&quot;users.selectOne&quot;,id) ; } public List&lt;User&gt; selectAll() { return getSqlSession().selectList(&quot;users.selectAll&quot;); } } a [OrderDao.java] 略 9.创建BaseService&lt;T&gt;.java接口 + UserService.java + UserServcieImpl.java [BaseService.java] package com.it18zhang.springmybatis.service; import java.util.List; /** * Created by Administrator on 2017/4/7. */ public interface BaseService&lt;T&gt; { public void insert(T t); public void update(T t); public void delete(Integer id); public T selectOne(Integer id); public List&lt;T&gt; selectAll(); } [BaseServiceImpl.java] public abstract class BaseServiceImpl&lt;T&gt; implements BaseService&lt;T&gt; { private BaseDao&lt;T&gt; dao ; public void setDao(BaseDao&lt;T&gt; dao) { this.dao = dao; } public void insert(T t) { dao.insert(t); } ... } [UserService.java] public interface UserService extends BaseService&lt;User&gt; { } [UserServiceImpl.java] @Service(&quot;userService&quot;) public class UserServiceImpl extends BaseServiceImpl&lt;User&gt; implements UserService{ /*** 重写该方法，注入指定的Dao对象 ***/ @Resource(name=&quot;userDao&quot;) public void setDao(BaseDao&lt;User&gt; dao) { super.setDao(dao); } } 10.完善spring的配置文件. &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;tx:advice id=”txAdvice” transaction-manager=”txManager”&gt; tx:attributes &lt;tx:method name=”“ propagation=”REQUIRED” isolation=”DEFAULT”/&gt; &lt;/tx:attributes&gt; &lt;/tx:advice&gt; aop:config &lt;!–advisor代表切入点通知，吧事务txAdvice加到一个地方去，这个地方叫做切入点，哪里执行execution呢，就是任何地方 第一个匹配的是函数的返回值 任何函数返回值都可以，然后.代表包，任何包以及包的子包，然后.service第一个点代表 包和类的分割符，后面就是以Service结尾的任何接口或类，然后最后括号里面的..代表参数不限，随便什么参数都可以–&gt; &lt;aop:advisor advice-ref=”txAdvice” pointcut=”execution( ..Service.(..))” /&gt; &lt;/aop:config&gt; &lt;context:component-scan base-package=”com.it18zhang.springmybatis.dao,com.it18zhang.springmybatis.service” /&gt; 11.测试UserService @Test public void testUserService() throws Exception { ApplicationContext ac = new ClassPathXmlApplicationContext(&quot;beans.xml&quot;); UserService us = (UserService)ac.getBean(&quot;userService&quot;); User u = new User(); u.setName(&quot;jerry&quot;); u.setAge(12); us.insert(u); } select i.id iid,i.itemname iitemname,o.id oid,o.orderno oorderno , u.id uid ,u.name uname ,u.age uagefrom items i left outer join orders o on i.oid = o.id left outer join users u on o.uid = u.idwhere i.id = 2 我们一般吧最基础的增删改查里面放到baseservice这个接口里面，这里面是公共的功能，所以在下面还需要有分开的叉开的，需要有userservice继承自baseservice,他也是一个接口，然后在userservice里面有什么需要加的功能加到这个里面，也就是说在服务层需要有什么功能的都加到这个接口里面，避免都要实现。这个地方是要继承的。 一个spring整合mybatis的rose图： 整个结构：这个里面如果直接接受dao可以在调试中get到他的具体的内容，如果接受service就不行因为事务管理封装起了sevice。所以接收到的是一个事务]]></content>
      <tags>
        <tag>SSM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase第一天]]></title>
    <url>%2F2018%2F11%2F16%2FHbase%E7%AC%AC%E4%B8%80%E5%A4%A9%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Hbase第二天]]></title>
    <url>%2F2018%2F11%2F16%2FHbase%E7%AC%AC%E4%BA%8C%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[start-hbase.sh hbase-daemon.sh start master habse-daemon.sh start regionserver hbase的ha设置： 直接打开S202或者s203的master进程即可，启动命令如上图。 hbase shell操作： $&gt;hbaes shell $hbase&gt;help namespace 类似于Mysql库的概念 insert intonosql: not only SQLkey-valueput用来放kv对。在建表的时候没有指定列明，指定的是列族名，然后这个列族下的列是可以增减的。help ‘put’ habase shell 操作： $&gt;hbase shell //登陆shell终端 $hbase&gt;help // $hbase&gt;help &apos;list_namespace&apos; //查看特定 的命令帮助 $hbase&gt;list_namespace //列出名字空间（数据库） $hbase&gt;list_namespace_tables &apos;default&apos; //列出名字空间 $hbase&gt;create_namespace &apos;ns1&apos; //创建名字空间 $hbase&gt;help &apos;create&apos; // $hbase&gt;create &apos;ns1:t1&apos;,&apos;f1&apos; //创建表，指定空间下 $hbase&gt;put &apos;ns1:t1&apos;,&apos;row1&apos;,&apos;f1:id&apos;,100 $hbase&gt;put &apos;ns1:t1&apos;,&apos;row1&apos;,&apos;f1:name&apos;,tom $hbase&gt;get &apos;ns1:t1&apos;,&apos;row1&apos; //指定查询row $hbase&gt;scan &apos;ns1:t1&apos; //权标扫描扫描ns1列族的t1列 三级坐标定位，一个是列族，一个是row一个是时间戳如下图; 通过java api操作hbase: package com.it18zhang.hbasedemo; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.hbase.HBaseConfiguration; import org.apache.hadoop.hbase.TableName; import org.apache.hadoop.hbase.client.Connection; import org.apache.hadoop.hbase.client.ConnectionFactory; import org.apache.hadoop.hbase.client.Put; import org.apache.hadoop.hbase.client.Table; import org.apache.hadoop.hbase.util.Bytes; import org.junit.Test; eate 2018/11/17 11:56 */ public class TestCRUD { @Test public void put() throws Exception { Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); TableName tname = TableName.valueOf(&quot;ns1:t1&quot;); Table table = conn.getTable(tname); byte[] rowid = Bytes.toBytes(&quot;row2&quot;); byte[] f1 = Bytes.toBytes(&quot;f1&quot;); byte[] id = Bytes.toBytes(&quot;id&quot;); byte[] value = Bytes.toBytes(102); //创建put对象 Put put = new Put(rowid); put.addColumn(f1, id, value); table.put(put); } } pom文件： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.it18zhang&lt;/groupId&gt; &lt;artifactId&gt;HbaseDemo&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-client&lt;/artifactId&gt; &lt;version&gt;1.2.3&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; hbase架构介绍： 关于区域服务器 每个大表按照rowkey来切分，rowkey是可排序的，每个切分的被分派到每个区域分五期里面。key是有序的。而且每个取余服务器的表都是有范围的，所以在get的时候要指定rowkey，然后查询的时候定位到这个rowkey在哪个区域服务器上，因为所有的区域服务器信息都在meta表这个自带的表里面，叫元数据表。里面存储着rowkey。也就是每个区域的rowkey的范围。 看里面的内容hbase；namespace,,14….,74….这个就是名字空间表，起始的位置，结束的rowkey位置。前4大段，是指的是一大行的前4列。两个逗号意味着没有切割。就只是在一个区域服务器里面 再看下面的ns1:t1表的都是一行，然后3列不同的列，如果查询这部分直接联系s204,如果204挂了， 这个表就会更新，变成承接的新表的数据。这个ns1:t1的这个表就没有分割。column是列，info是列族，server是列 在hbase查询的时候是3级定位。时间戳，列族（列），rowkey（行） 看一下下面的这个目录： hbase,数据，ns1名字空间，t1表，区域服务器,f1列族，然后列族下面就是文件了二进制存储，内容都在这个二进制文件里面。 一个表可以有多个区域，一个区域服务器肯定有多个区域，由于有副本的存在所以一个区域可以在多个服务器上，区域服务器是来提供服务的，读写过程是由区域服务器完成，相当于数据节点，这2个都是完成数据的读写过程的。 在区域里面有个store在里面有MemStore。所以这些内容都是在内存当中，如果内存满了，就会被溢出到磁盘当中，Storefile就是在磁盘当中了，Hfile通过和底层交互 hbase的写入过程： root这个地方写错了是老版本的，应该是meta表 现在妖王一个表里写数据，需要先查询这个表在那个区域服务器上，要先查询hbase空间下的meta表，因为这个表里面标识了类似于一个目录的内容，标识那个区域子在那个服务器上，也就是rowkey的范围。这个meta表在哪里呢怎么找到呢？ meta表要通过zk来找到。因为zk集群在，所以没有zk的情况下hbase都起不来。 进入到hbase shell里面 通过进入到shell里面发现这个meta表大概可能在这个里面，进去之后发现这个meta表在s203里面。如下图 所以实际上就是先联系zk然后通过zk找到他的位置在s203里面，这个meta表在s203里面 ta/hbase/meta/1588230740/info/da7a63e29d1c4fb588068ea9c187ae27 hbase基于hdfs【表数据的存储结构目录构成】 hdfs://s201:8020/hbase/data/${名字空间}/${表名}/区域名称/列族名称 相同列族的数据存放在一个文件中， 【WAL写前日志目录结构构成】 hdfs://s201:8020/hbase/wals/s202,16020,1542534586029/s202%2C16020%2C1542534586029.default.1542541792199 hdfs://s201:8020/hbase/wals/${区域服务器名称}/主机名，端口号，时间戳/ client端交互过程0.集群启动时，master负责分配区域到指定的区域服务器 1.联系zk找出meta表所在的区域服务器rs(regionserver) /meta/meta-region-server 定位到所在的服务器 2.定位rowkey，找到对应的rs(regionserver) 3.缓存信息到本地， 4.联系regionserver 5.HRegionServe负责open-HRegion对象打开H区域服务器对象，为每个列族创建store实例，说明store是和列族对应的，store包涵多个storefile实例，他们是对hfile的轻量级封装，每个store还对应一个memstroe（用于内存储速度快）， 在百万数据存储的时候：关闭WALS 代码如下： @Test public void biginsert() throws Exception { long start=System.currentTimeMillis(); Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); TableName tname = TableName.valueOf(&quot;ns1:t1&quot;); HTable table = (HTable) conn.getTable(tname); //不要自动清理缓冲区 table.setAutoFlushTo(false); for (int i = 0; i &lt; 1000000; i++) { Put put = new Put(Bytes.toBytes(&quot;row&quot; + i)); //关闭写前日志 put.setWriteToWAL(false); put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;), Bytes.toBytes(i)); put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;), Bytes.toBytes(&quot;tom&quot; + i)); put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;), Bytes.toBytes(i % 100)); table.put(put); if (i % 2000 == 0) { table.flushCommits(); } } table.flushCommits(); System.out.println(System.currentTimeMillis()-start); } hbase shell命令： 要想删除表，先要禁用表。 $hbase&gt;flush &apos;ns1:t1&apos; //清理内存数据到磁盘 $hbase&gt;count &apos;ns1:t1&apos; //统计函数 $hbase&gt;disable &apos;ns1:t1&apos; //删除表之前要禁用表 $hbase&gt;drop &apos;ns1:t1&apos; //删除表 $hbase&gt;count &apos;hbase:meta&apos; //查看元数据表 格式化代码，设置固定数字格式 @Test public void formatNum(){ DecimalFormat format =new DecimalFormat(); format.applyPattern(&quot;0000000&quot;); // format.applyPattern(&quot;###,###,00&quot;); System.out.println(format.format(8)); } 为什么要格式化rowid，因为rowid1008和8相比较8比1008还要大，所以要格式化一下。 经过格式化rowid的代码： @Test public void biginsert() throws Exception { DecimalFormat format =new DecimalFormat(); format.applyPattern(&quot;0000000&quot;); System.out.println(format.format(8)); long start=System.currentTimeMillis(); Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); TableName tname = TableName.valueOf(&quot;ns1:t1&quot;); HTable table = (HTable) conn.getTable(tname); //不要自动清理缓冲区 table.setAutoFlushTo(false); for (int i = 0; i &lt; 10000; i++) { Put put = new Put(Bytes.toBytes(&quot;row&quot; + format.format(i))); //关闭写前日志 put.setWriteToWAL(false); put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;), Bytes.toBytes(i)); put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;), Bytes.toBytes(&quot;tom&quot; + i)); put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;), Bytes.toBytes(i % 100)); table.put(put); if (i % 2000 == 0) { table.flushCommits(); } } table.flushCommits(); System.out.println(System.currentTimeMillis()-start); } flush命令$hbase:flush：清理内存数据到磁盘 region拆分切割 hbase默认切割文件是10G，超过切割。 $hbase&gt;count &apos;ns1:t1&apos; //统计函数 切割而一旦完成会保留原来的表结构，但是原来的数据内容已经没有了被删除了 hbase和hadoop的ha集成1.在hbase-env.sh文件添加hadoop配置文件目录到hbase_classpath环境变量[/soft/hbase/conf/hbase-env]并且分发。 export HBASE_CLASSPATH=$HBASE_CLASSPATH:/soft/hadoop/ etc/hadoop 2.在hbase/conf目录下创建到hadoop的hdfs-site.xml的符号链接 $&gt;ln -s /soft/hadoop/etc/hadoop/hdfs-site.xml /soft/hbase/conf/hdfs-site.xml 3.修改Hbase-site.xml文件中hbase.rootdir的目录值 /soft/hbase/conf/hbase-site.xml4.将之都分发出去。 继承了之后及时关闭了s201的namenode，hbase的Hmaster也不会关闭，也就是形成了高可用状态。 hbase手动移动区域手动移动区域 手动强行合并hbase块 手动切割： 拆分风暴： 在达到10G以后，几个区域同时增长，同时到达10G临界点，同时切割额，服务器工作瞬间增大。每个都在切割，就叫切割风暴，我们要避免这种情况，通过使用手动切割，避免拆分风暴。 代码操作增删改查 package com.it18zhang.hbasedemo; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.hbase.*; import org.apache.hadoop.hbase.client.*; import org.apache.hadoop.hbase.util.Bytes; import org.junit.Test; import java.io.IOException; import java.text.DecimalFormat; import java.util.Iterator; import java.util.Map; import java.util.NavigableMap; /** * @Title:TestCRUD * @Description: * @Auther: Eric Hunn * @Version:1.0 * @Create 2018/11/17 11:56 */ public class TestCRUD { @Test public void put() throws Exception { Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); TableName tname = TableName.valueOf(&quot;ns1:t1&quot;); Table table = conn.getTable(tname); byte[] rowid = Bytes.toBytes(&quot;row2&quot;); byte[] f1 = Bytes.toBytes(&quot;f1&quot;); byte[] id = Bytes.toBytes(&quot;id&quot;); byte[] value = Bytes.toBytes(102); //创建put对象 Put put = new Put(rowid); put.addColumn(f1, id, value); table.put(put); } @Test public void biginsert() throws Exception { DecimalFormat format = new DecimalFormat(); format.applyPattern(&quot;0000000&quot;); System.out.println(format.format(8)); long start = System.currentTimeMillis(); Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); TableName tname = TableName.valueOf(&quot;ns1:t1&quot;); HTable table = (HTable) conn.getTable(tname); //不要自动清理缓冲区 table.setAutoFlushTo(false); for (int i = 0; i &lt; 10000; i++) { Put put = new Put(Bytes.toBytes(&quot;row&quot; + format.format(i))); //关闭写前日志 put.setWriteToWAL(false); put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;), Bytes.toBytes(i)); put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;), Bytes.toBytes(&quot;tom&quot; + i)); put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;), Bytes.toBytes(i % 100)); table.put(put); if (i % 2000 == 0) { table.flushCommits(); } } table.flushCommits(); System.out.println(System.currentTimeMillis() - start); } @Test public void formatNum() { DecimalFormat format = new DecimalFormat(); format.applyPattern(&quot;0000000&quot;); // format.applyPattern(&quot;###,###,00&quot;); System.out.println(format.format(8)); } @Test public void createNamespace() throws Exception { //创建conf对象 Configuration conf = HBaseConfiguration.create(); //通过工厂创建连接对象 Connection conn = ConnectionFactory.createConnection(conf); Admin admin = conn.getAdmin(); NamespaceDescriptor nsd = NamespaceDescriptor.create(&quot;ns2&quot;).build(); admin.createNamespace(nsd); NamespaceDescriptor[] ns = admin.listNamespaceDescriptors(); for (NamespaceDescriptor n : ns) { System.out.println(n.getName()); } } @Test public void listNamespaces() throws Exception { //创建conf对象 Configuration conf = HBaseConfiguration.create(); //通过工厂创建连接对象 Connection conn = ConnectionFactory.createConnection(conf); Admin admin = conn.getAdmin(); NamespaceDescriptor[] ns = admin.listNamespaceDescriptors(); for (NamespaceDescriptor n : ns) { System.out.println(n.getName()); } } @Test public void createTables() throws Exception { //创建conf对象 Configuration conf = HBaseConfiguration.create(); //通过工厂创建连接对象 Connection conn = ConnectionFactory.createConnection(conf); Admin admin = conn.getAdmin(); //创建表名对象 TableName tbn = TableName.valueOf(&quot;ns2:t2&quot;); //创建表描述符对象 HTableDescriptor tbl = new HTableDescriptor(tbn); //在表描述符中添加列族创建列族描述符 HColumnDescriptor col = new HColumnDescriptor(&quot;f1&quot;); tbl.addFamily(col); admin.createTable(tbl); System.out.println(&quot;over&quot;); } @Test public void disableTable() throws Exception { Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); Admin admin = conn.getAdmin(); admin.disableTable(TableName.valueOf(&quot;ns2:t2&quot;)); } @Test public void dropTable() throws Exception { Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); Admin admin = conn.getAdmin(); admin.disableTable(TableName.valueOf(&quot;ns2:t2&quot;)); admin.deleteTable(TableName.valueOf(&quot;ns2:t2&quot;)); } @Test public void deleteData() throws IOException { Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); TableName tname = TableName.valueOf(&quot;ns1:t1&quot;); Table table = conn.getTable(tname); Delete del = new Delete(Bytes.toBytes(&quot;row0000001&quot;)); del.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;)); del.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;)); table.delete(del); System.out.println(&quot;over&quot;); } @Test public void scanall() throws IOException { Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); TableName tname = TableName.valueOf(&quot;ns1:t1&quot;); Table table = conn.getTable(tname); Scan scan = new Scan(); ResultScanner rs = table.getScanner(scan); Iterator&lt;Result&gt; it = rs.iterator(); while (it.hasNext()) { Result r = it.next();//理解这个r是对一整行的封装 byte[] value = r.getValue(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;)); System.out.println(Bytes.toString(value)); } } @Test public void scan() throws IOException { Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); TableName tname = TableName.valueOf(&quot;ns1:t1&quot;); Table table = conn.getTable(tname); Scan scan = new Scan(); scan.setStartRow(Bytes.toBytes(&quot;row0005000&quot;)); scan.setStopRow(Bytes.toBytes(&quot;row0008000&quot;)); ResultScanner rs = table.getScanner(scan); Iterator&lt;Result&gt; it = rs.iterator(); while (it.hasNext()) { Result r = it.next();//理解这个r是对一整行的封装 byte[] value = r.getValue(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;)); System.out.println(Bytes.toString(value)); } } //动态获取所有的列和列族，动态遍历，每个列和值的集合，由于强行getvalue转换成string类型，所以难免出现乱码情况 @Test public void scan2() throws IOException { Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); TableName tname = TableName.valueOf(&quot;ns1:t1&quot;); Table table = conn.getTable(tname); Scan scan = new Scan(); ResultScanner rs = table.getScanner(scan); Iterator&lt;Result&gt; it = rs.iterator(); while (it.hasNext()) { Result r = it.next();//理解这个r是对一整行的封装 Map&lt;byte[], byte[]&gt; map = r.getFamilyMap(Bytes.toBytes(&quot;f1&quot;)); for (Map.Entry&lt;byte[], byte[]&gt; entrySet : map.entrySet()) { String col = Bytes.toString(entrySet.getKey()); String val = Bytes.toString(entrySet.getValue()); System.out.println(col + &quot;:&quot; + val + &quot;,&quot;); } System.out.println(); } } @Test public void scan3() throws IOException { Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); TableName tname = TableName.valueOf(&quot;ns1:t1&quot;); Table table = conn.getTable(tname); Scan scan = new Scan(); ResultScanner rs = table.getScanner(scan); Iterator&lt;Result&gt; it = rs.iterator(); while (it.hasNext()) { Result r = it.next();//理解这个r是对一整行的封装 //得到一行的所有map，key=f1,value=Map&lt;Col,Map&lt;Timestamp,value&gt;&gt;这个结构 NavigableMap&lt;byte[], NavigableMap&lt;byte[], NavigableMap&lt;Long, byte[]&gt;&gt;&gt; map = r.getMap(); for (Map.Entry&lt;byte[], NavigableMap&lt;byte[], NavigableMap&lt;Long, byte[]&gt;&gt;&gt; entry : map.entrySet()) { //得到列族 String f = Bytes.toString(entry.getKey()); NavigableMap&lt;byte[], NavigableMap&lt;Long, byte[]&gt;&gt; colDataMap = entry.getValue(); for (Map.Entry&lt;byte[], NavigableMap&lt;Long, byte[]&gt;&gt; ets : colDataMap.entrySet()) { String c = Bytes.toString(ets.getKey()); Map&lt;Long, byte[]&gt; tsValueMap = ets.getValue(); for (Map.Entry&lt;Long, byte[]&gt; e : tsValueMap.entrySet()) { Long ts = e.getKey(); String value = Bytes.toString(e.getValue()); System.out.println(f + &quot;:&quot; + c + &quot;:&quot; + ts + &quot;=&quot; + value + &quot;,&quot;); } } } } } }]]></content>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper第二天]]></title>
    <url>%2F2018%2F11%2F15%2FZookeeper%E7%AC%AC%E4%BA%8C%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[leader推选过程(最小号选举法)1.所有节点在同一目录下创建临时序列节点。 2.节点下会生成/xxx/xx000000001等节点。 3.序号最小的节点就是leader，其余就是follower. 4.每个节点观察小于自己节点的主机。(注册观察者) 5.如果leader挂了，对应znode删除了。 6.观察者收到通知。 配置完全分布式zk集群1.挑选3台主机 s201 ~ s203 2.每台机器都安装zk tar 环境变量 3.配置zk配置文件 s201 ~ s203 [/soft/zk/conf/zoo.cfg] ... dataDir=/home/centos/zookeeper server.1=s201:2888:3888 server.2=s202:2888:3888 server.3=s203:2888:3888 4.在每台主机的/home/centos/zookeeper中添加myid,内容分别是1,2,3 [s201] $&gt;echo 1 &gt; /home/centos/zookeeper/myid [s202] $&gt;echo 2 &gt; /home/centos/zookeeper/myid [s203] $&gt;echo 3 &gt; /home/centos/zookeeper/myid 5.启动服务器集群 $&gt;zkServer.sh start ... 6.查看每台服务器的状态 $&gt;zkServer.sh status 7.修改zk的log目录 vi /soft/zk/conf/log4j.properties 修改如下： 8.创建log目录： xcall.sh &quot;mkdir /home/centos/zookeeper/log&quot; 部署细节1.在jn节点分别启动jn进程 $&gt;hadoop-daemon.sh start journalnode 2.启动jn之后，在两个NN之间进行disk元数据同步 a)如果是全新集群，先format文件系统,只需要在一个nn上执行。 [s201] $&gt;hadoop namenode -format b)如果将非HA集群转换成HA集群，复制原NN的metadata到另一个nn. 1.步骤一 [s201] $&gt;scp -r /home/centos/hadoop/dfs centos@s206:/home/centos/hadoop/ 2.步骤二 在新的nn(未格式化的nn)上运行一下命令，实现待命状态引导。 [s206] $&gt;hdfs namenode -bootstrapStandby //需要s201为启动状态,提示是否格式化,选择N. 3)在一个NN上执行以下命令，完成edit日志到jn节点的传输。 $&gt;hdfs namenode -initializeSharedEdits #查看s202,s203是否有edit数据. 4)启动所有节点. [s201] $&gt;hadoop-daemon.sh start namenode //启动名称节点 $&gt;hadoop-daemons.sh start datanode //启动所有数据节点 [s206] $&gt;hadoop-daemon.sh start namenode //启动名称节点 HA管理$&gt;hdfs haadmin -transitionToActive nn1 //切成激活态 $&gt;hdfs haadmin -transitionToStandby nn1 //切成待命态 $&gt;hdfs haadmin -transitionToActive --forceactive nn2//强行激活 $&gt;hdfs haadmin -failover nn1 nn2 //模拟容灾演示,从nn1切换到nn2 完全0开始部署hadoop HDFS的HA集群，使用zk实现自动容灾1.停掉hadoop的所有进程 2.删除所有节点的日志和本地数据. 删除/home/centos/hadoop下的所有和 /home/centos/journal下的所有 3.改换hadoop符号连接为ha 4.登录每台JN节点主机，启动JN进程. [s202-s204] $&gt;hadoop-daemon.sh start journalnode 5.登录其中一个NN,格式化文件系统(s201) $&gt;hadoop namenode -format 6.复制201目录的下nn的元数据到s206 $&gt;scp -r ~/hadoop/* centos@s206:/home/centos/hadoop 7.在未格式化的NN(s206)节点上做standby引导. 7.1)需要保证201的NN启动 $&gt;hadoop-daemon.sh start namenode 7.2)登录到s206节点，做standby引导. $&gt;hdfs namenode -bootstrapStandby 7.3)登录201，将s201的edit日志初始化到JN节点。 $&gt;hdfs namenode -initializeSharedEdits 8.启动所有数据节点. $&gt;hadoop-daemons.sh start datanode 9.登录到206,启动NN $&gt;hadoop-daemon.sh start namenode 10.查看webui http://s201:50070/ http://s206:50070/ 11.自动容灾 11.1)介绍 自动容灾引入两个组件，zk quarum + zk容灾控制器(ZKFC)。 运行NN的主机还要运行ZKFC进程，主要负责: a.健康监控 b.session管理 c.选举 11.2部署容灾 a.停止所有进程 $&gt;stop-all.sh b.配置hdfs-site.xml，启用自动容灾. [hdfs-site.xml] &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; c.配置core-site.xml，指定zk的连接地址. &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;s201:2181,s202:2181,s203:2181&lt;/value&gt; &lt;/property&gt; d.分发以上两个文件到所有节点。 12.登录其中的一台NN(s201),在ZK中初始化HA状态 $&gt;hdfs zkfc -formatZK 13.启动hdfs进程. $&gt;start-dfs.sh 14.测试自动容在(206是活跃节点) $&gt;kill -9 配置RM的HA自动容灾1.配置yarn-site.xml &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;cluster1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &lt;value&gt;s201&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &lt;value&gt;s206&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt; &lt;value&gt;s201:8088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt; &lt;value&gt;s206:8088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;s201:2181,s202:2181,s203:2181&lt;/value&gt; &lt;/property&gt; 2.使用管理命令 //查看状态 $&gt;yarn rmadmin -getServiceState rm1 //切换状态到standby $&gt;yarn rmadmin -transitionToStandby rm1 3.启动yarn集群 $&gt;start-yarn.sh 4.hadoop没有启动两个resourcemanager,需要手动启动另外一个 $&gt;yarn-daemon.sh start resourcemanager 5.查看webui 6.做容灾模拟. kill -9 hive的注意事项如果配置hadoop HA之前，搭建了Hive的话，在HA之后，需要调整路径信息. 主要是修改mysql中的dbs,tbls等相关表。 Hbasehadoop数据库，分布式可伸缩大型数据存储。 用户对随机、实时读写数据。 十亿行 x 百万列。 版本化、非关系型数据库。 FeatureLinear and modular scalability. //线性模块化扩展方式。 Strictly consistent reads and writes. //严格一致性读写 Automatic and configurable sharding of tables //自动可配置表切割 Automatic failover support between RegionServers. //区域服务器之间自动容在 Convenient base classes for backing Hadoop MapReduce jobs with Apache HBase tables. // Easy to use Java API for client access. //java API Block cache and Bloom Filters for real-time queries //块缓存和布隆过滤器用于实时查询 Query predicate push down via server side Filters //通过服务器端过滤器实现查询预测 Thrift gateway and a REST-ful Web service that supports XML, Protobuf, and binary data encoding options // Extensible jruby-based (JIRB) shell // Support for exporting metrics via the Hadoop metrics subsystem to files or Ganglia; or via JMX //可视化 面向列数据库。 hbase存储机制面向列存储，table是按row排序。 搭建hbase集群0.选择安装的主机 s201 ~ s204 1.jdk 略 2.hadoop 略 3.tar 略 4.环境变量 略 5.验证安装是否成功 $&gt;hbase version 5.配置hbase模式 5.1)本地模式 [hbase/conf/hbase-env.sh] EXPORT JAVA_HOME=/soft/jdk [hbase/conf/hbase-site.xml] ... &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;file:/home/hadoop/HBase/HFiles&lt;/value&gt; &lt;/property&gt; 5.2)伪分布式 [hbase/conf/hbase-env.sh] EXPORT JAVA_HOME=/soft/jdk [hbase/conf/hbase-site.xml] &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://localhost:8030/hbase&lt;/value&gt; &lt;/property&gt; 5.3)完全分布式(必做) [hbase/conf/hbase-env.sh] export JAVA_HOME=/soft/jdk export HBASE_MANAGES_ZK=false [hbse-site.xml] &lt;!-- 使用完全分布式 --&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hbase数据在hdfs上的存放路径 --&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://s201:8020/hbase&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置zk地址 --&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;s201:2181,s202:2181,s203:2181&lt;/value&gt; &lt;/property&gt; &lt;!-- zk的本地目录 --&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/home/centos/zookeeper&lt;/value&gt; &lt;/property&gt; 6.配置regionservers [hbase/conf/regionservers] s202 s203 s204 7.启动hbase集群(s201) $&gt;start-hbase.sh 8.登录hbase的webui http://s201:16010]]></content>
  </entry>
  <entry>
    <title><![CDATA[Avro&protobuf]]></title>
    <url>%2F2018%2F11%2F11%2FAvro-protobuf%2F</url>
    <content type="text"><![CDATA[hive数据倾斜. $hive&gt;SET hive.optimize.skewjoin=true; $hive&gt;SET hive.skewjoin.key=100000; $hive&gt;SET hive.groupby.skewindata=true; CREATE TABLE mydb.doc(line string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’ ; select t.word,count(*) from (select explode(split(line,’ ‘)) word from doc ) t group by t.word ; java串行化串行化系统protobuf，协议缓冲区。 在Hadoop里面的代码很多是通过相关语言自动生成的。 hadoop底层的rpc都是自动生成的。 java也有串行化，但是hadoop不适用，因为相对来讲比protobuf和avro复杂而且性能差一些。 当吧对象写入磁盘文件或者用于网络传输的时候要把对象变成字节数组。 串行化从本质上来讲是数据格式，就是一种格式，吧对象变成字节数组，也就是说所有数据都要落实到字节数组上，都需要这样，不管是影音文件还是别的文件，通过串行化吧他边恒字节数组 关于Javabean:标准javabean(pojo,plain old java object) 任何一个Java类也可以叫javabean.广义上。 狭义上的javabean：就是普通古老的java对象pojo:plain old java object 也就是私有的属性，getset方法，空的构造函数。比如person,学生，人。都是javabean。现实生活中的名词，在开发中都被定义成一个类，也就是说的javabean。 下面的代码就是一段javabean class Person{ public Person(){ } private String name; public void setName(String name){ this.name=name; } publc String genName(){ return name; }} google protobuf1.下载google protobuf.配置环境 protoc-2.5.0-win32.zip 1&apos;.pom.xml &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.protobuf&lt;/groupId&gt; &lt;artifactId&gt;protobuf-java&lt;/artifactId&gt; &lt;version&gt;2.5.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 2.设计对象 ... 3.描述对象 package tutorial; option java_package = &quot;com.example.tutorial&quot;; option java_outer_classname = &quot;AddressBookProtos&quot;; //这个是一个javabean，在protobuf里面叫message message Person { required string name = 1; required int32 id = 2; optional string email = 3; //下面这个是一个phonetype的枚举类 enum PhoneType { MOBILE = 0; HOME = 1; WORK = 2; } //这个也是一个javabean。 message PhoneNumber { required string number = 1; optional PhoneType type = 2 [default = HOME]; } repeated PhoneNumber phone = 4; } message AddressBook { repeated Person person = 1; } 4.编译描述 cmd&gt;protoc --java_out . xxx.proto 5.导入源代码到项目中 ... 6.使用对象 public class TestProtoBuf { @Test public void write() throws Exception{ AddressBookProtos.Person john = AddressBookProtos.Person.newBuilder() .setId(12345) .setName(&quot;tomas&quot;) .setEmail(&quot;123@123.123&quot;) .addPhone(AddressBookProtos.Person.PhoneNumber.newBuilder() .setNumber(&quot;+351 999 999 999&quot;) .setType(AddressBookProtos.Person.PhoneType.HOME) .build()) .build(); john.writeTo(new FileOutputStream(&quot;d:/prototbuf.data&quot;)); } @Test public void read() throws Exception{ AddressBookProtos.Person john = AddressBookProtos.Person.parseFrom(new FileInputStream(&quot;d:/prototbuf.data&quot;)); System.out.println(john.getName()); } } 上一段是用Protobuf进行编译和反编译的过程。然后下面这段图片是用java进行编译和反编译的过程： xml&lt;?xml version=&quot;1.0&quot;?&gt; &lt;persons&gt; &lt;person id=&quot;&quot; name=&quot;&quot;&gt; &lt;age&gt;12&lt;/age&gt; &lt;/person&gt; &lt;/person&gt; json[{ &quot;id&quot; : 1, &quot;nmae&quot; : &quot;tom&quot;, &quot;age&quot; : 20 }, { &quot;id&quot; : 2, &quot;nmae&quot; : &quot;tomas&quot;, &quot;age&quot; : 30 } ] avro (doug cutting)1.数据串行化系统 2.自描述语言. 数据结构和数据都存在文件中。跨语言。 使用json格式存储数据。 3.可压缩 + 可切割。 4.使用avro a)定义schema b)编译schema，生成java类 { //名字空间是这个，然后类型，然后名字，然后是字段数组 &quot;namespace&quot;: &quot;tutorialspoint.com&quot;, &quot;type&quot;: &quot;record&quot;, &quot;name&quot;: &quot;emp&quot;, &quot;fields&quot;: [ {&quot;name&quot;: &quot;name&quot;, &quot;type&quot;: &quot;string&quot;}, {&quot;name&quot;: &quot;id&quot;, &quot;type&quot;: &quot;int&quot;}, {&quot;name&quot;: &quot;salary&quot;, &quot;type&quot;: &quot;int&quot;}, {&quot;name&quot;: &quot;age&quot;, &quot;type&quot;: &quot;int&quot;}, {&quot;name&quot;: &quot;address&quot;, &quot;type&quot;: &quot;string&quot;} ] } c)使用java类 cmd&gt;java -jar avro-tools-1.7.7.jar compile schema emp.avsc . d)单元测试 package com.it18zhang.avrodemo.test; import org.apache.avro.Schema; import org.apache.avro.file.DataFileReader; import org.apache.avro.file.DataFileWriter; import org.apache.avro.generic.GenericData; import org.apache.avro.generic.GenericRecord; import org.apache.avro.io.DatumWriter; import org.apache.avro.specific.SpecificDatumReader; import org.apache.avro.specific.SpecificDatumWriter; import org.junit.Test; import java.io.File; import java.io.IOException; import java.util.Iterator; /** * Created by Administrator on 2017/3/23. */ public class TestAvro { // @Test // public void write() throws Exception { // //创建writer对象 // SpecificDatumWriter empDatumWriter = new SpecificDatumWriter&lt;Employee&gt;(Employee.class); // //写入文件 // DataFileWriter&lt;Employee&gt; empFileWriter = new DataFileWriter&lt;Employee&gt;(empDatumWriter); // // //创建对象 // Employee e1 = new Employee(); // e1.setName(&quot;tomas&quot;); // e1.setAge(12); // // //串行化数据到磁盘 // empFileWriter.create(e1.getSchema(), new File(&quot;d:/avro/data/e1.avro&quot;)); // empFileWriter.append(e1); // empFileWriter.append(e1); // empFileWriter.append(e1); // empFileWriter.append(e1); // //关闭流 // empFileWriter.close(); // } // // @Test // public void read() throws Exception { // //创建writer对象 // SpecificDatumReader empDatumReader = new SpecificDatumReader&lt;Employee&gt;(Employee.class); // //写入文件 // DataFileReader&lt;Employee&gt; dataReader = new DataFileReader&lt;Employee&gt;(new File(&quot;d:/avro/data/e1.avro&quot;) ,empDatumReader); // Iterator&lt;Employee&gt; it = dataReader.iterator(); // while(it.hasNext()){ // System.out.println(it.next().getName()); // } // } /** * 直接使用schema文件进行读写，不需要编译 */ @Test public void writeInSchema() throws Exception { //指定定义的avsc文件。 Schema schema = new Schema.Parser().parse(new File(&quot;d:/avro/emp.avsc&quot;)); //创建GenericRecord,相当于Employee GenericRecord e1 = new GenericData.Record(schema); //设置javabean属性 e1.put(&quot;Name&quot;, &quot;ramu&quot;); // e1.put(&quot;id&quot;, 001); // e1.put(&quot;salary&quot;, 30000); e1.put(&quot;age&quot;, 25); // e1.put(&quot;address&quot;, &quot;chennai&quot;); // DatumWriter&lt;GenericRecord&gt; empDatumWriter = new SpecificDatumWriter&lt;GenericRecord&gt;(GenericRecord.class); DataFileWriter&lt;GenericRecord&gt; empFileWriter = new DataFileWriter&lt;GenericRecord&gt;(empDatumWriter); empFileWriter.create(schema,new File(&quot;d:/avro/data/e2.avro&quot;)) ; empFileWriter.append(e1); empFileWriter.append(e1); empFileWriter.append(e1); empFileWriter.append(e1); empFileWriter.close(); } } 看一下AVSC这个编译好的avro文件里面的是什么结构： 他其实是一个json格式的结构： 非编译模式 --------------- @Test public void writeInSchema() throws Exception { //指定定义的avsc文件。 Schema schema = new Schema.Parser().parse(new File(&quot;d:/avro/emp.avsc&quot;)); //创建GenericRecord,相当于Employee GenericRecord e1 = new GenericData.Record(schema); //设置javabean属性 e1.put(&quot;name&quot;, &quot;ramu&quot;); // e1.put(&quot;id&quot;, 001); // e1.put(&quot;salary&quot;, 30000); e1.put(&quot;age&quot;, 25); // e1.put(&quot;address&quot;, &quot;chennai&quot;); // DatumWriter w1 = new SpecificDatumWriter (schema); DataFileWriter w2 = new DataFileWriter(w1); w2.create(schema,new File(&quot;d:/avro/data/e2.avro&quot;)) ; w2.append(e1); w2.append(e1); w2.close(); } /** * 反串行avro数据 */ @Test public void readInSchema() throws Exception { //指定定义的avsc文件。 Schema schema = new Schema.Parser().parse(new File(&quot;d:/avro/emp.avsc&quot;)); GenericRecord e1 = new GenericData.Record(schema); DatumReader r1 = new SpecificDatumReader (schema); DataFileReader r2 = new DataFileReader(new File(&quot;d:/avro/data/e2.avro&quot;),r1); while(r2.hasNext()){ GenericRecord rec = (GenericRecord)r2.next(); System.out.println(rec.get(&quot;name&quot;)); } r2.close(); }]]></content>
      <categories>
        <category>Avro&amp;Protobuf</category>
      </categories>
      <tags>
        <tag>Avro</tag>
        <tag>Protobuf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[笔记重点总结]]></title>
    <url>%2F2018%2F11%2F11%2F%E7%AC%94%E8%AE%B0%E9%87%8D%E7%82%B9%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[在Hadoop中重点是，全排序，二次排序。在视频的最后有一天是讲解一个二次排序的实例，我没有细看。 在Hive中我认为重点的是关于查询的内容，因为在hive中一般都是加载数据然后查询，查询之后有一个自定义函数UDF的内容，这部分的内容看的不是很清晰。 在avro和Protoc里面主要讲解2中反编译的方法，主要是一个面向对象开发的一个过程，还是java编程不熟悉，然后代码跟着走了一遍但是肯定还是有问题，视频里面的老师是在一遍看外文的书籍，根据里面的代码模板来写的。在自己敲的过程中protoc的代码引入maven依赖包之后，还是没有能够将编译好的文件识别出来，不知道什么问题，但是avo就引入maven之后解决了这个问题。 在讲解zookeeper的时候我觉得在选举算法的时候他没有细致的讲解，然后这边主要是zookeeper部署在hadoop集群和使用API控制zookeeper。在集群部署高可用的时候是201202203部署zk202203204部署journalnode节点，（journalnode节点已经快忘记了）。 容灾管理器进程ZKFC在配置的时候后来跟着yarn启动了起来。下面图是整个的进程图： 这个zookeeper的地方看的比较快，对于每个目录做什么的比较忘记，然后在配置的时候没有细究配置的意义。就是跟着走了一遍。但是最后配置了起来了。配置hdfs高可用的时候比较费时费力，配置内容比较多，配置yarn的时候比较简单，配置一下即可。 在hbase安装的过程中，由于开始的时候设置了zookeeper的HA自动容灾，所以最开始的时候s206自动设置为active模式，要把s206设置为standby模式才能在s201里面设置s201为HMaster。 也就是说要杀死掉s206里面的namenode或者说通过转换吧他变成standby然后s201设置为active模式。不然会导致出错 几个端口2181 8080 50070 8020 16010 在讲解hbase 第二天的动态遍历的那个三层For循环并没有看明白，也就是说Java的基础还是很薄弱的。讲解hbase一个三层循环嵌套来scan的没有弄明白。hbase的时候讲解的重点在于协处理器的理解和那个电信的一个calllogs的rowkey的设计。利用了一个二级索引的方式。这个地方的原理很重要。然后最后有一个平时用于生产的工具叫phoenix的工具，可以用类sql的语句写出来，避免了自己设二次索引。里面有大量协处理器的封装，可以直接用sql语句。这边又讲了一个hive和hbase集成的一个问题，这个直接在hive中直接写语句即可。注意看一下phoenix和hive集成的区别。]]></content>
      <categories>
        <category>笔记总结</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>重点</tag>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[心情日记——2018.11.09]]></title>
    <url>%2F2018%2F11%2F09%2F%E4%BA%BA%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%8A%AA%E5%8A%9B%EF%BC%9F%E2%80%94%E2%80%942018-11-09%2F</url>
    <content type="text"><![CDATA[无意之间从知乎里面看见的题目是：人这一辈子为什么要努力？ 泪目，与诸君共勉。 奶奶五十岁才来的美国，身无分文，也没有一技之长，随身携带的只有长年辛勤劳作留下的一身病根。 那是九十年代初，她去了唐人街的扎花厂上班，每天工作十个小时以上，一年工作三百六十五天。 扎花按件算钱，她眼神虽然不好，却比谁扎得都快。 有人弹吉他磨出了茧，有人搬砖头磨出了茧，她被针头扎出了茧。 回国看我的时候，她给我带了费列罗巧克力，翘胡子薯片，Jif花生酱。 她给我买好看的小西装，给我买一斤几十元的黄螺。 她带我去动物园，游乐园，森林公园，带我去北京看长城，看天安门，看毛主席。 “奶奶，美国比北京还好吗？” 她用最朴实的语言向我描述纽约的繁华，告诉我美国好极了，一切都在等着我。 知乎上经常讨论富养女孩，我有一个男孩被富养的故事。 有一年，我的巧克力吃完了，薯片吃完了，花生酱吃完了，奶奶还是没有回来。 那是我第一个不愿意知道的生活真相，她中风了，瘫了半边身子。 奶奶移民美国时带去了三个未成年的儿子，二叔，三叔，四叔。 二叔和三叔在登陆美国的第二天就打工去了，年幼的四叔读了几年书后也离开了学校。 他们在亲戚家的餐馆打工，每天工作十二个小时，每周工作六天。 餐馆一年营业三百六十四天，只在感恩节那天歇业。 奶奶瘫痪后，叔叔们轮流回国，给我带好吃的，带我出去玩。 我喜欢打乒乓球，二叔给我买了乒乓球，乒乓球拍，乒乓球桌。 有一年流行四驱车，三叔给我买了一辆遥控越野车，助我碾压所有的小伙伴。 四叔年少风流，出门把妹的时候总不忘带上我，要把我培养成下一代情圣。 他们绝口不提在美国生存的艰辛，那是大人们的秘密，和我无关。 奶奶出国五年后，爸妈也去了美国。 怎一个落魄了得？夫妻俩连属于自己的房间都没有。 扎花已经被时代淘汰，只有重活可以干。我爸当过屠夫，货柜工人，货运司机。 细节不必赘述，无非就是 12小时 x 365天的陈词滥调。 后来，他们四兄弟聚首，开了一家小超市，一家餐馆，后来又开了第二家小超市，第二家餐馆。 钱赚得越来越多，老家伙们拼命工作的老毛病却没有得到丝毫缓解。 爸妈出国五年后，我也来了美国，看清了生活本来的面目。 我在纽约生活了几个月，带着半身不遂的奶奶看遍了世界之都的繁华。 在这之前，她几乎没有走出过唐人街的范围，以前对我描绘纽约时一半是转述，一半靠想象。 后来，我回到了父母的身边，结束了长达五年的骨肉分离。 我爸来车站接我，把我带到了一栋小别墅前，很得意地告诉我：“这是我们家，知道你要来，刚买的。” 我去过奶奶刚来美国时住过的那个阴暗破旧的小公寓楼，也去过爸妈栖身过的那个散发着霉味的地下室，我知道我在新家会有一张床，一个房间，但我没想到，我会有一个别墅，一个前院，一个后院，一整个铺垫好的未来。 人这一生为什么要努力？ 奶奶的答案是我，爸妈的答案是我，我的答案是他们，以及把身家性命托付于我的媳妇和孩子。 对于我来说，长大了，责任多了，自然而然地想要努力，不是为了赚很多很多钱，而是为了过上自己想要的生活，过上奶奶和父母不曾拥有过的生活。他们替我吃完了所有的苦，帮我走了九十九步，我自己只需再走一步，哪敢迟疑？ 从外曾祖父母算起，我们家族四代八十多口人已经在美国奋斗了半个多世纪。我们人人都在努力，我们一代好过一代。 如果你的人生起点不高，不曾有人为你走过人生百步中的任何一步，不打紧，你尽管努力，多出来的步数不会被浪费掉，总有你在乎的人用得着，而你迟早会遇到你在乎的人。 与诸君共勉。 顾宇的知乎回答索引]]></content>
      <categories>
        <category>心情记</category>
      </categories>
      <tags>
        <tag>心情记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive第二天]]></title>
    <url>%2F2018%2F11%2F08%2FHive%E7%AC%AC%E4%BA%8C%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[hive数据仓库,在线分析处理。 HiveQL,类似sql语言。 表,metadata-&gt;rdbms. hive处理的数据是hdfs. MR,聚合操作。 hive的2个重点一个是分区一个是桶表内部表,管理表,托管表hive,drop ,数据也删除 外部表hive表结构。 分区表目录. where 缩小查询范围。 bucket表文件。 hash clustered by &apos;&apos; join横向连接，也就是作外连接和右外连接 union竖向连接。只能指定相匹配的字段 select id ,name from a1 union select id ,cid from a2; hive union操作select id,name from customers union select id,orderno from orders ; $&gt;hive //hive --service cli $&gt;hive --servic hiveserver2 //启动hiveserver2，10000 [thriftServer] $&gt;hive --service beeline //beeline hive使用jdbc协议实现远程访问hive$hive&gt;CREATE TABLE t3(id int,name string,age int) PARTITIONED BY (Year INT, Month INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ; export$hive&gt;EXPORT TABLE customers TO &apos;/user/centos/tmp.txt&apos;; //导出表结构+数据到hdfs目录。 看一下导出来的东西，是一个目录，包括表结构和表内容。 /order全排序$hive&gt;select * from orders order by id asc ; sort,map端排序,本地有序。$hive&gt;select * from orders sort by id asc ; distribute by &amp; cluster by &amp; sort by类似于mysql的group by,进行分区操作。 //select cid , ... from orders distribute by cid sort by name ; //注意顺序. $hive&gt;select id,orderno,cid from orders distribute by cid sort by cid desc ; //cluster by ===&gt; distribute by cid sort by cid destribut by 和cluster by图解对比： 这个地方没有细讲，说主要应用还是group by 只要记住这个cluster是dirstribute by 和sort by 的组合极客 函数mysql&gt;select concat(&apos;tom&apos;,1000) ; $hive&gt;select current_database(),current_user() ; $hive&gt;tab //查看帮助 设置作业参数$hive&gt;set hive.exec.reducers.bytes.per.reducer=xxx //设置reducetask的字节数。 $hive&gt;set hive.exec.reducers.max=0 //设置reduce task的最大任务数 $hive&gt;set mapreduce.job.reduces=0 //设置reducetask个数。 动态分区-严格模式-非严格模式动态分区模式:strict-严格模式，插入时至少指定一个静态分区，nonstrict-非严格模式-可以不指定静态分区。 set hive.exec.dynamic.partition.mode=nonstrict //设置非严格模式 $hive&gt;INSERT [OVERWRITE] into TABLE employees PARTITION (country, state) SELECT ..., se.cnty, se.st FROM staged_employees se WHERE se.cnty = &apos;US&apos;; 本来有一个已经分区好了的表，然后再建立一个表，比如也是相同的字段，然后新表没有分区要插进去老表，让他自动分区，就要用到动态分区，动态分区分为严格模式和非严格模式。如下图先使用了严格模式的情况，出错，说要至少指定一个分区也就是 $hive&gt;INSERT [OVERWRITE] into TABLE employees PARTITION (country=&apos;US&apos;, state) SELECT ..., se.cnty, se.st FROM staged_employees; 但是严格模式还是要制定这个分区，所以用一下非严格模式先设置一下： set hive.exec.dynamic.partition.mode=nonstrict //设置非严格模式 然后我们开始插入： $hive&gt;INSERT [OVERWRITE] into TABLE employees PARTITION (country, state) SELECT ..., se.cnty, se.st FROM staged_employees; 这样就实现了非严格模式下的动态分区 hive事务处理在&gt;0.13.0之后支持行级事务。 #（这个地方老师没讲清楚自己没搞出来）1.所有事务自动提交。 2.只支持orc格式。 3.使用bucket表。 4.配置hive参数，使其支持事务。 $hive&gt;SET hive.support.concurrency = true;$hive&gt;SET hive.enforce.bucketing = true;$hive&gt;SET hive.exec.dynamic.partition.mode = nonstrict;$hive&gt;SET hive.txn.manager = org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;$hive&gt;SET hive.compactor.initiator.on = true;$hive&gt;SET hive.compactor.worker.threads = 1; 5.使用事务性操作 $&gt;CREATE TABLE tx(id int,name string,age int) CLUSTERED BY (id) INTO 3 BUCKETS ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; stored as orc TBLPROPERTIES (&apos;transactional&apos;=&apos;true&apos;); mysql支持这样的操作，左边选择的查询的列，并不出现在右边的分组当中这样可以查询，但是hive中不允许这样。 但是在hive当中只可以如下图查询： 聚合处理每个用户购买商品订单大于1的：这个按照cid分组的，查询每个分组里面的count(*)，也就是每个不同的cid 的 $hive&gt;select cid,count(*) c ,max(price) from orders group by cid having c &gt; 1 ; wordcount创建新表:stats(word string,c int) ; 将查询结果插入到指定表中。 按照下图这个切割方法，用一个split的函数切割，切割之后形成一个数组类型的东西 explode这个函数传入的需要是一个经过split切割之后的数组，传入之后再去展开得到如上图所示的东西。但是切出来还是需要聚合，查询每个单词的个数 对下面这个SQL语句进行讲解。先看括号里面的最里面的括号，select explode(split(line,’ ‘)) as word from doc这句话都能理解，就是先切割成数组，然后对这个数据进行explode炸裂成单词，然后as word就是别名叫word，然后再把整个查询的as t别名t，然后select t.word,count(*) c from …group by t.word order by c desc limit2;就是对t的word字段进行分组，对分组的t的字段word和计数总和进行查询，然后按照降序排序，然后limit 2取最高的2个数。 $hive&gt;select t.word,count(*) c from ((select explode(split(line, &apos; &apos;)) as word from doc) as t) group by t.word order by c desc limit 2 ; view:视图,虚表是一个虚拟的表，类似于对这个语句的封装，或者说起了一个别名，并不是真的表。 这个地方创建视图的时候不能 $hive&gt;create view v1 as select a.*,b.*from customers a left outer join default.tt b on a.id = b.cid ; 上图这样是错误的，因为在a和b里面有相同的字段，要把相同的字段修改为不同的字段，如下图： //创建视图 $hive&gt;create view v1 as select a.id aid,a.name ,b.id bid , b.order from customers a left outer join default.tt b on a.id = b.cid ; 可以在续表的基础上在查询，如下所示： //查看视图 $hive&gt;show tables ; $hive&gt;select * from v1 ; view这个在hdfs上没有哦，而是存放在了mysql里面 一个mysql里面的操作：\G 行转列select * from tbls \G：结果如下： Map端连接$hive&gt;set hive.auto.convert.join=true //设置自动转换连接,默认开启了。 //使用mapjoin连接暗示实现mapjoin $hive&gt;select /*+ mapjoin(customers) */ a.*,b.* from customers a left outer join orders b on a.id = b.cid ; 调优1.explain 使用explain查看查询计划 hive&gt;explain [extended] select count(*) from customers ; hive&gt;explain select t.name , count(*) from (select a.name ,b.id,b.orderno from customers a ,orders b where a.id = b.cid) t group by t.name ; //设置limit优化测，避免全部查询.但是树上说因为是随机取样，可能会出问题。 hive&gt;set hive.limit.optimize.enable=true //本地模式 $hive&gt;set mapred.job.tracker=local; // $hive&gt;set hive.exec.mode.local.auto=true //自动本地模式,主要用于测试不用于实战 //并行执行,同时执行不存在依赖关系的阶段。?? $hive&gt;set hive.exec.parallel=true //是自动设置好的 //严格模式, $hive&gt;set hive.mapred.mode=strict //手动设置之后。1.分区表必须指定分区进行查询，否则不让查询。 //2.order by时必须使用limit子句。 //3.不允许笛卡尔积. //设置MR的数量 hive&gt; set hive.exec.reducers.bytes.per.reducer=750000000; //设置reduce处理的字节数。 //JVM重用 $hive&gt;set mapreduce.job.jvm.numtasks=1 //-1没有限制，使用大量小文件。 //UDF //User define function,用户自定义函数 //current_database(),current_user(); //显式所有函数 $hive&gt;show functions; $hive&gt;select array(1,2,3) ; //显式指定函数帮助 $hive&gt;desc function current_database(); //表生成函数,多行函数。 $hive&gt;explode(str,exp); //按照exp切割str. 自定义函数 上图带括号的都是函数，不带括号的是命令 1.创建类，继承UDF package com.it18zhang.hivedemo.udf; import org.apache.hadoop.hive.ql.exec.Description; import org.apache.hadoop.hive.ql.exec.UDF; /** * 自定义hive函数 */ @Description(name = &quot;myadd&quot;, value = &quot;myadd(int a , int b) ==&gt; return a + b &quot;, extended = &quot;Example:\n&quot; + &quot; myadd(1,1) ==&gt; 2 \n&quot; + &quot; myadd(1,2,3) ==&gt; 6;&quot;) public class AddUDF extends UDF { public int evaluate(int a ,int b) { return a + b ; } public int evaluate(int a ,int b , int c) { return a + b + c; } } 2.打成jar包。 cmd&gt;cd {classes所在目录} cmd&gt;jar cvf HiveDemo.jar -C x/x/x/x/classes/ . 3.添加jar包到hive的类路径 //添加jar到类路径 $&gt;cp /mnt/hgfs/downloads/bigdata/data/HiveDemo.jar /soft/hive/lib 3.重进入hive $&gt;.... 4.创建临时函数 // CREATE TEMPORARY FUNCTION myadd AS &apos;com.it18zhang.hivedemo.udf.AddUDF&apos;; 5.在查询中使用自定义函数 $hive&gt;select myadd(1,2) ; 6.定义日期函数 1)定义类 public class ToCharUDF extends UDF { /** * 取出服务器的当前系统时间 2017/3/21 16:53:55 */ public String evaluate() { Date date = new Date(); SimpleDateFormat sdf = new SimpleDateFormat(); sdf.applyPattern(&quot;yyyy/MM/dd hh:mm:ss&quot;); return sdf.format(date) ; } public String evaluate(Date date) { SimpleDateFormat sdf = new SimpleDateFormat(); sdf.applyPattern(&quot;yyyy/MM/dd hh:mm:ss&quot;); return sdf.format(date) ; } public String evaluate(Date date,String frt) { SimpleDateFormat sdf = new SimpleDateFormat(); sdf.applyPattern(frt); return sdf.format(date) ; } } 2)导出jar包，通过命令添加到hive的类路径(不需要重进hive)。 $hive&gt;add jar /mnt/hgfs/downloads/bigdata/data/HiveDemo-1.0-SNAPSHOT.jar 3)注册函数 $hive&gt;CREATE TEMPORARY FUNCTION to_char AS &apos;com.it18zhang.hivedemo.udf.ToCharUDF&apos;; $hive&gt;CREATE TEMPORARY FUNCTION to_date AS &apos;com.it18zhang.hivedemo.udf.ToCharUDF&apos;; 定义Nvl函数(这个是在英文版的Hadoop权威指南上给的)package com.it18zhang.hivedemo.udf; import org.apache.hadoop.hive.ql.exec.UDFArgumentException; import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException; import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException; import org.apache.hadoop.hive.ql.metadata.HiveException; import org.apache.hadoop.hive.ql.udf.generic.GenericUDF; import org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils; import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector; /** * 自定义null值处理函数 */ public class Nvl extends GenericUDF { private GenericUDFUtils.ReturnObjectInspectorResolver returnOIResolver; private ObjectInspector[] argumentOIs; public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException { argumentOIs = arguments; //检查参数个数 if (arguments.length != 2) { throw new UDFArgumentLengthException( &quot;The operator &apos;NVL&apos; accepts 2 arguments.&quot;); } returnOIResolver = new GenericUDFUtils.ReturnObjectInspectorResolver(true); //检查参数类型 if (!(returnOIResolver.update(arguments[0]) &amp;&amp; returnOIResolver .update(arguments[1]))) { throw new UDFArgumentTypeException(2, &quot;The 1st and 2nd args of function NLV should have the same type, &quot; + &quot;but they are different: \&quot;&quot; + arguments[0].getTypeName() + &quot;\&quot; and \&quot;&quot; + arguments[1].getTypeName() + &quot;\&quot;&quot;); } return returnOIResolver.get(); } public Object evaluate(DeferredObject[] arguments) throws HiveException { Object retVal = returnOIResolver.convertIfNecessary(arguments[0].get(), argumentOIs[0]); if (retVal == null) { retVal = returnOIResolver.convertIfNecessary(arguments[1].get(), argumentOIs[1]); } return retVal; } public String getDisplayString(String[] children) { StringBuilder sb = new StringBuilder(); sb.append(&quot;if &quot;); sb.append(children[0]); sb.append(&quot; is null &quot;); sb.append(&quot;returns&quot;); sb.append(children[1]); return sb.toString(); } } 2)添加jar到类路径 ... 3)注册函数 $hive&gt;CREATE TEMPORARY FUNCTION nvl AS &apos;org.apache.hadoop.hive.ql.udf.generic.GenericUDFNvl&apos;; 一些课上用的PPT]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
        <tag>内部表</tag>
        <tag>外部表</tag>
        <tag>托管表</tag>
        <tag>分区</tag>
        <tag>分桶</tag>
        <tag>join</tag>
        <tag>union</tag>
        <tag>export</tag>
        <tag>order</tag>
        <tag>sort</tag>
        <tag>动态分区</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive第一天]]></title>
    <url>%2F2018%2F11%2F06%2FHive%E7%AC%AC%E4%B8%80%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[hive在hadoop处理结构化数据的数据仓库。 不是: 关系数据库 不是OLTP 实时查询和行级更新。 hive特点hive存储数据结构(schema)在数据库中,处理的数据进入hdfs. OLAP HQL / HiveQL hive安装1.下载hive2.1-tar.gz 2.tar开 $&gt;tar -xzvf hive-2.1.0.tar.gz -C /soft //tar开 $&gt;cd /soft/hive-2.1.0 // $&gt;ln -s hive-2.1.0 hive //符号连接 3.配置环境变量 [/etc/profile] HIVE_HOME=/soft/hive PATH=...:$HIVE_HOME/bin 4.验证hive安装成功 $&gt;hive --v 5.配置hive,使用win7的mysql存放hive的元数据. a)复制mysql驱动程序到hive的lib目录下。 ... b)配置hive-site.xml 复制hive-default.xml.template为hive-site.xml 修改连接信息为mysql链接地址，将${system:...字样替换成具体路径。 [hive/conf/hive-site.xml] &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;Username to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://192.168.231.1:3306/hive2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt; &lt;/property&gt; 6)在msyql中创建存放hive信息的数据库 mysql&gt;create database hive2 ; 6)初始化hive的元数据(表结构)到mysql中。 $&gt;cd /soft/hive/bin $&gt;schematool -dbType mysql -initSchema hive命令行操作1.创建hive的数据库$hive&gt;hive --version // $hive&gt;hive --help // $hive&gt;create database mydb2 ; // $hive&gt;show databases ; $hive&gt;use mydb2 ; $hive&gt;create table mydb2.t(id int,name string,age int); $hive&gt;drop table t ; $hive&gt;drop table mydb2.t ; $hive&gt;select * from mydb2.t ; //查看指定库的表 $hive&gt;exit ; //退出 $&gt;hive //hive --service cli $&gt;hive //hive --service cli 2.通过远程jdbc方式连接到hive数据仓库1.启动hiveserver2服务器，监听端口10000 $&gt;hive --service hiveserver2 &amp; 2.通过beeline命令行连接到hiveserver2 $&gt;beeline //进入beeline命令行(于hive --service beeline) $beeline&gt;!help //查看帮助 $beeline&gt;!quit //退出 $beeline&gt;!connect jdbc:hive2://localhost:10000/mydb2//连接到hibve数据 $beeline&gt;show databases ; $beeline&gt;use mydb2 ; $beeline&gt;show tables; //显式表 使用Hive-jdbc驱动程序采用jdbc方式访问远程数据仓库1.创建java模块 2.引入maven 3.添加hive-jdbc依赖 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.it18zhang&lt;/groupId&gt; &lt;artifactId&gt;HiveDemo&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt; &lt;version&gt;2.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; 4.App package com.it18zhang.hivedemo; import java.sql.Connection; import java.sql.DriverManager; import java.sql.ResultSet; import java.sql.Statement; /** * 使用jdbc方式连接到hive数据仓库，数据仓库需要开启hiveserver2服务。 */ public class App { public static void main(String[] args) throws Exception { Class.forName(&quot;org.apache.hive.jdbc.HiveDriver&quot;); Connection conn = DriverManager.getConnection(&quot;jdbc:hive2://192.168.231.201:10000/mydb2&quot;); Statement st = conn.createStatement(); ResultSet rs = st.executeQuery(&quot;select id , name ,age from t&quot;); while(rs.next()){ System.out.println(rs.getInt(1) + &quot;,&quot; + rs.getString(2)) ; } rs.close(); st.close(); conn.close(); } } hive中表1.managed table托管表。 删除表时，数据也删除了。 2.external table外部表。 删除表时，数据不删。 hive命令//创建表,external 外部表 $hive&gt;CREATE external TABLE IF NOT EXISTS t2(id int,name string,age int) COMMENT &apos;xx&apos; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; STORED AS TEXTFILE ; //查看表数据 $hive&gt;desc t2 ; $hive&gt;desc formatted t2 ; //加载数据到hive表 $hive&gt;load data local inpath &apos;/home/centos/customers.txt&apos; into table t2 ; //local上传文件 $hive&gt;load data inpath &apos;/user/centos/customers.txt&apos; [overwrite] into table t2 ; //移动文件 //复制表 mysql&gt;create table tt as select * from users ; //携带数据和表结构 mysql&gt;create table tt like users ; //不带数据，只有表结构 hive&gt;create table tt as select * from users ; hive&gt;create table tt like users ; //count()查询要转成mr $hive&gt;select count(*) from t2 ; $hive&gt;select id,name from t2 ; // $hive&gt;select * from t2 order by id desc ; //MR //启用/禁用表 $hive&gt;ALTER TABLE t2 ENABLE NO_DROP; //不允许删除 $hive&gt;ALTER TABLE t2 DISABLE NO_DROP; //允许删除 分区表优化手段之一，从目录的层面控制搜索数据的范围。 //创建分区表. $hive&gt;CREATE TABLE t3(id int,name string,age int) PARTITIONED BY (Year INT, Month INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ; //显式表的分区信息 $hive&gt;SHOW PARTITIONS t3; //添加分区,创建目录 $hive&gt; alter table t3 add partition (year=2014, month=12) partition (year=2014,month=11); //删除分区 hive&gt;ALTER TABLE employee_partitioned DROP IF EXISTS PARTITION (year=2014, month=11); //分区结构 hive&gt;/user/hive/warehouse/mydb2.db/t3/year=2014/month=11 hive&gt;/user/hive/warehouse/mydb2.db/t3/year=2014/month=12 //加载数据到分区表 hive&gt;load data local inpath &apos;/home/centos/customers.txt&apos; into table t3 partition(year=2014,month=11); //查询分区表 hive&gt;select * from t3 where year = 2014 and month =11; 分区对应的是文件夹是目录，分桶对应的是文件，按照id分桶的意思就是按照id哈希不同的哈希进入不同的桶 //创建桶表 $hive&gt;CREATE TABLE t4(id int,name string,age int) CLUSTERED BY (id) INTO 3 BUCKETS ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ; //加载数据不会进行分桶操作 $hive&gt;load data local inpath &apos;/home/centos/customers.txt&apos; into table t4 ; //查询t3表数据插入到t4中。t3是分区表还多了年份和月份的2个字段所以要投影查询不能直接insert into t4 select * from t3;这是错的 $hive&gt;insert into t4 select id,name,age from t3 ; //桶表的数量如何设置? //评估数据量，保证每个桶的数据量block的2倍大小。 //连接查询 $hive&gt;CREATE TABLE customers(id int,name string,age int) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ; $hive&gt;CREATE TABLE orders(id int,orderno string,price float,cid int) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ; //加载数据到表 //内连接查询 hive&gt;select a.*,b.* from customers a , orders b where a.id = b.cid ; //左外 hive&gt;select a.*,b.* from customers a left outer join orders b on a.id = b.cid ; hive&gt;select a.*,b.* from customers a right outer join orders b on a.id = b.cid ; hive&gt;select a.*,b.* from customers a full outer join orders b on a.id = b.cid ; //explode,炸裂,表生成函数。 //使用hive实现单词统计 //1.建表 $hive&gt;CREATE TABLE doc(line string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop第十一天]]></title>
    <url>%2F2018%2F11%2F01%2FHadoop%E7%AC%AC%E5%8D%81%E4%B8%80%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[数据[customers.txt] 1,tom,12 2,tom,13 3,tom,14 4,tom,15 [orders.txt] 1,no001,12.23,1 2,no001,12.23,1 3,no001,12.23,2 4,no001,12.23,2 5,no001,12.23,2 6,no001,12.23,3 7,no001,12.23,3 8,no001,12.23,3 9,no001,12.23,3 map端join1.创建Mapper package com.it18zhang.hdfs.mr.mapjoin; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.FSDataInputStream; import org.apache.hadoop.fs.FileSystem; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.NullWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Mapper; import java.io.BufferedReader; import java.io.IOException; import java.io.InputStreamReader; import java.util.HashMap; import java.util.Map; /** * join操作，map端连接。 */ public class MapJoinMapper extends Mapper&lt;LongWritable,Text,Text,NullWritable&gt; { private Map&lt;String,String&gt; allCustomers = new HashMap&lt;String,String&gt;(); //启动,初始化客户信息 protected void setup(Context context) throws IOException, InterruptedException { try { Configuration conf = context.getConfiguration(); FileSystem fs = FileSystem.get(conf); FSDataInputStream fis = fs.open(new Path(&quot;file:///d:/mr/mapjoin/customers.txt&quot;)); //得到缓冲区阅读器 BufferedReader br = new BufferedReader(new InputStreamReader(fis)); String line = null ; while((line = br.readLine()) != null){ //得到cid String cid = line.substring(0,line.indexOf(&quot;,&quot;)); allCustomers.put(cid,line); } } catch (Exception e) { e.printStackTrace(); } } protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { //订单信息 String line = value.toString(); //提取customer id String cid = line.substring(line.lastIndexOf(&quot;,&quot;) + 1); //订单信息 String orderInfo = line.substring(0,line.lastIndexOf(&quot;,&quot;)); //连接customer + &quot;,&quot; + order String customerInfo = allCustomers.get(cid); context.write(new Text(customerInfo + &quot;,&quot; + orderInfo),NullWritable.get()); } } 2.创建App package com.it18zhang.hdfs.mr.mapjoin; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.NullWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; /** * */ public class MapJoinApp { public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;); Job job = Job.getInstance(conf); //设置job的各种属性 job.setJobName(&quot;MapJoinApp&quot;); //作业名称 job.setJarByClass(MapJoinApp.class); //搜索类 //添加输入路径 FileInputFormat.addInputPath(job,new Path(args[0])); //设置输出路径 FileOutputFormat.setOutputPath(job,new Path(args[1])); //没有reduce job.setNumReduceTasks(0); job.setMapperClass(MapJoinMapper.class); //mapper类 job.setMapOutputKeyClass(Text.class); // job.setMapOutputValueClass(NullWritable.class); // job.waitForCompletion(true); } } join端连接1.自定义key package com.it18zhang.hdfs.mr.mapjoin.reducejoin; import org.apache.hadoop.io.WritableComparable; import java.io.DataInput; import java.io.DataOutput; import java.io.IOException; /** */ public class ComboKey2 implements WritableComparable&lt;ComboKey2&gt; { //0-customer 1-order private int type ; private int cid ; private int oid ; private String customerInfo = &quot;&quot; ; private String orderInfo = &quot;&quot; ; public int compareTo(ComboKey2 o) { int type0 = o.type ; int cid0= o.cid; int oid0 = o.oid; String customerInfo0 = o.customerInfo; String orderInfo0 = o.orderInfo ; //是否同一个customer的数据 if(cid == cid0){ //同一个客户的两个订单 if(type == type0){ return oid - oid0 ; } //一个Customer + 他的order else{ if(type ==0) return -1 ; else return 1 ; } } //cid不同 else{ return cid - cid0 ; } } public void write(DataOutput out) throws IOException { out.writeInt(type); out.writeInt(cid); out.writeInt(oid); out.writeUTF(customerInfo); out.writeUTF(orderInfo); } public void readFields(DataInput in) throws IOException { this.type = in.readInt(); this.cid = in.readInt(); this.oid = in.readInt(); this.customerInfo = in.readUTF(); this.orderInfo = in.readUTF(); } } 2.自定义分区类 public class CIDPartitioner extends Partitioner&lt;ComboKey2,NullWritable&gt;{ public int getPartition(ComboKey2 key, NullWritable nullWritable, int numPartitions) { return key.getCid() % numPartitions; } } 3.创建Mapper package com.it18zhang.hdfs.mr.mapjoin.reducejoin; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.NullWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.InputSplit; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.lib.input.FileSplit; import java.io.IOException; /** * mapper */ public class ReduceJoinMapper extends Mapper&lt;LongWritable,Text,ComboKey2,NullWritable&gt; { protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { // String line = value.toString() ; //判断是customer还是order FileSplit split = (FileSplit)context.getInputSplit(); String path = split.getPath().toString(); //客户信息 ComboKey2 key2 = new ComboKey2(); if(path.contains(&quot;customers&quot;)){ String cid = line.substring(0,line.indexOf(&quot;,&quot;)); String custInfo = line ; key2.setType(0); key2.setCid(Integer.parseInt(cid)); key2.setCustomerInfo(custInfo); } //order info else{ String cid = line.substring(line.lastIndexOf(&quot;,&quot;) + 1); String oid = line.substring(0, line.indexOf(&quot;,&quot;)); String oinfo = line.substring(0, line.lastIndexOf(&quot;,&quot;)); key2.setType(1); key2.setCid(Integer.parseInt(cid)); key2.setOid(Integer.parseInt(oid)); key2.setOrderInfo(oinfo); } context.write(key2,NullWritable.get()); } } 4.创建Reducer package com.it18zhang.hdfs.mr.mapjoin.reducejoin; import org.apache.hadoop.io.NullWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Reducer; import java.io.IOException; import java.util.Iterator; /** * ReduceJoinReducer,reducer端连接实现。 */ public class ReduceJoinReducer extends Reducer&lt;ComboKey2,NullWritable,Text,NullWritable&gt; { protected void reduce(ComboKey2 key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException { Iterator&lt;NullWritable&gt; it = values.iterator(); it.next(); int type = key.getType(); int cid = key.getCid() ; String cinfo = key.getCustomerInfo() ; while(it.hasNext()){ it.next(); String oinfo = key.getOrderInfo(); context.write(new Text(cinfo + &quot;,&quot; + oinfo),NullWritable.get()); } } } 5.创建排序对比器 package com.it18zhang.hdfs.mr.mapjoin.reducejoin; import com.it18zhang.hdfs.maxtemp.allsort.secondarysort.ComboKey; import org.apache.hadoop.io.WritableComparable; import org.apache.hadoop.io.WritableComparator; /** * 组合Key排序对比器 */ public class ComboKey2Comparator extends WritableComparator { protected ComboKey2Comparator() { super(ComboKey2.class, true); } public int compare(WritableComparable a, WritableComparable b) { ComboKey2 k1 = (ComboKey2) a; ComboKey2 k2 = (ComboKey2) b; return k1.compareTo(k2); } } 6.分组对比器 package com.it18zhang.hdfs.mr.mapjoin.reducejoin; import com.it18zhang.hdfs.maxtemp.allsort.secondarysort.ComboKey; import org.apache.hadoop.io.WritableComparable; import org.apache.hadoop.io.WritableComparator; /** * CID分组对比器 */ public class CIDGroupComparator extends WritableComparator{ protected CIDGroupComparator() { super(ComboKey2.class, true); } public int compare(WritableComparable a, WritableComparable b) { ComboKey2 k1 = (ComboKey2) a; ComboKey2 k2 = (ComboKey2) b; return k1.getCid() - k2.getCid(); } } 7.App package com.it18zhang.hdfs.mr.mapjoin.reducejoin; import com.it18zhang.hdfs.maxtemp.allsort.secondarysort.*; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.NullWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.input.TextInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; /** * */ public class ReduceJoinApp { public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;); Job job = Job.getInstance(conf); //设置job的各种属性 job.setJobName(&quot;ReduceJoinApp&quot;); //作业名称 job.setJarByClass(ReduceJoinApp.class); //搜索类 //添加输入路径 FileInputFormat.addInputPath(job,new Path(&quot;D:\\mr\\reducejoin&quot;)); //设置输出路径 FileOutputFormat.setOutputPath(job,new Path(&quot;D:\\mr\\reducejoin\\out&quot;)); job.setMapperClass(ReduceJoinMapper.class); //mapper类 job.setReducerClass(ReduceJoinReducer.class); //reducer类 //设置Map输出类型 job.setMapOutputKeyClass(ComboKey2.class); // job.setMapOutputValueClass(NullWritable.class); // //设置ReduceOutput类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); // //设置分区类 job.setPartitionerClass(CIDPartitioner.class); //设置分组对比器 job.setGroupingComparatorClass(CIDGroupComparator.class); //设置排序对比器 job.setSortComparatorClass(ComboKey2Comparator.class); job.setNumReduceTasks(2); //reduce个数 job.waitForCompletion(true); } } hive在hadoop处理结构化数据的数据仓库。 不是: 关系数据库 不是OLTP 实时查询和行级更新。 hive特点hive存储数据结构(schema)在数据库中,处理的数据进入hdfs. OLAP HQL / HiveQL hive安装1.下载hive2.1-tar.gz 2.tar开 $&gt;tar -xzvf hive-2.1.0.tar.gz -C /soft //tar开 $&gt;cd /soft/hive-2.1.0 // $&gt;ln -s hive-2.1.0 hive //符号连接 3.配置环境变量 [/etc/profile] HIVE_HOME=/soft/hive PATH=...:$HIVE_HOME/bin 4.验证hive安装成功 $&gt;hive --v 5.配置hive,使用win7的mysql存放hive的元数据. a)复制mysql驱动程序到hive的lib目录下。 ... b)配置hive-site.xml 复制hive-default.xml.template为hive-site.xml 修改连接信息为mysql链接地址，将${system:...字样替换成具体路径。 [hive/conf/hive-site.xml] &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;Username to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://192.168.231.1:3306/hive2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt; &lt;/property&gt; 6)在msyql中创建存放hive信息的数据库 mysql&gt;create database hive2 ; 6)初始化hive的元数据(表结构)到mysql中。 $&gt;cd /soft/hive/bin $&gt;schematool -dbType mysql -initSchema hive命令行操作1.创建hive的数据库 $hive&gt;hive --version // $hive&gt;hive --help // $hive&gt;create database mydb2 ; // $hive&gt;show databases ; $hive&gt;use mydb2 ; $hive&gt;create table mydb2.t(id int,name string,age int); $hive&gt;drop table t ; $hive&gt;drop table mydb2.t ; $hive&gt;select * from mydb2.t ; //查看指定库的表 $hive&gt;exit ; //退出 $&gt;hive //hive --service cli $&gt;hive //hive --service cli 通过远程jdbc方式连接到hive数据仓库1.启动hiveserver2服务器，监听端口10000 $&gt;hive --service hiveserver2 &amp; 2.通过beeline命令行连接到hiveserver2 $&gt;beeline //进入beeline命令行(于hive --service beeline) $beeline&gt;!help //查看帮助 $beeline&gt;!quit //退出 $beeline&gt;!connect jdbc:hive2://localhost:10000/mydb2//连接到hibve数据 $beeline&gt;show databases ; $beeline&gt;use mydb2 ; $beeline&gt;show tables; //显式表 使用Hive-jdbc驱动程序采用jdbc方式访问远程数据仓库1.创建java模块 2.引入maven 3.添加hive-jdbc依赖 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.it18zhang&lt;/groupId&gt; &lt;artifactId&gt;HiveDemo&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt; &lt;version&gt;2.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; 4.App package com.it18zhang.hivedemo; import java.sql.Connection; import java.sql.DriverManager; import java.sql.ResultSet; import java.sql.Statement; /** * 使用jdbc方式连接到hive数据仓库，数据仓库需要开启hiveserver2服务。 */ public class App { public static void main(String[] args) throws Exception { Class.forName(&quot;org.apache.hive.jdbc.HiveDriver&quot;); Connection conn = DriverManager.getConnection(&quot;jdbc:hive2://192.168.231.201:10000/mydb2&quot;); Statement st = conn.createStatement(); ResultSet rs = st.executeQuery(&quot;select id , name ,age from t&quot;); while(rs.next()){ System.out.println(rs.getInt(1) + &quot;,&quot; + rs.getString(2)) ; } rs.close(); st.close(); conn.close(); } } hive中表1.managed table 托管表。 删除表时，数据也删除了。 2.external table 外部表。 删除表时，数据不删。 hive命令//创建表,external 外部表 $hive&gt;CREATE external TABLE IF NOT EXISTS t2(id int,name string,age int) COMMENT &apos;xx&apos; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; STORED AS TEXTFILE ; //查看表数据 $hive&gt;desc t2 ; $hive&gt;desc formatted t2 ; //加载数据到hive表 $hive&gt;load data local inpath &apos;/home/centos/customers.txt&apos; into table t2 ; //local上传文件 $hive&gt;load data inpath &apos;/user/centos/customers.txt&apos; [overwrite] into table t2 ; //移动文件 //复制表 mysql&gt;create table tt as select * from users ; //携带数据和表结构 mysql&gt;create table tt like users ; //不带数据，只有表结构 hive&gt;create table tt as select * from users ; hive&gt;create table tt like users ; //count()查询要转成mr $hive&gt;select count(*) from t2 ; $hive&gt;select id,name from t2 ; // $hive&gt;select * from t2 order by id desc ; //MR //启用/禁用表 $hive&gt;ALTER TABLE t2 ENABLE NO_DROP; //不允许删除 $hive&gt;ALTER TABLE t2 DISABLE NO_DROP; //允许删除 //分区表,优化手段之一，从目录的层面控制搜索数据的范围。 //创建分区表. $hive&gt;CREATE TABLE t3(id int,name string,age int) PARTITIONED BY (Year INT, Month INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ; //显式表的分区信息 $hive&gt;SHOW PARTITIONS t3; //添加分区,创建目录 $hive&gt;alter table t3 add partition (year=2014, month=12); //删除分区 hive&gt;ALTER TABLE employee_partitioned DROP IF EXISTS PARTITION (year=2014, month=11); //分区结构 hive&gt;/user/hive/warehouse/mydb2.db/t3/year=2014/month=11 hive&gt;/user/hive/warehouse/mydb2.db/t3/year=2014/month=12 //加载数据到分区表 hive&gt;load data local inpath &apos;/home/centos/customers.txt&apos; into table t3 partition(year=2014,month=11); //创建桶表 $hive&gt;CREATE TABLE t4(id int,name string,age int) CLUSTERED BY (id) INTO 3 BUCKETS ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ; //加载数据不会进行分桶操作 $hive&gt;load data local inpath &apos;/home/centos/customers.txt&apos; into table t4 ; //查询t3表数据插入到t4中。 $hive&gt;insert into t4 select id,name,age from t3 ; //桶表的数量如何设置? //评估数据量，保证每个桶的数据量block的2倍大小。 //连接查询 $hive&gt;CREATE TABLE customers(id int,name string,age int) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ; $hive&gt;CREATE TABLE orders(id int,orderno string,price float,cid int) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ; //加载数据到表 //内连接查询 hive&gt;select a.*,b.* from customers a , orders b where a.id = b.cid ; //左外 hive&gt;select a.*,b.* from customers a left outer join orders b on a.id = b.cid ; hive&gt;select a.*,b.* from customers a right outer join orders b on a.id = b.cid ; hive&gt;select a.*,b.* from customers a full outer join orders b on a.id = b.cid ; //explode,炸裂,表生成函数。 //使用hive实现单词统计 //1.建表 $hive&gt;CREATE TABLE doc(line string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>join</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop第十天]]></title>
    <url>%2F2018%2F10%2F28%2FHadoop%E7%AC%AC%E5%8D%81%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[HA3个JournalNode,edit log 两个NN，active | standby 2NN ha的管理命令hdfs haadmin -getServiceState nn1 //查看服务 hdfs haadmin -transitionToActive nn1 //激活 hdfs haadmin -transitionToStandby nn2 //待命 hdfs haadmin -failover nn1 nn2 //对调 数据仓库OLAP //online analyze process,在线分析处理 //延迟性高. 在于后期海量数据的查询统计hive是延迟性比较高的操作实时性不好都是批量计算 数据库OLTP //online transaction process在线事务处理. //实时性好。延迟性很低 mysql针对事务性处理保证ACI特性。 HA3个JournalNode,edit log 两个NN，active | standby 2NN ha的管理命令hdfs haadmin -getServiceState nn1 //查看服务 hdfs haadmin -transitionToActive nn1 //激活 hdfs haadmin -transitionToStandby nn2 //待命 hdfs haadmin -failover nn1 nn2 //对调 数据仓库OLAP //online analyze process,在线分析处理 //延迟性高. 数据库OLTP //online transaction process在线事务处理. //实时性好。 jdbcjava database connection,java数据库连接。 java数据库连接设计套接字编程吗：面向接口编程，依托mysql驱动程序和mysql操作， jdbc就是socket，musql就是serversocket。 0 1.创建mysql数据库和表 create table users(id int primary key auto_increment , name varchar(20) , age int); 2.idea中创建jdbcDemo模块 事务:transaction,和数据库之间的一组操作。 特点. a //atomic,原子性,不可分割. c //consistent,不能破坏掉 i //isolate,隔离型. d //durable.永久性 truncate截断表，类似于delete操作，速度快，数据无法回滚。在Mysql里面操作 truncate table users ; sql语句1. 2. 3. Transactioncommit //提交 rollback //回滚 savePoint //保存点 插入10万条数据不用预处理语句import org.junit.Test; import java.sql.Connection; import java.sql.DriverManager; import java.sql.Statement; //测试增删改查基本功能 public class TestCRUD { @Test public void testStatement() throws Exception{ long start=System.currentTimeMillis(); //创建连接 String diverClass = &quot;com.mysql.jdbc.Driver&quot;; String url = &quot;jdbc:mysql://localhost:3306/big4&quot;; String username = &quot;root&quot;; String password = &quot;root&quot;; Class.forName(diverClass); Connection conn = DriverManager.getConnection(url, username, password); //关闭自动提交 conn.setAutoCommit(false); //创建语句对象 Statement st = conn.createStatement(); for(int i=0;i&lt;100000;i++){ String sql = &quot;insert into users(name,age) values(&apos;tomas&quot; + i + &quot;&apos;,&quot; + (i % 100) + &quot;)&quot;; st.execute(sql); } conn.commit(); st.close(); conn.close(); System.out.println(System.currentTimeMillis() - start); } } 使用预处理语句：import org.junit.Test; import java.sql.Connection; import java.sql.DriverManager; import java.sql.PreparedStatement; //测试增删改查基本功能 public class TestCRUD { @Test public void testPreparedStatement() throws Exception{ long start=System.currentTimeMillis(); //创建连接 String diverClass = &quot;com.mysql.jdbc.Driver&quot;; String url = &quot;jdbc:mysql://localhost:3306/big4&quot;; String username = &quot;root&quot;; String password = &quot;root&quot;; Class.forName(diverClass); Connection conn = DriverManager.getConnection(url, username, password); //关闭自动提交 conn.setAutoCommit(false); //创建语句对象 String sql = &quot;insert into users(name,age) value(?,?)&quot;; PreparedStatement ppst = conn.prepareStatement(sql); for(int i =0;i &lt; 10000;i++){ ppst.setString(1,&quot;tom&quot;+i); ppst.setInt(2,i%100); ppst.executeUpdate();//执行更新 } conn.commit(); ppst.close(); conn.close(); System.out.println(System.currentTimeMillis() - start); } } 上述代码加一个批处理：import org.junit.Test; import java.sql.Connection; import java.sql.DriverManager; import java.sql.PreparedStatement; //测试增删改查基本功能 public class TestCRUD { @Test public void testPreparedStatement() throws Exception { long start = System.currentTimeMillis(); //创建连接 String diverClass = &quot;com.mysql.jdbc.Driver&quot;; String url = &quot;jdbc:mysql://localhost:3306/big4&quot;; String username = &quot;root&quot;; String password = &quot;root&quot;; Class.forName(diverClass); Connection conn = DriverManager.getConnection(url, username, password); //关闭自动提交 conn.setAutoCommit(false); //创建语句对象 String sql = &quot;insert into users(name,age) value(?,?)&quot;; PreparedStatement ppst = conn.prepareStatement(sql); for (int i = 0; i &lt; 10000; i++) { ppst.setString(1, &quot;tom&quot; + i); ppst.setInt(2, i % 100); ppst.addBatch();//这个地方相当于一个缓冲区，相当于缓冲区里面放了2000个在传出去，减少了网络带宽速度会变快 if (i % 200 == 0) { ppst.executeUpdate();//执行更新 } } ppst.addBatch();//最后不够2000在进行一个批处理 conn.commit(); ppst.close(); conn.close(); System.out.println(System.currentTimeMillis() - start); } } 这边的一个通过自回环传输一百万次插入，传输量巨大。但是要是直接在Mysql端直接给个命令做一百万次插入，就很快。不用和服务器端交互很多的数据量。通过存储过程来写，存储过程就是一组sql语句。这些语句已经预先在数据库中存放了。无非就是循环了。要执行这些语句，就要发一条执行存储过程的指令，服务器已收到命令，就直接执行。 速度很快的。 100000条数据通过普通事务，预处理，批处理时间Statement //46698 PreparedStatent //43338 CallableStatement //14385 mysql存储过程msyql&gt;– 定义新的终止符,*不要带空格这个是注释*mysql&gt;delimiter // mysql&gt;– 创建存储过程mysql&gt;CREATE PROCEDURE simpleproc (OUT param1 INT) BEGIN SELECT COUNT(*) INTO param1 FROM users; – into 是赋值方式之一 END // mysql&gt;– 查看存储过程的状态mysql&gt;show procedure status // mysql&gt;– 查看指定存储过程创建语句mysql&gt;show create procedure simpleproc ; mysql&gt;– 调用存储过程,@a在命令中定义变量mysql&gt;call simpleproc(@a) mysql&gt;– 删除存储过程mysql&gt;show drop procedure simpleproc ; mysql&gt;– 定义加法存储过程,set赋值语句 :=mysql&gt;create procedure sp_add(in a int,in b int, out c int) begin set c := a + b ; end // java访问存储过程（调用的是上一步c=a+b这个过程）import org.junit.Test; import java.sql.*; /** * 测试基本操作 */ public class TestCRUD { /** * 存储过程 */ @Test public void testCallableStatement() throws Exception { long start = System.currentTimeMillis(); //创建连接 String driverClass = &quot;com.mysql.jdbc.Driver&quot;; String url = &quot;jdbc:mysql://localhost:3306/big4&quot;; String username = &quot;root&quot;; String password = &quot;root&quot;; Class.forName(driverClass); Connection conn = DriverManager.getConnection(url, username, password); //关闭自动提交 conn.setAutoCommit(false); //创建可调用语句，调用存储过程 CallableStatement cst = conn.prepareCall(&quot;{call sp_add(?,?,?)}&quot;); cst.setInt(1,2); //绑定参数 cst.setInt(2,3); //注册输出参数类型 cst.registerOutParameter(3,Types.INTEGER); cst.execute(); int sum = cst.getInt(3); System.out.println(sum); conn.commit(); conn.close(); System.out.println(System.currentTimeMillis() - start); } } 百万数据插入，存储过程的性能1.创建存储过程 mysql&gt;create procedure sp_batchinsert(in n int) begin DECLARE name0 varchar(20); -- 定义在begin内部 DECLARE age0 int; DECLARE i int default 0 ; while i &lt; n do set name0 := concat(&apos;tom&apos;,i) ; set age0 := i % 100 ; insert into users(name,age) values(name0,age0); set i := i + 1 ; end while ; end // 2.java代码 @Test public void testCallableStatement() throws Exception { long start = System.currentTimeMillis(); //创建连接 String driverClass = &quot;com.mysql.jdbc.Driver&quot;; String url = &quot;jdbc:mysql://localhost:3306/big4&quot;; String username = &quot;root&quot;; String password = &quot;root&quot;; Class.forName(driverClass); Connection conn = DriverManager.getConnection(url, username, password); //关闭自动提交 conn.setAutoCommit(false); //创建可调用语句，调用存储过程 CallableStatement cst = conn.prepareCall(&quot;{call sp_batchinsert(?)}&quot;); cst.setInt(1,1000000); //绑定参数 //注册输出参数类型 cst.execute(); conn.commit(); conn.close(); System.out.println(System.currentTimeMillis() - start); } mysql函数这个是创建函数的SQL参考手册： 1.函数和存储过程相似，只是多了返回值声明. 2.创建函数 mysql&gt;create function sf_add(a int ,b int) returns int begin return a + b ; end // 3.显式创建的函数 mysql&gt;show function status -- mysql&gt;show function status like &apos;%add%&apos; -- mysql&gt;select sf_add(1,2) -- 4.java调用函数 @Test public void testFunction() throws Exception { long start = System.currentTimeMillis(); //创建连接 String driverClass = &quot;com.mysql.jdbc.Driver&quot;; String url = &quot;jdbc:mysql://localhost:3306/big4&quot;; String username = &quot;root&quot;; String password = &quot;root&quot;; Class.forName(driverClass); Connection conn = DriverManager.getConnection(url, username, password); //关闭自动提交 conn.setAutoCommit(false); //创建可调用语句，调用存储过程 CallableStatement cst = conn.prepareCall(&quot;{? = call sf_add(?,?)}&quot;); cst.setInt(2,100);//设置第二个参数是100第三个参数是200，第一个参数是输出。 cst.setInt(3,200); cst.registerOutParameter(1,Types.INTEGER); //注册输出参数类型 cst.execute(); System.out.println(cst.getInt(1)); conn.commit(); conn.close(); System.out.println(System.currentTimeMillis() - start); } MVCCmultiple version concurrent control,多版本并发控制。 数据库的记录存储有多条，每一条都附加版本。所以可以实现隔离级别。是因为有MVCC多版本并发控制 事务的并发执行，容易出现的几个现象1.脏读 读未提交,一个事务读取了另外一个事务改写还没有提交的数据，如果另外一个 事务在稍后的时候回滚。 2.不可重复读 一个事务进行相同条件查询连续的两次或者两次以上，每次结果都不同。这个是对数据进行了更改 有其他事务做了update操作。 3.幻读 和(2)很像，其他事务做了insert操作.1时间查询到了10个，2时刻查询到了20个，是因为中间有做了插入操作 隔离级别（由于有MVCC）为了避免出现哪种并发现象的。 1 //read uncommitted ,读未提交 导致脏读导致不可重复读 2 //read committed ,读已提交 这里避免了脏读 4 //repeatable read ,可以重复读 这里避免了不可重复读。还是可以让他插入更新，但是读取的是更新之前的值 8 //serializable ,串行化(悲观锁) 这里避免了换读，串行化不支持并发了就已经。 演示mysql事务隔离级别1.开启mysql客户端 mysql&gt; 2.关闭自动提交 mysql&gt;set autocommit 0 ; 3.每次操作数据,都要开启事务，提交事务。 脏读现象[A] 1)mysql&gt;start transaction ; -- 开始事务 2)msyql&gt;update users set age = age + 1 where id = 1 ; -- 更新数据,没有提交 6)mysql&gt;rollback ; -- 回滚 7)mysql&gt;select * from users ; [B] 3)mysql&gt;set session transaction isolation level read uncommitted ; -- 读未提交 4)msyql&gt;start transaction ; -- 开始事务 5)mysql&gt;select * from users ; -- 13 避免脏读[A] 1)mysql&gt;start transaction ; -- 开始事务 2)msyql&gt;update users set age = age + 1 where id = 1 ; -- 更新数据,没有提交 6)mysql&gt;rollback ; -- 回滚 7)mysql&gt;select * from users ; [B] 3)mysql&gt;set session transaction isolation level read committed ; -- 读已提交 4)msyql&gt;start transaction ; -- 开始事务 5)mysql&gt;select * from users ; -- 13 测试不可重复读(隔离级别设置为读已提交不能避免不可重复读。)[A] 1)mysql&gt;commit ; 2)mysql&gt;set session transaction isolation level read committed ; -- 读已提交 3)mysql&gt;start transaction ; -- 开始事务 4)mysql&gt;select * from users ; -- 查询 9)mysql&gt;select * from users ; [B] 5)mysql&gt;commit; 6)mysql&gt;start transaction ; 7)mysql&gt;update users set age = 15 where id = 1 ; -- 更新 8)mysql&gt;commit; 测试避免不可重复读(隔离级别设置为读已提交不能避免不可重复读。)[A] 1)mysql&gt;commit ; 2)mysql&gt;set session transaction isolation level repeatable read ; -- 可以重复读 3)mysql&gt;start transaction ; -- 开始事务 4)mysql&gt;select * from users ; -- 查询 9)mysql&gt;select * from users ; [B] 5)mysql&gt;commit; 6)mysql&gt;start transaction ; 7)mysql&gt;update users set age = 15 where id = 1 ; -- 更新 8)mysql&gt;commit; 测试幻读(隔离级别设置为repeatable)[A] 1)mysql&gt;commit ; 2)mysql&gt;set session transaction isolation level serializable; -- 串行化 3)mysql&gt;start transaction ; -- 开始事务 4)mysql&gt;select * from users ; -- 查询 9)mysql&gt;select * from users ; [B] 5)mysql&gt;commit; 6)mysql&gt;start transaction ; 7)mysql&gt;insert into users(name,age) values(&apos;tomas&apos;,13); -- 更新 8)mysql&gt;commit; ANSI SQL美国国家标准结构SQL组 select * from users for update ; MySQL1.支持四种隔离级别。 2.默认隔离级别是可以重复读。 3.隔离级别是seriable,不支持并发写。 表级锁LOCK TABLE t WRITE; -- 加锁(表级锁,read) UNLOCK TABLES ; -- 解除自己所有的所有表级锁 表级锁只能通过命令来解锁。 编程实现脏读现象package com.it18zhang.jdbcdemo.test; import org.junit.Test; import java.sql.*; /** * 测试隔离级别 */ public class TestIsolationLevel { /** * 执行写，不提交 */ @Test public void testA() throws Exception{ //创建连接 String driverClass = &quot;com.mysql.jdbc.Driver&quot;; String url = &quot;jdbc:mysql://localhost:3306/big4&quot;; String username = &quot;root&quot;; String password = &quot;root&quot;; Class.forName(driverClass); Connection conn = DriverManager.getConnection(url, username, password); conn.setAutoCommit(false); Statement st = conn.createStatement(); st.execute(&quot;update users set age = 80 where id = 1&quot;); System.out.println(&quot;===============&quot;); conn.commit(); conn.close(); } /** * 查询，查到别人没有提交的数据 */ @Test public void testB() throws Exception{ //创建连接 String driverClass = &quot;com.mysql.jdbc.Driver&quot;; String url = &quot;jdbc:mysql://localhost:3306/big4&quot;; String username = &quot;root&quot;; String password = &quot;root&quot;; Class.forName(driverClass); Connection conn = DriverManager.getConnection(url, username, password); //设置隔离级别读未提交==&gt;导致脏读 /************************** 设置隔离级别 ***************************************/ conn.setTransactionIsolation(Connection.TRANSACTION_READ_UNCOMMITTED); conn.setAutoCommit(false); Statement st = conn.createStatement(); ResultSet rs = st.executeQuery(&quot;select age from users where id = 1&quot;); rs.next(); int age = rs.getInt(1) ; System.out.println(age); System.out.println(&quot;===============&quot;); conn.commit(); conn.close(); } 共享读锁独占写锁一个事务写操作，另一个塞住。行级别的锁 在独占锁这块，开启事务之后，资源存在征用的情况，如果A修改&gt;=2，b修改&lt;=3那么就出现先来先用的情况，没有提交事务之前，B一直要等待状态。然后事务是you原子性的，肯定就是非交集和交集的部分都不能更改。因为原子性。查询是由隔离级别控制。在查询的时候为了不让别人更新设置隔离级别为最高级别，就是8串行化，但是很难受，连接不关掉放到连接池里面，被下一个调用，还是串行化状态，不能写入，性能差。除了设置隔离级别，我们还可以加上一个select * from users for update。这样的话就代表仅仅是对这一个查询语句的时候执行独占写锁。这个时候不需要再设置隔离级别为8了。 SQL// insert into users(name,age,...) values(&apos;&apos;,12,..) ; -- insert update users set name = &apos;xxx&apos;,age = xxx ,... where id = xxx ; -- update delete from users where id = xxx -- delete -- 投影查询 projection. select id,name from users where ... order by limit xxx --select -- 查询时直接上独占写锁 select * from users for update ; 连接查询1.准备表[mysql.sql] drop table if exists customers; -- 删除表 drop table if exists orders ; -- 删除表 create table customers(id int primary key auto_increment , name varchar(20) , age int); -- 创建customers表 create table orders(id int primary key auto_increment , orderno varchar(20) , price float , cid int); -- 创建orders表 -- 插入数据 insert into customers(name,age) values(&apos;tom&apos;,12); insert into customers(name,age) values(&apos;tomas&apos;,13); insert into customers(name,age) values(&apos;tomasLee&apos;,14); insert into customers(name,age) values(&apos;tomason&apos;,15); -- 插入订单数据 insert into orders(orderno,price,cid) values(&apos;No001&apos;,12.25,1); insert into orders(orderno,price,cid) values(&apos;No002&apos;,12.30,1); insert into orders(orderno,price,cid) values(&apos;No003&apos;,12.25,2); insert into orders(orderno,price,cid) values(&apos;No004&apos;,12.25,2); insert into orders(orderno,price,cid) values(&apos;No005&apos;,12.25,2); insert into orders(orderno,price,cid) values(&apos;No006&apos;,12.25,3); insert into orders(orderno,price,cid) values(&apos;No007&apos;,12.25,3); insert into orders(orderno,price,cid) values(&apos;No008&apos;,12.25,3); insert into orders(orderno,price,cid) values(&apos;No009&apos;,12.25,3); insert into orders(orderno,price,cid) values(&apos;No0010&apos;,12.25,NULL); ---执行SQL文件 source d:/SQL/mysql.sql 2.查询--连接查询 mysql&gt;-- 笛卡尔积查询,无连接条件查询 mysql&gt;select a.*,b.* from customers a , orders b ; mysql&gt;-- 内连接,查询符合条件的记录. mysql&gt;select a.*,b.* from customers a , orders b where a.id = b.cid ; mysql&gt;-- 左外连接,查询符合条件的记录. mysql&gt;select a.*,b.* from customers a left outer join orders b on a.id = b.cid ; mysql&gt;-- 右外连接,查询符合条件的记录. mysql&gt;select a.*,b.* from customers a right outer join orders b on a.id = b.cid ; mysql&gt;-- 全外连接,查询符合条件的记录(mysql不支持全外链接) mysql&gt;select a.*,b.* from customers a full outer join orders b on a.id = b.cid ; 2.查询–分组 字段列表 表 条件 分组 组内条件 排序 分页 mysql&gt;select ... from ... where ... group by ... having ... order by ... limit .. mysql&gt;-- 去重查询 mysql&gt;select distinct price,cid from orders ; mysql&gt;-- 条件查询 mysql&gt;select price,cid from orders where price &gt; 12.27 ; mysql&gt;-- 聚集查询 mysql&gt;select max(price) from orders ; mysql&gt;select min(price) from orders ; mysql&gt;select avg(price) from orders ; mysql&gt;select sum(price) from orders ; mysql&gt;select count(id) from orders ; mysql&gt;-- 分组查询 mysql&gt;select max(price) from orders where cid is not null group by cid ; mysql&gt;-- 分组查询(组内过滤) mysql&gt;select cid ,orderno,max(price) as max_price,min(price) from orders where cid is not null group by cid having max_price &gt; 20 ; mysql&gt;-- 降序查询 mysql&gt;select cid ,orderno,max(price) as max_price,min(price) from orders where cid is not null group by cid having max_price &gt; 20 order by max_price desc; mysql&gt;-- 模糊查询 mysql&gt;select * from customers where name like &apos;toma%&apos; mysql&gt;select * from customers where name not like &apos;toma%&apos; mysql&gt;-- 范围查询 mysql&gt;select * from customers where id in (1,2,3) mysql&gt;select * from customers where id not in (1,2,3) mysql&gt;-- between 1 and 10,闭区间 mysql&gt;select * from customers where id between 1 and 3 ; mysql&gt;select * from customers where id &gt;= 1 and id &lt;= 3 ; mysql&gt;-- 嵌套子查询(查询没有订单的客户) mysql&gt;select * from customers where id not in (select distinct cid from orders where cid is not null); mysql&gt;-- 嵌套子查询(查询订单数量&gt;2的客户) mysql&gt;select * from customers where id in (select cid from orders group by cid having count(cid) &gt; 2); mysql&gt;select * from customers where id in ( select t.cid from (select cid,count(*) as c from orders group by cid having c &gt; 2) as t); mysql&gt;--向已有表中添加列 mysql&gt;--alter table orders add column area int; mysql&gt;--设置area字段的值 mysql&gt;--update orders set area = 2 where id in(1,3,,6,7); mysql&gt;--设置area字段的值 mysql&gt;--update orders set area = 1 where id not in(1,3,,6,7); mysql&gt;--设置area字段的值 mysql&gt;--update orders set area = 1 where id not in(1,3,,6,7); 稍微看一下下面这个SQL语句： mysq&gt;--select cid,area,count(price) from orders group by cid,area;//这句话的意思是对cid和area联合分组，什么意思：就是说要cid和area要都相同才能进入到同一个组里面去。 mysq&gt;--select cid,area,count(price),max(price),min(price) from orders group by cid,area order by cid asc, area desc ;//这句话的意思是对cid和area联合分组，什么意思：就是说要cid和area要都相同才能进入到同一个组里面去。然后这个cid升序area降序是在1里面对area降序，也就是说先对cid整体升序，然后对升序完之后相同的cid里面进行降序。 mysql&gt;-- 嵌套子查询(查询客户id,客户name,订单数量,最贵订单，最便宜订单，平均订单价格 where 订单数量&gt;2的客户) mysql&gt;select a.id,a.name,b.c,b.max,b.min,b.avg from customers a,((select cid,count(cid) c , max(price) max ,min(price) min,avg(price) avg from orders group by cid having c &gt; 2) as b) where a.id = b.cid ; hadoopMR 左外连接.]]></content>
      <categories>
        <category>Hadoop</category>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>jdbc加强</tag>
        <tag>事务操作</tag>
        <tag>批处理</tag>
        <tag>预处理</tag>
        <tag>自定义mysql存储过程</tag>
        <tag>mysql函数</tag>
        <tag>百万数据插入</tag>
        <tag>HA管理命令</tag>
        <tag>数据仓库</tag>
        <tag>数据库</tag>
        <tag>mysql事务隔离级别</tag>
        <tag>独占写锁</tag>
        <tag>共享读锁</tag>
        <tag>脏读</tag>
        <tag>不可重复读</tag>
        <tag>幻读</tag>
        <tag>串行化</tag>
        <tag>行级锁</tag>
        <tag>连接查询</tag>
        <tag>模糊查询</tag>
        <tag>外连接查询</tag>
        <tag>笛卡尔积查询</tag>
        <tag>分组查询</tag>
        <tag>嵌套子查询</tag>
        <tag>条件查询</tag>
        <tag>去重查询</tag>
        <tag>范围查询</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop第九天]]></title>
    <url>%2F2018%2F10%2F25%2FHadoop%E7%AC%AC%E4%B9%9D%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[复习：1.链式job编程 MR //Mapper+ / Reduce Mapper* 2.DBWritable 和数据库交互。 3.Sqoop 4.全排序 对reduce输出的所有结果进行排序。 5.二次排序 对value进行排序。 6.数据倾斜 1.reduce 2.自定义分区函数 数据结果错 + 二次job 3.重新设计key 数据结果错 + 二次job 机架感知机架有一块网卡。可以连接很多主机，机架上每个刀片服务器主机都连接到这个机架网卡上的。 比如一台刀片服务器上有2个进程，2个进程之间通信需要走机架上的交换机吗。不需要直接走自回环网络即可。走Localhost即可。所以2个进程之间不需要网卡交互数据。2个进程之间的距离就是0。每个进程所在的主机节点，连接他们之间的网络通路的跃点数就是他们之间的距离 如果b中绿色进程和a中红色进程需要交互就需要通过交换机，距离就是2.怎么算的，就是从每个服务器到交换机之间的距离相加，就是1+1； 如果是2个机架也要通过一个交换机连接起来。左边机架a中绿色和右边机架红色通信。距离就是4. 同一个机房通信最多就是4。也就是在通过一个交换机。 Hadoop都有机架感知能力：是他里面的一种策略，都需人为维护。在默认情况下所有节点都在同一个机架上。/default-rack。但是在临近节点数据量非常大的时候，也是不会使用的，就近原则只是一种策略，还有其他策略。 fault tolerance容错. 针对业务。 map或reduce任务失败，的这种错误。 fail over容灾. 针对硬件故障。 master / slave主(master,namenode)从(slave,datanode)结构. topology.node.switch.mapping.impl 客户端请求Namenode来读取datanodes的过程Namenode只存放文件系统的数据信息，而不存放各个节点的IP地址。存放块ID，块列表。 可靠性提供数据安全的能力。 可用性提供持续服务的能力。 默认的副本放置策略首选在本地机架的一个node存放副本,另一个副本在本地机架的另一个不同节点。 最后一个副本在不同机架的不同节点上。 hads oiv //image data metadata. 离线镜像查看器hads oev //edit 编辑日志 镜像文件存放的是元信息，元信息包括块，块ID，块列表，是不是上下级关系，副本数，权限，块大小。除了数据内容本身内容之外的。 通过实现接口改变配置实现一个机架感知。 自定义机架感知(优化hadoop集群一种方式)1.自定义实现类 package com.it18zhang.hdfs.rackaware; import org.apache.hadoop.net.DNSToSwitchMapping; import java.io.FileWriter; import java.io.IOException; import java.util.ArrayList; import java.util.List; /*机架感知实现类 吧203以下的机器设置为机架1，吧203以上的机架设置为机架2 */ public class MyRackAware implements DNSToSwitchMapping { public List&lt;String&gt; resolve(List&lt;String&gt; names) { ArrayList&lt;String&gt; list = new ArrayList&lt;String&gt;(); //true表示是不是追加模式 FileWriter fw = null; try { fw = new FileWriter(&quot;/home/centos/rackaware.txt&quot;, true); for (String str : names) { fw.write(str + &quot;\r\n&quot;); if (str.startsWith(&quot;192&quot;)) { //192.168.192.202 String ip = str.substring(str.lastIndexOf(&quot;.&quot;) + 1); if (Integer.parseInt(ip) &lt;= 203) { list.add(&quot;/rack1/&quot; + ip); } else { list.add(&quot;/rack2&quot; + ip); } } else if (str.startsWith(&quot;s&quot;)) { String ip = str.substring(1); if (Integer.parseInt(ip) &lt;= 203) { list.add(&quot;/rack1/&quot; + ip); } else { list.add(&quot;/rack2&quot; + ip); } } } } catch (IOException e) { e.printStackTrace(); } return list; } public void reloadCachedMappings() { } public void reloadCachedMappings(List&lt;String&gt; names) { } } 2.配置core-site.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://192.168.231.201/&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/centos/hadoop&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;topology.node.switch.mapping.impl&lt;/name&gt; &lt;value&gt;com.it18zhang.hdfs.rackaware.MyRackAware&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 3.导出jar包 4.复制jar到/soft/hadoop/shared/hadoop/common/lib（这个是hadoop的类路径） 5.分发jar.(可以不做) 实际上不需要分发，只在名称节点上运行。 6.重启名称节点 $&gt;hadoop-daemon.sh stop namenode $&gt;hadoop-daemon.sh start namenode 在s202上传一个文件，最后得出来确实和副本存放策略一致： 首选在本地机架的一个node存放副本,另一个副本在本地机架的另一个不同节点。 最后一个副本在不同机架的不同节点上。 关于HDFS下面这一段是关于HDFS的讲解，HDFS包括namenode和datanode，名称节点和数据节点。名称节点只是保存名称信息。那么如果在HDFS中文件移动的话怎么办，就是名称节点改变目录来移动即可。1kb和1G的速度是一样的。在HDFS中移动只是改变路径而已，块根本就没变，只能重新删掉在put才是真正移动了块。这一节就是通过实验验证了这个问题。 去IOEIBM // Oracle // EMC // HA1.NFS 网络共享存储设备。 2.QJM Quorum Journal Manager 3.两个名称节点 active //激活 standby //待命 active //激活deactive //钝化 SPOFsingle point of failure,单点故障。 事务是个特性a //atomic 原子性 c //consistent一致性 i //isolate 隔离型 d //durable 永久性· majority 大部分. HA高可用配置high availability,高可用. /home/centos/hadoop/dfs/data/current/BP-2100834435-192.168.231.201-1489328949370/current/finalized/subdir0/subdir0 两个名称节点，一个active(激活态)，一个是standby(slave待命),slave节点维护足够多状态以便于容灾。 和客户端交互的active节点,standby不交互. 两个节点都和JN守护进程构成组的进行通信。 数据节点配置两个名称节点，分别报告各自的信息。 同一时刻只能有一个激活态名称节点。 脑裂:两个节点都是激活态。 为防止脑裂，JNs只允许同一时刻只有一个节点向其写数据。容灾发生时，成为active节点的namenode接管 向jn的写入工作。 硬件资源名称节点: 硬件配置相同。 JN节点 : 轻量级进程，至少3个节点,允许挂掉的节点数 (n - 1) / 2. 不需要再运行辅助名称节点。 部署配置细节0.s201和s206具有完全一致的配置，尤其是ssh. 1.配置nameservice [hdfs-site.xml] &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;mycluster&lt;/value&gt; &lt;/property&gt; 2.dfs.ha.namenodes.[nameservice ID] [hdfs-site.xml] &lt;!-- myucluster下的名称节点两个id --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; 3.dfs.namenode.rpc-address.[nameservice ID].[name node ID] [hdfs-site.xml] 配置每个nn的rpc地址。 &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;s201:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;s206:8020&lt;/value&gt; &lt;/property&gt; 4.dfs.namenode.http-address.[nameservice ID].[name node ID] 配置webui端口 [hdfs-site.xml] &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;s201:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;s206:50070&lt;/value&gt; &lt;/property&gt; 5.dfs.namenode.shared.edits.dir 名称节点共享编辑目录. [hdfs-site.xml] &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://s202:8485;s203:8485;s204:8485/mycluster&lt;/value&gt; &lt;/property&gt; 6.dfs.client.failover.proxy.provider.[nameservice ID] java类，client使用它判断哪个节点是激活态。 [hdfs-site.xml] &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; 7.dfs.ha.fencing.methods 脚本列表或者java类，在容灾保护激活态的nn. [hdfs-site.xml] &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt; sshfence shell(/bin/true) &lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/home/centos/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt; 8.fs.defaultFS 配置hdfs文件系统名称服务。 [core-site.xml] &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://mycluster&lt;/value&gt; &lt;/property&gt; 9.dfs.journalnode.edits.dir 配置JN存放edit的本地路径。 [hdfs-site.xml] &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/home/centos/hadoop/journal&lt;/value&gt; &lt;/property&gt; 部署细节1.在jn节点分别启动jn进程 $&gt;hadoop-daemon.sh start journalnode 2.启动jn之后，在两个NN之间进行disk元数据同步 a)如果是全新集群，先format文件系统,只需要在一个nn上执行。 [s201] $&gt;hadoop namenode -format 格式化文件系统主要就是生成一个新的VERSION，里面内容就是生成一个名称空间ID集群ID时间块池ID b)如果将非HA集群转换成HA集群，复制原NN的metadata到另一个nn. 1.步骤一 [s201] $&gt;scp -r /home/centos/hadoop/dfs centos@s206:/home/centos/hadoop/ 2.步骤二 在新的nn(未格式化的nn)上运行一下命令，实现待命状态引导。 [s206] $&gt;hdfs namenode -bootstrapStandby //需要s201为启动状态,提示是否格式化,选择N. 3)在一个NN上执行以下命令，完成edit日志到jn节点的传输。 $&gt;hdfs namenode -initializeSharedEdits #查看s202,s203是否有edit数据. 4)启动所有节点. [s201] $&gt;hadoop-daemon.sh start namenode //启动名称节点 $&gt;hadoop-daemons.sh start datanode //启动所有数据节点 [s206] $&gt;hadoop-daemon.sh start namenode //启动名称节点 HA管理$&gt;hdfs haadmin -transitionToActive nn1 //切成激活态 $&gt;hdfs haadmin -transitionToStandby nn1 //切成待命态 $&gt;hdfs haadmin -transitionToActive --forceactive nn2//强行激活 $&gt;hdfs haadmin -failover nn1 nn2 //模拟容灾演示,从nn1切换到nn2]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>机架感知</tag>
        <tag>机架感知实现</tag>
        <tag>HA</tag>
        <tag>手动移动数据块</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop第八天]]></title>
    <url>%2F2018%2F10%2F22%2FHadoop%E7%AC%AC%E5%85%AB%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[二次排序链条化分组不能解决数据倾斜问题，已经进入到了同一个reduce对象里面了，分一个组出来就进入同一个reduce方法里面。每次分组只是决定调用不调用同一个reduce方法。已经晚了，数据已经进入到了同一个reduce了，分区可以决定数据倾斜问题。 reduce分为方法和对象，有的时候说reduce是指的是方法，有的时候指的是对象，要分开对待。 单词统计做统计肯定不能哈希分区，因为哈希同一个值进入到同一个reduce对象利民区去了 数据倾斜问题 随机分区 二次MR1. 2. 3. 4. 如果正常按照wordcount来处理会分为 reduce不设置就是1，map没机会设置数量。但是也可以设置map，但是意义，以为mr提交的时候map的个数取决于切片的个数。 切片的计算公式：min block、maxsplit、 blocksize取中间值等于blocksize。 现在文件如下，现在3个文件每个文件有一百万数据且有一百万个hello就对应了3个map。（这里没明白为什么3个切片也就是为啥3个map） 每个切片里，分区个数等于reduce个数，分区的个数取决于reduce个数，之前自己已经在WCAPP里面设置好了reduce个数。 我有3个map，4个reduce。那么每个map里面就有4个分区了。 要解决哈希分区带来的数据倾斜问题，可以通过随机分区，然后在进行二次mr来解决这个问题 组合combineor是对分区里面的预先聚合，下面的hello就不会发送一百万次，而是提前聚合了发送了hello 一百万次。这个样子。能解决目前的倾斜问题，但是不可能这么多数据，可能量很大 下面这种情况下，所有的hello根据哈希分区就进入到同一个分区，然后一百万个tom分4个分区 通过随机分区可以解决数据倾斜问题，但是会产生新的问题，就是说hello被分到4个不同的reduce里面，导致reduce1产生hello10reduce2产生hello13这种问题，所以通过二次mr来解决这个问题，让hello这个单词统计，进入到同一个reduce结果当中。所以也就是通过随即分区和二次mr解决 [1.txt]1000000 hello tom1 hello tom2 hello tom3 hello tom4 hello tom5 hello tom6 hello tom7 hello tom8 hello tom9 hello tom10 [2.txt]1000000 hello tom11 hello tom12 hello tom13 hello tom14 hello tom15 hello tom16 hello tom17 hello tom18 hello tom19 hello tom20 [3.txt]1000000 hello tom21 hello tom22 hello tom23 hello tom24 hello tom25 hello tom26 hello tom27 hello tom28 hello tom29 hello tom30 代码如下： //自定义分区函数 public class RandomPartitioner extends Partitioner&lt;Text,IntWritable&gt; { @Override public int getPartition(Text text, IntWritable intWritable, int numPartitions) { return new Random().nextInt(numPartitions); } } //解决数据倾斜问题： public class WCSkueApp { public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;); Job job = Job.getInstance(conf); // 设置作业的各种属性 job.setJobName(&quot;MaxTempApp&quot;); //作业名称 job.setJarByClass(WCSkueApp.class); //搜索类路径 job.setInputFormatClass(TextInputFormat.class);//设置输入格式类 //添加输入路径 FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew&quot;)); //设置输出路径 org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(&quot;e:/mr/skew/out&quot;)); //设置随机分区： job.setPartitionerClass(RandomPartitioner.class); job.setMapperClass(WCSkueMapper.class); //mapper类 job.setReducerClass(WCSkueReducer.class); //reduce类 job.setNumReduceTasks(4); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); job.setOutputKeyClass(Text.class); //设置输出类型 job.setOutputValueClass(IntWritable.class); job.waitForCompletion(true); } } public class WCSkueMapper extends Mapper &lt;LongWritable, Text, Text, IntWritable&gt;{ public WCSkueMapper(){ System.out.println(&quot;new WCMapper&quot;); } @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { String[] arr = value.toString().split(&quot; &quot;); Text keyout= new Text(); IntWritable valueout = new IntWritable(); for(String s:arr){ keyout.set(s); valueout.set(1); context.write(keyout,valueout); } } } public class WCSkueReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{ /** * reduce */ protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException { int count = 0 ; for(IntWritable iw : values){ count = count + iw.get() ; } context.write(key,new IntWritable(count)); } } //解决数据倾斜问题： public class WCSkueApp2 { public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;); Job job = Job.getInstance(conf); // 设置作业的各种属性 job.setJobName(&quot;MaxTempApp&quot;); //作业名称 job.setJarByClass(WCSkueApp2.class); //搜索类路径 job.setInputFormatClass(TextInputFormat.class);//设置输入格式类 //添加输入路径 FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00000&quot;)); FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00001&quot;)); FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00002&quot;)); FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00003&quot;)); //设置输出路径 org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(&quot;e:/mr/skew/out2&quot;)); job.setMapperClass(WCSkueMapper2.class); //mapper类 job.setReducerClass(WCSkueReducer2.class); //reduce类 job.setNumReduceTasks(4); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); job.setOutputKeyClass(Text.class); //设置输出类型 job.setOutputValueClass(IntWritable.class); job.waitForCompletion(true); } } public class WCSkueMapper2 extends Mapper &lt;LongWritable, Text, Text, IntWritable&gt;{ public WCSkueMapper2(){ System.out.println(&quot;new WCMapper&quot;); } protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { String[] arr = value.toString().split(&quot;\t&quot;); Text keyout= new Text(); IntWritable valueout = new IntWritable(); context.write(new Text(arr[0]),new IntWritable(Integer.parseInt(arr[1]))); } } public class WCSkueReducer2 extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{ /** * reduce */ protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException { int count = 0 ; for(IntWritable iw : values){ count = count + iw.get() ; } context.write(key,new IntWritable(count)); } } 上面这个就是通过2次mr解决数据倾斜问题 但是不用那么麻烦的去切分了直接使用Keyvalueinputformat即可实现：如下代码： //解决数据倾斜问题： public class WCSkueApp2 { public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;); Job job = Job.getInstance(conf); // 设置作业的各种属性 job.setJobName(&quot;MaxTempApp&quot;); //作业名称 job.setJarByClass(WCSkueApp2.class); //搜索类路径 job.setInputFormatClass(KeyValueTextInputFormat.class);//设置输入格式类 //添加输入路径 FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00000&quot;)); FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00001&quot;)); FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00002&quot;)); FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00003&quot;)); //设置输出路径 org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(&quot;e:/mr/skew/out2&quot;)); job.setMapperClass(WCSkueMapper2.class); //mapper类 job.setReducerClass(WCSkueReducer2.class); //reduce类 job.setNumReduceTasks(4); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); job.setOutputKeyClass(Text.class); //设置输出类型 job.setOutputValueClass(IntWritable.class); job.waitForCompletion(true); } } public class WCSkueMapper2 extends Mapper &lt;Text, Text, Text, IntWritable&gt;{ public WCSkueMapper2(){ System.out.println(&quot;new WCMapper&quot;); } protected void map(Text key, Text value, Context context) throws IOException, InterruptedException { context.write(key,new IntWritable(Integer.parseInt(value.toString()))); } } public class WCSkueReducer2 extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{ /** * reduce */ protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException { int count = 0 ; for(IntWritable iw : values){ count = count + iw.get() ; } context.write(key,new IntWritable(count)); } } 链条式编程 讲解上图：首先通过MapMapper1来切割，然后通过MapMapper1过滤掉法轮功，真个是一个链条，在map端，执行完之后开始聚合，在reduce端，在聚合的时候，每个单词都有各自的个数，对产生的结果再次过滤，过滤掉少于5的单词，当然可以放到同一个map里面，但是如果变成模块方法，易于维护。更加方便使用。m阶段可以有多个m但是R阶段只能有一个r。但是r阶段可以有多个m。 代码如下图：//链条式job任务 public class WCChainApp { public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;); Job job = Job.getInstance(conf); // 设置作业的各种属性 job.setJobName(&quot;MaxTempApp&quot;); //作业名称 job.setJarByClass(WCChainApp.class); //搜索类路径 job.setInputFormatClass(TextInputFormat.class);//设置输入格式类 //添加输入路径 FileInputFormat.addInputPath(job, new Path(&quot;e:/mr/skew&quot;)); //设置输出路径 org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(&quot;e:mr/skew/out&quot;)); //在map链条上添加一个mapper的环节 ChainMapper.addMapper(job,WCMapMapper1.class,LongWritable.class,Text.class,Text.class,IntWritable.class,conf); ChainMapper.addMapper(job,WCMapMapper2.class,Text.class,IntWritable.class,Text.class,IntWritable.class,conf); //在reduce链条上设置reduce ChainReducer.setReducer(job,WCReducer.class,Text.class,IntWritable.class,Text.class,IntWritable.class,conf); ChainReducer.addMapper(job,WCReduceMapper1.class,Text.class,IntWritable.class,Text.class,IntWritable.class,conf); job.setNumReduceTasks(3); job.waitForCompletion(true); } } public class WCMapMapper1 extends Mapper &lt;LongWritable, Text, Text, IntWritable&gt;{ protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { Text keyout= new Text(); IntWritable valueout = new IntWritable(); String[] arr = value.toString().split(&quot; &quot;); for(String s:arr){ keyout.set(s); valueout.set(1); context.write(keyout,valueout); } } } public class WCMapMapper2 extends Mapper &lt;Text, IntWritable, Text, IntWritable&gt;{ protected void map(Text key, IntWritable value, Context context) throws IOException, InterruptedException { if(!key.toString().equals(&quot;falungong&quot;)){ context.write(key , value); } } } //过滤单词个数 public class WCReduceMapper1 extends Mapper&lt;Text,IntWritable,Text,IntWritable&gt; { @Override protected void map(Text key, IntWritable value, Context context) throws IOException, InterruptedException { if(value.get() &gt; 5){ context.write(key,value); } } } /** * Reducer */ public class WCReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{ /** * reduce */ protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException { int count = 0 ; for(IntWritable iw : values){ count = count + iw.get() ; } context.write(key,new IntWritable(count)); } } FileInputFormat（读源码）获取切片集合。 子类都要重写方法isSplittable(); 负责创建RecordReader对象。 设置IO路径。 RecordReader（读源码）负责从InputSplit中读取KV对。 jdbc笔记模板：[写操作] Class.forName(&quot;com.mysql.jdbc.Driver&quot;); Connection conn = DriverMananger.getConnection(&quot;jdbc:mysql://localhost:3306/big4&quot;,&quot;root&quot;,&quot;root&quot;); //预处理语句 PreparedStatement ppst = conn.preparedStatement(&quot;insert into test(id,name,age) values(?,?,?)&quot;); //绑定参数 ppst.setInteger(1,1); ppst.setInteger(2,&quot;tom&quot;); ppst.setInteger(3,12); ppst.executeUpdate(); ppst.close(); conn.close(); [读操作] Class.forName(&quot;com.mysql.jdbc.Driver&quot;); Connection conn = DriverMananger.getConnection(&quot;jdbc:mysql://localhost:3306/big4&quot;,&quot;root&quot;,&quot;root&quot;); ppst = conn.preparedStatement(&quot;select id,name from test &quot;); //结果集 ResultSet rs = ppst.executeQuery(); while(rs.next()){ int id = rs.getInt(&quot;id&quot;); String name = rs.getInt(&quot;name&quot;); } rs.close(); conn.close(); 看一下下源码：看一下写和读入都把结果集和预处理语句都已经封装进去了，剩下的只要填空就好了。 以下是从数据库中读入的代码模板： 使用DBWritable向数据库从数据库中读取1.准备数据库create database big4 ; use big4 ; create table words(id int primary key auto_increment , name varchar(20) , txt varchar(255)); insert into words(name,txt) values(&apos;tomas&apos;,&apos;hello world tom&apos;); insert into words(txt) values(&apos;hello tom world&apos;); insert into words(txt) values(&apos;world hello tom&apos;); insert into words(txt) values(&apos;world tom hello&apos;); 2.编写hadoop MyDBWritable.import org.apache.hadoop.mapreduce.lib.db.DBWritable; import java.io.DataInput; import java.io.DataOutput; import java.io.IOException; import java.sql.PreparedStatement; import java.sql.ResultSet; import java.sql.SQLException; /** * MyDBWritable */ public class MyDBWritable implements DBWritable,Writable { private int id ; private String name ; private String txt ; public int getId() { return id; } public void setId(int id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public String getTxt() { return txt; } public void setTxt(String txt) { this.txt = txt; } public void write(DataOutput out) throws IOException { out.writeInt(id); out.writeUTF(name); out.writeUTF(txt); } public void readFields(DataInput in) throws IOException { id = in.readInt(); name = in.readUTF(); txt = in.readUTF(); } /** * 写入db */ public void write(PreparedStatement ppst) throws SQLException { ppst.setInt(1,id); ppst.setString(2,name); ppst.setString(3,txt); } /** * 从db读取 */ public void readFields(ResultSet rs) throws SQLException { id = rs.getInt(1); name = rs.getString(2); txt = rs.getString(3); } } 3.WcMapperpublic class WCMapper extends Mapper&lt;LongWritable,MyDBWritable,Text,IntWritable&gt; { protected void map(LongWritable key, MyDBWritable value, Context context) throws IOException, InterruptedException { System.out.println(key); String line = value.getTxt(); System.out.println(value.getId() + &quot;,&quot; + value.getName()); String[] arr = line.split(&quot; &quot;); for(String s : arr){ context.write(new Text(s),new IntWritable(1)); } } } 4.WCReducerprotected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException { int count = 0 ; for(IntWritable w : values){ count = count + w.get() ; } context.write(key,new IntWritable(count)); } 5.WCApppublic static void main(String[] args) throws Exception { Configuration conf = new Configuration(); Job job = Job.getInstance(conf); //设置job的各种属性 job.setJobName(&quot;MySQLApp&quot;); //作业名称 job.setJarByClass(WCApp.class); //搜索类 //配置数据库信息 String driverclass = &quot;com.mysql.jdbc.Driver&quot; ; String url = &quot;jdbc:mysql://localhost:3306/big4&quot; ; String username= &quot;root&quot; ; String password = &quot;root&quot; ; //设置数据库配置 DBConfiguration.configureDB(job.getConfiguration(),driverclass,url,username,password); //设置数据输入内容 DBInputFormat.setInput(job,MyDBWritable.class,&quot;select id,name,txt from words&quot;,&quot;select count(*) from words&quot;); //设置输出路径 FileOutputFormat.setOutputPath(job,new Path(&quot;e:/mr/sql/out&quot;)); //设置分区类 job.setMapperClass(WCMapper.class); //mapper类 job.setReducerClass(WCReducer.class); //reducer类 job.setNumReduceTasks(3); //reduce个数 job.setMapOutputKeyClass(Text.class); // job.setMapOutputValueClass(IntWritable.class); // job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); // job.waitForCompletion(true); } 6.pom.xml增加mysql驱动&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.17&lt;/version&gt; &lt;/dependency&gt; 7.将mr的统计结果写入mysql数据库a)准备表 create table stats(word varchar(50),c int); b)设置App的DBOutputFormat类 com.it18zhang.hdfs.mr.mysql.WCApp d) e) f) mysql分页查询如图所示吧： 使用DBWritable向数据库从数据库中写入public class MyDBWritalbe implements DBWritable,Writable { private int id=0; private String name=&quot;&quot;; private String txt=&quot;&quot;; private String word=&quot;&quot;; private int wordcount=0; public String getWord() { return word; } public void setWord(String word) { this.word = word; } public int getWordcount() { return wordcount; } public void setWordcount(int wordcount) { this.wordcount = wordcount; } public int getId() { return id; } public void setId(int id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public String getTxt() { return txt; } public void setTxt(String txt) { this.txt = txt; } public void write(DataOutput out) throws IOException { out.writeInt(id); out.writeUTF(name); out.writeUTF(txt); out.writeUTF(word); out.writeInt(wordcount); } public void readFields(DataInput in) throws IOException { id=in.readInt(); name=in.readUTF(); txt=in.readUTF(); word=in.readUTF(); wordcount=in.readInt(); } //向数据库中写入DB public void write(PreparedStatement ppst) throws SQLException { //要求定制字段列表的时候先单词后个数。 ppst.setString(1,word); ppst.setInt(2,wordcount); } //从DB中读出 public void readFields(ResultSet rs) throws SQLException { id=rs.getInt(1); name=rs.getString(2); txt=rs.getString(3); } } public class WCApp { // // public static void main(String[] args) throws Exception { // Configuration conf = new Configuration(); // // Job job = Job.getInstance(conf); // //// 设置作业的各种属性 // job.setJobName(&quot;MySQLApp&quot;); //作业名称 // job.setJarByClass(WCApp.class); //搜索类路径 // // //配置数据库信息 // String driverclass = &quot;com.mysql.jdbc.Driver&quot;; // String url = &quot;jdbc:mysql://localhost:3306/big4&quot;; // String usrname = &quot;root&quot;; // String password = &quot;root&quot;; // DBConfiguration.configureDB(conf,driverclass,url,usrname,password); // //设置数据输入内容 // DBInputFormat.setInput(job,DBWritable.class,&quot;select id,name,txt from words&quot;,&quot;select count(*) from words&quot;); // //设置输出路径 // FileOutputFormat.setOutputPath(job, new Path(&quot;d:/mr/sql/out&quot;)); // // job.setMapperClass(WCMapper.class); //mapper类 // job.setReducerClass(WCReducer.class); //reduce类 // // job.setNumReduceTasks(3); // // // job.setMapOutputKeyClass(Text.class); // job.setMapOutputValueClass(IntWritable.class); // // job.setOutputKeyClass(Text.class); //设置输出类型 // job.setOutputValueClass(IntWritable.class); // // job.waitForCompletion(true); // // } //} public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); Job job = Job.getInstance(conf); //设置job的各种属性 job.setJobName(&quot;MySQLApp&quot;); //作业名称 job.setJarByClass(WCApp.class); //搜索类 //配置数据库信息 String driverclass = &quot;com.mysql.jdbc.Driver&quot;; String url = &quot;jdbc:mysql://localhost:3306/big4&quot;; String username = &quot;root&quot;; String password = &quot;root&quot;; //设置数据库配置 DBConfiguration.configureDB(job.getConfiguration(), driverclass, url, username, password); //设置数据输入内容 DBInputFormat.setInput(job, MyDBWritalbe.class, &quot;select id,name,txt from words&quot;, &quot;select count(*) from words&quot;); //这里定制字段列表，先单词后个数 DBOutputFormat.setOutput(job,&quot;stats&quot;,&quot;word&quot;,&quot;c&quot;); //设置分区类 job.setMapperClass(WCMapper.class); //mapper类 job.setReducerClass(WCReducer.class); //reducer类 job.setNumReduceTasks(3); //reduce个数 job.setMapOutputKeyClass(Text.class); // job.setMapOutputValueClass(IntWritable.class); // job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); // job.waitForCompletion(true); } } public class WCMapper extends Mapper&lt;LongWritable, MyDBWritalbe, Text, IntWritable&gt; { @Override protected void map(LongWritable key, MyDBWritalbe value, Context context) throws IOException, InterruptedException { System.out.println(key); String line = value.getTxt(); System.out.println(value.getId() + &quot;,&quot; + value.getName()); String[] arr = line.split(&quot; &quot;); for (String s : arr) { context.write(new Text(s), new IntWritable(1)); } } } public class WCReducer extends Reducer&lt;Text, IntWritable, MyDBWritalbe, NullWritable&gt; { protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException { int count = 0; for (IntWritable w : values) { count = count + w.get(); } MyDBWritalbe keyout = new MyDBWritalbe(); keyout.setWord(key.toString()); keyout.setWordcount(count); context.write(keyout, NullWritable.get()); } } 在虚拟机中跑wordcount写入数据库问题：需要修改的地方：1.修改url 2.修改core-site.xml或者删除掉 3.在虚拟机中分发mysql-connector-java-5.1.7-bin.jar将之分发到每个节点的/soft/hadoop/share/hadoop/common/lib目录下。 4.在运行 hadoop jar HdfsDemo.jar com.it18zhang.hdfs.mr.mysql.WCApp 出现的问题：1.出现连接不上的报错： 百度了一下完美解决，是由于mysql没有对所有用户开启权限导致： https://blog.csdn.net/OldTogether/article/details/79612627?utm_source=blogxgwz1 2.出现一下问题： 原因是有一个s205的防火墙没有关掉导致的： https://blog.csdn.net/shirdrn/article/details/7280040 完]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>二次排序链条化</tag>
        <tag>数据倾斜</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop第七天]]></title>
    <url>%2F2018%2F10%2F17%2FHadoop%E7%AC%AC%E4%B8%83%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[多输入问题在IDEA里面代码： 首先明确一点InputInputformat也就是文本输入格式的输入是Longwritblele和Text，然后SequenceFileputFormat的输入是intwritble和text。 public class WCApp { public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;); Job job = Job.getInstance(conf); // 设置作业的各种属性 job.setJobName(&quot;WCAppMulti&quot;); //作业名称 job.setJarByClass(WCApp.class); //搜索类路径 //多个输入 MultipleInputs.addInputPath(job,new Path(&quot;file:///e:/mr/txt&quot;),TextInputFormat.class,WCTextMapper.class); MultipleInputs.addInputPath(job,new Path(&quot;file:///e:/mr/seq&quot;),SequenceFileInputFormat.class,WCSeqMapper.class); //设置输出 FileOutputFormat.setOutputPath(job,new Path(args[0])); job.setReducerClass(WCReducer.class);//reducer类 job.setNumReduceTasks(3);//reducer个数 job.setOutputKeyClass(Text.class); //设置输出类型 job.setOutputValueClass(IntWritable.class); job.waitForCompletion(true); } } ublic class WCReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{ /** * reduce */ protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException { int count = 0 ; for(IntWritable iw : values){ count = count + iw.get() ; } String tno = Thread.currentThread().getName(); System.out.println(tno + &quot;WCReducer:&quot; + key.toString() + &quot;=&quot; + count); context.write(key,new IntWritable(count)); } } public class WCTextMapper extends Mapper &lt;LongWritable, Text, Text, IntWritable&gt;{ @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { String[] arr = value.toString().split(&quot; &quot;); Text keyout= new Text(); IntWritable valueout = new IntWritable(); for(String s:arr){ keyout.set(s); valueout.set(1); context.write(keyout,valueout); } } } public class WCTextMapper extends Mapper &lt;LongWritable, Text, Text, IntWritable&gt;{ @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { String[] arr = value.toString().split(&quot; &quot;); Text keyout= new Text(); IntWritable valueout = new IntWritable(); for(String s:arr){ keyout.set(s); valueout.set(1); context.write(keyout,valueout); } } } 下图是配置本地的路径。要明确其中的arg[]也就是这个里配置的那个Program arguments的路径。因为只需要一个参数就可以了，因为在代码中已经写好了输入路径了。 计数器 日志目录：/soft/hadoop/logs/userlogs 用户打印的日志。Hadoop输出的日志都是在外层的文件之中。userlogs是自己的日志打印。 计数器是其他的内容都不做更改，在Mapper和Reducer里面添加这个语句即可 context.getCounter(“r”, “WCReducer.reduce”).increment(1); 然后扔到虚拟机里面去运行： hadoop jar HdfsDemo.jar com.it18zhang.hdfs.mr.WCApp hdfs://user/centos/data/ hdfs://user/centos/data/out 单独配置2nn到独立节点配置core-site文件 [hdfs-site.xml] &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;s206:50090&lt;/value&gt; &lt;/property&gt; 跟踪运行器信息 添加一个工具类： public class Util { public static String getInfo(Object o,String msg ){ return getHostname() + &quot;:&quot; + getPID() + &quot;:&quot; + getTID()+&quot;:&quot;+getObjInfo(o) +msg; } //得到主机名 public static String getHostname() { try { return InetAddress.getLocalHost().getHostName(); } catch (UnknownHostException e) { e.printStackTrace(); } return null; } //获得当前程序的所在的进程ID。 public static int getPID() { String info = ManagementFactory.getRuntimeMXBean().getName(); return Integer.parseInt(info.substring(0,info.indexOf(&quot;@&quot;))); } //返回当前线程ID， public static String getTID(){ return Thread.currentThread().getName(); } public static String getObjInfo(Object o){ String sname = o.getClass().getSimpleName(); return sname + &quot;@&quot;+o.hashCode(); } } 然后在map和reduce阶段添加： //每执行一次，计数器对这个组+1 context.getCounter(&quot;m&quot;,Util.getInfo(this,&quot;map&quot;)).increment(1); 效果如下图 全排序## 普通排序求最高年份温度 ## 代码如下： public class MaxTempApp { public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;); Job job = Job.getInstance(conf); // 设置作业的各种属性 job.setJobName(&quot;MaxTempApp&quot;); //作业名称 job.setJarByClass(MaxTempApp.class); //搜索类路径 job.setInputFormatClass(TextInputFormat.class);//设置输入格式类 //添加输入路径 FileInputFormat.addInputPath(job, new Path(args[0])); //设置输出路径 org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(args[1])); job.setMapperClass(MaxTempMapper.class); //mapper类 job.setReducerClass(MaxTempReducer.class); //reduce类 job.setNumReduceTasks(3); //reduce个数 job.setMapOutputKeyClass(IntWritable.class); job.setMapOutputValueClass(IntWritable.class); job.setOutputKeyClass(IntWritable.class); //设置输出类型 job.setOutputValueClass(IntWritable.class); job.waitForCompletion(true); } } public class MaxTempMapper extends Mapper&lt;LongWritable, Text, IntWritable, IntWritable&gt; { public MaxTempMapper() { System.out.println(&quot;new MaxTempMapper&quot;); } @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { String line = value.toString(); String arr[] = line.split(&quot; &quot;); context.write(new IntWritable(Integer.parseInt(arr[0])), new IntWritable((Integer.parseInt(arr[1])))); } } public class MaxTempReducer extends Reducer&lt;IntWritable, IntWritable, IntWritable, IntWritable&gt;{ /** * reduce */ protected void reduce(IntWritable key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException { int max =Integer.MIN_VALUE; for(IntWritable iw : values){ max = max &gt; iw.get() ? max : iw.get() ; } context.write(key,new IntWritable(max)); } } 全排序代码上面代码产生的会是3个文件，文件内部各自排序，但是不是全排序的文件。全排序2种办法： 1、设置分区数是1，但是数据倾斜 在每个分区里设置0-30，30-60，60-100即可,然后每个分区返回0,1,2进入分成不同的区代码如下图： public class YearPartitioner extends Partitioner&lt;IntWritable, IntWritable&gt; { public int getPartition(IntWritable year, IntWritable temp, int parts) { int y = year.get() - 1970; if (y &lt; 33) { return 0; } if (y &gt; 33 &amp;&amp; y &lt; 66) { return 1; } else { return 2; } } } public class MaxTempReducer extends Reducer&lt;IntWritable, IntWritable, IntWritable, IntWritable&gt;{ /** * reduce */ protected void reduce(IntWritable key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException { int max =Integer.MIN_VALUE; for(IntWritable iw : values){ max = max &gt; iw.get() ? max : iw.get() ; } context.write(key,new IntWritable(max)); } } public class MaxTempMapper extends Mapper&lt;LongWritable, Text, IntWritable, IntWritable&gt; { public MaxTempMapper() { System.out.println(&quot;new MaxTempMapper&quot;); } @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { String line = value.toString(); String arr[] = line.split(&quot; &quot;); context.write(new IntWritable(Integer.parseInt(arr[0])), new IntWritable((Integer.parseInt(arr[1])))); } } public class MaxTempApp { public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;); Job job = Job.getInstance(conf); // 设置作业的各种属性 job.setJobName(&quot;MaxTempApp&quot;); //作业名称 job.setJarByClass(MaxTempApp.class); //搜索类路径 job.setInputFormatClass(TextInputFormat.class);//设置输入格式类 job.setPartitionerClass(YearPartitioner.class); //添加输入路径 FileInputFormat.addInputPath(job, new Path(args[0])); //设置输出路径 org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(args[1])); job.setMapperClass(MaxTempMapper.class); //mapper类 job.setReducerClass(MaxTempReducer.class); //reduce类 job.setNumReduceTasks(3); //reduce个数 job.setMapOutputKeyClass(IntWritable.class); job.setMapOutputValueClass(IntWritable.class); job.setOutputKeyClass(IntWritable.class); //设置输出类型 job.setOutputValueClass(IntWritable.class); job.waitForCompletion(true); } } 其实无非就是加一个partitioner的这个类而已。 全排序采样器1.定义1个reduce 2.自定义分区函数。： 自行设置分解区间。 3.使用hadoop采样机制。 通过采样器生成分区文件，结合hadoop的TotalOrderPartitioner进行分区划分 TotalOrderPartitioner //全排序分区类,读取外部生成的分区文件确定区间。 使用时采样代码在最后端,否则会出现错误。 //分区文件设置，设置的job的配置对象，不要是之前的conf.这里面的getconfiguration()是对最开始new的conf的拷贝，TotalOrderPartitioner.setPartitionFile(job.getConfiguration(),new Path(“d:/mr/par.lst”)); 首先一段产生随机年份，温度的代码public class PrepareTempData { @Test public void makeData() throws IOException { FileWriter fw = new FileWriter(&quot;e:/mr/temp.txt&quot;); for(int i=0;i&lt;6000;i++){ int year=1970+ new Random().nextInt(100); int temp=-30 + new Random().nextInt(600); fw.write(&quot; &quot;+ year + &quot; &quot;+ temp + &quot;\r\n&quot; ); } fw.close(); } } 全排序采样器代码 public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS&quot;, &quot;file:///&quot;); Job job = Job.getInstance(conf); 设置作业的各种属性 job.setJobName(&quot;MaxTempApp&quot;); //作业名称 job.setJarByClass(MaxTempApp.class); //搜索类路径 job.setInputFormatClass(SequenceFileInputFormat.class);//设置输入格式类 //添加输入路径 FileInputFormat.addInputPath(job, new Path(args[0])); //设置输出路径 org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(args[1])); job.setMapperClass(MaxTempMapper.class); //mapper类 job.setReducerClass(MaxTempReducer.class); //reduce类 job.setMapOutputKeyClass(IntWritable.class); job.setMapOutputValueClass(IntWritable.class); job.setOutputKeyClass(IntWritable.class); //设置输出类型 job.setOutputValueClass(IntWritable.class); //设置 全排序分区类 job.setPartitionerClass(TotalOrderPartitioner.class); //将sample数据 写入分区文件 TotalOrderPartitioner.setPartitionFile(conf, new Path(&quot;e:/mr/par.lst&quot;)); //创建随机采样器对象 InputSampler.Sampler&lt;IntWritable, IntWritable&gt; sampler = new InputSampler.RandomSampler&lt;IntWritable, IntWritable&gt;(0.1, 10000, 10); job.setNumReduceTasks(3); //reduce个数 InputSampler.writePartitionFile(job, sampler); job.waitForCompletion(true); } } public class MaxTempMapper extends Mapper&lt;IntWritable, IntWritable, IntWritable, IntWritable&gt; { public MaxTempMapper() { System.out.println(&quot;new MaxTempMapper&quot;); } @Override protected void map(IntWritable key, IntWritable value, Context context) throws IOException, InterruptedException { context.write(key,value); } } public class MaxTempReducer extends Reducer&lt;IntWritable, IntWritable, IntWritable, IntWritable&gt;{ /** * reduce */ protected void reduce(IntWritable key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException { int max =Integer.MIN_VALUE; for(IntWritable iw : values){ max = max &gt; iw.get() ? max : iw.get() ; } context.write(key,new IntWritable(max)); } } public class YearPartitioner extends Partitioner&lt;IntWritable, IntWritable&gt; { public int getPartition(IntWritable year, IntWritable temp, int parts) { int y = year.get() - 1970; if (y &lt; 33) { return 0; } if (y &gt; 33 &amp;&amp; y &lt; 66) { return 1; } else { return 2; } } } 全排序官方笔记: 1.定义1个reduce 2.自定义分区函数. 自行设置分解区间。 3.使用hadoop采样机制。 通过采样器生成分区文件，结合hadoop的TotalOrderPartitioner进行分区划分。 TotalOrderPartitioner //全排序分区类,读取外部生成的分区文件确定区间。 使用时采样代码在最后端,否则会出现错误。 //分区文件设置，设置的job的配置对象，不要是之前的conf. TotalOrderPartitioner.setPartitionFile(job.getConfiguration(),new Path(&quot;d:/mr/par.lst&quot;)); 全排序和部分排序二次排序在面试比例很重的分区在map端，分组在reduce端。二次排序就是把value和key组合到一起，然后value就是nullwritable这样子。合在一起成为一个combokey。Combokey不但要可比较还要串行化，hadoop的串行化机制是writable ，串行就是写入到流离去，反串行就是读出来。IntWritable LongWritable这些都是经过串行化实现的类，Writable有很多子类。 value本身不能排序，为了能让他排序，吧value做到key里面去。吧年份和气温都做到key里面去，这个key是自定义的key,然后在combokey中定义里面的排序规则。自定义key,自定义对比器。 wirtable是串行化机制，java本身也有串行化机制，本质是对象输出流和输入流objectInputStream,objectOutputStream。需要写入和读取就可以了，但是java的串行化效率比较低而且复杂，不能够夸语言，所以hadoop有一个自己的串行化Wriable过程。 这端是reduce端，经过map端的combokey输出的combokey类是这个样子的，如果不经过处理直接聚合，就会发先，每一个combo都是一个新的都不相同，但是我们想让相同年份的进入到同一个组里面去。所以要重写分组。根据年份写分组，只要是同一个年就是同一组 reduce端里面有reduce对象和reduce方法。我们说的分几个区进入到几个reduce的意思是进入到几个reduce对象。 现在讲一下下图：下图说在没有分组的情况下，一个key对应好多value,也就是一个1978对应好多个12。好多个12进入到迭代器里面去，不断it.next是下一个v。但是key也改变，其实每次都是变化的， 讲一下二次排序和全排序：全排序就是整个年份-温度数据，分成几个区，让第一个区的最大值小于第二个区的最小值，然后这样子排下去，既解决了分布式的问题，又解决了数据倾斜的问题，但是value是不能排序的，因为mapreduce天生value就不能够排序。那么如何解决让温度也排序呢，就是要把年份和温度做成一个key，传入，然后再排序。就是二次排序，既实现了年份的排序，又实现了温度的排序，这里说的温度的排序是指同一个年份温度升序降序的问题。 二次排序代码如下： /** * 自定义组合key */ public class ComboKey implements WritableComparable&lt;ComboKey&gt; { private int year; private int temp; public int getYear() { return year; } public void setYear(int year) { this.year = year; } public int getTemp() { return temp; } public void setTemp(int temp) { this.temp = temp; } /** * 对key进行比较实现 */ public int compareTo(ComboKey o) { int y0 = o.getYear(); int t0 = o.getTemp(); //年份相同(升序) if (year == y0) { //气温降序 return -(temp - t0); // //这个地方为什么说temp-t0是升序排列，因为这个temp-t0是默认的。默认就是升序排列加一个负号就是降序排列。本身升序排列的就是temp-t0是大于0的 } else { return year - y0; } } /** * 串行化过程 */ public void write(DataOutput out) throws IOException { //年份 out.writeInt(year); //气温 out.writeInt(temp); } public void readFields(DataInput in) throws IOException { year = in.readInt(); temp = in.readInt(); } } /** *ComboKeyComparator */ public class ComboKeyComparator extends WritableComparator { protected ComboKeyComparator() { super(ComboKey.class, true); } public int compare(WritableComparable a, WritableComparable b) { ComboKey k1 = (ComboKey) a; ComboKey k2 = (ComboKey) b; return k1.compareTo(k2); } } public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS&quot;, &quot;file:///&quot;); Job job = Job.getInstance(conf); // 设置作业的各种属性 job.setJobName(&quot;SecondarySortApp&quot;); //作业名称 job.setJarByClass(MaxTempApp.class); //搜索类路径 job.setInputFormatClass(TextInputFormat.class);//设置输入格式类 //添加输入路径 FileInputFormat.addInputPath(job, new Path(args[0])); //设置输出路径 org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(args[1])); job.setMapperClass(MaxTempMapper.class); //mapper类 job.setReducerClass(MaxTempReducer.class); //reduce类 //设置map输出类型 job.setMapOutputKeyClass(ComboKey.class); job.setMapOutputValueClass(NullWritable.class); //设置Reduceoutput类型 job.setOutputKeyClass(IntWritable.class); //设置输出类型 job.setOutputValueClass(IntWritable.class); //设置分区类 job.setPartitionerClass(YearPartitioner.class); //设置分组对比器。 job.setGroupingComparatorClass(YearGroupComparator.class); //设置排序对比器，听说不写那个ComboKeyComparator不设置也可以 job.setSortComparatorClass(ComboKeyComparator.class); //reduce个数 job.setNumReduceTasks(3); job.waitForCompletion(true); } } public class MaxTempMapper extends Mapper&lt;LongWritable, Text, ComboKey, NullWritable&gt; { @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { String line = value.toString(); String[] arr = line.split(&quot; &quot;); ComboKey keyout = new ComboKey(); keyout.setYear(Integer.parseInt(arr[0])); keyout.setTemp(Integer.parseInt(arr[1])); context.write(keyout, NullWritable.get()); } } /** * Reducer */ public class MaxTempReducer extends Reducer&lt;ComboKey, NullWritable, IntWritable, IntWritable&gt;{ protected void reduce(ComboKey key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException { int year = key.getYear(); int temp = key.getTemp(); context.write(new IntWritable(year),new IntWritable(temp)); } } public class YearGroupComparator extends WritableComparator { protected YearGroupComparator() { super(ComboKey.class, true); } public int compare(WritableComparable a, WritableComparable b) { ComboKey k1 = (ComboKey) a ; ComboKey k2 = (ComboKey) b ; return k1.getYear() - k2.getYear() ; } } //自定义分区，在map端执行，是map中的一个阶段，mapkv进kv出，kv出去之后要有一个分区的过程。 //默认是哈希分区，这里边是修改了分区规则。 public class YearPartitioner extends Partitioner&lt;ComboKey,NullWritable&gt; { public int getPartition(ComboKey key, NullWritable nullWritable, int numPartitions) { int year = key.getYear(); return year % numPartitions; } }]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>多输入问题</tag>
        <tag>计数器使用</tag>
        <tag>跟踪运行信息</tag>
        <tag>产生随机数文件</tag>
        <tag>全排序</tag>
        <tag>全排序采样器</tag>
        <tag>二次排序</tag>
        <tag>倒排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop第六天之Yarn作业提交]]></title>
    <url>%2F2018%2F10%2F11%2FHadoop%E7%AC%AC%E5%85%AD%E5%A4%A9%E4%B9%8BYarn%E4%BD%9C%E4%B8%9A%E6%8F%90%E4%BA%A4%2F</url>
    <content type="text"><![CDATA[本地模式job提交流程mr.Job = new Job(); job.setxxx(); JobSubmitter.提交 LocalJobRunner.Job(); start(); hdfs writepacket, hdfs 切片计算方式getFormatMinSplitSize() = 1 //最小值(&gt;=1) 1 0 long minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job)); //最大值(&lt;= Long.Max , mapreduce.input.fileinputformat.split.maxsize=) long maxSize = getMaxSplitSize(job); //得到block大小 long blockSize = file.getBlockSize(); //minSplit maxSplit blockSize //Math.max(minSize, Math.min(maxSize, blockSize)); LF : Line feed,换行符private static final byte CR = ‘\r’;private static final byte LF = ‘\n’; 压缩1.Windows 源文件大小:82.8k 源文件类型:txt 压缩性能比较 | DeflateCodec GzipCodec BZip2Codec Lz4Codec SnappyCodec |结论 ------------|-------------------------------------------------------------------|---------------------- 压缩时间(ms)| 450 7 196 44 不支持 |Gzip &gt; Lz4 &gt; BZip2 &gt; Deflate ------------|-------------------------------------------------------------------|---------------------- 解压时间(ms)| 444 66 85 33 |lz4 &gt; gzip &gt; bzip2 &gt; Deflate ------------|-------------------------------------------------------------------|---------------------- 占用空间(k) | 19k 19k 17k 31k 不支持 |Bzip &gt; Deflate = Gzip &gt; Lz4 | | 2.CentOS 源文件大小:82.8k 源文件类型:txt | DeflateCodec GzipCodec BZip2Codec Lz4Codec LZO SnappyCodec |结论 ------------|---------------------------------------------------------------------------|---------------------- 压缩时间(ms)| 944 77 261 53 77 不支持 |Gzip &gt; Lz4 &gt; BZip2 &gt; Deflate ------------|---------------------------------------------------------------------------|---------------------- 解压时间(ms)| 67 66 106 52 73 |lz4 &gt; gzip &gt; Deflate&gt; lzo &gt; bzip2 ------------|---------------------------------------------------------------------------|---------------------- 占用空间(k) | 19k 19k 17k 31k 34k |Bzip &gt; Deflate = Gzip &gt; Lz4 &gt; lzo 远程调试1.设置服务器java vm的-agentlib:jdwp选项. [server] //windwos //set JAVA_OPTS=%JAVA_OPTS% -agentlib:jdwp=transport=dt_socket,address=8888,server=y,suspend=n //linux export HADOOP_CLIENT_OPTS=-agentlib:jdwp=transport=dt_socket,address=8888,server=y,suspend=y 2.在server启动java程序 hadoop jar HdfsDemo.jar com.it18zhang.hdfs.mr.compress.TestCompress 3.server会暂挂在8888. Listening ... 4.客户端通过远程调试连接到远程主机的8888. 5.客户端就可以调试了。 hadoop jarjava hadoop jar com.it18zhang.hdfs.mr.compress.TestCompress export HADOOP_CLIENT_OPTS=-agentlib:jdwp=transport=dt_socket,address=8888,server=y,suspend=y 在pom.xml中引入新的插件(maven-antrun-plugin),实现文件的复制.[pom.xml] &lt;project&gt; ... &lt;build&gt; &lt;plugins&gt; ... &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-antrun-plugin&lt;/artifactId&gt; &lt;version&gt;1.8&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;run&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;tasks&gt; &lt;echo&gt;---------开始复制jar包到共享目录下----------&lt;/echo&gt; &lt;delete file=&quot;D:\downloads\bigdata\data\HdfsDemo-1.0-SNAPSHOT.jar&quot;&gt;&lt;/delete&gt; &lt;copy file=&quot;target/HdfsDemo-1.0-SNAPSHOT.jar&quot; toFile=&quot;D:\downloads\bigdata\data\HdfsDemo.jar&quot;&gt; &lt;/copy&gt; &lt;/tasks&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; ... &lt;/project&gt; 在centos上使用yum安装snappy压缩库文件[google snappy] $&gt;sudo yum search snappy #查看是否有snappy库 $&gt;sudo yum install -y snappy.x86_64 #安装snappy压缩解压缩库 库文件windows :dll(dynamic linked library) linux :so(shared object) LZO1.在pom.xml引入lzo依赖 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.it18zhang&lt;/groupId&gt; &lt;artifactId&gt;HdfsDemo&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins &lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-antrun-plugin&lt;/artifactId&gt; &lt;version&gt;1.8&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;run&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;tasks&gt; &lt;echo&gt;---------开始复制jar包到共享目录下----------&lt;/echo&gt; &lt;delete file=&quot;D:\downloads\bigdata\data\HdfsDemo-1.0-SNAPSHOT.jar&quot;&gt;&lt;/delete&gt; &lt;copy file=&quot;target/HdfsDemo-1.0-SNAPSHOT.jar&quot; toFile=&quot;D:\downloads\bigdata\data\HdfsDemo.jar&quot;&gt; &lt;/copy&gt; &lt;/tasks&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.7.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-yarn-common&lt;/artifactId&gt; &lt;version&gt;2.7.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-yarn-client&lt;/artifactId&gt; &lt;version&gt;2.7.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-yarn-server-resourcemanager&lt;/artifactId&gt; &lt;version&gt;2.7.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.anarres.lzo&lt;/groupId&gt; &lt;artifactId&gt;lzo-hadoop&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; 2.在centos上安装lzo库 $&gt;sudo yum -y install lzo 3.使用mvn命令下载工件中的所有依赖 进入pom.xml所在目录，运行cmd： mvn -DoutputDirectory=./lib -DgroupId=com.it18zhang -DartifactId=HdfsDemo -Dversion=1.0-SNAPSHOT dependency:copy-dependencies 4.在lib下存放依赖所有的第三方jar 5.找出lzo-hadoop.jar + lzo-core.jar复制到hadoop的响应目录下。 $&gt;cp lzo-hadoop.jar lzo-core.jar /soft/hadoop/shared/hadoop/common/lib 6.执行远程程序即可。 修改maven使用aliyun镜像。[maven/conf/settings.xml] &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;settings xmlns=&quot;http://maven.apache.org/SETTINGS/1.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd&quot;&gt; &lt;pluginGroups&gt; &lt;/pluginGroups&gt; &lt;proxies&gt; &lt;/proxies&gt; &lt;servers&gt; &lt;server&gt; &lt;id&gt;releases&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;snapshots&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;Tomcat7&lt;/id&gt; &lt;username&gt;tomcat&lt;/username&gt; &lt;password&gt;tomcat&lt;/password&gt; &lt;/server&gt; &lt;/servers&gt; &lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;*&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;/mirror&gt; &lt;/mirrors&gt; &lt;/settings&gt; 文件格式:SequenceFile1.SequenceFile Key-Value对方式。 2.不是文本文件，是二进制文件。 3.可切割 因为有同步点。 reader.sync(pos); //定位到pos之后的第一个同步点。 writer.sync(); //写入同步点 4.压缩方式 不压缩 record压缩 //只压缩value 块压缩 //按照多个record形成一个block. 文件格式:MapFile1.Key-value 2.key按升序写入(可重复)。 3.mapFile对应一个目录，目录下有index和data文件,都是序列文件。 4.index文件划分key区间,用于快速定位。 自定义分区函数1.定义分区类 public class MyPartitioner extends Partitioner&lt;Text, IntWritable&gt;{ public int getPartition(Text text, IntWritable intWritable, int numPartitions) { return 0; } } 2.程序中配置使用分区类 job.setPartitionerClass(MyPartitioner.class); combinerd（合成） 继承了Reducer 任何的Reducer类都能被他使用 Map端的Reducer 预先化简1，为了减少网络带宽 将Map端发出的的数据进行聚合 并不是所有的都可以用combiner2,combiner 切片个数是四个，mapper就需要也是4个 下图是一个客户端的写入过程：首先形成一个管线，因为有很多节点，节点形成管线，现在本地构建2个队列，一个发送队列sendQueue，一个是确认队列ackQueue，客户端吧消息封装成Packet放到队列里面，而且结束后会放一个空包进去，发送的话，是以管线的方式发给A，A在发给B，B在发给C，在发送的过程中有一个校验的过程chunk(4+512)就是4个校验位+512个数据字节，一个Packet是64K+33个头的字节，在确认队列里面是一个编号seqNo如果在丢列中ABC中都回执，就吧seqNo在ackQueue中移除。发送如果失败，就吧管线重新建立，吧没法送的包返回到队列之前的一个容错机制。 输入格式定位文件，在输入格式中间有一个交互reader，在Map和Reduce都有这么一些方法，在Mapper和Reducer中有回调方法，就是本来使用的是子类的方法，但是子类中调用有父类的的run方法，然后run方法里面有reduce方法也就又调用了子类的方法，这叫做回调 。Mapper是通过reader读取下一条数据传进来的。Map和Reduce中间有一个shuffer的混洗过程，网络间分发数据的过程， Map里面先分区，分区个数等于Reduce的个数，所以有几个reduce就有几个分区，默认情况下，Map分区通过哈希算法来分区的，对K哈希。只要K相同，一定会被哈希到一个分区里面，分区也就是桶的概念，也就是只要K相同进入到同一个桶里面，只要是同一个分区就是进入到同一个reduce，默认情况下inputformat的格式，他的K就是偏移量。如果是图片格式的话就可能是别的。只有在文本输入的时候才是偏移量，当然也可以自己定义Inputformat格式。在mr计算之前切片就已经算好了，产生的切片文件和配置XML文件放到了临时目录下了。 看下图，首先客户端运行作业，第一步run job，第二部走ResourceManager，是Yarn层面，负责资源调度，所有集群的资源都属于ResourceManager，第二部get new application，首先得到应用的ID，第三部copyt job resources，复制切片信息和xml配置信息和jar包到HDFS，第四部提交给资源管理器，然后资源管理器start container，然后通过节点管理器启动一个MRAPPMaster（应用管理程序），第六步初始化Job，第七部检索切片信息，第八步请求资源管理器去分配一个资源列表，第九步启动节点管理器，通过节点管理器来启动一个JVM虚拟机，在虚拟机里面启动yarn子进程，第十部通过子进程读取切片信息，然后在运行MapTast或者ReduceTask 客户端提取一个作业，然后联系资源管理器，为了得到id,在完全分布式下客户端client叫job，一旦提交就叫做application了，先请求，然后得到了appid返回给了客户端，第三部将作业信息上传到HDFS的临时目录，存档切片信息和jar包，然后找到节点管理器，然后NM孵化一个MRAPPMaster，然后检索HDFS文件系统，获取到有多少个要执行的并发任务，拿到任务之后，要求资源管理器分配一个集合，哪些节点可以用。 然后MRAppmaster在联系其他的NM，让NM去开启Yarn子进程，子进程在去检索HDFS信息，在开启Map和Reduce hdfs 切片计算方式getFormatMinSplitSize() = 1 //最小值(&gt;=1) 1 0 long minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job)); //最大值(&lt;= Long.Max , mapreduce.input.fileinputformat.split.maxsize=) long maxSize = getMaxSplitSize(job); //得到block大小 long blockSize = file.getBlockSize(); //minSplit maxSplit blockSize //Math.max(minSize, Math.min(maxSize, blockSize)); 在计算切片的时候一般块大小不设置，是最好的，一般就是128M，采用本地优先策略，一个快大小是128m如果一旦把他边变1m那么切片大小就会变成1M，一个，然后128M的块大小会被分发128份切片分别传输给MR去计算，这样的话，就会增加网络间的传输，增加网络负载。所以128m是最合适的 CR=‘\r’回车符LF+’\n’换行符 windows系统里面是\r\n。回车带换行。linux系统只有一个\n 切片是定义大方向的，阅读器recorderreader是控制细节的。切片定义任务数量，开几个map但是不一定是每个map都有任务。 切片问题是物理设置，但是是逻辑读取。 打个比方说上图这个东西比如说第一个例子，前面的设置物理划分，根据字节数划分成了4个切片，也就是有4个Mapper,然后四个切片开始读取，其中第一个物理切割到t但是实际上，由于切片读取到换行符所以，第一个Mapper也就是读取到了hello world tom，然后第二个，第二个偏移量是13+13.偏移量是13，在物理读取13个直接，但是由于Mapper检测不是行首，所以直接从下一行读取也在读取一整行。这个就是每个切片128MB的缩影，吧128M变成了13而已。然后其实Textinputformat下一个环节是。recoderreader。这个会先检测是否是行首， 压缩问题package com.it18zhang.hdfs.mr.compress; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.io.IOUtils; import org.apache.hadoop.io.compress.*; import org.apache.hadoop.util.ReflectionUtils; import org.junit.Test; import java.io.FileInputStream; import java.io.FileOutputStream; /** * @Title:TestCompress * @Description: * @Auther: Eric Hunn * @Version:1.0 * @Create 2018/10/14 19:39 */ public class TestCompress { @Test public void deflateCompress() throws Exception { Class[] zipClasses = { DeflateCodec.class, GzipCodec.class, BZip2Codec.class, Lz4Codec.class, }; for(Class c : zipClasses){ zip(c); } } /*压缩测试 * * */ public void zip(Class codecClass) throws Exception { long start = System.currentTimeMillis(); //实例化对象 CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, new Configuration()); //创建文件输出流，得到默认拓展名 FileOutputStream fos = new FileOutputStream(&quot;e:/comp/b&quot; + codec.getDefaultExtension()); //得到压缩流 CompressionOutputStream zipOut = codec.createOutputStream(fos); IOUtils.copyBytes(new FileInputStream(&quot;e:/11111.txt/&quot;), zipOut, 1024); zipOut.close(); System.out.println(codecClass.getSimpleName() + &quot; ：&quot;); } } 压缩一般是在文件Map输出的时候压缩，Map输出的时候内存不够一般要往磁盘里面溢出，在溢出的时候可以让他压缩溢出，这样reduce的时候就要解压缩。上面代码测试过lz4不行，但是视频上是可以的，暂时不知道哪里出错了。 package com.it18zhang.hdfs.mr.compress; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.io.IOUtils; import org.apache.hadoop.io.compress.*; import org.apache.hadoop.util.ReflectionUtils; import org.junit.Test; import java.io.FileInputStream; import java.io.FileOutputStream; /** * @Title:TestCompress * @Description: * @Auther: Eric Hunn * @Version:1.0 * @Create 2018/10/14 19:39 */ public class TestCompress { @Test public void deflateCompress() throws Exception { Class[] zipClasses = { DeflateCodec.class, GzipCodec.class, BZip2Codec.class, // Lz4Codec.class, // SnappyCodec.class, }; for(Class c : zipClasses){ unzip(c); } } /*压缩测试 * * */ public void zip(Class codecClass) throws Exception { long start = System.currentTimeMillis(); //实例化对象 CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, new Configuration()); //创建文件输出流，得到默认拓展名 FileOutputStream fos = new FileOutputStream(&quot;e:/comp/b&quot; + codec.getDefaultExtension()); //得到压缩流 CompressionOutputStream zipOut = codec.createOutputStream(fos); IOUtils.copyBytes(new FileInputStream(&quot;e:/11111.txt/&quot;), zipOut, 1024); zipOut.close(); System.out.println(codecClass.getSimpleName() + &quot; ：&quot;); } public void unzip(Class codecClass) throws Exception { long start = System.currentTimeMillis(); //实例化对象 CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, new Configuration()); //创建文件输出流，得到默认拓展名 FileInputStream fis = new FileInputStream(&quot;e:/comp/b&quot; + codec.getDefaultExtension()); //得到压缩流 CompressionInputStream zipIn = codec.createInputStream(fis); IOUtils.copyBytes(zipIn,new FileOutputStream(&quot;e:/comp/b&quot; + codec.getDefaultExtension() + &quot;.txt&quot;), 1024); zipIn.close(); System.out.println(codecClass.getSimpleName() + &quot; ：&quot; + (System.currentTimeMillis() - start)); } } 在集群上运package com.it18zhang.hdfs.mr.compress; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.io.IOUtils; import org.apache.hadoop.io.compress.*; import org.apache.hadoop.util.ReflectionUtils; import java.io.FileInputStream; import java.io.FileOutputStream; /** * @Title:TestCompress * @Description: * @Auther: Eric Hunn * @Version:1.0 * @Create 2018/10/14 19:39 */ public class TestCompress { public static void main(String[] args) throws Exception { Class[] zipClasses = { DeflateCodec.class, GzipCodec.class, BZip2Codec.class, // Lz4Codec.class, // SnappyCodec.class, }; for(Class c : zipClasses){ zip(c); } System.out.println(&quot;==================================&quot;); for(Class c : zipClasses){ unzip(c); } } /*压缩测试 * * */ public static void zip(Class codecClass) throws Exception { long start = System.currentTimeMillis(); //实例化对象 CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, new Configuration()); //创建文件输出流，得到默认拓展名 FileOutputStream fos = new FileOutputStream(&quot;/home/centos/zip/b&quot; + codec.getDefaultExtension()); //得到压缩流 CompressionOutputStream zipOut = codec.createOutputStream(fos); IOUtils.copyBytes(new FileInputStream(&quot;/home/centos/zip/a.txt&quot;), zipOut, 1024); zipOut.close(); System.out.println(codecClass.getSimpleName() + &quot; ：&quot;); } public static void unzip(Class codecClass) throws Exception { long start = System.currentTimeMillis(); //实例化对象 CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, new Configuration()); //创建文件输出流，得到默认拓展名 FileInputStream fis = new FileInputStream(&quot;/home/centos/zip/b&quot; + codec.getDefaultExtension()); //得到压缩流 CompressionInputStream zipIn = codec.createInputStream(fis); IOUtils.copyBytes(zipIn,new FileOutputStream(&quot;/home/centos/zip/b&quot; + codec.getDefaultExtension() + &quot;.txt&quot;), 1024); zipIn.close(); System.out.println(codecClass.getSimpleName() + &quot; ：&quot; + (System.currentTimeMillis() - start)); } } 远程调试1.设置服务器java vm的-agentlib:jdwp选项. [server] //windwos //set JAVA_OPTS=%JAVA_OPTS% -agentlib:jdwp=transport=dt_socket,address=8888,server=y,suspend=n //linux 2.在server启动java程序 hadoop jar HdfsDemo.jar com.it18zhang.hdfs.mr.compress.TestCompress 3.server会暂挂在8888. Listening ... 4.客户端通过远程调试连接到远程主机的8888. 5.客户端就可以调试了。 通过MapFile来写入 /*写操作 * */ @Test public void save() throws Exception { Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS &quot;, &quot;file:///&quot;); FileSystem fs = FileSystem.get(conf); Path p = new Path(&quot;e:/seq/1.seq&quot;); MapFile.Writer writer = new MapFile.Writer(conf, fs, &quot;e:/map&quot;, IntWritable.class, Text.class); for (int i = 0; i &lt; 100000; i++) { writer.append(new IntWritable(i), new Text(&quot;tom&quot; + i)); } // for(int i =0 ;i &lt; 10 ; i++){// writer.append(new IntWritable(i),new Text(“tom” + i));// } writer.close(); } 通过MapFile来读取/*读取Mapfile文件 * */ @Test public void readMapfile() throws Exception { Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS &quot;, &quot;file:///&quot;); FileSystem fs = FileSystem.get(conf); Path p = new Path(&quot;e:/seq/1.seq&quot;); MapFile.Reader reader = new MapFile.Reader(fs, &quot;e:/map&quot;, conf); IntWritable key = new IntWritable(); Text value = new Text(); while (reader.next(key, value)) { System.out.println(key.get() + &quot;:&quot; + value.toString()); } reader.close(); } Map的分区是哈希分区 combiner的好处是：map端的reduce在map端做集合，减少了shuffle量。统计每个单词hello 1发了十万次，不如发一个hello1在map端做聚合发过去hello十万，即刻。就是在map端口预先reduce化简的过程。不是所有的作业都可以把reduce做成combiner。最大值最小值都可以combiner但是平均值就不行了。 第六天的最后两个视频没有往下写代码，因为太多太杂了。主要讲解MR计数器，数据倾斜，Mapfile 序列化读取，压缩这些。]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Yan作业提交</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop第五天01之hdfs写入剖析]]></title>
    <url>%2F2018%2F10%2F05%2FHadoop%E7%AC%AC%E4%BA%94%E5%A4%A901%E4%B9%8Bhdfs%E5%86%99%E5%85%A5%E5%89%96%E6%9E%90%2F</url>
    <content type="text"><![CDATA[一段HDFS写入流源码分析。 首先这个得到了一个抽象类，这个FileSystemm是一个抽象类。 这个抽象类有子类，左边有一个箭头，点击得到了一个这个抽象类的所有的子类，简单看下，因为Hadoop包类的hadfs这个，得到的子类是DistributedFileSystem这个分布式文件系统。 也可以吧鼠标放到fs上会显示返回的类 也可以在IDEA的右下角的类标签里面找到： 也就是说返回了一个DistributedFIleSystem, 然后文件系统fs.create调用了create方法返回了一个FSDataOutputStream流，这个是一个HDFS数据输出流 单机F5单部进入第一个： 看这个，进到下一步，调用checkClosed()，方法，这个方法其实是继承FSOutput流里面的东西。 看到这个里面out=DFSOutputStream。这个是被装饰的流。调用这个流的write方法 HDFS流是对DFS输出流的包装 进去这个是装饰模式。 在这个构造模式中也声明了字段。 下一步： 调用了Close方法因为是继承 都是FSoutput流的子类。一个检查的方法，判断是否数组越界。 下面这个for是个循环，循环写入，。 然后下一步，进入到write1方法。 里面的buf是一个缓冲区，count是一个成员常量 上图的buf看注释，是一个缓冲区用哦过来存储数据，在他被checksum校验和之前 校验和就是检查是否是比之前设置的最小块大小大，而且还必须是512字节的倍数 上图点击进入到一个校验和类的里面看注释，校验和用于数据传输之前。 上图这两行，通过一个算法来实现校验和。通过CRC32算法类似于哈希算法。 不管他，回到buf缓冲这个地方，单部进入 首先在缓冲区进行一个判定 拷贝本地数据到本地的一个buf，显示Localbuffer=4608,count=0.要拷贝的字节是4608。如果拷贝的内容没有缓冲区大就考数据内容，要是比缓冲区大的话，就拷贝缓冲区大小的内容。 在单部进入到上图。 返回到代码。进入到源代码中，如上图 在单部进入到这个Close里面：如下图： 这个close是out.close。out是调用父类的close方法，而且父类还是一个包装类，所以进入了包装的列的close方法。再进入到close()方法。如下图： out的本类HDFSOutputStream，然后这个又是一个装饰类，本质上是DFSOutputStream这个类的方法。 单部进入到Out里面，看一下调用的到底是谁的out。再单部进入到Out里面 看注释，关闭输出流并且 释放与之相关联的系统资源。上图最终进入到了DFSOutputstream的close()方法里面了。 接上图，看到在这个close(）方法里面有一个closeImp（)方法。调用自己的关闭实现方法还在这个类里面执行呢：继续在这个类里面往下走:如下图： 这个里面有一个flushbuffer()方法， 在单部进入到这个方法里面。如下图： 看到这个图里面this和当前类不太一样说明这两个东西也是继承关系。 清理缓冲区两个参数，看注释：强制任何输出流字节被校验和并且被写入底层的输出流。 再单步进入： 看到如上图的内容。然后到了writeChecksumChunks这个方法里面，单部进入到这个里面： 对给定的数据小块来生成校验和，并且根据小块和校验和给底层的数据输出流。 单部进入到这个sum.calculateChunckedSum方法里面。 下一步 上图吧数据写入了底层里面去了。 下一步 下一步 下一步 上图就是将底层的内容打成包，再去通过网络传输。在包里面加编号和序号，也就是加顺序。 往下不写了太多了。现在的视频位置是hdfs写入剖析2的开头。]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>hdfs写入</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop第四天之滚动日志-安全模式]]></title>
    <url>%2F2018%2F10%2F04%2FHadoop%E7%AC%AC%E5%9B%9B%E5%A4%A9%E4%B9%8B%E6%BB%9A%E5%8A%A8%E6%97%A5%E5%BF%97-%E5%AE%89%E5%85%A8%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Hadoop第四天之最小块设置-指定副本数]]></title>
    <url>%2F2018%2F10%2F04%2FHadoop%E7%AC%AC%E5%9B%9B%E5%A4%A9%E4%B9%8B%E6%9C%80%E5%B0%8F%E5%9D%97%E8%AE%BE%E7%BD%AE-%E6%8C%87%E5%AE%9A%E5%89%AF%E6%9C%AC%E6%95%B0%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[遇到未解决的问题]]></title>
    <url>%2F2018%2F10%2F01%2F%E9%81%87%E5%88%B0%E6%9C%AA%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[在eclipse中，ctrl+左键找到源码，为什么旁边在pakage explore中有有轮廓啊？ 点击两个箭头即可转换。 这个东西是在IDEA中怎么调出来的。 克隆centos之后有时候出现这种情况 IDEA的使用问题：这个是什么快捷键。 在增强虚拟机工具的时候按照视频的方法没有成功，然后通过挂载的方法实现了，但是hgfs文件夹和文件夹里面的内容都是root权限的，只能修改文件夹的所有者，但是不能修改里面文件的所有者。 有一个分组的问题： 分组就是说组合key出来本来是每一个组合key大多都是不同的key。相同年份不同温度也是不同的key，但是为了相同的年份的组合key能够进入到同一个reduce，所以要通过分组让他这个样子，但是为什么要这个样 经过分区已经说达到了将不同年份的 在mapreduce阶段有一个大表和小表连接 hive里面也有一个小表和小表连接就在map端口，然后小表和大表连接在reduce端口。 这个地方的问题没有搞明白。具体的hive在hive第二天07mr的忘记在哪里了 然后mr的最后一天hadoop第十一天的二次排序没有搞得特别明白。和hive阶段的自定义函数UDF没有特别清楚 在avro和rotobuf第一天 在讲解protobuf的时候写代码写到下图的时候听说这个com.example.tuorial是源码包里面的东西，我是没找到，我觉得可能是源码包里面的例子，里面包括的一些类，但是还是没有这个东西。 已经解决，见大坑 在一个spring项目里面有这样一个问题： dao层类dao一个方法，然后在servicce层里面没有new出一个dao类的对象，直接写一个属性Private Dao d;然后直接d.insert(); 这种方法为什么可以 这种是什么调用方法：]]></content>
      <categories>
        <category>问题</category>
      </categories>
      <tags>
        <tag>未解决的问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[haoop第三天之脚本分析，单个进程启动]]></title>
    <url>%2F2018%2F09%2F30%2Fhaoop%E7%AC%AC%E4%B8%89%E5%A4%A9%E4%B9%8B%E8%84%9A%E6%9C%AC%E5%88%86%E6%9E%90%EF%BC%8C%E5%8D%95%E4%B8%AA%E8%BF%9B%E7%A8%8B%E5%90%AF%E5%8A%A8%2F</url>
    <content type="text"><![CDATA[ssh权限问题1.~/.ssh/authorized_keys 644 2.$/.ssh 700 3.root 配置SSH生成密钥对 $&gt;ssh-keygen -t rsa -P &apos;&apos; -f ~/.ssh/id_rsa 添加认证文件 $&gt;cat ~/.ssh/id_rsa.pub &gt; ~/.ssh/authorized_keys 权限设置,文件和文件夹权限除了自己之外，别人不可写。 $&gt;chmod 700 ~/.ssh $&gt;chmod 644 ~/.ssh/authorized_keys scp远程复制. rsync远程同步,支持符号链接。 rsync -lr xxx xxx 完全分布式1.配置文件 [core-site.xml] fs.defaultFS=hdfs://s201:8020/ [hdfs-site.xml] replication=1 //伪分布 replication=3 //完全分布 [mapred-site.xml] mapreduce.framework.name=yarn [yarn-site.xml] rm.name=s201 [slaves] s202 s203 s204 2.分发文件 a)ssh openssh-server //sshd openssh-clients //ssh openssh //ssh-keygen b)scp/rsync 3.格式化文件系统 $&gt;hadoop namenode -format 4.启动hadoop所有进程 //start-dfs.sh + start-yarn.sh $&gt;start-all.sh 5.xcall.sh jps /usr/local/bin/jps /usr/local/bin/java 6.查看jps进程 $&gt;xcall.sh jps 7.关闭centos的防火墙 $&gt;sudo service firewalld stop // &lt;=6.5 start/stop/status/restart $&gt;sudo systemctl stop firewalld // 7.0 停止 start/stop/status/restart $&gt;sudo systemctl disable firewalld //关闭 $&gt;sudo systemctl enable firewalld //启用 7.最终通过webui http://s201:50070/ 符号连接1.修改符号连接的owner $&gt;chown -h centos:centos xxx //-h:针对连接本身，而不是所指文件. 2.修改符号链接 $&gt;ln -sfT index.html index //覆盖原有的连接。 hadoop模块common // hdfs // mapreduce // yarn // 进程[hdfs]start-dfs.sh NameNode NN DataNode DN SecondaryNamenode 2NN [yarn]start-yarn.sh ResourceMananger RM NodeManager NM 脚本分析sbin/start-all.sh -------------- libexec/hadoop-config.sh start-dfs.sh start-yarn.sh sbin/start-dfs.sh -------------- libexec/hadoop-config.sh sbin/hadoop-daemons.sh --config .. --hostname .. start namenode ... sbin/hadoop-daemons.sh --config .. --hostname .. start datanode ... sbin/hadoop-daemons.sh --config .. --hostname .. start sescondarynamenode ... sbin/hadoop-daemons.sh --config .. --hostname .. start zkfc ... // sbin/start-yarn.sh -------------- libexec/yarn-config.sh bin/yarn-daemon.sh start resourcemanager bin/yarn-daemons.sh start nodemanager sbin/hadoop-daemons.sh ---------------------- libexec/hadoop-config.sh slaves hadoop-daemon.sh sbin/hadoop-daemon.sh ----------------------- libexec/hadoop-config.sh bin/hdfs .... sbin/yarn-daemon.sh ----------------------- libexec/yarn-config.sh bin/yarn bin/hadoop ------------------------ hadoop verion //版本 hadoop fs //文件系统客户端. hadoop jar // hadoop classpath hadoop checknative bin/hdfs ------------------------ dfs // === hadoop fs classpath namenode -format secondarynamenode namenode journalnode zkfc datanode dfsadmin haadmin fsck balancer jmxget mover oiv oiv_legacy oev fetchdt getconf groups snapshotDiff lsSnapshottableDir portmap nfs3 cacheadmin crypto storagepolicies version hdfs常用命令$&gt;hdfs dfs -mkdir /user/centos/hadoop $&gt;hdfs dfs -ls -r /user/centos/hadoop $&gt;hdfs dfs -lsr /user/centos/hadoop $&gt;hdfs dfs -put index.html /user/centos/hadoop $&gt;hdfs dfs -get /user/centos/hadoop/index.html a.html $&gt;hdfs dfs -rm -r -f /user/centos/hadoop no route 关闭防火墙。 $&gt;su root $&gt;xcall.sh &quot;service firewalld stop&quot; $&gt;xcall.sh &quot;systemctl disable firewalld&quot; hdfs500G 1024G = 2T/4T 切割。 寻址时间:10ms左右 磁盘速率 : 100M /s 64M 128M //让寻址时间占用读取时间的1%. 1ms 1 / 100 size = 181260798 block-0 : 134217728 block-1 : 47043070 -------------------- b0.no : 1073741829 b1.no : 1073741830 HAhigh availability,高可用性。通常用几个9衡量。 99.999% SPOF:single point of failure,单点故障。 secondarynamenode找到所有的配置文件1.tar开hadoop-2.7.3.tar.gz hadoop-2.7.3\share\hadoop\common\hadoop-common-2.7.3.jar\core-default.xml hadoop-2.7.3\share\hadoop\hdfs\hadoop-hdfs-2.7.3.jar\hdfs-default.xml hadoop-2.7.3\share\hadoop\mapreduce\hadoop-mapreduce-client-core-2.7.3.jar\mapred-default.xml hadoop-2.7.3\share\hadoop\yarn\hadoop-yarn-common-2.7.3.jar\yarn-site.xml 本地模式[core-site.xml] fs.defaultFS=file:/// //默认值 配置hadoop临时目录1.配置[core-site.xml]文件 &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://s201/&lt;/value&gt; &lt;/property&gt; &lt;!--- 配置新的本地目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/centos/hadoop&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; //以下属性均由hadoop.tmp.dir决定,在hdfs-site.xml文件中配置。 dfs.namenode.name.dir=file://${hadoop.tmp.dir}/dfs/name dfs.datanode.data.dir=file://${hadoop.tmp.dir}/dfs/data dfs.datanode.data.dir=file://${hadoop.tmp.dir}/dfs/data dfs.namenode.checkpoint.dir=file://${hadoop.tmp.dir}/dfs/namesecondary dfs.namenode.checkpoint.dir=file://${hadoop.tmp.dir}/dfs/namesecondary 2.分发core-site.xml文件 $&gt;xsync core-site.xml 3.格式化文件系统,只对namenode的本地目录进行初始化。 $&gt;hadoop namenode -format //hdfs namenode -format 4.启动hadoop $&gt;start-dfs.sh 使用xcall.sh在所有节点上创建jps符号连接，指向/soft/jdk/bin/jps1.切换到root用户 $&gt;su root 2.创建符号连接 $&gt;xcall.sh &quot;ln -sfT /soft/jdk/bin/jps /usr/local/bin/jps&quot; 3.修改jps符号连接的owner $&gt;xcall.sh &quot;chown -h centos:centos /usr/local/bin/jps&quot; 4.查看所有主机上的java进程 $&gt;xcall.sh jps 在centos桌面版中安装eclipse1.下载eclipse linux版 eclipse-jee-mars-R-linux-gtk-x86_64.tar.gz 2.tar开到/soft下, $&gt;tar -xzvf eclipse-jee-mars-R-linux-gtk-x86_64.tar.gz -C /soft 3.启动eclipse $&gt;cd /soft/eclipse $&gt;./eclipse &amp; //后台启动 4.创建桌面快捷方式 $&gt;ln -s /soft/eclipse/eclipse ~/Desktop/eclipse 5. 收集hadoop的所有jar包使用hadoop客户端api访问hdfs1.创建java项目 2.导入hadoop类库 3. 4. 5. 网络拓扑1. 2. 3. 4. 作业1.使用hadoop API递归输出整个文件系统 2.]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>脚本分析</tag>
        <tag>单个进程启动</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop第二天之搭建]]></title>
    <url>%2F2018%2F09%2F29%2FHadoop%E7%AC%AC%E4%BA%8C%E5%A4%A9%E4%B9%8B%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[hadoop1.独立模式(standalone|local) nothing! 本地文件系统。 不需要启用单独进程。 2.pesudo(伪分布模式) 等同于完全分布式，只有一个节点。 SSH: //(Socket), //public + private //server : sshd ps -Af | grep sshd //clint : ssh //ssh-keygen:生成公私秘钥。 //authorized_keys 需要使用644 //ssh 192.168.231.201 yes [配置文件] core-site.xml //fs.defaultFS=hdfs://localhost/ hdfs-site.xml //replication=1 mapred-site.xml // yarn-site.xml // 3.full distributed(完全分布式) 让命令行提示符显式完整路径1.编辑profile文件，添加环境变量PS1 [/etc/profile] export PS1=&apos;[\u@\h `pwd`]\$&apos; 2.source $&gt;source /etc/profile 配置hadoop，使用符号连接的方式，让三种配置形态共存。1.创建三个配置目录,内容等同于hadoop目录 ${hadoop_home}/etc/local ${hadoop_home}/etc/pesudo ${hadoop_home}/etc/full 2.创建符号连接 $&gt;ln -s 3.对hdfs进行格式化 $&gt;hadoop namenode -format 4.修改hadoop配置文件，手动指定JAVA_HOME环境变量 [${hadoop_home}/etc/hadoop/hadoop-env.sh] ... export JAVA_HOME=/soft/jdk ... 5.启动hadoop的所有进程 $&gt;start-all.sh 6.启动完成后，出现以下进程 $&gt;jps 33702 NameNode 33792 DataNode 33954 SecondaryNameNode 29041 ResourceManager 34191 NodeManager 7.查看hdfs文件系统 $&gt;hdfs dfs -ls / 8.创建目录 $&gt;hdfs dfs -mkdir -p /user/centos/hadoop 9.通过webui查看hadoop的文件系统 http://localhost:50070/ 10.停止hadoop所有进程 $&gt;stop-all.sh 11.centos防火墙操作 [cnetos 6.5之前的版本] $&gt;sudo service firewalld stop //停止服务 $&gt;sudo service firewalld start //启动服务 $&gt;sudo service firewalld status //查看状态 [centos7] $&gt;sudo systemctl enable firewalld.service //&quot;开机启动&quot;启用 $&gt;sudo systemctl disable firewalld.service //&quot;开机自启&quot;禁用 $&gt;sudo systemctl start firewalld.service //启动防火墙 $&gt;sudo systemctl stop firewalld.service //停止防火墙 $&gt;sudo systemctl status firewalld.service //查看防火墙状态 [开机自启] $&gt;sudo chkconfig firewalld on //&quot;开启自启&quot;启用 $&gt;sudo chkconfig firewalld off //&quot;开启自启&quot;禁用 hadoop的端口50070 //namenode http port 50075 //datanode http port 50090 //2namenode http port 8020 //namenode rpc port 50010 //datanode rpc port hadoop四大模块common hdfs //namenode + datanode + secondarynamenode mapred yarn //resourcemanager + nodemanager 启动脚本1.start-all.sh //启动所有进程 2.stop-all.sh //停止所有进程 3.start-dfs.sh // 4.start-yarn.sh [hdfs] start-dfs.sh stop-dfs.sh NN DN 2NN [yarn] start-yarn.sh stop-yarn.sh RM NM 修改主机名1./etc/hostname s201 2./etc/hosts 127.0.0.1 localhost 192.168.231.201 s201 192.168.231.202 s202 192.168.231.203 s203 192.168.231.204 s204 完全分布式1.克隆3台client(centos7) 右键centos-7--&gt;管理-&gt;克隆-&gt; ... -&gt; 完整克隆 2.启动client 3.启用客户机共享文件夹。 4.修改hostname和ip地址文件 [/etc/hostname] s202 [/etc/sysconfig/network-scripts/ifcfg-ethxxxx] ... IPADDR=.. 5.重启网络服务 $&gt;sudo service network restart 6.修改/etc/resolv.conf文件 nameserver 192.168.231.2 7.重复以上3 ~ 6过程. 准备完全分布式主机的ssh1.删除所有主机上的/home/centos/.ssh/* 2.在s201主机上生成密钥对 $&gt;ssh-keygen -t rsa -P &apos;&apos; -f ~/.ssh/id_rsa 3.将s201的公钥文件id_rsa.pub远程复制到202 ~ 204主机上。 并放置/home/centos/.ssh/authorized_keys $&gt;scp id_rsa.pub centos@s201:/home/centos/.ssh/authorized_keys $&gt;scp id_rsa.pub centos@s202:/home/centos/.ssh/authorized_keys $&gt;scp id_rsa.pub centos@s203:/home/centos/.ssh/authorized_keys $&gt;scp id_rsa.pub centos@s204:/home/centos/.ssh/authorized_keys 4.配置完全分布式(${hadoop_home}/etc/hadoop/) [core-site.xml] &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://s201/&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; [hdfs-site.xml] &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; [mapred-site.xml] 不变 [yarn-site.xml] &lt;?xml version=&quot;1.0&quot;?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;s201&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; [slaves] s202 s203 s204 [hadoop-env.sh] ... export JAVA_HOME=/soft/jdk ... 5.分发配置 $&gt;cd /soft/hadoop/etc/ $&gt;scp -r full centos@s202:/soft/hadoop/etc/ $&gt;scp -r full centos@s203:/soft/hadoop/etc/ $&gt;scp -r full centos@s204:/soft/hadoop/etc/ 6.删除符号连接 $&gt;cd /soft/hadoop/etc $&gt;rm hadoop $&gt;ssh s202 rm /soft/hadoop/etc/hadoop $&gt;ssh s203 rm /soft/hadoop/etc/hadoop $&gt;ssh s204 rm /soft/hadoop/etc/hadoop 7.创建符号连接 $&gt;cd /soft/hadoop/etc/ $&gt;ln -s full hadoop $&gt;ssh s202 ln -s /soft/hadoop/etc/full /soft/hadoop/etc/hadoop $&gt;ssh s203 ln -s /soft/hadoop/etc/full /soft/hadoop/etc/hadoop $&gt;ssh s204 ln -s /soft/hadoop/etc/full /soft/hadoop/etc/hadoop 8.删除临时目录文件 $&gt;cd /tmp $&gt;rm -rf hadoop-centos $&gt;ssh s202 rm -rf /tmp/hadoop-centos $&gt;ssh s203 rm -rf /tmp/hadoop-centos $&gt;ssh s204 rm -rf /tmp/hadoop-centos 9.删除hadoop日志 $&gt;cd /soft/hadoop/logs $&gt;rm -rf * $&gt;ssh s202 rm -rf /soft/hadoop/logs/* $&gt;ssh s203 rm -rf /soft/hadoop/logs/* $&gt;ssh s204 rm -rf /soft/hadoop/logs/* 10.格式化文件系统 $&gt;hadoop namenode -format 11.启动hadoop进程 $&gt;start-all.sh rsync四个机器均安装rsync命令。 远程同步. $&gt;sudo yum install rsync 将root用户实现无密登录1.同 编写脚本1.xcall.sh 2.xsync.sh xsync.sh /home/etc/a.txt rsync -lr /home/etc/a.txt centos@s202:/home/etc netstat -anop 查看进程]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>搭建</tag>
        <tag>第二天</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop安装配置]]></title>
    <url>%2F2018%2F09%2F24%2FHadoop%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"></content>
      <categories>
        <category>-Hadoop</category>
      </categories>
      <tags>
        <tag>-Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux基础之Yum命令]]></title>
    <url>%2F2018%2F09%2F20%2FLinux%E5%9F%BA%E7%A1%80%E4%B9%8BYum%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[软件源Repository //仓库. URL //http:// .d //directory目录 xxxd //daemon 查看仓库文件/etc/yum.repos.d/xxx.repo curl传输url上的数据的。 [下载文件到指定目录] curl -o /etc/yum.repos.d/ali.repo http://mirrors.aliyun.com/repo/Centos-7.repo 更换centos的软件源1.下载源仓库文件,xxx.repo curl -o /etc/yum.repos.d/ali.repo http://mirrors.aliyun.com/repo/Centos-7.repo 2.将repo文件保存到/etc/yum.repos.d/目录中。 屏蔽软件仓库1.将/etc/yum.repos.d/xxx.repo文件删除或者更换扩展名即可。 修改centos能够使用sudo命令[/etc/sudoers] $&gt;su root $&gt;nano /etc/sudoers ... centos ALL 使用yum进行软件包安装卸载$&gt;yum list //列出所有软件包 $&gt;yum list installed //列出已经安装的软件包 $&gt;yum list installed | grep nano //列出已经安装的软件包 $&gt;yum search nano //在yum的软件源中搜索软件 $&gt;yum remove nano //卸载软件 $&gt;yum -y install nano //直接安装，不需要yes确认. $&gt;yum list installed | grep nano //查看是否安装了Nano $&gt;mkdir /home/centos/rpms $echo 以下命令只下载软件，不安装软件 $&gt;sudo yum install --downloadonly //只下载 --downloaddir=/home/centos/rpms //指定下载目录 wget //下载已经安装的软件 $&gt;sudo yum reinstall --downloadonly --downloaddir=/home/centos/rpms wget $&gt;sudo yum localinstall xxx.rpm //从本地rpm文件直接安装软件 $&gt;su root $&gt;yum search ifconfig $&gt;yum -y install net-tools //安装网络工具 #==========修改网络地址====================== //需要重启network服务 $&gt;sudo nano /etc/sysconfig/network-scripts/ifcfg-eth1677736 [/etc/sysconfig/network-scripts/ifcfg-eth1677736] ... IPADDR=192.168.231.201 GATEWAY=192.168.231.2 DNS=192.168.231.2 $&gt;service network restart //重启网络服务。 $&gt;sudo nano /etc/resolv.conf //修改该文件不需要重启network服务 [/etc/resolv.conf] nameserver 192.168.231.2 在没有nano时，使用自带的vi文本编辑器1.vi xx.txt 2.模式切换 esc //切换到命令模式,退出编辑模式 //:q! 不保存退出 //:wq 保存退出 //x 删除一个字符 //dd 删除一行 insert //切换到编辑模式,退出命令模式 //del backspace Which命令which命令用于查找并显示给定命令的绝对路径，环境变量PATH中保存了查找命令时需要遍历的目录。 which指令会在环境变量$PATH设置的目录里查找符合条件的文件。也就是说，使用which命令，就可以看到某个系统命令是否存在，以及执行的到底是哪一个位置的命令。 语法which(选项)(参数)]]></content>
      <tags>
        <tag>Linux基础</tag>
        <tag>Yum命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux基础第之循环命令]]></title>
    <url>%2F2018%2F09%2F19%2FLinux%E5%9F%BA%E7%A1%80%E7%AC%AC%E4%B9%8B%E5%BE%AA%E7%8E%AF%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[访问文件(夹)物理位置pwd命令是显示当前的逻辑位置。 物理位置是针对符号连接也是针对软连接的。 //进入/t的物理位置 $&gt;cd -P /t //显式当前目录的物理位置 $&gt;pwd -P 访问环境变量echo ${PATH} //ok echo $PATH //ok echo &quot;$PATH&quot; //ok echo &apos;$PATH&apos; //&apos;&apos;原样输出这个不行，显示“PATH” export定义环境变量,只在session中有效 (当前会话)$&gt;export name=${PATH}:tom 设置name为${Var1}的值，Var1没有设置为${Var2}的值。 $&gt;export name=${Var1:-${Var2}} 命令执行过程$? //命令的返回值存储变量,0:成功 1:失败。 $# //参数个数 $1 //第几个参数 $0 //当前脚本(命令)名称 $@ //取出所有参数 shift //参数左移 ${a/b/c} // 下面一个例子 向左移位解释： if 命令讲解语法: 中括号是可以选择的： if COMMANDS; then COMMANDS; [ elif COMMANDS; then COMMANDS; ]... [ else COMMANDS; ] fi if [ $# -lt 3 ]; then xx ; fi 3,5 使用for循环输出1 - 100个数看一下for的帮助文档，注意从冒号之后是开始的： for NAME [in WORDS ... ] ; do COMMANDS; done for x in a b c d ; do echo $x ; done ; 通过for循环打印一个三角形：首先看一下 然后这段是命令： while语法for: for NAME [in WORDS ... ] ; do COMMANDS; done for ((: for (( exp1; exp2; exp3 )); do COMMANDS; done 一个例子： #!/bin/bash ((i=0)) while ((i&lt;100)) ; do echo $i; i=$((i+1)) done]]></content>
      <tags>
        <tag>linux</tag>
        <tag>循环命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux基础之hostname]]></title>
    <url>%2F2018%2F09%2F19%2FLinux%E5%9F%BA%E7%A1%80%E4%B9%8Bhostname%2F</url>
    <content type="text"><![CDATA[命令嵌套使用$&gt;echo `cat b.txt` //命令解析,无法嵌套 $&gt;$(... $()) //支持命令的嵌套 创建用户0.用户和组之间，一个用户可以属于多个组。 但是有一个首要组。 1.adduser,等同于useradd 符号链接。 /usr/sbin/adduser --&gt; /usr/sbin/useradd. 2.useradd 输入新密码. 重复输入 $&gt;su root $&gt;useradd -m centos2 $&gt;su root $&gt;passwd centos2 -m, --create-home create the user&apos;s home directory -p, --password PASSWORD encrypted password of the new account 3.使用方法 $&gt;su root $&gt;userdel -r centos2 //用户所在组目录也会被删除.在删除用户时候要用exit退出要删除的用户,删除的时候可能会exit好多次，因为会来回su。]]></content>
      <tags>
        <tag>Linux基础</tag>
        <tag>基础命令</tag>
        <tag>hostname</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux基础之进程查看]]></title>
    <url>%2F2018%2F09%2F18%2FLinux%E5%9F%BA%E7%A1%80%E4%B9%8B%E8%BF%9B%E7%A8%8B%E6%9F%A5%E7%9C%8B%2F</url>
    <content type="text"><![CDATA[job 放到后台运行的进程.1.将程序放到后台运行,以&amp;结尾. $&gt;nano b.txt &amp; 2.查看后台运行的jobs数 $&gt;jobs 3.切换后台作业到前台来. $&gt;fg %n //n是job编号. 4.前台正在的进程，放到后台。 ctrl + z 5.让后作业运行 $&gt;bg %1 // 6.杀死作业 $&gt;kill %1 // man + 命令 ：查看该命令详细帮助 进程查看,prcess show $&gt;ps -Af |grep gnome //-A:所有进程 -f:所有列格式. $&gt;top //动态显示进程信息。含有cpu、内存的使用情况. //q,按照q退出。 cut剪切显示文件的每一行。 $&gt;cut -c 1-5 a.txt //从第一个字符开始,下标从1开始。 $&gt;ps -Af | cut -c 45-80 | more //吧PS里面获得的内容剪切显示，显示没一行的45-80，翻页查看 查看帮助$&gt;help //查看os内置的命令 $&gt;man ifcon fig //查看特定命令 $&gt;ifconfig --help $&gt;ifconfig -h $&gt;info ifconfig // 磁盘分区使用$&gt;fdisk -l /dev/sda 里面的中括号是可选的，尖括号是必须要写的 里面sad是磁盘，sda1是分区。sd1，sd2，sd3是磁盘的三个分区。 查看磁盘使用情况(disk free)$&gt;df -ah /home/centos //查看 dirname取出指定地址的上级目录. $&gt;dirname /a/b/c/d $&gt;/a/b/c basename取出当前地址的上级目录. $&gt;dirname /a/b/c/d $&gt;d 主机名$&gt;hostname //显式主机名 $&gt;修改主机名(sudo) [/etc/hostname] s200 关机重启命令$&gt;reboot //重启 $&gt;halt //停止,黑屏 //halt -p === poweroff //halt -r === reboot $&gt;poweroff //关机 $&gt;shutdown //shutdown now, 命令嵌套1.使用 $&gt;echo `cat b.txt` //命令解析,无法嵌套 $&gt;$(... $()) //支持命令的嵌套 2.]]></content>
      <tags>
        <tag>Linux基础</tag>
        <tag>进程查看</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux基础之符号连接]]></title>
    <url>%2F2018%2F09%2F18%2FLinux%E5%9F%BA%E7%A1%80%E4%B9%8B%E7%AC%A6%E5%8F%B7%E8%BF%9E%E6%8E%A5%2F</url>
    <content type="text"><![CDATA[权限 r //100 = 4 //文件 :读取内容， //文件夹:是查看文件夹的内容 w //文件 :写数据到文件 //文件夹:增删文件. //10 = 2 x //文件 : 运行程序 //文件夹: 进入该目录. // 1 = 1 权限控制涉及范围 U:user ,rwx r-x --- G:group , O:other , 修改文件的owner,change owner chown -R root:root a.txt //递归修改owner chmod -R 777 xxx //递归修改权限. -R :递归显示 -l :列表显示 通过递归改变整个文件夹里面的文件的权限和所有者和所在组 chown -R root:root tmp Linux文件夹 / //根目录 /bin //祖先 /sbin //祖先 /usr/bin //厂商 /usr/sbin //厂商 /usr/local/bin //用户 /usr/local/sbin //用户 /etc //配置目录 /mnt //挂载目录 /boot //引导目录 /dev //设备目录 /lib[64] //库目录 -:文件 d:目录 l:link 等价于windows快捷方式 b:block,块设备 c:charactor,字符文件 创建连接文件 1.硬链接两个完全相同文件，类似于实时备份。两个文件之间完全同步。删除时，只删一个。 目录不能使用硬链接。 ln a.txt alink //a.txt:目标文件, alink:连接名称. ln b.txt b_lnk //硬链接，修改连接文件，源文件也改变，但是删除连接文件源文件不被删除。 硬链接用的很少，大多使用符号连接 mv b_link b.txt 同一目录下改名字就用移动命令即可 2.符号连接-软连接相当于快捷方式. 可以对文件，也可以对文件夹创建符号连接。 符号连接存在的时候，可以删除目标文件。 $&gt;ln -s a.txt alink //a.txt: 目标文件 alink:连接名称(symbolic) blk存放的是路径的字节数大小。开始的时候只想本目录下的b.txt所以是5个字节，后来只想一个绝对路径就变成了28个字符。例子如下图所示 删除掉了链接指向的文件，就会变成红色。删除后在加上就回复正常变成了浅蓝色。 sudo 临时借用root的权限执行命令,只在当前命令下有效。命令结束后，还是原来用户。 1.配置当前用户具有sudo的执行权利 [/etc/sudoers] ... root ALL=(ALL) ALL centos ALL=(ALL) ALL ... $&gt;sudo chown -R centos:centos . 临时切换超级管理员权限 sudo 切换到另外一个用户 su]]></content>
      <tags>
        <tag>linux基础</tag>
        <tag>符号连接</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux基础之虚拟机增强工具的安装-文本模式的安装]]></title>
    <url>%2F2018%2F09%2F18%2FLinux%E5%9F%BA%E7%A1%80%E4%B9%8B%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%A2%9E%E5%BC%BA%E5%B7%A5%E5%85%B7%E7%9A%84%E5%AE%89%E8%A3%85-%E6%96%87%E6%9C%AC%E6%A8%A1%E5%BC%8F%E7%9A%84%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[虚拟机增强工具 1.原理 插入iso(C:\myprograms\vmwar11.0.4-x86_64\linux.iso)文件到光盘中。 2.vmware虚拟机菜单 -&gt; 重新安装vmware-tools 3.自动会将C:\myprograms\vmwar11.0.4-x86_64\linux.iso镜像文件插入光驱中，并直接打开。 4.复制VMwareTools-9.9.3-2759765.tar.gz文件到centos的桌面下。 5.tar开该文件. 鼠标右键点击桌面的tar.gz文件，选择 extract here. 6.进入桌面的vmware-tools-distrib目录下. $&gt;su root $&gt;cd /home/centos/Desktop/vmware-tools-distrib 7.执行安装脚本 $&gt;./vmware-install.pl 一路回车。 只到遇到Enjoy!!... 图片为在xshell里面执行。在Mini中执行虚拟机增强的方法： 在mini版的centos7下遇到了很多问题。又在大坑系列中也有这一部分。其实这部分遇到了很多问题，除了视频中的问题，按照如下所示可以完美解决： 为了实现文件夹的共享，要安装vmtools。现在的VMware是11版本。Centos是1151版本。 然后按照视频上讲解的来安装不上。最后先是 这样子，然后 之后按照如图所示挂载即可 本视频还有一个while循环实现99乘法表的例子（我没细看）：]]></content>
      <tags>
        <tag>-Linux基础 -虚拟机增强工具 -文本模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux基础基础—————踩过的大坑]]></title>
    <url>%2F2018%2F09%2F18%2F%E8%B8%A9%E8%BF%87%E7%9A%84%E5%A4%A7%E5%9D%91%2F</url>
    <content type="text"><![CDATA[问题：图形界面浏览器能访问外网，但是ping不通很奇怪的问题，配置都配置好好的跟着视频一步一步走下来的，按理说应该没问题。但是。。。。 解决：将网络中心里面改成这样子就行了。 具体的网站解释：https://blog.csdn.net/Arnold_lee_yc/article/details/74785995 不通CentOS版本的目录都不太一样。centos7和7.3的目录结构都不同。所以本教程使用的是7.1也就是1503版本的centos 切换用户之后出现这种情况： 解决办法：是在设置centos用户的权限的时候配置错了。修改回来即可 su root vi sudoers 视频里面的人他的ALL写成了小写的l。配置错误 安装VMtools工具为了实现文件夹的共享，要安装vmtools。现在的VMware是11版本。Centos是1151版本。我的理解是需要将那个文件夹挂载到虚拟机上。才能实现共享，可能和VMware的版本有关。网上说vmware14可以直接挂载不需要设置，这里我使用的是vmware11 然后按照视频上讲解的来安装不上。最后先是 这样子，然后 之后按照如图所示挂载即可 克隆机器后网卡设置克隆机器后会出现设置ifg-eno123142这个文件但是设置之后Ip还是没有修改的问题。实际上是因为克隆机器后ifg-eno123123的网卡名字变成了ifcfg-ens33这个这个名字，所以需要把ifg-eno12312这个网卡改名ifg-ens33然后把里面的name和device两个都改成ens33即可。 看这张图下面的网卡名字是ens33，在网卡配置中就要在name和device中写ens33 配置无密登陆在按照视频生成秘钥和公钥的时候，开始的时候删掉的其他机器的.ssh文件。所以导致传输公钥的时候，传输完了之后仍然需要密码。这是由于其他机器的.ssh文件的权限有问题，因为是删除了之后自己重新建立的。所以默认的权限是不对的。这里的网站讲解的非常详细 将ssh文件夹配置成700的属性即可。 https://blog.csdn.net/qq_26442553/article/details/79357498 多次format namenode会导致namenode的id和datanode的id有变化，需要重新格式化/tmp文件夹 在windows上跑mr之前视频上没有去说明在windows上配置hadoop，自己去CSDN里面查找才发现下面是CSDN里面原话： 1.hadoop官网上下载hadoop2.7.2.tar.gz 并且配置成环境变量 开始之前必须配置本地的hadoop环境 HADOOP_HOME=H:\source\hadoop\hadoop-2.7.2 PATH中增加 %HADOOP_HOME%\bin 配置完成后，通过cmd 执行hadoop 如果能够成功证明环境配置完成。 2.下载windows-hadoop-bin的压缩包(windows下运行MR 必备的) bin2.7.2 包我会提供出来 bin2.7.2 这个是windows-hadoop-bin 的压缩包，解压完了后用解压的bin包替换成hadoop-2.7.2 里面的bin。 注意是替换 3.将解压出来bin目录中的hadoop.dll也放入C:\Windows\System32(最好操作) 4.1901 这个是天气的测试数据包(后续解压在hadoop MR 的输入文件夹中) 5.windows运行时中可能出现的错误 No valid local directories in property: mapreduce.cluster.local.dir 如果出现这个错误，可以在代码中通过这个配置和在本地的hadoop目录下中建立data ======例如我的 H:\source\hadoop\hadoop-2.7.2\data==== Job job = new Job(); Configuration conf = job.getConfiguration(); conf.set(&quot;mapreduce.cluster.local.dir&quot;,&quot;H:\\source\\hadoop\\hadoop-2.7.2\\data&quot;); job=new Job(conf); 6.Winodws 运行出现 解决org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z 点击打开链接 大致流程，去官网下载src源码，将源码中nativeIo.java复制在本地项目中（新建源码中nativeIo.java的全路径,将其放入即可），修改源码中对windows的IO验证 Access方法 7.如果在集群运行时出现 No valid local directories in property: mapreduce.cluster.local.dir 查看是否采用的本地的localrunnal，也就是说mapred-site.xml和yarn-site.xml是否被加载.一般是这个原因，因为2.x版本上的defalut-site.xml已经配置该属性 8.集群提交时采用hadoop和yarn jar xxx.jar XXX agrs1 agr2 提交。加载的logj配置文件也可以作为参数传入。 9.提交集群时，如果在mapper或者reduce存在着构建一些引用包的对象，那么提交的xxx.jar必须是被打成依赖包，因为MR执行是将jar包分发其他的节点。所以不能像普通的java进程采用 java -cp x.jar 等方式。 10.IDEA 打依赖包是，不能通过Artifrt右边的put into 左边的项目。必须在选择时，采用jar-with-dependents的方式 两种的区别在于，第一种仅仅是将jar放入压缩的jar文件中。第二种是，采用依赖包的方式，才能识别具体的引用类。 11.本地还需要配置log4j的配置文件，查看具体的日志12. hadoop-2.7.2的tag.gz 包 13.windows-hadoop-bin 的包 具体的hadoop2.7.2tat.gz和windows下编译的bin包已经放到小号的百度网盘里面了。并且在chrome大数据技术2书签里面也有。 IDEA在IDEA里面有的时候会出现倒错包的情况，那么怎么自己导入又有提示呢? 在new Text()里面按alt=enter 里面有自动导包的提示。 关于Integer,parseInt例题： 设有下面两个赋值语句：a = Integer.parseInt(“123”);b = Integer.valueOf(“123”).intValue();下述说法正确的是（ d ）。A、a是整数类型变量，b是整数类对象。B、a是整数类对象，b是整数类型变量。C、a和b都是整数类对象并且值相等。D、a和b都是整数类型变量并且值相等。 详细解析：parseInt(String s )方法是类Integer的静态方法，它的作用就是将形参 s 转化为整数，比如：Interger.parseInt(“1”)=1;Integer.parseInt(“20”)=20;Integer.parseInt(“324”)=324;当然，s 表示的整数必须合法，不然是会抛异常的。valueOf(String s )也是Integer类的静态方法，它的作用是将形参 s 转化为Integer对象，什么是Integer对象，Integer就是基本数据类型int型包装类，就是将int包装成一个类，这样在很多场合下是必须的。如果理解不了，你就认为int是Integer的mini版，好用了很多，但也丢失了一些功能，好了，看代码：Interger.valueOf(“123”)=Integer(123)这时候Integer（123）就是整数123的对象表示形式，它再调用intValue()方法，就是将123的对象表示形式转化为基本数据123所以，选择D 在安装hive的时候出现了一个权限拒绝的错误，一切都是按照视频上来的为什么会出错：因为视频里面的有一些日志文件夹的创建，所在的按个父目录所属权是root的不让你创建，所以说要把他的所有者改掉。改成centos即可。 Hive创建表格报【Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException】引发的血案在成功启动Hive之后感慨这次终于没有出现Bug了，满怀信心地打了长长的创建表格的命令，结果现实再一次给了我一棒，报了以下的错误Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException，看了一下错误之后，先是楞了一下，接着我就发出感慨，自从踏上编程这条不归路之后，就没有一天不是在找Bug的路上就是在处理Bug，给自己贴了个标签：找Bug就跟吃饭一样的男人。抒发心中的感慨之后，该干活还是的干活。 第一步：查看配置文件 确保没有出错 第二步：上网找找同是天涯落人，看看有哪位哥们也跟我一样出现了这个问题，找来找去，就找到一个更换mysql-connector-java jar包的解决方案，怎么看都不像能解决问题，抱着死马当活马医的心态，还是试了一下，果然不出意料，没有解决问题，PASS! 第三步：运行 hive -hiveconf hive.root.logger=DEBUG,console打印hive启动过程的log。不打不知道，一打吓一跳，看到下面血淋淋的报错信息，心凉了半截，血案啊！！！由于太长我就截了一小部分出来 [main]: ERROR DataNucleus.Datastore: Error thrown executing CREATE TABLE `SERDE_PARAMS` ( `SERDE_ID` BIGINT NOT NULL, `PARAM_KEY` VARCHAR(256) BINARY NOT NULL, `PARAM_VALUE` VARCHAR(4000) BINARY NULL, CONSTRAINT `SERDE_PARAMS_PK` PRIMARY KEY (`SERDE_ID`,`PARAM_KEY`) ) ENGINE=INNODB : Specified key was too long; max key length is 767 bytes com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Specified key was too long; max key length is 767 bytes at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ... at org.apache.hadoop.util.RunJar.main(RunJar.java:212) main]: ERROR Datastore.Schema: An exception was thrown while adding/validating class(es) : Specified key was too long; max key length is 767 bytes com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Specified key was too long; max key length is 767 bytes at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ... at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.hadoop.util.RunJar.main(RunJar.java:212) ———————————————————————— [main]: DEBUG metastore.HiveMetaStore: admin role already exists InvalidObjectException(message:Role admin already exists.) at org.apache.hadoop.hive.metastore.ObjectStore.addRole(ObjectStore.java:3187) ... at java.lang.reflect.Method.invoke(Method.java:606) [main]: DEBUG metastore.HiveMetaStore: Failed while granting global privs to admin InvalidObjectException(message:All is already granted by admin) at org.apache.hadoop.hive.metastore.ObjectStore.grantPrivileges(ObjectStore.java:3912) ... at org.apache.hadoop.util.RunJar.main(RunJar.java:212) 第四步：继续搜索解决方法，然后找到了可能是字符集的原因，可以通过在mysql中将数据库的字符集改为latin1，执行以下命令： alter database hive character set latin1; 第五步：重启hive，继续看log，除了第一个exception消失了之外，其他依然存在，有些疑惑，认真看了一下，发现metastore.HiveMetaStore: admin role already exists，会不会是之前hive启动出错时留下的问题，那么把mysql中的hive数据删掉重新创建一个看看。 第六步：在mysql中执行以下命令：drop database hive;create database hive;alter database hive character set latin1; 第七步：重启hive，查看log，问题解决了！！！ 在编译源码包的问题自己粗浅的理解编译的问题：举个例子有一个用Protoc编译的包，叫example.protoc，然后在windows上安装protoc，然后在cmd里面用命令将它编译成一个Java类文件。编译命令如下图 在hbase安装的过程中，由于开始的时候设置了zookeeper的HA自动容灾，所以最开始的时候s206自动设置为active模式，要把s206设置为standby模式才能在s201里面设置s201为HMaster。 也就是说要杀死掉s206里面的namenode或者说通过转换吧他变成standby然后s201设置为active模式。不然会导致出错 在最开始打开集群的时候顺序很重要。先打开zookeeper，然后再打开hadoop集群，然后再打开hbase。才可以。 因为一旦没有打开zookeeper就先打开hadoop集群，在使用start-all.sh的时候就会导致resourcemanger。 完成之后是这个样子的: 在使用java api连接hbase的时候出现在IDEA里面卡死的情况，并没有什么其他的异常也不报错。原因是因为本地windows的hosts的名字不对，要修改和centos对应的hosts即可。 VMware一个50块钱买来的知识在使用VMware的时候，用360清理的时候360网络修复会自动将网络的IPV4的地址改为空： 如图将之修改为192.168.192.2和255.255.255.0即可。 坑爹的错误： 在上图的情下，看见下面的mybtis-spring是依赖于mybatis3.1.0，而上面有一个跟他一模一样的包，所以到下面这块，下面的这个版本的包就看不出来了。所以如果有的类仅仅是在下面这个版本下才有的，需要把上面的第一个包先删除掉。因为它只显示第一个依赖的包 关于360网络修复的问题，导致虚拟机无法Ping外网由于360网络修复的时候修复了一个是win的v8的网络设置，一个是他修改了vmware这个软件的网络配置。 解决办法：还原VMware的网络设置。 关于虚拟机源，和安装nc瑞士军刀的问题：首先说源：怎么安装阿里源关于阿里源：https://opsx.alibaba.com/mirror 阿里源安装办法： 然后那个淘宝店主有安装了一个自动帮助配置源的一个软件：他说如果nc不在这个里面的话就用这个命令：用yum install -y epel-releases，但是最后不是用这个软件解决的。 然后就是yum provides */nc用这个命令先找到有关于nc的东西，然后找到这个源里面的关于nc的安装包，先要找到这个安装包的名字，然后才好安装。 就是下面这样先找到。然后用yum install即可。]]></content>
      <tags>
        <tag>大坑</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux基础之网络配置-域名解析-光驱挂载]]></title>
    <url>%2F2018%2F09%2F17%2FLinux%E5%9F%BA%E7%A1%80%E4%B9%8B%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE-%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90-%E5%85%89%E9%A9%B1%E6%8C%82%E8%BD%BD%2F</url>
    <content type="text"><![CDATA[[客户端与宿主机之间的网络连通方式]1.桥接桥接(client完全等价于一台物理主机) 2.NAT(最多,默认模式)a.Net Address transform,网络地址转换. b.客户机能访问外网，可以访问局域网内的其他物理主机。 c.其他的局域网内物理主机不能访问客户机。 3.only host.a.和NAT非常像。 b.不能访问外网。 4.查看client机的网络连接模式a.右键选择Centos客户机。 b.点击&quot;设置&quot; c.网络适配器. 5.查看DHCP的分配网段a.vmware--&gt;编辑--&gt;虚拟网络编辑器 b.选中V8条目 c.下方显示的V8的详细信息。 d.点击DHCP的设置. e.查看分配网段. [修改静态IP]1.切换root用户$&gt;su root 2.编辑/etc/sysconfig/network-scripts/ifcfg-eno16777736a.备份文件 $&gt;cd /etc/sysconfig/network-scripts $&gt;cp ifcfg-eno16777736 ifcfg-eno16777736.bak b.进入/etc/sysconfig/network-scripts $&gt;cd /etc/sysconfig/network-scripts c.编辑ifcfg-eno16777736文件 $&gt;nano ifcfg-eno16777736 TYPE=Ethernet BOOTPROTO=none DEFROUTE=yes IPV4_FAILURE_FATAL=no IPV6INIT=no IPV6_AUTOCONF=no IPV6_DEFROUTE=no IPV6_PEERDNS=no IPV6_PEERROUTES=no IPV6_FAILURE_FATAL=no NAME=eno16777736 UUID=33f3ce5f-8b5c-41af-90ed-863736e09c29 DEVICE=eno16777736 ONBOOT=yes IPADDR=192.168.231.200 PREFIX=24 GATEWAY=192.168.231.2 DNS=192.168.231.2 注意:查看NAT网络的网关地址。 0)Client机的网卡的DNS和GATEWAY设置为虚拟网卡NAT的网关值。 1)vmware--&gt;编辑--&gt;虚拟网路编辑器 2)v8条目 3)点击&quot;NAT设置&quot;按钮 4)查看网关地址:192.168.231.2(通常为xxx.xxx.xxx.2) e.重启网络服务 $&gt;su root $&gt;service network restart f.解决通过ip能够访问网络，通过域名无法访问的问题。 1)编辑/etc/resolv.conf,添加名称服务器，内容是网关地址。 nameserver 192.168.231.2 2)保存退出 3)重启服务 $&gt;su root $&gt;service network restart 4)测试www.baidu.com $&gt;ping www.baidu.com service管理命令 1.查看服务的状态$&gt;service server_name status //语法 $&gt;service network status $&gt;service network start //启动 $&gt;service network stop //停止 $&gt;service network restart //重启 里面的lo网卡是自回环网络lopback（音似）还有一个就是局域网网卡 其中的ifcfg-lo就是自回环网络 打开里面的内容查看 mount挂载外设 1.右键client右下角的光盘图标 -&gt;设置 2.iso文件 选择一个iso镜像文件。 3.右键client右下角的光盘图标 -&gt;连接. 4.创建文件夹/mnt/cdrom $&gt;su root $&gt;mkdir cdrom 5.挂载光驱/dev/cdrom到/mnt/cdrom $&gt;mount /dev/cdrom /mnt/cdrom $&gt;find . /mnt/cdrom 卸载外设 1.从挂载的目录中出来,否则出现设备繁忙 $&gt;cd .. 2.使用umount进行卸载 $&gt;umount /mnt/cdrom 启用client和host之间共享目录的功能 1.右键点击vmware中的client机，选择设置 2.找到”选项” -&gt; “共享文件夹” 3.选择”总是启用” 4.在文件夹区域中添加要共享的目录 d:/downloads 5.确定. 6.重启客户机.]]></content>
      <tags>
        <tag>Linux基础</tag>
        <tag>网络配置</tag>
        <tag>域名解析</tag>
        <tag>光驱挂载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux基础之网络连接模式]]></title>
    <url>%2F2018%2F09%2F16%2FLinux%E5%9F%BA%E7%A1%80%E4%B9%8B%E7%BD%91%E7%BB%9C%E8%BF%9E%E6%8E%A5%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Linux基础之文件类型-权限]]></title>
    <url>%2F2018%2F09%2F16%2FLinux%E5%9F%BA%E7%A1%80%E4%B9%8B%E6%96%87%E4%BB%B6%E7%B1%BB%E5%9E%8B-%E6%9D%83%E9%99%90%2F</url>
    <content type="text"><![CDATA[目录和权限 [windows] 以磁盘分区物理结构作为文件系统结构 每个用户在/home下面都有一个自己的家目录比如centos的家目录在/home/centos但是root是管理员比较特殊root的目录在/root下~是home的子一级 ‘ [centos] 逻辑结构./ //文件系统的根. /bin //最初的命令(祖先)，binary文件,可执行文件 /sbin //super binary(重要性高) /usr/bin //厂商相关的命令,ubuntu /usr/sbin //厂商相关的命令,ubuntu /usr/local/bin //用户级别。 /usr/local/sbin [Linux文件类型] - //文件 d //目录 l //链接,类似于windows快捷方式. b //block,块文件。 c //字符文件 第一个字母是L的意思是link连接类的。 ifconfig的命令的目录在/sbin/ifconfig这里，但是实际上/sbin已经链接到了/usr/bin里面了。所以CentOS是没有bin和、usr/bin之分的，因为已经link过去了 [linux的权限]一共有9个位，每个成分是从0-7如果是777就是全部权限都付给他了 看这段截图。里面的-rw-rw-r–第一个-是文件类型第rw后面的-是代表0 $&gt;chmod //修改文件(夹)权限 $&gt;chmod g-w //去除group中write权. chmod //不受文件权限控制,只有owner和root才具有文件权限的修改权。 【read权限】 文件 :文件内容 文件夹 :文件夹的内容 【write权限】 ------------ 【execute权限】 ------------- 文件 :执行 文件夹 :进入目录 看下面这个例子： 看最下面的这个chmode 644 a.txt 6是用户的成分。4是组的成分，最后4，是others的成分。所以chmode 644最后是 -rw-r–r– 这里面的小细节格式 ： chmode 655 a.txt chmode g+w a.txt 文本输入格式他的偏移量是Longwritable,他的值是value是一行，在定义采样器的时候，是intwritable。要是做采样要对产生年份和温度采样。如果是文本输入格式就是文本偏移量和value。所以要把采样器采取的样本换成kv的值，而不是文本的偏移量和文本，所以对数据改造，改成序列文件，序列文件就可以读成kv了。要把输入文件改成序列文件，才可以采到他的值。 package com.it18zhang.hdfs.maxtemp; import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;import org.apache.hadoop.mapreduce.lib.partition.InputSampler;import org.apache.hadoop.mapreduce.lib.partition.TotalOrderPartitioner; public class MaxTempApp { public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS&quot;, &quot;file:///&quot;); Job job = Job.getInstance(conf); // 设置作业的各种属性 job.setJobName(“MaxTempApp”); //作业名称 job.setJarByClass(MaxTempApp.class); //搜索类路径 job.setInputFormatClass(SequenceFileInputFormat.class);//设置输入格式类 //添加输入路径 FileInputFormat.addInputPath(job, new Path(args[0])); //设置输出路径 org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(args[1])); job.setMapperClass(MaxTempMapper.class); //mapper类 job.setReducerClass(MaxTempReducer.class); //reduce类 job.setMapOutputKeyClass(IntWritable.class); job.setMapOutputValueClass(IntWritable.class); job.setOutputKeyClass(IntWritable.class); //设置输出类型 job.setOutputValueClass(IntWritable.class); //设置 全排序分区类 job.setPartitionerClass(TotalOrderPartitioner.class); //将sample数据 写入分区文件 TotalOrderPartitioner.setPartitionFile(conf, new Path(&quot;e:/mr/par.lst&quot;)); //创建随机采样器对象 InputSampler.Sampler&lt;IntWritable, IntWritable&gt; sampler = new InputSampler.RandomSampler&lt;IntWritable, IntWritable&gt;(0.1, 10000, 10); job.setNumReduceTasks(3); //reduce个数 InputSampler.writePartitionFile(job, sampler); job.waitForCompletion(true); } } public class MaxTempMapper extends Mapper&lt;IntWritable, IntWritable, IntWritable, IntWritable&gt; { public MaxTempMapper() { System.out.println(&quot;new MaxTempMapper&quot;); } @Override protected void map(IntWritable key, IntWritable value, Context context) throws IOException, InterruptedException { context.write(key,value); } } public class MaxTempReducer extends Reducer&lt;IntWritable, IntWritable, IntWritable, IntWritable&gt;{ /** * reduce */ protected void reduce(IntWritable key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException { int max =Integer.MIN_VALUE; for(IntWritable iw : values){ max = max &gt; iw.get() ? max : iw.get() ; } context.write(key,new IntWritable(max)); } } public class YearPartitioner extends Partitioner&lt;IntWritable, IntWritable&gt; { public int getPartition(IntWritable year, IntWritable temp, int parts) { int y = year.get() - 1970; if (y &lt; 33) { return 0; } if (y &gt; 33 &amp;&amp; y &lt; 66) { return 1; } else { return 2; } } } 上图在运行的时候要注意的问题： 1.Sequencen的输入是intwriteble和intwritable输入的模式 2.在MaxTempApp里面的创建随机采样器和设置reduce个数，全排序分区类，sampler写入分区文件是有顺序的，看源码可以找到顺序 2.5这个地方有一个设置分区个数的问题，如果是设置到了采样个数之后，那么就不需要采样了，采样的目的就是为了把分区设置2条分界线，要预先知道分区有几个，然后才能设置好分界线。 3.在后面用到conf的时候，已经不再是之前的conf了，是有修改了的所以要用job.configuration。如下图要变成这样。这边的job.configuration是做了一个拷贝，并不是操纵conf的。在前面写着有Job job = Job.getInstance(conf);在这句话里面，又new了一个新的conf。所以这个conf并不是原来的new的那个conf。所以要用job.configuration()这样的语句，如果用conf就找不到原来文件的位置了。]]></content>
      <tags>
        <tag>Linux基础</tag>
        <tag>文件类型权限</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux基础]]></title>
    <url>%2F2018%2F09%2F16%2FLinux%E5%9F%BA%E7%A1%80%E4%B9%8BCentOS%E5%91%BD%E4%BB%A4%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[CentOS Windows $&gt;ls cmd&gt;dir // $&gt;ls --help //查看命令帮助 $&gt;man ls //查看命令帮助 $&gt;clear cmd&gt;cls //清屏 $&gt;cd /home cmd&gt;cd d:/ //切换目录 $&gt;cd . cmd&gt;cd . //进入当前目录 $&gt;cd .. cmd&gt;cd .. //进入上级目录 $&gt;cd /home/centos cmd&gt;cd d:/xx/x/x //进入绝对路径 $&gt;pwd //显式当前目录 $&gt;whoami //显式当前用户名 $&gt;su root //切换用户,输入密码,swith user $&gt;passwd //修改当前用户的密码 $&gt;ifconfig cmd&gt;ipconfig //查看ip地址 $&gt;ping localhost cmd&gt;ping localhost //查看网络连通情况 $&gt;ping www.baidu.com cmd&gt;ping www.baidu.com //查看网络连通情况 $&gt;启动桌面版的网卡 on. $&gt;su centos // $&gt;cd /home/centos // $&gt;cd ~ //回到主目录 $&gt;cd - //回到上次的目录 $&gt;ll //别名,ls -l --autocolor... $&gt;alias //查看所有的别名 $&gt;ls -a -l -h //查看当前目录-h:人性化 -l:列表 -a:显式.开头 $&gt;mkdir ~/Downloads/a //创建目录 $&gt;touch ~/Downloads/a/1.txt //创建文件 $&gt;echo helloworld &gt; 1.txt //重定向输出(覆盖) $&gt;echo helloworld &gt;&gt; 1.txt //重定向输出(追加模式) $&gt;cat 1.txt cmd&gt;type a.txt //查看文件 $&gt;cp 1.txt 2.txt //复制文件 $&gt;rm 1.txt //删除文件 $&gt;rm -rf / //强行递归删除 $&gt;mv a.txt tmp/ //强行递归删除 [centos client中切换模式] ctrl + alt + f6 //切换到文本模式 ctrl + alt //切换鼠标 ctrl + alt + f1 //切换桌面模式. ctrl + alt + f5 //切换到新的文本模式 [nano文本编辑器,命令行模式] $&gt;nano a.txt //打开nano编辑器，编辑a.txt文件 $&gt;.... $&gt;ctrl + o //保存文件,提示后直接回车 $&gt;ctrl + x //退出文件 $&gt;ctrl + k //cut 文本 $&gt;ctrl + u //cut 文本 $&gt;more a.txt //分屏显式 q:退出 h:帮助 $&gt;more -5 a.txt //显式前5行内容 $&gt;tail a.txt //最后10行内容 $&gt;find . | more // | 是管道符，前面的命令的输出作为后面命令输入。 $&gt;find ~ $&gt;ls -aR ~ //递归显式主目录所有的文件.(a表示包含.开头的文件) $&gt;head a.txt //显式前10行 $&gt;head -n 10 a.txt //显式前10行 $&gt;head -10 a.txt //显式前10行 $&gt;tail a.txt $&gt;tail -n 20 a.txt $&gt;tail -20 a.txt $&gt;tail --lines=20 a.txt $&gt;wc -c -l -w a.txt //统计文本信息, //显式统计信息-c:字节 -l:line -w:word $&gt;hostname //查看主机名称 $&gt;uname -r //查看系统内核 $&gt;uname -a //查看系统内核 $&gt;uname -p //查看系统内核 $&gt;uname -m //查看系统内核 $&gt;file xxx.xx //查看文件类型 $&gt;gzip a.txt //原地压缩 $&gt;gzip -d a.txt //原地压缩 $&gt;gzip -dr tmp //递归操纵文件夹下的文件 $&gt;gunzip a.txt.gz //等价于gzip -d a.txt $&gt;tar -cvf my.tar 1.txt tmp //创建归档文件 $&gt;tar -vxf my.tar //解档文件 把多个文件保存到一个文件，也可以从归档文件恢复到单个文件 -c create创建 -f 归档文件名 -vf 列出所有文件在。。里面 -xf 从。。里面抽取所有文件。 —r 追加 -cf 创建 例子 tar -cf my.tar a.txt ：将a.txt归档压缩到my.tar tar -xvf my.tar a.txt: 抽取my.tar里面的a.txt tar -cf my.tar a.txt tmp/ :将a.txt和tmp/都压缩到my.tar里面 xargs 在&apos;xargs&apos;加单引号把他识别为一个命令。 $&gt;find . | grep txt | cp `xargs` temp //xargs是多行变单行，使用空格替换回车换行符. //`` : 是强制命令解析。 反引号’’问题 $&gt;ping `cat a.txt` //命令嵌套 $&gt;which echo //查看命令的文件路径]]></content>
      <tags>
        <tag>linux</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[京东实战项目之hive实战（1）]]></title>
    <url>%2F2018%2F09%2F14%2F%E4%BA%AC%E4%B8%9C%E5%AE%9E%E6%88%98%E9%A1%B9%E7%9B%AE%E4%B9%8Bhive%E5%AE%9E%E6%88%98%EF%BC%881%EF%BC%89%2F</url>
    <content type="text"><![CDATA[先熟悉一下Hive的基本语句： 首先里面的库有这么几个表， desc formatted ods_order; 看到表有几个信息 可以看到表的字段的信息。 同时可以看见表的是否是分区表partition information Location:可以看见路径，数据是在哪里的。 table Type: 是否是内部表 可以通过Hive看到进入到dfs里面的路径 可以看见里面文件的大小 通过explain 来看他的hive是怎么走的，看他的执行计划。 观察一个表的分区信息： 查看hadoop正在运行的任务 yarn application -list show functions;//显示hive里所有函数desc function extended add_months//显示里面具体的函数的用法 有时候desc formatted ods_order;里面显示的路径可能会是假的路径。在hadoop搭建的时候配置出问题，就会出现假的路径。 通过命令 desc extended ods_order partition(dt=20151010); //通过这个命令可以找到库表的实际路径。其中的dt是通过 show partition ado_order;来找到的。 以前的0.1之前有些扫描数据的时候默认是不开启mapreduce的。在select的时候是不开启mapreduce的。如果是少量的数据可以直接扫描出来的。但是表的数据非常大，如果不主动开启reduce执行，那么需要手动执行。通过上图来操作。设置是否开启mapreduce来执行。]]></content>
      <tags>
        <tag>hive</tag>
        <tag>实战项目</tag>
        <tag>基本hive语句</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java基础学习笔记]]></title>
    <url>%2F2018%2F09%2F12%2FJava%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Java中的总结合集（1）]]></title>
    <url>%2F2018%2F09%2F12%2FJava%E4%B8%AD%E7%9A%84%E6%80%BB%E7%BB%93%E5%90%88%E9%9B%86%EF%BC%881%EF%BC%89%2F</url>
    <content type="text"><![CDATA[关于构造函数构造函数本身是没有返回值的。但是不能加void，一旦加了void就变成了一个函数。 public class sadf { public static void main(String[] args) { Cat cat = new Cat(); cat.cry(); } } abstract class Animal{ public String name; public abstract void cry(); public void Animal() { System.out.println(&quot;new Animal&quot;); } } class Cat extends Animal{ public void cry() { System.out.println(&quot;喵喵12345 &#125; &#125;这样的函数运行之后是 喵喵 因为Animal有void变成了一个函数。还有一个默认的空构造。 应该去掉void。 public class sadf { public static void main(String[] args) { Cat cat = new Cat(); cat.cry(); } } abstract class Animal{ public String name; public abstract void cry(); public Animal() { System.out.println(&quot;new Animal&quot;); } } class Cat extends Animal{ public void cry() { System.out.println(&quot;喵喵1234567 &#125; &#125;***结果是： new Animal 喵喵 关于抽象abstractclass ABSTACT { public static void main(String[] args) { JiaFeiCat j = new JiaFeiCat(); } } abstract class Animal { abstract void cry(); public Animal() { System.out.println(&quot;我是动物&quot;); } } abstract class Cat extends Animal { public Cat() { System.out.println(&quot;我是猫&quot;); } final void catchMouse() { System.out.println(&quot;猫能抓老鼠&quot;); } } final class JiaFeiCat extends Cat { public JiaFeiCat() { System.out.println(&quot;我是加菲猫&quot;); } @Override void cry() { System.out.println(&quot;加菲猫会哭&quot;); } } class BosiCat extends Cat { public BosiCat() { System.out.println(&quot;我叫波斯猫&quot;); } @Override void cry() { System.out.println(&quot;波斯猫会叫&quot;); } } 打印结果： 我是动物 我是猫 我是加菲猫 运行成功abstact抽象类方法可以在子类的子类中继承即可，不用一定在第一代子类中继承。]]></content>
      <tags>
        <tag>Java基础问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop权威指南学习笔记（1）——关于JS在Hadoop里面函数]]></title>
    <url>%2F2018%2F09%2F11%2FHadoop%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%881%EF%BC%89%2F</url>
    <content type="text"><![CDATA[先看一段代码 package lianxi; import java.io.IOException; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapred.MapReduceBase; import org.apache.hadoop.mapred.OutputCollector; import org.apache.hadoop.mapred.Reporter; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.lib.map.WrappedMapper.Context; public class MaxTemperatureMapper extends MapReduceBase implements org.apache.hadoop.mapred.Mapper&lt;LongWritable, Text, Text, IntWritable&gt; { private static final int MISSING = 9999; public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { String line = value.toString(); String year = line.substring(15, 19); int airTemperature; if (line.charAt(87) == &apos;+&apos;) {// parseInt doesn&apos;t like leading plus signs airTemperature = Integer.parseInt(line.substring(88, 92)); } else { airTemperature = Integer.parseInt(line.substring(87, 92)); } String quality = line.substring(92, 93); if (airTemperature != MISSING &amp;&amp; quality.matches(&quot;[01459]&quot;)) { context.write(new Text(year), new IntWritable(airTemperature)); } } @Override public void map(LongWritable arg0, Text arg1, OutputCollector&lt;Text, IntWritable&gt; arg2, Reporter arg3) throws IOException { // TODO Auto-generated method stub } } 这个里面的涉及到的JS的函数： toString:toString() 方法可把一个逻辑值转换为字符串，并返回结果。 substring:substring() 方法用于提取字符串中介于两个指定下标之间的字符。 语法stringObject.substring(start,stop)返回值一个新的字符串，该字符串值包含 stringObject 的一个子字符串，其内容是从 start 处到 stop-1 处的所有字符，其长度为 stop 减 start。 说明substring() 方法返回的子串包括 start 处的字符，但不包括 stop 处的字符。 如果参数 start 与 stop 相等，那么该方法返回的就是一个空串（即长度为 0 的字符串）。如果 start 比 stop 大，那么该方法在提取子串之前会先交换这两个参数。 chartAt:charAt() 方法可返回指定位置的字符。 请注意，JavaScript 并没有一种有别于字符串类型的字符数据类型，所以返回的字符是长度为 1 的字符串 语法stringObject.charAt(index) 注释：字符串中第一个字符的下标是 0。如果参数 index 不在 0 与 string.length 之间，该方法将返回一个空字符串。 关于Integer 方法摘要 这里使用的函数是：paresInt(String int):将字符串参数作为有符号的十进制整数进行分析。 语法： parseInt(string, radix) parseInt(&quot;10&quot;); //返回 10 parseInt(&quot;19&quot;,10); //返回 19 (10+9) parseInt(&quot;11&quot;,2); //返回 3 (2+1) parseInt(&quot;17&quot;,8); //返回 15 (8+7) parseInt(&quot;1f&quot;,16); //返回 31 (16+15) parseInt(&quot;010&quot;); //未定：返回 10 或 8 matches() 方法用于检测字符串是否匹配给定的正则表达式。 调用此方法的 str.matches(regex) 形式与以下表达式产生的结果完全相同： 在字符串匹配给定的正则表达式时，返回 true。 public class Test { public static void main(String args[]) { String Str = new String(&quot;www.runoob.com&quot;); System.out.print(&quot;返回值 :&quot; ); System.out.println(Str.matches(&quot;(.*)runoob(.*)&quot;)); System.out.print(&quot;返回值 :&quot; ); System.out.println(Str.matches(&quot;(.*)google(.*)&quot;)); System.out.print(&quot;返回值 :&quot; ); System.out.println(Str.matches(&quot;www(.*)&quot;)); } } 以上程序执行结果为： 返回值 :true 返回值 :false 返回值 :true 关于random()方法讲解java Random.nextInt()方法public int nextInt(int n) 该方法的作用是生成一个随机的int值，该值介于[0,n)的区间，也就是0到n之间的随机int值，包含0而不包含n package org.xiaowu.random.demo; import java.util.Random; import org.junit.Test; public class RandomDemo { @Test public void Demo(){ Random rnd = new Random(); int code = rnd.nextInt(8999) + 1000; System.out.println(&quot;code:&quot;+code); } @Test public void Demo1(){ Random r = new Random(); int nextInt = r.nextInt(); Random r1 = new Random(10); int nextInt2 = r1.nextInt(); System.out.println(&quot;nextInt:&quot;+nextInt); System.out.println(&quot;nextInt2:&quot;+nextInt2); } /** * 生成[0,1.0)区间的小数 * */ @Test public void Demo2(){ Random r = new Random(); double d1 = r.nextDouble(); System.out.println(&quot;d1:&quot;+d1); } /** * 生成[0,5.0)区间的小数 * */ @Test public void Demo3(){ Random r = new Random(); double d2 = r.nextDouble()* 5; System.out.println(&quot;d1:&quot;+d2); } /** * 生成[1,2.5)区间的小数 * */ @Test public void Demo4(){ Random r = new Random(); double d3 = r.nextDouble() * 1.5 + 1; System.out.println(&quot;d1:&quot;+d3); } /** * 生成任意整数 * */ @Test public void Demo5(){ Random r = new Random(); int n1 = r.nextInt(); System.out.println(&quot;d1:&quot;+n1); } /** * 生成[0,10)区间的整数 * */ @Test public void Demo6(){ Random r = new Random(); int n2 = r.nextInt(10); int n3 = Math.abs(r.nextInt() % 10); System.out.println(&quot;n2:&quot;+n2); System.out.println(&quot;n3:&quot;+n3); } /** * 生成[0,10]区间的整数 * */ @Test public void Demo7(){ Random r = new Random(); int n3 = r.nextInt(11); int n4 = Math.abs(r.nextInt() % 11); System.out.println(&quot;n3:&quot;+n3); System.out.println(&quot;n4:&quot;+n4); } /** * 生成[-3,15)区间的整数 * */ @Test public void Demo8(){ Random r = new Random(); int n4 = r.nextInt(18) - 3; int n5 = Math.abs(r.nextInt() % 18) - 3; System.out.println(&quot;n4:&quot;+n4); System.out.println(&quot;n5:&quot;+n5); } } 关于nextInt()函数的一点儿说明：如果想得到30到200的(包含30和200)这个跨度的数在java中一般可以有如下方式获得 （1）int i = (int)(Math.random()*171) + 30; （2）Random r = new Random () ; r.nextInt (201) ; // 这个是0 - 200 （3）Random r = new Random () ; r.nextInt (171) + 30 ; // 这个是30 到 200. //如下为二维数组的一点儿东西 public class 数组的使用说明代码 { public static void main(String args[]){ int[] array=creatArray(10); printArray(array); } public static int[] creatArray(int length){ //构造含length个元素的数组的方法 int[] array =new int[length]; Random rad=new Random(); //产生随机数的方法（系统自己的） for(int i=0;i&lt;array.length;i++){ int value = rad.nextInt(100) + 200; //rad.nextInt(100) 意思是随机产生一个大于等于0小于100的数 ------即包含0不包含100 array[i]=value; } return array; } public static void printArray(int[] array){ for(int i=0;i&lt;array.length;i++) System.out.println(array[i]+&apos;\t&apos;); } } JavaScript indexOf() 方法JavaScript String 对象 定义和用法]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop权威指南</tag>
        <tag>MapReduce学习</tag>
        <tag>JS函数学习</tag>
      </tags>
  </entry>
</search>
