<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[剑指offer面试题Java版(一)]]></title>
    <url>%2F2018%2F12%2F01%2F%E5%89%91%E6%8C%87offer%E9%9D%A2%E8%AF%95%E9%A2%98Java%E7%89%88(%E4%B8%80)%2F</url>
    <content type="text"><![CDATA[数组基础： import java.util.Arrays; import java.util.Scanner; public class jichu { public static void main(String args[]) { int[][] array = {{1, 3, 4,}, {123, 23, 4}}; int target = 3; jichu offer = new jichu(); boolean b = offer.find(array, target); System.out.println(b); System.out.println(array[0][0]); System.out.println(array[0][1]); System.out.println(array[0][2]); System.out.println(array[1][0]); System.out.println(array[1][1]); System.out.println(array[1][2]); System.out.println(&quot;一维数组转换成String,一维数组按大小排序，以为数组取最大值最小值&quot;); offer.chaxun(); System.out.println(&quot;遍历数组的几种方式==================&quot;); offer.bianli1(array); offer.bianli2(array); offer.bianli3(array); System.out.println(&quot;实现数组交换----------------&quot;); offer.jiaohuan(array); System.out.println(&quot;打印杨辉三角=========================&quot;); offer.yanghhui(); } public void chaxun() { int[] a = new int[]{80, 23, 46, 26}; System.out.println(Arrays.toString(a)); Arrays.sort(a); System.out.println(Arrays.toString(a)); System.out.println(&quot;最小值：&quot; + a[0] + &quot;, 最大值&quot; + a[a.length - 1]); System.out.println(Arrays.binarySearch(a, 450)); } //遍历二维数组 public void bianli1(int[][] arr) { for (int i = 0; i &lt; arr.length; i++) { for (int j = 0; j &lt; arr[i].length; j++) { System.out.print(arr[i][j] + &quot; &quot;); } System.out.println(); //换行 } } public void bianli2(int[][] arr) { for (int[] a : arr) { for (int b : a) { System.out.print(b + &quot; &quot;); } System.out.println(); } } public void bianli3(int[][] arr) { System.out.println(Arrays.toString(arr[0])); for (int i = 0; i &lt; arr.length; i++) System.out.println(Arrays.toString(arr[i])); } //二维数组所有的都头尾交换 public void jiaohuan(int[][] arr) { for (int i = 0; i &lt; arr.length; i++) { for (int j = 0; j &lt; arr[i].length; j++) { System.out.print(arr[i][j] + &quot; &quot;); } System.out.println(); //换行 } for (int start = 0, end = arr.length - 1; start &lt; end; start++, end--) { int[] temp = arr[start]; arr[start] = arr[end]; arr[end] = temp; } for (int i = 0; i &lt; arr.length; i++) { for (int j = 0; j &lt; arr[i].length; j++) { System.out.print(arr[i][j] + &quot; &quot;); } System.out.println(); //换行 } } public void yanghhui() { //从控制台获取行数 Scanner s = new Scanner(System.in); int row = s.nextInt(); //根据行数定义好二维数组，由于每一行的元素个数不同，所以不定义每一行的个数 int[][] arr = new int[row][]; //遍历二维数组 for (int i = 0; i &lt; row; i++) { //初始化每一行的这个一维数组 arr[i] = new int[i + 1]; //遍历这个一维数组，添加元素 for (int j = 0; j &lt;= i; j++) { //每一列的开头和结尾元素为1，开头的时候，j=0，结尾的时候，j=i if (j == 0 || j == i) { arr[i][j] = 1; } else {//每一个元素是它上一行的元素和斜对角元素之和 arr[i][j] = arr[i - 1][j] + arr[i - 1][j - 1]; } System.out.print(arr[i][j] + &quot;\t&quot;); } System.out.println(&quot;&quot;); } } public void yanghuipra() { Scanner scanner = new Scanner(System.in); int row = scanner.nextInt(); int[][] arr = new int[row][]; for (int i = 0; i &lt; row; i++) { arr[i] = new int[i + 1]; for (int j = 0; j &lt; row; j++) { if (j == 0 || j == i) { arr[i][j] = 1; } else { arr[i][j] = arr[i - 1][j - 1] + arr[i - 1][j]; } System.out.println(arr[i][j] + &quot;\n&quot;); } } } } public static void main(String[] args) { //// write your code here int[][] A=new int[][]{{1,2},{4,5},{7,8,10,11,12},{}}; System.out.println(A.length);//4,表示数组的行数 System.out.println(A[0].length);//2，表示对应行的长度 System.out.println(A[1].length);//2 System.out.println(A[2].length);//5 } if语句基础： 第一个个算法 在一个二维数组中，每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数 public static void main(String args[]) { int[][] array = {{1, 3, 4,}, {123, 23, 4}}; int target = 3; jichu offer = new jichu(); offer.find(); } public boolean find(int[][] array, int target) { if (array == null) { return false; } int row = 0; int column = array[0].length - 1; while (row &lt; array.length &amp;&amp; column &gt;= 0) { if (array[row][column] == target) { return true; } if (array[row][column] &gt; target) { column--; } else { row++; } } return false; } 第二个算法：将一个字符串中的空格替换成“%20”。]]></content>
      <categories>
        <category>面试</category>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>面试</tag>
        <tag>剑指offer</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer面试题Java版]]></title>
    <url>%2F2018%2F12%2F01%2F%E5%89%91%E6%8C%87offer%E9%9D%A2%E8%AF%95%E9%A2%98Java%E7%89%88%2F</url>
    <content type="text"></content>
      <categories>
        <category>算法</category>
        <category>面试</category>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>面试</tag>
        <tag>剑指offer</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSM第三天]]></title>
    <url>%2F2018%2F11%2F29%2FSSM%E7%AC%AC%E4%B8%89%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[部署tomcat服务器1.下载apache-tomcat-7.0.70-windows-x64.zip 2.解压文件，不要放到中文或者空格目录下。 3.进入加压目录. cmd&gt;cd {tomcat_home}/bin 4.执行命令 cmd&gt;startup.bat 5.通过浏览器访问页面，查看是否启动成功。 http://localhost:8080/ tomcat目录结构bin //执行文件 conf //配置文件目录,server.xml,修改8080. webapps //项目部署目录,项目打成war包，运行期间自行解压。 work //临时目录 logs //日志目录 在idea中开发web项目1.在idea中配置tomcat服务器 settings &gt; applications server -&gt; + --&gt; 定位到tomcat解压目录 -&gt;ok 2.创建java模块 + javaEE支持 + maven支持. 3.运行web程序 配置idea中tomcat热部署0.关闭tomcat服务器 1.run --&gt; edit configuration 2.Server选项卡--&gt; VM options部分 on &quot;Update&quot; action : update Classes and resources on Frame deactivation : update Classes and resources 3.启动服务器要选择&quot;debug&quot;模式. 在web模块中添加mvn支持这个地方controller返回的是modelandview然后在返回给分发器程序的。返回的是模型视图的逻辑名。而且还需要配置一个视图解析器。 handlemapping controller viewresolve需要配置到beans.xml里面，这个文件可以随意配置名字。 1.在pom.xml引入springmvc的依赖 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.it18zhang&lt;/groupId&gt; &lt;artifactId&gt;myspringmvc&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;4.3.3.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; 2.在web/WEB-INF/web.xml文件中配置DispatcherServlet分发器. &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;web-app xmlns=&quot;http://xmlns.jcp.org/xml/ns/javaee&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_3_1.xsd&quot; version=&quot;3.1&quot;&gt; &lt;!-- 配置分发器Servlet --&gt; &lt;servlet&gt; &lt;servlet-name&gt;dispatcher&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;dispatcher&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;/web-app&gt; 3.配置springmvc配置文件，使用注解驱动配置项(默认名称就是dispatcher-servlet.xml) [web/WEB-INF/dispatcher-servlet.xml] &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc-4.3.xsd&quot;&gt; &lt;!-- 使用注解驱动 --&gt; &lt;mvc:annotation-driven /&gt; &lt;/beans&gt; 4.编写控制器类 [HomeController.java] package com.it18zhang.springmvc.web.controller; import org.springframework.stereotype.Controller; import org.springframework.web.bind.annotation.RequestMapping; /** * HomeController */ @Controller public class HomeController { /** * 打开主页 */ @RequestMapping(&quot;/home&quot;) public String openHome(){ System.out.println(&quot;hello world&quot;); return null ; } } 5.配置dispatcher-servlet.xml文件，增加扫描路径配置。 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc-4.3.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.3.xsd&quot;&gt; &lt;!-- 配置扫描路径 --&gt; &lt;context:component-scan base-package=&quot;com.it18zhang.springmvc.web.controller&quot; /&gt; &lt;!-- 使用注解驱动 --&gt; &lt;mvc:annotation-driven /&gt; &lt;/beans&gt; 6.启动程序，访问地址 http://localhost:9090/ 7.出现类找不到的原因。 idea的web项目默认不会将依赖类库放置到web-inf/lib下，需要手动设置详情见PPT。 project structure -&gt; artifacts -&gt; myspringmvc:war exploded -&gt; 选择 output layout选项卡 -&gt; 选择右侧的available elements下myspringmvc条目的所有类库 -&gt;右键 -&gt; put into WEB-INF/lib即可。 8.运行程序。 9.配置Spring MVC是视图解析器. [web/WEB-INF/dispatcher-servlet.xml] &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc-4.3.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.3.xsd&quot;&gt; &lt;!-- 配置扫描路径 --&gt; &lt;context:component-scan base-package=&quot;com.it18zhang.springmvc.web.controller&quot; /&gt; &lt;!-- 使用注解驱动 --&gt; &lt;mvc:annotation-driven /&gt; &lt;!-- 内部资源视图解析器 --&gt; &lt;bean class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt; &lt;property name=&quot;prefix&quot; value=&quot;/&quot; /&gt; &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot;/&gt; &lt;/bean&gt; &lt;/beans&gt; 10.添加jsp页面和控制代码 [HomeController.java] package com.it18zhang.springmvc.web.controller; import org.springframework.stereotype.Controller; import org.springframework.web.bind.annotation.RequestMapping; /** * HomeController */ @Controller public class HomeController { /** * 打开主页 */ @RequestMapping(&quot;/home&quot;) public String openHome(){ System.out.println(&quot;hello world&quot;); return &quot;index&quot;; } /** * 打开主页 */ @RequestMapping(&quot;/home2&quot;) public String home2(){ System.out.println(&quot;how are you???&quot;); return &quot;index2&quot;; } } 11./web/index.jsp + /web/index2.jsp [/web/index2.jsp] &lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;index2.jsp&lt;/title&gt; &lt;/head&gt; &lt;body&gt; welcome to spring mvc !!! &lt;/body&gt; &lt;/html&gt; html:标签类型 inline //行内标签,自己不占一行,和其他标签可以在一行.&lt;br&gt; block //块标签,自己占一行。 &lt;a href=&quot;&quot;&gt;百度&lt;/a&gt; 模拟注册行为1.创建/web/reg.jsp &lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;reg.jsp&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;form action=&quot;/doReg2&quot; method=&quot;post&quot;&gt; UserName : &lt;input type=&quot;text&quot; name=&quot;username&quot;&gt;&lt;br&gt; Password : &lt;input type=&quot;password&quot; name=&quot;password&quot;&gt;&lt;br&gt; &lt;input type=&quot;submit&quot;/&gt; &lt;/form&gt; &lt;/body&gt; &lt;/html&gt; 2&apos;.引入Servlet API类库 [pom.xml] &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.it18zhang&lt;/groupId&gt; &lt;artifactId&gt;myspringmvc&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;4.3.3.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;servlet-api&lt;/artifactId&gt; &lt;version&gt;2.5&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; 2.创建RegController.java package com.it18zhang.springmvc.web.controller; import org.springframework.stereotype.Controller; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RequestParam; import javax.servlet.http.HttpServletRequest; /** * */ @Controller public class RegController { @RequestMapping(&quot;/toReg&quot;) public String toRegView(){ return &quot;reg&quot; ; } @RequestMapping(&quot;/doReg&quot;) public String doReg(@RequestParam(&quot;username&quot;) String username, @RequestParam(&quot;password&quot;) String password){ System.out.println(&quot;插入数据&quot;); System.out.println(username + &quot;,&quot; + password); return &quot;index&quot; ; } @RequestMapping(&quot;/doReg2&quot;) public String doReg(HttpServletRequest req) { System.out.println(&quot;插入数据222&quot;); String user = req.getParameter(&quot;username&quot;); System.out.println(user); return &quot;index&quot;; } } 引入jstl标签库,jee标准标签库。1.添加pom.xml依赖 &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;jstl&lt;/artifactId&gt; &lt;version&gt;1.2&lt;/version&gt; &lt;/dependency&gt; 2.修改jsp页面,声明标签库并使用标签 &lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt; &lt;%@ taglib uri=&quot;http://java.sun.com/jsp/jstl/core&quot; prefix=&quot;c&quot; %&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;reg.jsp&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;form action=&apos;&lt;c:url value=&quot;/reg.jsp&quot; /&gt;&apos; method=&quot;post&quot;&gt; UserName : &lt;input type=&quot;text&quot; name=&quot;username&quot;&gt;&lt;br&gt; Password : &lt;input type=&quot;password&quot; name=&quot;password&quot;&gt;&lt;br&gt; &lt;input type=&quot;submit&quot;/&gt; &lt;/form&gt; &lt;/body&gt; &lt;/html&gt; 3.修改上下文名称 project structure --&gt; artifacts --&gt; edit 4. 5. 模拟查询-查询一个User对象1.添加方法 class RegController{ ... /*****从请求中提取uid参数******/ @RequestMapping(&quot;/selectOne&quot;) public String selectOne(Model model , @RequestParam(&quot;uid&quot;) int uid){ System.out.println(&quot;接受到了参数 : uid = &quot; + uid); String username =&quot;tomson&quot; ; //将数据存放到model中，向jsp传递. model.addAttribute(&quot;myusername&quot;, username); return &quot;selectOne&quot; ; } } 2.创建selectOne.jsp [selectOne.jsp] &lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt; &lt;%@ taglib uri=&quot;http://java.sun.com/jsp/jstl/core&quot; prefix=&quot;c&quot; %&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;selectOne.jsp&lt;/title&gt; &lt;/head&gt; &lt;body&gt; username : &lt;c:out value=&quot;${myusername}&quot; /&gt; &lt;/body&gt; &lt;/html&gt; 3.打开浏览器输入地址; http://localhost:9090/selectOne?uid=100 模拟查询-查询全部信息1.定义User类。 public class User { private Integer id; private String name; private int age; ... //get/set } 2.在RegController中添加方法 class RegController{ ... @RequestMapping(&quot;/selectAll&quot;) public String selectAll(Model m){ List&lt;User&gt; list = new ArrayList&lt;User&gt;(); for(int i = 1 ; i &lt;= 50 ; i ++){ User u = new User(); u.setId(i); u.setName(&quot;tom&quot; + i); u.setAge(i % 20); list.add(u) ; } // m.addAttribute(&quot;allUsers&quot;,list); return &quot;userList&quot; ; } 3.创建userList.jsp &lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt; &lt;%@ taglib uri=&quot;http://java.sun.com/jsp/jstl/core&quot; prefix=&quot;c&quot; %&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;selectOne.jsp&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;table border=&quot;1px&quot;&gt; &lt;tr&gt; &lt;td&gt;ID&lt;/td&gt; &lt;td&gt;NAME&lt;/td&gt; &lt;td&gt;AGE&lt;/td&gt; &lt;/tr&gt; &lt;c:forEach items=&quot;${allUsers}&quot; var=&quot;u&quot;&gt; &lt;tr&gt; &lt;td&gt;&lt;c:out value=&quot;${u.id}&quot;/&gt;&lt;/td&gt; &lt;td&gt;&lt;c:out value=&quot;${u.name}&quot;/&gt;&lt;/td&gt; &lt;td&gt;&lt;c:out value=&quot;${u.age}&quot;/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/c:forEach&gt; &lt;/table&gt; &lt;/body&gt; &lt;/html&gt; 4.启动服务器,输入地址 http://localhost:9090/selectAll forward:请求转发，在服务器内部完成。客户端不参与，地址栏不改变。 而且只能转发到本应用的其他路径上。共享请求参数。 redirect 重定向，客户端参与，地址栏变，可以重定向到任意url地址。 不能共享变量。]]></content>
      <tags>
        <tag>SSM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java基础（关于面试）]]></title>
    <url>%2F2018%2F11%2F28%2FJava%E5%9F%BA%E7%A1%80%EF%BC%88%E5%85%B3%E4%BA%8E%E9%9D%A2%E8%AF%95%EF%BC%89%2F</url>
    <content type="text"><![CDATA[bit意为“位”或“比特”，是电子计算机中最小的数据单位，是计算机存储设备的最小单位，每一位的状态只能是0或1。 Byte意为“字节”，8个二进制位构成1个”字节(Byte)”，即1Byte=8bit,两者换算是1：8的关系，字节是计算机处理数据的基本单位，即以字节为单位解释信息。1个字节可以储存1个英文字母或者半个汉字，换句话说，1个汉字占据2个字节的存储空间。 发现 数据类型占内存的位数实际上与操作系统的位数和编译器（不同编译器支持的位数可能有所不同）都有关，具体某种数据类型占字节数得编译器根据操作系统位数两者之间进行协调好后分配内存大小。具体在使用的时候如想知道具体占内存的位数通过sizeof(int)可以得到准确的答案。 int 和byte[]之间的转换 ArrayList和LinkedList的区别-完整总结1.ArrayList是实现了基于动态数组的数据结构，每个元素在内存中存储地址是连续的；LinkedList基于链表的数据结构，每个元素内容包扩previous, next, element（其中，previous是该节点的上一个节点，next是该节点的下一个节点，element是该节点所包含的值），也是由于这一性质支持了每个元素在内存中分布存储。 2.为了使得突破动态长度数组而衍生的ArrayList初始容量为10，每次扩容会固定为之前的1.5倍，所以当你ArrayList达到一定量之后会是一种很大的浪费，并且每次扩容的过程是内部复制数组到新数组；LinkedList的每一个元素都需要消耗一定的空间 3.对于每个元素的检索，ArrayList要优于LinkedList。因为ArrayList从一定意义上来说，就是复杂的数组，所以基于数组index的 检索性能显然高于通过for循环来查找每个元素的LinkedList。 4.元素插入删除的效率对比，要视插入删除的位置来分析，各有优劣 在列表首位添加（删除）元素，LnkedList性能远远优于ArrayList,原因在于ArrayList要后移（前移）每个元素的索引和数组扩容（删除元素时则不需要扩容）。（测试的时候当然插入一次是看不出来什么的，我自己测试插入十万次，就会有数组扩容arraycopy的因素）而LinkedList则直接增加元素，修改原第一元素该节点的上一个节点即可，删除同理 在列表中间位置添加（删除）元素，总的来说位置靠前则LnkedList性能优于ArrayList，靠后则相反。出现这种情况的原因在于ArrayList性能主要损耗在后移（前移）该位置之后的元素索引，而LinkedList损耗在for循环从第一位检索该位置的元素。这个性能反转的临界点不固定，我自己测试插入十万次，在15000次左右损耗时间相比出现变化 在列表末尾位置添加（删除）元素，性能相差不大。]]></content>
      <categories>
        <category>Java基础</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>Java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka]]></title>
    <url>%2F2018%2F11%2F25%2FKafka%2F</url>
    <content type="text"><![CDATA[flume收集日志、移动、聚合框架。 基于事件。 agentsource //接收数据,生产者 //put() //NetcatSource //ExecSource,实时收集 tail -F xxx.txt //spooldir //seq //Stress //avroSource channel //暂存数据，缓冲区, //非永久性:MemoryChannel //永久性 :FileChannel,磁盘. //SpillableMemoryChannel :Mem + FileChannel.Capacity sink //输出数据,消费者 //从channel提取take()数据,write()destination. //HdfsSink //HbaseSink //avroSink 看一下一边都是存储到哪里，如果是电信的那种需要经常查询的就需要放到Hbase里面，如果是放到hdfs里面就只能是全表扫描了。hbase可以随机定位。瞬间定位。 JMSjava message service,java消息服务。 queue //只有能有一个消费者。P2P模式(点对点). //发布订阅(publish-subscribe,主题模式)， kafka分布式流处理平台。 在系统之间构建实时数据流管道。 以topic分类对记录进行存储 每个记录包含key-value+timestamp 每秒钟百万消息吞吐量。 producer //消息生产者 consumer //消息消费者 consumer group //消费者组 kafka server //broker,kafka服务器也叫broker topic //主题,副本数,分区. zookeeper //hadoop namenoade + RM HA | hbase | kafka 安装kafka0.选择s202 ~ s204三台主机安装kafka 1.准备zk 略 2.jdk 略 3.tar文件 4.环境变量 略 5.配置kafka [kafka/config/server.properties] ... broker.id=202 ... listeners=PLAINTEXT://:9092 ... log.dirs=/home/centos/kafka/logs ... zookeeper.connect=s201:2181,s202:2181,s203:2181 6.分发server.properties，同时修改每个文件的broker.id 7.启动kafka服务器 a)先启动zk b)启动kafka [s202 ~ s204] $&gt;bin/kafka-server-start.sh config/server.properties c)验证kafka服务器是否启动 $&gt;netstat -anop | grep 9092 8.创建主题 $&gt;bin/kafka-topics.sh --create --zookeeper s201:2181 --replication-factor 3 --partitions 3 --topic test 9.查看主题列表 $&gt;bin/kafka-topics.sh --list --zookeeper s201:2181 10.启动控制台生产者 $&gt;bin/kafka-console-producer.sh --broker-list s202:9092 --topic test1 11.启动控制台消费者 $&gt;bin/kafka-console-consumer.sh --bootstrap-server s202:9092 --topic test1 --from-beginning --zookeeper s202:2181 12.在生产者控制台输入hello world kafka集群在zk的配置/controller ===&gt; {&quot;version&quot;:1,&quot;brokerid&quot;:202,&quot;timestamp&quot;:&quot;1490926369148&quot; /controller_epoch ===&gt; 1 /brokers /brokers/ids //记载kfk集群每个服务器的信息 /brokers/ids/202 ===&gt; {&quot;jmx_port&quot;:-1,&quot;timestamp&quot;:&quot;1490926370304&quot;,&quot;endpoints&quot;:[&quot;PLAINTEXT://s202:9092&quot;],&quot;host&quot;:&quot;s202&quot;,&quot;version&quot;:3,&quot;port&quot;:9092} //每个节点的连接信息 /brokers/ids/203 /brokers/ids/204 //每个主题下分区数据，主题是有分区的。 /brokers/topics/test/partitions/0/state ===&gt;{&quot;controller_epoch&quot;:1,&quot;leader&quot;:203,&quot;version&quot;:1,&quot;leader_epoch&quot;:0,&quot;isr&quot;:[203,204,202]} /brokers/topics/test/partitions/1/state ===&gt;... /brokers/topics/test/partitions/2/state ===&gt;... leader是针对于主题来说的，是针对主题上的分区来说的。每个分区test主题下。controller也是kfk注册的他和broker是一个层级。说明在kfk集群里面s202是类似于Leader的身份。在 /brokers/seqid ===&gt; null /admin /admin/delete_topics/test ===&gt;标记删除的主题 /isr_change_notification /consumers/xxxx/ /config 下图是生产者没有连接zk别的都是有连接zk的 容错创建主题//2个副本5个分区，2乘以5对应了10个文件夹分配到3个节点所以s202里面有3个， repliation_factor 2 partitions 5 $&gt;kafka-topic.sh --zookeeper s202:2181 --replication_factor 3 --partitions 4 --create --topic test3 2 x 5 = 10 //是个文件夹 [s202] test2-1 // test2-2 // test2-3 // [s203] test2-0 test2-2 test2-3 test2-4 [s204] test2-0 test2-1 test2-4 重新布局分区和副本，手动再平衡$&gt;kafka-topics.sh --alter --zookeeper s202:2181 --topic test2 --replica-assignment 203:204,203:204,203:204,203:204,203:204 副本broker存放消息以消息达到顺序存放。生产和消费都是副本感知的。 支持到n-1故障。每个分区都有leader，follow. leader挂掉时，消息分区写入到本地log或者，向生产者发送消息确认回执之前，生产者向新的leader发送消息。 新leader的选举是通过isr进行，第一个注册的follower成为leader。 kafka支持副本模式[同步复制] 1.producer联系zk识别leader 2.向leader发送消息 3.leadr收到消息写入到本地log 4.follower从leader pull消息 5.follower向本地写入log 6.follower向leader发送ack消息 7.leader收到所有follower的ack消息 8.leader向producer回传ack [异步副本] 和同步复制的区别在与leader写入本地log之后， 直接向client回传ack消息，不需要等待所有follower复制完成。 通过java API实现消息生产者，发送消息package com.it18zhang.kafkademo.test; import org.junit.Test; import kafka.javaapi.producer.Producer; import kafka.producer.KeyedMessage; import kafka.producer.ProducerConfig; import java.util.HashMap; import java.util.Properties; /** * Created by Administrator on 2017/3/31. */ public class TestProducer { @Test public void testSend(){ Properties props = new Properties(); //broker列表 props.put(&quot;metadata.broker.list&quot;, &quot;s202:9092&quot;); //串行化 props.put(&quot;serializer.class&quot;, &quot;kafka.serializer.StringEncoder&quot;); // props.put(&quot;request.required.acks&quot;, &quot;1&quot;); //创建生产者配置对象 ProducerConfig config = new ProducerConfig(props); //创建生产者 Producer&lt;String, String&gt; producer = new Producer&lt;String, String&gt;(config); KeyedMessage&lt;String, String&gt; msg = new KeyedMessage&lt;String, String&gt;(&quot;test3&quot;,&quot;100&quot; ,&quot;hello world tomas100&quot;); producer.send(msg); System.out.println(&quot;send over!&quot;); } } 消息消费者/** * 消费者 */ @Test public void testConumser(){ // Properties props = new Properties(); props.put(&quot;zookeeper.connect&quot;, &quot;s202:2181&quot;); props.put(&quot;group.id&quot;, &quot;g3&quot;); props.put(&quot;zookeeper.session.timeout.ms&quot;, &quot;500&quot;); props.put(&quot;zookeeper.sync.time.ms&quot;, &quot;250&quot;); props.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;); props.put(&quot;auto.offset.reset&quot;, &quot;smallest&quot;); //创建消费者配置对象 ConsumerConfig config = new ConsumerConfig(props); // Map&lt;String, Integer&gt; map = new HashMap&lt;String, Integer&gt;(); map.put(&quot;test3&quot;, new Integer(1)); Map&lt;String, List&lt;KafkaStream&lt;byte[], byte[]&gt;&gt;&gt; msgs = Consumer.createJavaConsumerConnector(new ConsumerConfig(props)).createMessageStreams(map); List&lt;KafkaStream&lt;byte[], byte[]&gt;&gt; msgList = msgs.get(&quot;test3&quot;); for(KafkaStream&lt;byte[],byte[]&gt; stream : msgList){ ConsumerIterator&lt;byte[],byte[]&gt; it = stream.iterator(); while(it.hasNext()){ byte[] message = it.next().message(); System.out.println(new String(message)); } } } flume集成kafka1.KafkaSink [生产者] a1.sources = r1 a1.sinks = k1 a1.channels = c1 a1.sources.r1.type=netcat a1.sources.r1.bind=localhost a1.sources.r1.port=8888 a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink a1.sinks.k1.kafka.topic = test3 a1.sinks.k1.kafka.bootstrap.servers = s202:9092 a1.sinks.k1.kafka.flumeBatchSize = 20 a1.sinks.k1.kafka.producer.acks = 1 a1.channels.c1.type=memory a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 2.KafkaSource [消费者] a1.sources = r1 a1.sinks = k1 a1.channels = c1 a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource a1.sources.r1.batchSize = 5000 a1.sources.r1.batchDurationMillis = 2000 a1.sources.r1.kafka.bootstrap.servers = s202:9092 a1.sources.r1.kafka.topics = test3 a1.sources.r1.kafka.consumer.group.id = g4 a1.sinks.k1.type = logger a1.channels.c1.type=memory a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 3.Channel 生产者 + 消费者 a1.sources = r1 a1.sinks = k1 a1.channels = c1 a1.sources.r1.type = avro a1.sources.r1.bind = localhost a1.sources.r1.port = 8888 a1.sinks.k1.type = logger a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel a1.channels.c1.kafka.bootstrap.servers = s202:9092 a1.channels.c1.kafka.topic = test3 a1.channels.c1.kafka.consumer.group.id = g6 a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 flume重启后会重新读取里面的内容，那么怎么解决重复读取？要引进行序号，每一行都是一个事件，所以在sink里面之后写入redis或者hbase里面因为是高速的，下次再启动flume，虽然从头开始读，也往通道里面放，但是对于sink来讲，会判断他的行号，如果序号比我的数据存的要早，就滤过。比这个大就往里写。相当于一个拦截器]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hbase第四天]]></title>
    <url>%2F2018%2F11%2F22%2FHbase%E7%AC%AC%E5%9B%9B%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[hbase协处理器. Observer //触发器,基于事件激活的。 Endpoint //存储过程,客户端调用。 RegionObserver //system --&gt; user[加载顺序] 100 00-99 callerId - 201703 : hashcode % 100 = 00-99 01,139xxxx,138yyy,.... 热点让数据均匀分散。 create ‘ns1:calllogs’ , SPLITS=&gt;[01,02,03,,…99,] rowkey按照byte排序。 create table xxx(){ } rowkey分区编号 xx,callerId,callTime,calleeId startkey = xx,19626675332, startkey = xx,19626675333, 对通话记录表的设计：（具体在HBase的设计原则2） 对于首先创建的主叫的表是上面这张表，在查询主叫的时候只需要指定xx，主叫时间,时间片。即可。但是查询被叫的时候就不行了。几乎要全表扫描。用00,138xx,2017010101,139xxx这样查询。所以每次向这个表写记录的时候，我们都知道被叫是谁，如果知道被叫的话。我们可以在设计一张表，叫calleeLog,他的rowkey有calleid，time,callerid构成。被叫表存的value存的是主叫表rowkey里面的被叫。然后根据这个查到主叫表后面的内容。但是如果只想知道谁给你打的电话，所以在被叫表的rowkey里面加了一个callerid，如果想查询谁给你打了电话，就在被叫表rowkey里面加了一个callerid。主叫表的rowkey里面都是作为主叫出现的，被叫表里面的数据都是作为被叫出现的。主叫表的内容被叫表里没有，被叫表的内容主叫表也没有的。而且应该吧时长duration也放在里面。也就是说要把最经常使用的信息都编入rowkey里面去，能不查具体的value就尽量不查询具体的value，但是如果要查询具体的数据，什么基站，那个口啊，就是要查询主叫表里面的value的值。这个下图所示的也就是叫二次索引。在写入主叫表的时候也在往被叫表里面写入，用什么写入？也就是用协处理器来处理，怎么处理呢，就是在你写入主叫表的时候，协处理器立刻截获，然后重写里面的方法，往被叫表里面写入就可以了。所以这个就是一个电信HBase的设计原则。 通化记录1.创建表 create &apos;ns1:calllogs&apos;,&apos;f1&apos; 2.创建单元测试 @Test public void put() throws Exception { Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); TableName tname = TableName.valueOf(&quot;ns1:calllogs&quot;); Table table = conn.getTable(tname); String callerId = &quot;13845456767&quot; ; String calleeId = &quot;139898987878&quot; ; SimpleDateFormat sdf = new SimpleDateFormat(); sdf.applyPattern(&quot;yyyyMMddHHmmss&quot;); String callTime = sdf.format(new Date()); int duration = 100 ; DecimalFormat dff = new DecimalFormat(); dff.applyPattern(&quot;00000&quot;); String durStr = dff.format(duration); //区域00-99 int hash = (callerId + callTime.substring(0, 6)).hashCode(); hash = (hash &amp; Integer.MAX_VALUE) % 100 ; //hash区域号 DecimalFormat df = new DecimalFormat(); df.applyPattern(&quot;00&quot;); String regNo = df.format(hash); //拼接rowkey //xx , callerid , time , direction, calleid ,duration String rowkey = regNo + &quot;,&quot; + callerId + &quot;,&quot; + callTime + &quot;,&quot; + &quot;0,&quot; + calleeId + &quot;,&quot; + durStr ; byte[] rowid = Bytes.toBytes(rowkey); Put put = new Put(rowid); put.addColumn(Bytes.toBytes(&quot;f1&quot;),Bytes.toBytes(&quot;callerPos&quot;),Bytes.toBytes(&quot;河北&quot;)); put.addColumn(Bytes.toBytes(&quot;f1&quot;),Bytes.toBytes(&quot;calleePos&quot;),Bytes.toBytes(&quot;河南&quot;)); //执行插入 table.put(put); System.out.println(&quot;over&quot;); } 3.创建协处理器 public class CalleeLogRegionObserver extends BaseRegionObserver{ public void postPut(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Put put, WALEdit edit, Durability durability) throws IOException { super.postPut(e, put, edit, durability); // TableName callLogs = TableName.valueOf(&quot;calllogs&quot;); //得到当前的TableName对象 TableName tableName = e.getEnvironment().getRegion().getRegionInfo().getTable(); if(!callLogs.equals(tableName)){ return ; } //得到主叫的rowkey //xx , callerid , time , direction, calleid ,duration //被叫:calleid,time, String rowkey = Bytes.toString(put.getRow()); String[] arr = rowkey.split(&quot;,&quot;); String hash = Util.getRegNo(arr[4],arr[2]); //hash String newRowKey = hash + &quot;,&quot; + arr[4] + &quot;,&quot; + arr[2] + &quot;,1,&quot; + arr[1] + &quot;,&quot; + arr[5] ; Put newPut = new Put(Bytes.toBytes(newRowKey)); Table t = e.getEnvironment().getTable(tableName); t.put(newPut); } } 4.配置hbase-site.xml并分发分发jar包。 &lt;property&gt; &lt;name&gt;hbase.coprocessor.region.classes&lt;/name&gt; &lt;value&gt;com.it18zhang.hbasedemo.coprocessor.CalleeLogRegionObserver&lt;/value&gt; &lt;/property&gt; 5.启动hbase集群. BloomFilter布隆过滤器。 代码如下: phonix1.安装phonix a)下载apache-phoenix-4.10.0-HBase-1.2-bin.tar.gz b)tar c)复制xxx-server.jar到hbase的lib目录，并且分发,删除以前的phonixjar包。 d)重启hbase 2.使用phonix的命令行程序 $&gt;phonix/bin/.sqlline.py s202 //连接的是zk服务器 $phonix&gt;!tables $phonix&gt;!help //查看帮助 phoenix在创建表的时候要使用大量的协处理器，他是在建表时候不区分大小写的，而且hbase不可以识别得出来他的表，但是hbase shell里面建的表他能识别。 2.SQL Client安装 a)下载squirrel-sql-3.7.1-standard.jar 该文件是安装文件，执行的安装程序。 $&gt;jar -jar squirrel-sql-3.7.1-standard.jar $&gt;下一步... b)复制phoenix-4.10.0-HBase-1.2-client.jar到SQuerrel安装目录的lib下(c:\myprograms\squirrel)。 c)启动SQuirrel(GUI) 定位安装目录-&gt;执行squirrel-sql.bat d)打开GUI界面 d)在左侧的边栏选中&quot;Drivers&quot;选项卡， 点击 &quot;+&quot; -&gt; URL : jdbc:phoenix:192.168.231.202 Driverclass : org.apache.phoenix.jdbc.PhoenixDriver jdbc:phoenix: s202 d)测试。 3.SQLLine客户端操作 //建表 $jdbc:phoenix&gt;create table IF NOT EXISTS test.Person (IDCardNum INTEGER not null primary key, Name varchar(20),Age INTEGER); //插入数据 $jdbc:phoenix&gt;UPSERT INTO test.PERSON(IDCardNum , Name,Age) VALUES (1,&apos;tom&apos;,12); //删除数据 $jdbc:phoenix&gt;delete from test.persion where idcardnum = 1 ; //更新数据 //upsert into test.PERSON(IDCardNum , Name,Age) VALUES (1,&apos;tom&apos;,12);]]></content>
  </entry>
  <entry>
    <title><![CDATA[Flume]]></title>
    <url>%2F2018%2F11%2F21%2FFlume%2F</url>
    <content type="text"><![CDATA[hbaseNoSQL. 面向列族。 随机定位 实时读写。 分布式 可伸缩 HA zookeeper version(列族) rowkey/famil+qualifier/timestamp = value rowkey //唯一性,散列性,定长,不要太长,加盐. 二次索引 byte[] hive离线计算。 MR:MapReducehadoop : DBWritable + WritableComparable : 将hbase的表影射到hive上，使用hive的查询语句。CREATE TABLE mydb.t11(key string, name string) STORED BY &apos;org.apache.hadoop.hive.hbase.HBaseStorageHandler&apos; WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,cf:name&quot;) TBLPROPERTIES(&quot;hbase.table.name&quot; = &quot;ns1:t11&quot;); select count(*) from mydb.t11 ; flume收集、移动、聚合大量日志数据的服务。 基于流数据的架构，用于在线日志分析。 基于事件。 在生产和消费者之间启动协调作用。 提供了事务保证，确保消息一定被分发。 Source 多种 sink多种. multihop //多级跃点.可以从一个flume到另外一个flume 水平扩展: //加节点， 竖直扩展 //增加硬件。 Source接受数据，类型有多种。 Channel临时存放地，对Source中来的数据进行缓冲，直到sink消费掉。 Sink从channel提取数据存放到中央化存储(hadoop / hbase)。 实时产生的数据， Flume的优点以下是使用Flume的优点：使用Apache Flume，我们可以将数据存储到任何集中存储中（HBase，HDFS）。当传入数据的速率超过可以写入数据的速率时目的地，Flume充当数据生产者和数据生成者之间的中介集中存储并在它们之间提供稳定的数据流。Flume提供了上下文路由的功能。 FLUME - 介绍App Flume2Flume中的交易是基于渠道的，其中两个交易（一个发件人为每条消息维护一个接收器。它保证了可靠的信息交货。Flume具有可靠性，容错性，可扩展性，可管理性和可定制性。水槽的特点Flume的一些显着特征如下：Flume将来自多个Web服务器的日志数据提取到集中存储（HDFS，HBase）有效。使用Flume，我们可以立即将来自多个服务器的数据导入Hadoop。与日志文件一起，Flume还用于导入大量事件数据由Facebook和Twitter等社交网站和电子商务制作亚马逊和Flipkart等网站。Flume支持大量源和目标类型。Flume支持多跳流，扇入扇出流，上下文路由等。水槽可以水平缩放 安装flume1.下载 2.tar 3.环境变量 4.验证flume是否成功 $&gt;flume-ng version //next generation.下一代. 配置flume1.创建配置文件 [/soft/flume/conf/hello.conf] #声明三种组件 a1.sources = r1 a1.channels = c1 a1.sinks = k1 #定义source信息 a1.sources.r1.type=netcat a1.sources.r1.bind=localhost a1.sources.r1.port=8888 #定义sink信息 a1.sinks.k1.type=logger #定义channel信息 a1.channels.c1.type=memory #绑定在一起 a1.sources.r1.channels=c1 a1.sinks.k1.channel=c1 2.运行 a)启动flume agent $&gt;bin/flume-ng agent -f ../conf/helloworld.conf -n a1 -Dflume.root.logger=INFO,console b)启动nc的客户端 $&gt;nc localhost 8888 $nc&gt;hello world c)在flume的终端输出hello world. 安装nc$&gt;sudo yum install nmap-ncat.x86_64 清除仓库缓存$&gt;修改ali.repo --&gt; ali.repo.bak文件。 $&gt;sudo yum clean all $&gt;sudo yum makecache #例如阿里基本源 $&gt;sudo wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo #阿里epel源 $&gt;sudo wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo flume source1.netcat nc .. 2.exec 实时日志收集,实时收集日志。 a1.sources = r1 a1.sinks = k1 a1.channels = c1 a1.sources.r1.type=exec a1.sources.r1.command=tail -F /home/centos/test.txt a1.sinks.k1.type=logger a1.channels.c1.type=memory a1.sources.r1.channels=c1 a1.sinks.k1.channel=c1 3.批量收集 监控一个文件夹，静态文件。 收集完之后，会重命名文件成新文件。.compeleted. a)配置文件 [spooldir_r.conf] a1.sources = r1 a1.channels = c1 a1.sinks = k1 a1.sources.r1.type=spooldir a1.sources.r1.spoolDir=/home/centos/spool a1.sources.r1.fileHeader=true a1.sinks.k1.type=logger a1.channels.c1.type=memory a1.sources.r1.channels=c1 a1.sinks.k1.channel=c1 b)创建目录 $&gt;mkdir ~/spool c)启动flume $&gt;bin/flume-ng agent -f ../conf/helloworld.conf -n a1 -Dflume.root.logger=INFO,console 4.序列source [seq] a1.sources = r1 a1.channels = c1 a1.sinks = k1 a1.sources.r1.type=seq a1.sources.r1.totalEvents=1000 a1.sinks.k1.type=logger a1.channels.c1.type=memory a1.sources.r1.channels=c1 a1.sinks.k1.channel=c1 [运行] $&gt;bin/flume-ng agent -f ../conf/helloworld.conf -n a1 -Dflume.root.logger=INFO,console 5.StressSource a1.sources = stresssource-1 a1.channels = memoryChannel-1 a1.sources.stresssource-1.type = org.apache.flume.source.StressSource a1.sources.stresssource-1.size = 10240 a1.sources.stresssource-1.maxTotalEvents = 1000000 a1.sources.stresssource-1.channels = memoryChannel-1 flume sink1.hdfs a1.sources = r1 a1.channels = c1 a1.sinks = k1 a1.sources.r1.type = netcat a1.sources.r1.bind = localhost a1.sources.r1.port = 8888 a1.sinks.k1.type = hdfs a1.sinks.k1.hdfs.path = /flume/events/%y-%m-%d/%H/%M/%S a1.sinks.k1.hdfs.filePrefix = events- #是否是产生新目录,每十分钟产生一个新目录,一般控制的目录方面。round是决定是否产生新文件的，滚动是决定是否产生新文件的。 #2017-12-12 --&gt; #2017-12-12 --&gt;%H%M%S a1.sinks.k1.hdfs.round = true a1.sinks.k1.hdfs.roundValue = 10 a1.sinks.k1.hdfs.roundUnit = second a1.sinks.k1.hdfs.useLocalTimeStamp=true #是否产生新文件。 a1.sinks.k1.hdfs.rollInterval=10 a1.sinks.k1.hdfs.rollSize=10 a1.sinks.k1.hdfs.rollCount=3 a1.channels.c1.type=memory a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 2.hive 略 3.hbase a1.sources = r1 a1.channels = c1 a1.sinks = k1 a1.sources.r1.type = netcat a1.sources.r1.bind = localhost a1.sources.r1.port = 8888 a1.sinks.k1.type = hbase a1.sinks.k1.table = ns1:t12 a1.sinks.k1.columnFamily = f1 a1.sinks.k1.serializer = org.apache.flume.sink.hbase.RegexHbaseEventSerializer a1.channels.c1.type=memory a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 4.kafka 数据进入到源里面去，最终进入通道里面，有很多通道c1,c2,c3，这个取决于通道选择器，ChannelProcessor，对事件进行处理，先经过一堆拦截器也是有很多种 。拦截器在各个文件前加东西，拦截之后再回到选择器。拦截器是典型的批处理，把加到东西流水线似的加到头文件里面。不管是什么对象，通过什么source进来的，都被转换成envent对象。在拦截器这个地方是链式技术。把事件放到通道里面，每个通道放置事件都是一个事务，保证能成功， 这个图是source到通道的图 sink的图： 使用avroSource和AvroSink实现跃点agent处理1.创建配置文件 [avro_hop.conf] #a1 a1.sources = r1 a1.sinks= k1 a1.channels = c1 a1.sources.r1.type=netcat a1.sources.r1.bind=localhost a1.sources.r1.port=8888 a1.sinks.k1.type = avro a1.sinks.k1.hostname=localhost a1.sinks.k1.port=9999 a1.channels.c1.type=memory a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 #a2 a2.sources = r2 a2.sinks= k2 a2.channels = c2 a2.sources.r2.type=avro a2.sources.r2.bind=localhost a2.sources.r2.port=9999 a2.sinks.k2.type = logger a2.channels.c2.type=memory a2.sources.r2.channels = c2 a2.sinks.k2.channel = c2 2.启动a2 $&gt;flume-ng agent -f /soft/flume/conf/avro_hop.conf -n a2 -Dflume.root.logger=INFO,console 3.验证a2 $&gt;netstat -anop | grep 9999 4.启动a1 $&gt;flume-ng agent -f /soft/flume/conf/avro_hop.conf -n a1 5.验证a1 $&gt;netstat -anop | grep 8888 channel1.MemoryChannel 略 2.FileChannel a1.sources = r1a1.sinks= k1a1.channels = c1 a1.sources.r1.type=netcata1.sources.r1.bind=localhosta1.sources.r1.port=8888 a1.sinks.k1.type=logger a1.channels.c1.type = file a1.channels.c1.checkpointDir = /home/centos/flume/fc_check a1.channels.c1.dataDirs = /home/centos/flume/fc_data a1.sources.r1.channels=c1a1.sinks.k1.channel=c1 可溢出文件通道a1.channels = c1a1.channels.c1.type = SPILLABLEMEMORY #0表示禁用内存通道，等价于文件通道a1.channels.c1.memoryCapacity = 0 #0,禁用文件通道，等价内存通道。a1.channels.c1.overflowCapacity = 2000 a1.channels.c1.byteCapacity = 800000a1.channels.c1.checkpointDir = /user/centos/flume/fc_checka1.channels.c1.dataDirs = /user/centos/flume/fc_data 创建Flume模块1.添加pom.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.it18zhang&lt;/groupId&gt; &lt;artifactId&gt;FluemDemo&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flume&lt;/groupId&gt; &lt;artifactId&gt;flume-ng-core&lt;/artifactId&gt; &lt;version&gt;1.7.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt;]]></content>
      <tags>
        <tag>flume</tag>
        <tag>source</tag>
        <tag>sink</tag>
        <tag>channel</tag>
        <tag>安装flume</tag>
        <tag>avro跃点</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试理论知识]]></title>
    <url>%2F2018%2F11%2F21%2F%E9%9D%A2%E8%AF%95%E7%90%86%E8%AE%BA%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[flume：Apache Flume是一种工具/服务/数据提取机制，用于收集聚合和从各种传输大量的流数据，如日志文件，事件（等等）源集中数据存储。 Flume是一种高度可靠，分布式和可配置的工具。 它主要是为了设计的将流数据（日志数据）从各种Web服务器复制到HDFS。]]></content>
  </entry>
  <entry>
    <title><![CDATA[SSM第一天]]></title>
    <url>%2F2018%2F11%2F19%2FSSM%E7%AC%AC%E4%B8%80%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[SSM第二天三大框架的整合： 数据库层：mybatis数据持久化层：dao在交互mybatisdao:数据访问层：数据访问对象，存放简单的增删改查。每个表都有这个业务service层：业务层，和dao交互，对dao的方法进行组合调用。形成套餐springmvc展现层：做网页，做web开发的，springmvc应该和业务层交互，在springmvc里面的controller层和业务层交互，展现的东西就是jsp做web开发做网站的，做web应用的。通过web架构来实现。springmvc就是web架构里面的一个。 Mybais和数据库整合 1.在pom中添加依赖： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.it18zhang&lt;/groupId&gt; &lt;artifactId&gt;SpringmybatisDemo&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;3.4.6&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.17&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.mchange&lt;/groupId&gt; &lt;artifactId&gt;c3p0&lt;/artifactId&gt; &lt;version&gt;0.9.5.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;version&gt;4.3.3.RElEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;version&gt;4.3.3.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aop&lt;/artifactId&gt; &lt;version&gt;4.3.3.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt; &lt;version&gt;1.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; 2.创建包 com.it18zhang.springmybatis.dao com.it18zhang.springmybatis.service com.it18zhang.springmybatis.utils 3.配置beans.xml在source下 坑爹的错误： 在上图的情下，看见下面的mybtis-spring是依赖于mybatis3.1.0，而上面有一个跟他一模一样的包，所以到下面这块，下面的这个版本的包就看不出来了。所以如果有的类仅仅是在下面这个版本下才有的，需要把上面的第一个包先删除掉。因为它只显示第一个依赖的包 复杂应用1.准备数据 sql.sql 2.创建java类. [Order.java] public class Order { private Integer id ; private String orderNo ; //简历关联关系 private User user ; //get/set } [Item.java] public class Item { private Integer id; private String itemName; //订单项和订单之间的关联关系 private Order order; //get/set } 3.创建Order映射文件 [resource/OrderMapper.xml] &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt; &lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt; &lt;mapper namespace=&quot;orders&quot;&gt; &lt;insert id=&quot;insert&quot;&gt; insert into orders(orderno,uid) values(#{orderNo},#{user.id}) &lt;/insert&gt; &lt;!-- findById --&gt; &lt;select id=&quot;selectOne&quot; parameterType=&quot;int&quot; resultMap=&quot;RM_Order&quot;&gt; select o.id oid , o.orderno oorderno , o.uid uid , u.name uname , u.age uage from orders o left outer join users u on o.uid = u.id where o.id = #{id} &lt;/select&gt; &lt;!-- findAll --&gt; &lt;select id=&quot;selectAll&quot; resultMap=&quot;RM_Order&quot;&gt; select o.id oid , o.orderno oorderno , o.uid uid , u.name uname , u.age uage from orders o left outer join users u on o.uid = u.id &lt;/select&gt; &lt;!-- 自定义结果映射 --&gt; &lt;resultMap id=&quot;RM_Order&quot; type=&quot;com.it18zhang.mybatisdemo.domain.Order&quot;&gt; &lt;id property=&quot;id&quot; column=&quot;oid&quot;/&gt; &lt;result property=&quot;orderNo&quot; column=&quot;oorderno&quot;/&gt; &lt;association property=&quot;user&quot; javaType=&quot;com.it18zhang.mybatisdemo.domain.User&quot;&gt; &lt;id property=&quot;id&quot; column=&quot;uid&quot; /&gt; &lt;result property=&quot;name&quot; column=&quot;uname&quot; /&gt; &lt;result property=&quot;age&quot; column=&quot;uage&quot; /&gt; &lt;/association&gt; &lt;/resultMap&gt; &lt;/mapper&gt; 4.修改配置文件,添加映射。 [resource/mybatis-config.xml] &lt;!-- 引入映射文件 --&gt; &lt;mappers&gt; &lt;mapper resource=&quot;*Mapper.xml&quot;/&gt; &lt;/mappers&gt; 5.测试类 public class TestOrder { /** * insert */ @Test public void insert() throws Exception { String resource = &quot;mybatis-config.xml&quot;; InputStream inputStream = Resources.getResourceAsStream(resource); SqlSessionFactory sf = new SqlSessionFactoryBuilder().build(inputStream); SqlSession s = sf.openSession(); User u = new User(); u.setId(2); Order o = new Order(); o.setOrderNo(&quot;No005&quot;); o.setUser(u); s.insert(&quot;orders.insert&quot;,o); s.commit(); s.close(); } @Test public void selectOne() throws Exception { String resource = &quot;mybatis-config.xml&quot;; InputStream inputStream = Resources.getResourceAsStream(resource); SqlSessionFactory sf = new SqlSessionFactoryBuilder().build(inputStream); SqlSession s = sf.openSession(); Order order = s.selectOne(&quot;orders.selectOne&quot;,1); System.out.println(order.getOrderNo()); s.commit(); s.close(); } @Test public void selectAll() throws Exception { String resource = &quot;mybatis-config.xml&quot;; InputStream inputStream = Resources.getResourceAsStream(resource); SqlSessionFactory sf = new SqlSessionFactoryBuilder().build(inputStream); SqlSession s = sf.openSession(); List&lt;Order&gt; list = s.selectList(&quot;orders.selectAll&quot;); for(Order o : list){ System.out.println(o.getOrderNo() + &quot; : &quot; + o.getUser().getName()); } s.commit(); s.close(); } } 配置一对多1.在User中增加orders集合。 public class User { ... private List&lt;Order&gt; orders ; //get/set } 2.改造UserMapper.xml 组合多对一和一对多关联关系到一个实体(Order)中1.关系 Order(*) -&gt; (1)User Order(1) -&gt; (*)Item 2.Order.java class Order{ ... List&lt;Item&gt; items ; //get/set } 2&apos;.修改配置文件增加别名 [resources/mybatis-config.xml] &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt; &lt;!DOCTYPE configuration PUBLIC &quot;-//mybatis.org//DTD Config 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot;&gt; &lt;configuration&gt; &lt;typeAliases&gt; &lt;typeAlias type=&quot;com.it18zhang.mybatisdemo.domain.User&quot; alias=&quot;_User&quot;/&gt; &lt;typeAlias type=&quot;com.it18zhang.mybatisdemo.domain.Order&quot; alias=&quot;_Order&quot;/&gt; &lt;typeAlias type=&quot;com.it18zhang.mybatisdemo.domain.Item&quot; alias=&quot;_Item&quot;/&gt; &lt;/typeAliases&gt; &lt;environments default=&quot;development&quot;&gt; &lt;environment id=&quot;development&quot;&gt; &lt;transactionManager type=&quot;JDBC&quot;/&gt; &lt;dataSource type=&quot;POOLED&quot;&gt; &lt;property name=&quot;driver&quot; value=&quot;com.mysql.jdbc.Driver&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/mybatis&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;root&quot;/&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;!-- 引入映射文件 --&gt; &lt;mappers&gt; &lt;mapper resource=&quot;UserMapper.xml&quot;/&gt; &lt;mapper resource=&quot;OrderMapper.xml&quot;/&gt; &lt;/mappers&gt; &lt;/configuration&gt; 3.OrderMapper.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt; &lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt; &lt;mapper namespace=&quot;orders&quot;&gt; &lt;insert id=&quot;insert&quot;&gt; insert into orders(orderno,uid) values(#{orderNo},#{user.id}) &lt;/insert&gt; &lt;!-- findById --&gt; &lt;select id=&quot;selectOne&quot; parameterType=&quot;int&quot; resultMap=&quot;RM_Order&quot;&gt; select o.id oid , o.orderno oorderno , o.uid uid , u.name uname , u.age uage , i.id iid, i.itemname iitemname from orders o left outer join users u on o.uid = u.id left outer join items i on o.id = i.oid where o.id = #{id} &lt;/select&gt; &lt;!-- findAll --&gt; &lt;select id=&quot;selectAll&quot; resultMap=&quot;RM_Order&quot;&gt; select o.id oid , o.orderno oorderno , o.uid uid , u.name uname , u.age uage , i.id iid, i.itemname iitemname from orders o left outer join users u on o.uid = u.id left outer join items i on o.id = i.oid &lt;/select&gt; &lt;!-- 自定义结果映射 --&gt; &lt;resultMap id=&quot;RM_Order&quot; type=&quot;com.it18zhang.mybatisdemo.domain.Order&quot;&gt; &lt;id property=&quot;id&quot; column=&quot;oid&quot;/&gt; &lt;result property=&quot;orderNo&quot; column=&quot;oorderno&quot;/&gt; &lt;association property=&quot;user&quot; javaType=&quot;com.it18zhang.mybatisdemo.domain.User&quot;&gt; &lt;id property=&quot;id&quot; column=&quot;uid&quot; /&gt; &lt;result property=&quot;name&quot; column=&quot;uname&quot; /&gt; &lt;result property=&quot;age&quot; column=&quot;uage&quot; /&gt; &lt;/association&gt; &lt;collection property=&quot;items&quot; ofType=&quot;_Item&quot;&gt; &lt;id property=&quot;id&quot; column=&quot;iid&quot; /&gt; &lt;result property=&quot;itemName&quot; column=&quot;iitemname&quot; /&gt; &lt;/collection&gt; &lt;/resultMap&gt; &lt;/mapper&gt; 4.测试 @Test public void selectOne() throws Exception { String resource = &quot;mybatis-config.xml&quot;; InputStream inputStream = Resources.getResourceAsStream(resource); SqlSessionFactory sf = new SqlSessionFactoryBuilder().build(inputStream); SqlSession s = sf.openSession(); Order order = s.selectOne(&quot;orders.selectOne&quot;,1); System.out.println(order.getOrderNo() + order.getUser().getName()); for(Item i : order.getItems()){ System.out.println(i.getId() + &quot;:&quot; + i.getItemName()); } s.commit(); s.close(); } 改造项目1.引入Util类 package com.it18zhang.mybatisdemo.util; import org.apache.ibatis.io.Resources; import org.apache.ibatis.session.SqlSession; import org.apache.ibatis.session.SqlSessionFactory; import org.apache.ibatis.session.SqlSessionFactoryBuilder; import java.io.InputStream; /** * 工具类 */ public class Util { // private static SqlSessionFactory sf ; static{ try { String resource = &quot;mybatis-config.xml&quot;; InputStream inputStream = Resources.getResourceAsStream(resource); sf = new SqlSessionFactoryBuilder().build(inputStream); } catch (Exception e) { e.printStackTrace(); } } /** * 开启会话 */ public static SqlSession openSession(){ return sf.openSession() ; } /** * 关闭会话 */ public static void closeSession(SqlSession s){ if(s != null){ s.close(); } } /** * 关闭会话 */ public static void rollbackTx(SqlSession s) { if (s != null) { s.rollback(); } } } 2.设计模板类DaoTemplate和回调MybatisCallback接口 [DaoTemplate.java] package com.it18zhang.mybatisdemo.dao; import com.it18zhang.mybatisdemo.util.Util; import org.apache.ibatis.session.SqlSession; /** * 模板类 */ public class DaoTemplate { /** * 执行 */ public static Object execute(MybatisCallback cb){ SqlSession s = null; try { s = Util.openSession(); Object ret = cb.doInMybatis(s); s.commit(); return ret ; } catch (Exception e) { Util.rollbackTx(s); } finally { Util.closeSession(s); } return null ; } } [MybatisCallback.java] package com.it18zhang.mybatisdemo.dao; import org.apache.ibatis.session.SqlSession; /** * 回调接口 */ public interface MybatisCallback { public Object doInMybatis(SqlSession s); } 3.通过模板类+回调接口实现UserDao.java [UserDao.java] package com.it18zhang.mybatisdemo.dao; import com.it18zhang.mybatisdemo.domain.User; import com.it18zhang.mybatisdemo.util.Util; import org.apache.ibatis.session.SqlSession; import java.util.List; /** * UserDao */ public class UserDao { /** * 插入操作 */ public void insert(final User user){ DaoTemplate.execute(new MybatisCallback() { public Object doInMybatis(SqlSession s) { s.insert(&quot;users.insert&quot;,user); return null ; } }); } /** * 插入操作 */ public void update(final User user){ DaoTemplate.execute(new MybatisCallback() { public Object doInMybatis(SqlSession s) { s.update(&quot;users.update&quot;, user); return null ; } }); } public User selctOne(final Integer id){ return (User)DaoTemplate.execute(new MybatisCallback() { public Object doInMybatis(SqlSession s) { return s.selectOne(&quot;users.selectOne&quot;,id); } }); } public List&lt;User&gt; selctAll(){ return (List&lt;User&gt;)DaoTemplate.execute(new MybatisCallback() { public Object doInMybatis(SqlSession s) { return s.selectList(&quot;users.selectAll&quot;); } }); } } 4.App测试 public static void main(String[] args) { UserDao dao = new UserDao(); User u = dao.selctOne(1); System.out.println(u.getName()); } 回调接口的一个画图分析：]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hbase第三天]]></title>
    <url>%2F2018%2F11%2F19%2FHbase%E7%AC%AC%E4%B8%89%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[复习Hbase第二天随机定位+实时读写 nosql:not only sql数据库 key-value对形式的存储 key: rowkey/family+col/timstamp = value rowkey 排序,byte[] 客户端先联系zk找到元数据表hbae:meta，存放了整个数据库的表和区域服务器信息，相当于目录，类似于名称节点。当找到了之后就可以定位到区域服务器，所以hbase数据读写和HRegionServer来交互，有很多regionServer构成了一个集群。数据先进入到写前日志，写前日志用于容错，用于恢复，所以在交互的时候client先交互HRegionServer然后在网Hlog里写入数据，然后在溢出之后写入HRegion，对于HRegion来说有个内存储MemStore在内存中存储数据，用来提高速度，MemStore达到一定值溢出到磁盘，所以还有一个StoreFile存储，用来和底层交互，底层就是Hfile。通过Hfile对象来跟HDFS交互，就找到了HDFS客户端DFSClient了，这个DFSClient就是hdfs范畴了，最终数据存储到HDFS里面了。 表的切割指的是切割表或者切割区域，按照rowkey来切分，因为rowkey是有序的，相当于建立索引，通过切割可以实现负载均衡，如果所有东西都在一个点就会出现热点问题。 hbase的增删改查是： put(rowkey).addColumn(). put(Put) delete get() 更新也是put scan() merge合并。 移动区域，目的减少某一个服务器的压力。可以任意配置区域所在地，由那个区域服务器承载。 切割风暴：达到10G之后同时到达临界点，同时切割，为了避免可以让这个10G的值变大再切割，也就是不让他自动切割了。可以手动切割避免。或者预切割来处理。 hbase存储的荣誉量比较大，因为它存储的时候都是以kv的方式来存储，而key是三极坐标，rowkey，列，列族，时间戳+一个value，所以前三个值都要存放很多次。所以要求列族和列的名称和rowkey的名字不能太长。一旦过长，就会发现存储的时候被坐标占用了大量的空间，而value的很少，最好列和列族名字不要太长。 本天会涉及rowkey的设计问题。 预先切割创建表时可以预先对表进行切割。 切割线就是rowkey create ‘ns1:t2’,’f1’,SPLITES=&gt;[‘row3000’,’row6000] 预先切割创建表时，预先对表进行切割。 切割线是rowkey. $hbase&gt;create &apos;ns1:t2&apos;,&apos;f1&apos;,SPLITS=&gt;[&apos;row3000&apos;,&apos;row6000&apos;] 创建表时指定列族的版本数,该列族的所有列都具有相同数量版本$hbase&gt;create &apos;ns1:t3&apos;,{NAME=&gt;&apos;f1&apos;,VERSIONS=&gt;3} //创建表时，指定列族的版本数。 $hbase&gt;get &apos;ns1:t3&apos;,&apos;row1&apos;,{COLUMN=&gt;&apos;f1&apos;,VERSIONS=&gt;4} //检索的时候，查询多少个版本。 $hbase&gt;put &apos;ns1:t3&apos;,&apos;row1&apos;,&apos;f1:name&apos;,&apos;tom&apos; 关于查询的命令行： 关于创建表的命令： @Test public void getWithVersions() throws IOException { Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); TableName tname = TableName.valueOf(&quot;ns1:t3&quot;); Table table = conn.getTable(tname); Get get = new Get(Bytes.toBytes(&quot;row1&quot;)); //检索所有版本 get.setMaxVersions(); Result r = table.get(get); List&lt;Cell&gt; cells = r.getColumnCells(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;)); for(Cell c : cells){ String f = Bytes.toString(c.getFamily()); String col = Bytes.toString(c.getQualifier()); long ts = c.getTimestamp(); String val = Bytes.toString(c.getValue()); System.out.println(f + &quot;/&quot; + col + &quot;/&quot; + ts + &quot;=&quot; + val); } } 原生扫描(专家)1.原生扫描$hbase&gt;scan &apos;ns1:t3&apos;,{COLUMN=&gt;&apos;f1&apos;,RAW=&gt;true,VERSIONS=&gt;10} //包含标记了delete的数据 2.删除数据$hbase&gt;delete &apos;nd1:t3&apos;,&apos;row1&apos;,&apos;f1:name&apos;,148989875645 //删除数据，标记为删除. //小于该删除时间的数据都作废。 3.TTLtime to live ,存活时间。 影响所有的数据，包括没有删除的数据。 超过该时间，原生扫描也扫不到数据。 $hbase&gt;create &apos;ns1:tx&apos; , {NAME=&gt;&apos;f1&apos;,TTL=&gt;10,VERSIONS} 4.KEEP_DELETED_CELLS删除key之后，数据是否还保留。 $hbase&gt;create &apos;ns1:tx&apos; , {NAME=&gt;&apos;f1&apos;,TTL=&gt;10,VERSIONS,KEEP_DELETED_CELLS=&gt;true} 缓存和批处理 1.开启服务器端扫描器缓存 a)表层面(全局)只需要配置一个属性即可 &lt;property&gt; &lt;name&gt;hbase.client.scanner.caching&lt;/name&gt; &lt;!-- 整数最大值 --&gt; &lt;value&gt;2147483647&lt;/value&gt; &lt;source&gt;hbase-default.xml&lt;/source&gt; &lt;/property&gt; b)操作层面 //设置量 scan.setCaching(10); 2. 3. cache row nums : 1000 //632 cache row nums : 5000 //423 cache row nums : 1 //7359 扫描器缓存面向行级别的。 @Test public void getScanCache() throws IOException { Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); TableName tname = TableName.valueOf(&quot;ns1:t1&quot;); Scan scan = new Scan(); scan.setCaching(5000); Table t = conn.getTable(tname); ResultScanner rs = t.getScanner(scan); long start = System.currentTimeMillis() ; Iterator&lt;Result&gt; it = rs.iterator(); while(it.hasNext()){ Result r = it.next(); System.out.println(r.getColumnLatestCell(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;))); } System.out.println(System.currentTimeMillis() - start); } 批量扫描是面向列级别 控制每次next()服务器端返回的列的个数。 scan.setBatch(5); //每次next返回5列。 测试缓存和批处理 */ @Test public void testBatchAndCaching() throws IOException { Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); TableName tname = TableName.valueOf(&quot;ns1:t7&quot;); Scan scan = new Scan(); scan.setCaching(2); scan.setBatch(3); Table t = conn.getTable(tname); ResultScanner rs = t.getScanner(scan); Iterator&lt;Result&gt; it = rs.iterator(); while (it.hasNext()) { Result r = it.next(); System.out.println(&quot;========================================&quot;); //得到一行的所有map,key=f1,value=Map&lt;Col,Map&lt;Timestamp,value&gt;&gt; NavigableMap&lt;byte[], NavigableMap&lt;byte[], NavigableMap&lt;Long, byte[]&gt;&gt;&gt; map = r.getMap(); // for (Map.Entry&lt;byte[], NavigableMap&lt;byte[], NavigableMap&lt;Long, byte[]&gt;&gt;&gt; entry : map.entrySet()) { //得到列族 String f = Bytes.toString(entry.getKey()); Map&lt;byte[], NavigableMap&lt;Long, byte[]&gt;&gt; colDataMap = entry.getValue(); for (Map.Entry&lt;byte[], NavigableMap&lt;Long, byte[]&gt;&gt; ets : colDataMap.entrySet()) { String c = Bytes.toString(ets.getKey()); Map&lt;Long, byte[]&gt; tsValueMap = ets.getValue(); for (Map.Entry&lt;Long, byte[]&gt; e : tsValueMap.entrySet()) { Long ts = e.getKey(); String value = Bytes.toString(e.getValue()); System.out.print(f + &quot;/&quot; + c + &quot;/&quot; + ts + &quot;=&quot; + value + &quot;,&quot;); } } } System.out.println(); } } 先插入数据 代码运行结果： 上面代码和上图对应的。设置2个cach和3个batch视频上说是2行3列，但是我觉得应该是2个列族3个列的这样子去输出。然后最后这一行还剩下2个输出2个，也就是3个输出，2个输出，3个输出，2个输出，。 ======================================== f1/id/1490595148588=1,f2/addr/1490595182150=hebei,f2/age/1490595174760=12,f2/id/1490595164473=1,f2/name/1490595169589=tom, ======================================== f1/id/1490595196410=2,f1/name/1490595213090=tom2.1,f2/addr/1490595264734=tangshan,f2/age/1490595253996=13,f2/id/1490595233568=2,f2/name/1490595241891=tom2.2, ======================================== f1/age/1490595295427=14,f1/id/1490595281251=3,f1/name/1490595289587=tom3.1,f2/addr/1490595343690=beijing,f2/age/1490595336300=14,f2/id/1490595310966=3, ========================================f2/name/1490595327531=tom3.2, Filter过滤器远程服务器收到scan对象进行反序列化，恢复成scan对象进行过滤，对每个区域进行过滤，每个区域服务器有很多区域，每个区域里面有区域扫描器， RegionScanner，会使用区域过滤器。 1.RowFilter select * from ns1:t1 where rowkey &lt;= row100 /** /** * 测试RowFilter过滤器 */ @Test public void testRowFilter() throws IOException { Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); TableName tname = TableName.valueOf(&quot;ns1:t1&quot;); Scan scan = new Scan(); RowFilter rowFilter = new RowFilter(CompareFilter.CompareOp.LESS_OR_EQUAL, new BinaryComparator(Bytes.toBytes(&quot;row0100&quot;))); scan.setFilter(rowFilter); Table t = conn.getTable(tname); ResultScanner rs = t.getScanner(scan); Iterator&lt;Result&gt; it = rs.iterator(); while (it.hasNext()) { Result r = it.next(); System.out.println(Bytes.toString(r.getRow())); } } /** * 测试FamilyFilter过滤器 */ @Test public void testFamilyFilter() throws IOException { Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); TableName tname = TableName.valueOf(&quot;ns1:t7&quot;); Scan scan = new Scan(); FamilyFilter filter = new FamilyFilter(CompareFilter.CompareOp.LESS, new BinaryComparator(Bytes.toBytes(&quot;f2&quot;))); scan.setFilter(filter); Table t = conn.getTable(tname); ResultScanner rs = t.getScanner(scan); Iterator&lt;Result&gt; it = rs.iterator(); while (it.hasNext()) { Result r = it.next(); byte[] f1id = r.getValue(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;)); byte[] f2id = r.getValue(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;id&quot;)); System.out.println(f1id + &quot; : &quot; + f2id); } } /** * 测试QualifierFilter(列过滤器) */ @Test public void testColFilter() throws IOException { Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); TableName tname = TableName.valueOf(&quot;ns1:t7&quot;); Scan scan = new Scan(); QualifierFilter colfilter = new QualifierFilter(CompareFilter.CompareOp.EQUAL, new BinaryComparator(Bytes.toBytes(&quot;id&quot;))); scan.setFilter(colfilter); Table t = conn.getTable(tname); ResultScanner rs = t.getScanner(scan); Iterator&lt;Result&gt; it = rs.iterator(); while (it.hasNext()) { Result r = it.next(); byte[] f1id = r.getValue(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;)); byte[] f2id = r.getValue(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;id&quot;)); byte[] f2name = r.getValue(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;name&quot;)); System.out.println(f1id + &quot; : &quot; + f2id + &quot; : &quot; + f2name); } } /** * 测试ValueFilter(值过滤器) * 过滤value的值，含有指定的字符子串 */ @Test public void testValueFilter() throws IOException { Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); TableName tname = TableName.valueOf(&quot;ns1:t7&quot;); Scan scan = new Scan(); ValueFilter filter = new ValueFilter(CompareFilter.CompareOp.EQUAL, new SubstringComparator(&quot;to&quot;)); scan.setFilter(filter); Table t = conn.getTable(tname); ResultScanner rs = t.getScanner(scan); Iterator&lt;Result&gt; it = rs.iterator(); while (it.hasNext()) { Result r = it.next(); byte[] f1id = r.getValue(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;)); byte[] f2id = r.getValue(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;id&quot;)); byte[] f1name = r.getValue(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;)); byte[] f2name = r.getValue(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;name&quot;)); System.out.println(f1id + &quot; : &quot; + f2id + &quot; : &quot; + Bytes.toString(f1name) + &quot; : &quot; + Bytes.toString(f2name)); } } /** * 依赖列过滤器 */ @Test public void testDepFilter() throws IOException { Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); TableName tname = TableName.valueOf(&quot;ns1:t7&quot;); Scan scan = new Scan(); DependentColumnFilter filter = new DependentColumnFilter(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;addr&quot;), true, CompareFilter.CompareOp.NOT_EQUAL, new BinaryComparator(Bytes.toBytes(&quot;beijing&quot;)) ); //ValueFilter filter = new ValueFilter(CompareFilter.CompareOp.EQUAL, new SubstringComparator(&quot;to&quot;)); scan.setFilter(filter); Table t = conn.getTable(tname); ResultScanner rs = t.getScanner(scan); Iterator&lt;Result&gt; it = rs.iterator(); while (it.hasNext()) { Result r = it.next(); byte[] f1id = r.getValue(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;)); byte[] f2id = r.getValue(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;id&quot;)); byte[] f1name = r.getValue(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;)); byte[] f2name = r.getValue(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;name&quot;)); System.out.println(f1id + &quot; : &quot; + f2id + &quot; : &quot; + Bytes.toString(f1name) + &quot; : &quot; + Bytes.toString(f2name)); } } /** * 单列值value过滤， * */ @Test public void testSingleColumValueFilter() throws IOException { Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); TableName tname = TableName.valueOf(&quot;ns1:t7&quot;); Scan scan = new Scan(); SingleColumnValueFilter filter = new SingleColumnValueFilter(Bytes.toBytes(&quot;f2&quot;, Bytes.toBytes(&quot;name&quot;), CompareFilter.CompareOp.NOT_EQUAL), new BinaryComparator(Bytes.toBytes(&quot;tom2.1&quot;))); //ValueFilter filter = new ValueFilter(CompareFilter.CompareOp.EQUAL, new SubstringComparator(&quot;to&quot;)); scan.setFilter(filter); Table t = conn.getTable(tname); ResultScanner rs = t.getScanner(scan); Iterator&lt;Result&gt; it = rs.iterator(); while (it.hasNext()) { Result r = it.next(); byte[] f1id = r.getValue(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;)); byte[] f2id = r.getValue(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;id&quot;)); byte[] f1name = r.getValue(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;)); byte[] f2name = r.getValue(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;name&quot;)); System.out.println(f1id + &quot; : &quot; + f2id + &quot; : &quot; + Bytes.toString(f1name) + &quot; : &quot; + Bytes.toString(f2name)); } } 复杂查询select * from t7 where ((age &lt;= 13) and (name like &apos;%t&apos;) or (age &gt; 13) and (name like &apos;t%&apos;)) 指定列族，指定列，指定对比方式，指定值(小于用二进制比较，等于用正则表达式串对比器) 复杂查询实现方式:FilterList@Test public void testComboFilter() throws IOException { Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); TableName tname = TableName.valueOf(&quot;ns1:t7&quot;); Scan scan = new Scan(); //where ... f2:age &lt;= 13 SingleColumnValueFilter ftl = new SingleColumnValueFilter( Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;age&quot;), CompareFilter.CompareOp.LESS_OR_EQUAL, new BinaryComparator(Bytes.toBytes(&quot;13&quot;)) ); //where ... f2:name like %t SingleColumnValueFilter ftr = new SingleColumnValueFilter( Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;name&quot;), CompareFilter.CompareOp.EQUAL, new RegexStringComparator(&quot;^t&quot;)//以t开头 ); //ft FilterList ft = new FilterList(FilterList.Operator.MUST_PASS_ALL); ft.addFilter(ftl); ft.addFilter(ftr); //where ... f2:age &gt; 13 SingleColumnValueFilter fbl = new SingleColumnValueFilter( Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;age&quot;), CompareFilter.CompareOp.GREATER, new BinaryComparator(Bytes.toBytes(&quot;13&quot;)) ); //where ... f2:name like %t SingleColumnValueFilter fbr = new SingleColumnValueFilter( Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;name&quot;), CompareFilter.CompareOp.EQUAL, new RegexStringComparator(&quot;t$&quot;)//以t结尾 ); //ft FilterList fb = new FilterList(FilterList.Operator.MUST_PASS_ALL); fb.addFilter(fbl); fb.addFilter(fbr); FilterList fall = new FilterList(FilterList.Operator.MUST_PASS_ONE); fall.addFilter(ft); fall.addFilter(fb); scan.setFilter(fall); Table t = conn.getTable(tname); ResultScanner rs = t.getScanner(scan); Iterator&lt;Result&gt; it = rs.iterator(); while (it.hasNext()) { Result r = it.next(); byte[] f1id = r.getValue(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;)); byte[] f2id = r.getValue(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;id&quot;)); byte[] f1name = r.getValue(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;)); byte[] f2name = r.getValue(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;name&quot;)); System.out.println(f1id + &quot; : &quot; + f2id + &quot; : &quot; + Bytes.toString(f1name) + &quot; : &quot; + Bytes.toString(f2name)); } } 计数器$hbase&gt;incr &apos;ns1:t8&apos;,&apos;row1&apos;,&apos;f1:click&apos;,1 $hbase&gt;get_counter &apos;ns1:t8&apos;,&apos;row1&apos;,&apos;f1:click&apos; //得到计数器的值 [API编程] @Test public void testIncr() throws IOException { Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); TableName tname = TableName.valueOf(&quot;ns1:t8&quot;); Table t = conn.getTable(tname); Increment incr = new Increment(Bytes.toBytes(&quot;row1&quot;)); incr.addColumn(Bytes.toBytes(&quot;f1&quot;),Bytes.toBytes(&quot;daily&quot;),1); incr.addColumn(Bytes.toBytes(&quot;f1&quot;),Bytes.toBytes(&quot;weekly&quot;),10); incr.addColumn(Bytes.toBytes(&quot;f1&quot;),Bytes.toBytes(&quot;monthly&quot;),100); t.increment(incr); } coprocessor协处理器工作过程： 批处理的，等价于存储过程或者触发器 [Observer] 观察者,类似于触发器，基于事件。发生动作时，回调相应方法。 RegionObserver //RegionServer区域观察者 MasterObserver //Master节点。 WAlObserver // [Endpoint] 终端,类似于存储过程。 1.加载 [hbase-site.xml] &lt;property&gt; &lt;name&gt;hbase.coprocessor.region.classes&lt;/name&gt; &lt;value&gt;coprocessor.RegionObserverExample, coprocessor.AnotherCoprocessor&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.coprocessor.master.classes&lt;/name&gt; &lt;value&gt;coprocessor.MasterObserverExample&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.coprocessor.wal.classes&lt;/name&gt; &lt;value&gt;coprocessor.WALObserverExample, bar.foo.MyWALObserver&lt;/value&gt; &lt;/property&gt; 2.自定义观察者 [MyRegionObserver] package com.it18zhang.hbasedemo.coprocessor; import org.apache.hadoop.hbase.Cell; import org.apache.hadoop.hbase.CoprocessorEnvironment; import org.apache.hadoop.hbase.client.Delete; import org.apache.hadoop.hbase.client.Durability; import org.apache.hadoop.hbase.client.Get; import org.apache.hadoop.hbase.client.Put; import org.apache.hadoop.hbase.coprocessor.BaseRegionObserver; import org.apache.hadoop.hbase.coprocessor.ObserverContext; import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment; import org.apache.hadoop.hbase.regionserver.wal.WALEdit; import org.apache.hadoop.hbase.util.Bytes; import java.io.FileWriter; import java.io.IOException; import java.util.List; /** * 自定义区域观察者 */ public class MyRegionObserver extends BaseRegionObserver{ private void outInfo(String str){ try { FileWriter fw = new FileWriter(&quot;/home/centos/coprocessor.txt&quot;,true); fw.write(str + &quot;\r\n&quot;); fw.close(); } catch (Exception e) { e.printStackTrace(); } } public void start(CoprocessorEnvironment e) throws IOException { super.start(e); outInfo(&quot;MyRegionObserver.start()&quot;); } public void preOpen(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e) throws IOException { super.preOpen(e); outInfo(&quot;MyRegionObserver.preOpen()&quot;); } public void postOpen(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e) { super.postOpen(e); outInfo(&quot;MyRegionObserver.postOpen()&quot;); } @Override public void preGetOp(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Get get, List&lt;Cell&gt; results) throws IOException { super.preGetOp(e, get, results); String rowkey = Bytes.toString(get.getRow()); outInfo(&quot;MyRegionObserver.preGetOp() : rowkey = &quot; + rowkey); } public void postGetOp(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Get get, List&lt;Cell&gt; results) throws IOException { super.postGetOp(e, get, results); String rowkey = Bytes.toString(get.getRow()); outInfo(&quot;MyRegionObserver.postGetOp() : rowkey = &quot; + rowkey); } public void prePut(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Put put, WALEdit edit, Durability durability) throws IOException { super.prePut(e, put, edit, durability); String rowkey = Bytes.toString(put.getRow()); outInfo(&quot;MyRegionObserver.prePut() : rowkey = &quot; + rowkey); } @Override public void postPut(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Put put, WALEdit edit, Durability durability) throws IOException { super.postPut(e, put, edit, durability); String rowkey = Bytes.toString(put.getRow()); outInfo(&quot;MyRegionObserver.postPut() : rowkey = &quot; + rowkey); } @Override public void preDelete(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Delete delete, WALEdit edit, Durability durability) throws IOException { super.preDelete(e, delete, edit, durability); String rowkey = Bytes.toString(delete.getRow()); outInfo(&quot;MyRegionObserver.preDelete() : rowkey = &quot; + rowkey); } @Override public void postDelete(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Delete delete, WALEdit edit, Durability durability) throws IOException { super.postDelete(e, delete, edit, durability); String rowkey = Bytes.toString(delete.getRow()); outInfo(&quot;MyRegionObserver.postDelete() : rowkey = &quot; + rowkey); } } 2.注册协处理器并分发 &lt;property&gt; &lt;name&gt;hbase.coprocessor.region.classes&lt;/name&gt; &lt;value&gt;com.it18zhang.hbasedemo.coprocessor.MyRegionObserver&lt;/value&gt; &lt;/property&gt; 3.导出jar包。 4.复制jar到共享目录，分发到jar到hbase集群的hbase lib目录下. [/soft/hbase/lib]]]></content>
  </entry>
  <entry>
    <title><![CDATA[SSM第二天]]></title>
    <url>%2F2018%2F11%2F19%2FSSM%E7%AC%AC%E4%BA%8C%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[mybatis持久化技术。jdbc sql Spring 业务层框架。 管理bean的。 new Map&lt;String,Object&gt; 体验spring1.创建模块 ,添加pom.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.it18zhang&lt;/groupId&gt; &lt;artifactId&gt;springdemo&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;4.3.3.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; 2.创建java类 public class WelcomeService { private String message ; public String getMessage() { return message; } public void setMessage(String message) { this.message = message; } public void sayHello(){ System.out.println(message); } } 3.创建配置文件 [resources/beans.xml] &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;bean id=&quot;ws&quot; class=&quot;com.it18zhang.springdemo.service.WelcomeService&quot;&gt; &lt;property name=&quot;message&quot; value=&quot;hello world&quot; /&gt; &lt;/bean&gt; &lt;/beans&gt; 4.创建App [App.java] public static void main(String[] args) { //创建容器 ApplicationContext ac = new ClassPathXmlApplicationContext(&quot;beans.xml&quot;); WelcomeService ws = (WelcomeService)ac.getBean(&quot;ws&quot;); ws.sayHello(); } spring的注解方式使用0.增加pom.xml文件 &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;version&gt;4.3.3.RELEASE&lt;/version&gt; &lt;/dependency&gt; 1.UserDao增加@Repository注解. @Repository(&quot;userDao&quot;) public class UserDao{ ... } 2.Service增加 @Service注解。 @Service(&quot;ws&quot;) public class WelcomeService { ... //注入指定的dao对象 @Resource(name = &quot;userDao&quot;) public void setDao(UserDao dao) { this.dao = dao; } } 3.修改beans.xml文件，引入context空间，使用组件扫描。 [resrouces/beans.xml] &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.3.xsd&quot;&gt; &lt;context:component-scan base-package=&quot;com.it18zhang.springdemo.dao,com.it18zhang.springdemo.service&quot; /&gt; 4.测试App.java public class App { public static void main(String[] args) { //创建容器 ApplicationContext ac = new ClassPathXmlApplicationContext(&quot;beans.xml&quot;); WelcomeService ws = (WelcomeService)ac.getBean(&quot;ws&quot;); ws.sayHello(); } } spring 整合mybatis1.创建模块 pom.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.it18zhang&lt;/groupId&gt; &lt;artifactId&gt;springmybatisdemo&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.17&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;c3p0&lt;/groupId&gt; &lt;artifactId&gt;c3p0&lt;/artifactId&gt; &lt;version&gt;0.9.1.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;version&gt;4.3.3.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;version&gt;4.3.3.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aop&lt;/artifactId&gt; &lt;version&gt;4.3.3.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt; &lt;version&gt;4.3.3.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt; &lt;version&gt;1.3.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt; &lt;version&gt;1.8.10&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; 2.创建包 com.it18zhang.springmybatis.dao com.it18zhang.springmybatis.service com.it18zhang.springmybatis.util 3.配置beans.xml [resources/beans.xml] &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;!-- 数据源 --&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;com.mchange.v2.c3p0.ComboPooledDataSource&quot;&gt; &lt;property name=&quot;driverClass&quot; value=&quot;com.mysql.jdbc.Driver&quot;/&gt; &lt;property name=&quot;jdbcUrl&quot; value=&quot;jdbc:mysql://localhost:3306/mybatis&quot;/&gt; &lt;property name=&quot;user&quot; value=&quot;root&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;root&quot;/&gt; &lt;property name=&quot;maxPoolSize&quot; value=&quot;10&quot;/&gt; &lt;property name=&quot;minPoolSize&quot; value=&quot;2&quot;/&gt; &lt;property name=&quot;initialPoolSize&quot; value=&quot;3&quot;/&gt; &lt;property name=&quot;acquireIncrement&quot; value=&quot;2&quot;/&gt; &lt;/bean&gt; &lt;/beans&gt; 4.编写单元测试 @Test public void testConn() throws Exception { ApplicationContext ac = new ClassPathXmlApplicationContext(&quot;beans.xml&quot;); DataSource ds = (DataSource)ac.getBean(&quot;dataSource&quot;); System.out.println(ds.getConnection()); } 5.添加domain类 User Order Item 略 6.添加Mapper.xml映射文件 //注意：修改类的别名 resources/UserMapper.xml resources/OrderMapper.xml 7.添加mybatis-config.xml [resources/mybatis-config.xml] &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt; &lt;!DOCTYPE configuration PUBLIC &quot;-//mybatis.org//DTD Config 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot;&gt; &lt;configuration&gt; &lt;typeAliases&gt; &lt;typeAlias type=&quot;com.it18zhang.springmybatis.domain.User&quot; alias=&quot;_User&quot;/&gt; &lt;typeAlias type=&quot;com.it18zhang.springmybatis.domain.Order&quot; alias=&quot;_Order&quot;/&gt; &lt;typeAlias type=&quot;com.it18zhang.springmybatis.domain.Item&quot; alias=&quot;_Item&quot;/&gt; &lt;/typeAliases&gt; &lt;!-- 引入映射文件 --&gt; &lt;mappers&gt; &lt;mapper resource=&quot;UserMapper.xml&quot;/&gt; &lt;mapper resource=&quot;OrderMapper.xml&quot;/&gt; &lt;/mappers&gt; &lt;/configuration&gt; 8.创建Dao接口和实现类. [BaseDao.java] package com.it18zhang.springmybatis.dao; import java.util.List; /** *基本Dao接口 */ public interface BaseDao&lt;T&gt; { public void insert(T t) ; public void update(T t) ; public void delete(Integer id) ; public T selectOne(Integer id) ; public List&lt;T&gt; selectAll() ; } [UserDao.java] package com.it18zhang.springmybatis.dao; import com.it18zhang.springmybatis.domain.User; import org.mybatis.spring.support.SqlSessionDaoSupport; import java.util.List; /** */ @Repository(&quot;userDao&quot;) public class UserDao extends SqlSessionDaoSupport implements BaseDao&lt;User&gt; { public void insert(User user) { getSqlSession().insert(&quot;users.insert&quot;,user); } public void update(User user) { getSqlSession().update(&quot;users.update&quot;, user); } public void delete(Integer id ) { getSqlSession().delete(&quot;users.delete&quot;, id); } public User selectOne(Integer id) { return getSqlSession().selectOne(&quot;users.selectOne&quot;,id) ; } public List&lt;User&gt; selectAll() { return getSqlSession().selectList(&quot;users.selectAll&quot;); } } a [OrderDao.java] 略 9.创建BaseService&lt;T&gt;.java接口 + UserService.java + UserServcieImpl.java [BaseService.java] package com.it18zhang.springmybatis.service; import java.util.List; /** * Created by Administrator on 2017/4/7. */ public interface BaseService&lt;T&gt; { public void insert(T t); public void update(T t); public void delete(Integer id); public T selectOne(Integer id); public List&lt;T&gt; selectAll(); } [BaseServiceImpl.java] public abstract class BaseServiceImpl&lt;T&gt; implements BaseService&lt;T&gt; { private BaseDao&lt;T&gt; dao ; public void setDao(BaseDao&lt;T&gt; dao) { this.dao = dao; } public void insert(T t) { dao.insert(t); } ... } [UserService.java] public interface UserService extends BaseService&lt;User&gt; { } [UserServiceImpl.java] @Service(&quot;userService&quot;) public class UserServiceImpl extends BaseServiceImpl&lt;User&gt; implements UserService{ /*** 重写该方法，注入指定的Dao对象 ***/ @Resource(name=&quot;userDao&quot;) public void setDao(BaseDao&lt;User&gt; dao) { super.setDao(dao); } } 10.完善spring的配置文件. &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;tx:advice id=”txAdvice” transaction-manager=”txManager”&gt; tx:attributes &lt;tx:method name=”“ propagation=”REQUIRED” isolation=”DEFAULT”/&gt; &lt;/tx:attributes&gt; &lt;/tx:advice&gt; aop:config &lt;!–advisor代表切入点通知，吧事务txAdvice加到一个地方去，这个地方叫做切入点，哪里执行execution呢，就是任何地方 第一个匹配的是函数的返回值 任何函数返回值都可以，然后.代表包，任何包以及包的子包，然后.service第一个点代表 包和类的分割符，后面就是以Service结尾的任何接口或类，然后最后括号里面的..代表参数不限，随便什么参数都可以–&gt; &lt;aop:advisor advice-ref=”txAdvice” pointcut=”execution( ..Service.(..))” /&gt; &lt;/aop:config&gt; &lt;context:component-scan base-package=”com.it18zhang.springmybatis.dao,com.it18zhang.springmybatis.service” /&gt; 11.测试UserService @Test public void testUserService() throws Exception { ApplicationContext ac = new ClassPathXmlApplicationContext(&quot;beans.xml&quot;); UserService us = (UserService)ac.getBean(&quot;userService&quot;); User u = new User(); u.setName(&quot;jerry&quot;); u.setAge(12); us.insert(u); } select i.id iid,i.itemname iitemname,o.id oid,o.orderno oorderno , u.id uid ,u.name uname ,u.age uagefrom items i left outer join orders o on i.oid = o.id left outer join users u on o.uid = u.idwhere i.id = 2 我们一般吧最基础的增删改查里面放到baseservice这个接口里面，这里面是公共的功能，所以在下面还需要有分开的叉开的，需要有userservice继承自baseservice,他也是一个接口，然后在userservice里面有什么需要加的功能加到这个里面，也就是说在服务层需要有什么功能的都加到这个接口里面，避免都要实现。这个地方是要继承的。 一个spring整合mybatis的rose图： 整个结构：这个里面如果直接接受dao可以在调试中get到他的具体的内容，如果接受service就不行因为事务管理封装起了sevice。所以接收到的是一个事务]]></content>
      <tags>
        <tag>SSM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase第一天]]></title>
    <url>%2F2018%2F11%2F16%2FHbase%E7%AC%AC%E4%B8%80%E5%A4%A9%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Hbase第二天]]></title>
    <url>%2F2018%2F11%2F16%2FHbase%E7%AC%AC%E4%BA%8C%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[start-hbase.sh hbase-daemon.sh start master habse-daemon.sh start regionserver hbase的ha设置： 直接打开S202或者s203的master进程即可，启动命令如上图。 hbase shell操作： $&gt;hbaes shell $hbase&gt;help namespace 类似于Mysql库的概念 insert intonosql: not only SQLkey-valueput用来放kv对。在建表的时候没有指定列明，指定的是列族名，然后这个列族下的列是可以增减的。help ‘put’ habase shell 操作： $&gt;hbase shell //登陆shell终端 $hbase&gt;help // $hbase&gt;help &apos;list_namespace&apos; //查看特定 的命令帮助 $hbase&gt;list_namespace //列出名字空间（数据库） $hbase&gt;list_namespace_tables &apos;default&apos; //列出名字空间 $hbase&gt;create_namespace &apos;ns1&apos; //创建名字空间 $hbase&gt;help &apos;create&apos; // $hbase&gt;create &apos;ns1:t1&apos;,&apos;f1&apos; //创建表，指定空间下 $hbase&gt;put &apos;ns1:t1&apos;,&apos;row1&apos;,&apos;f1:id&apos;,100 $hbase&gt;put &apos;ns1:t1&apos;,&apos;row1&apos;,&apos;f1:name&apos;,tom $hbase&gt;get &apos;ns1:t1&apos;,&apos;row1&apos; //指定查询row $hbase&gt;scan &apos;ns1:t1&apos; //权标扫描扫描ns1列族的t1列 三级坐标定位，一个是列族，一个是row一个是时间戳如下图; 通过java api操作hbase: package com.it18zhang.hbasedemo; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.hbase.HBaseConfiguration; import org.apache.hadoop.hbase.TableName; import org.apache.hadoop.hbase.client.Connection; import org.apache.hadoop.hbase.client.ConnectionFactory; import org.apache.hadoop.hbase.client.Put; import org.apache.hadoop.hbase.client.Table; import org.apache.hadoop.hbase.util.Bytes; import org.junit.Test; eate 2018/11/17 11:56 */ public class TestCRUD { @Test public void put() throws Exception { Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); TableName tname = TableName.valueOf(&quot;ns1:t1&quot;); Table table = conn.getTable(tname); byte[] rowid = Bytes.toBytes(&quot;row2&quot;); byte[] f1 = Bytes.toBytes(&quot;f1&quot;); byte[] id = Bytes.toBytes(&quot;id&quot;); byte[] value = Bytes.toBytes(102); //创建put对象 Put put = new Put(rowid); put.addColumn(f1, id, value); table.put(put); } } pom文件： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.it18zhang&lt;/groupId&gt; &lt;artifactId&gt;HbaseDemo&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-client&lt;/artifactId&gt; &lt;version&gt;1.2.3&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; hbase架构介绍： 关于区域服务器 每个大表按照rowkey来切分，rowkey是可排序的，每个切分的被分派到每个区域分五期里面。key是有序的。而且每个取余服务器的表都是有范围的，所以在get的时候要指定rowkey，然后查询的时候定位到这个rowkey在哪个区域服务器上，因为所有的区域服务器信息都在meta表这个自带的表里面，叫元数据表。里面存储着rowkey。也就是每个区域的rowkey的范围。 看里面的内容hbase；namespace,,14….,74….这个就是名字空间表，起始的位置，结束的rowkey位置。前4大段，是指的是一大行的前4列。两个逗号意味着没有切割。就只是在一个区域服务器里面 再看下面的ns1:t1表的都是一行，然后3列不同的列，如果查询这部分直接联系s204,如果204挂了， 这个表就会更新，变成承接的新表的数据。这个ns1:t1的这个表就没有分割。column是列，info是列族，server是列 在hbase查询的时候是3级定位。时间戳，列族（列），rowkey（行） 看一下下面的这个目录： hbase,数据，ns1名字空间，t1表，区域服务器,f1列族，然后列族下面就是文件了二进制存储，内容都在这个二进制文件里面。 一个表可以有多个区域，一个区域服务器肯定有多个区域，由于有副本的存在所以一个区域可以在多个服务器上，区域服务器是来提供服务的，读写过程是由区域服务器完成，相当于数据节点，这2个都是完成数据的读写过程的。 在区域里面有个store在里面有MemStore。所以这些内容都是在内存当中，如果内存满了，就会被溢出到磁盘当中，Storefile就是在磁盘当中了，Hfile通过和底层交互 hbase的写入过程： root这个地方写错了是老版本的，应该是meta表 现在妖王一个表里写数据，需要先查询这个表在那个区域服务器上，要先查询hbase空间下的meta表，因为这个表里面标识了类似于一个目录的内容，标识那个区域子在那个服务器上，也就是rowkey的范围。这个meta表在哪里呢怎么找到呢？ meta表要通过zk来找到。因为zk集群在，所以没有zk的情况下hbase都起不来。 进入到hbase shell里面 通过进入到shell里面发现这个meta表大概可能在这个里面，进去之后发现这个meta表在s203里面。如下图 所以实际上就是先联系zk然后通过zk找到他的位置在s203里面，这个meta表在s203里面 ta/hbase/meta/1588230740/info/da7a63e29d1c4fb588068ea9c187ae27 hbase基于hdfs【表数据的存储结构目录构成】 hdfs://s201:8020/hbase/data/${名字空间}/${表名}/区域名称/列族名称 相同列族的数据存放在一个文件中， 【WAL写前日志目录结构构成】 hdfs://s201:8020/hbase/wals/s202,16020,1542534586029/s202%2C16020%2C1542534586029.default.1542541792199 hdfs://s201:8020/hbase/wals/${区域服务器名称}/主机名，端口号，时间戳/ client端交互过程0.集群启动时，master负责分配区域到指定的区域服务器 1.联系zk找出meta表所在的区域服务器rs(regionserver) /meta/meta-region-server 定位到所在的服务器 2.定位rowkey，找到对应的rs(regionserver) 3.缓存信息到本地， 4.联系regionserver 5.HRegionServe负责open-HRegion对象打开H区域服务器对象，为每个列族创建store实例，说明store是和列族对应的，store包涵多个storefile实例，他们是对hfile的轻量级封装，每个store还对应一个memstroe（用于内存储速度快）， 在百万数据存储的时候：关闭WALS 代码如下： @Test public void biginsert() throws Exception { long start=System.currentTimeMillis(); Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); TableName tname = TableName.valueOf(&quot;ns1:t1&quot;); HTable table = (HTable) conn.getTable(tname); //不要自动清理缓冲区 table.setAutoFlushTo(false); for (int i = 0; i &lt; 1000000; i++) { Put put = new Put(Bytes.toBytes(&quot;row&quot; + i)); //关闭写前日志 put.setWriteToWAL(false); put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;), Bytes.toBytes(i)); put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;), Bytes.toBytes(&quot;tom&quot; + i)); put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;), Bytes.toBytes(i % 100)); table.put(put); if (i % 2000 == 0) { table.flushCommits(); } } table.flushCommits(); System.out.println(System.currentTimeMillis()-start); } hbase shell命令： 要想删除表，先要禁用表。 $hbase&gt;flush &apos;ns1:t1&apos; //清理内存数据到磁盘 $hbase&gt;count &apos;ns1:t1&apos; //统计函数 $hbase&gt;disable &apos;ns1:t1&apos; //删除表之前要禁用表 $hbase&gt;drop &apos;ns1:t1&apos; //删除表 $hbase&gt;count &apos;hbase:meta&apos; //查看元数据表 格式化代码，设置固定数字格式 @Test public void formatNum(){ DecimalFormat format =new DecimalFormat(); format.applyPattern(&quot;0000000&quot;); // format.applyPattern(&quot;###,###,00&quot;); System.out.println(format.format(8)); } 为什么要格式化rowid，因为rowid1008和8相比较8比1008还要大，所以要格式化一下。 经过格式化rowid的代码： @Test public void biginsert() throws Exception { DecimalFormat format =new DecimalFormat(); format.applyPattern(&quot;0000000&quot;); System.out.println(format.format(8)); long start=System.currentTimeMillis(); Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); TableName tname = TableName.valueOf(&quot;ns1:t1&quot;); HTable table = (HTable) conn.getTable(tname); //不要自动清理缓冲区 table.setAutoFlushTo(false); for (int i = 0; i &lt; 10000; i++) { Put put = new Put(Bytes.toBytes(&quot;row&quot; + format.format(i))); //关闭写前日志 put.setWriteToWAL(false); put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;), Bytes.toBytes(i)); put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;), Bytes.toBytes(&quot;tom&quot; + i)); put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;), Bytes.toBytes(i % 100)); table.put(put); if (i % 2000 == 0) { table.flushCommits(); } } table.flushCommits(); System.out.println(System.currentTimeMillis()-start); } flush命令$hbase:flush：清理内存数据到磁盘 region拆分切割 hbase默认切割文件是10G，超过切割。 $hbase&gt;count &apos;ns1:t1&apos; //统计函数 切割而一旦完成会保留原来的表结构，但是原来的数据内容已经没有了被删除了 hbase和hadoop的ha集成1.在hbase-env.sh文件添加hadoop配置文件目录到hbase_classpath环境变量[/soft/hbase/conf/hbase-env]并且分发。 export HBASE_CLASSPATH=$HBASE_CLASSPATH:/soft/hadoop/ etc/hadoop 2.在hbase/conf目录下创建到hadoop的hdfs-site.xml的符号链接 $&gt;ln -s /soft/hadoop/etc/hadoop/hdfs-site.xml /soft/hbase/conf/hdfs-site.xml 3.修改Hbase-site.xml文件中hbase.rootdir的目录值 /soft/hbase/conf/hbase-site.xml4.将之都分发出去。 继承了之后及时关闭了s201的namenode，hbase的Hmaster也不会关闭，也就是形成了高可用状态。 hbase手动移动区域手动移动区域 手动强行合并hbase块 手动切割： 拆分风暴： 在达到10G以后，几个区域同时增长，同时到达10G临界点，同时切割额，服务器工作瞬间增大。每个都在切割，就叫切割风暴，我们要避免这种情况，通过使用手动切割，避免拆分风暴。 代码操作增删改查 package com.it18zhang.hbasedemo; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.hbase.*; import org.apache.hadoop.hbase.client.*; import org.apache.hadoop.hbase.util.Bytes; import org.junit.Test; import java.io.IOException; import java.text.DecimalFormat; import java.util.Iterator; import java.util.Map; import java.util.NavigableMap; /** * @Title:TestCRUD * @Description: * @Auther: Eric Hunn * @Version:1.0 * @Create 2018/11/17 11:56 */ public class TestCRUD { @Test public void put() throws Exception { Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); TableName tname = TableName.valueOf(&quot;ns1:t1&quot;); Table table = conn.getTable(tname); byte[] rowid = Bytes.toBytes(&quot;row2&quot;); byte[] f1 = Bytes.toBytes(&quot;f1&quot;); byte[] id = Bytes.toBytes(&quot;id&quot;); byte[] value = Bytes.toBytes(102); //创建put对象 Put put = new Put(rowid); put.addColumn(f1, id, value); table.put(put); } @Test public void biginsert() throws Exception { DecimalFormat format = new DecimalFormat(); format.applyPattern(&quot;0000000&quot;); System.out.println(format.format(8)); long start = System.currentTimeMillis(); Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); TableName tname = TableName.valueOf(&quot;ns1:t1&quot;); HTable table = (HTable) conn.getTable(tname); //不要自动清理缓冲区 table.setAutoFlushTo(false); for (int i = 0; i &lt; 10000; i++) { Put put = new Put(Bytes.toBytes(&quot;row&quot; + format.format(i))); //关闭写前日志 put.setWriteToWAL(false); put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;), Bytes.toBytes(i)); put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;), Bytes.toBytes(&quot;tom&quot; + i)); put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;), Bytes.toBytes(i % 100)); table.put(put); if (i % 2000 == 0) { table.flushCommits(); } } table.flushCommits(); System.out.println(System.currentTimeMillis() - start); } @Test public void formatNum() { DecimalFormat format = new DecimalFormat(); format.applyPattern(&quot;0000000&quot;); // format.applyPattern(&quot;###,###,00&quot;); System.out.println(format.format(8)); } @Test public void createNamespace() throws Exception { //创建conf对象 Configuration conf = HBaseConfiguration.create(); //通过工厂创建连接对象 Connection conn = ConnectionFactory.createConnection(conf); Admin admin = conn.getAdmin(); NamespaceDescriptor nsd = NamespaceDescriptor.create(&quot;ns2&quot;).build(); admin.createNamespace(nsd); NamespaceDescriptor[] ns = admin.listNamespaceDescriptors(); for (NamespaceDescriptor n : ns) { System.out.println(n.getName()); } } @Test public void listNamespaces() throws Exception { //创建conf对象 Configuration conf = HBaseConfiguration.create(); //通过工厂创建连接对象 Connection conn = ConnectionFactory.createConnection(conf); Admin admin = conn.getAdmin(); NamespaceDescriptor[] ns = admin.listNamespaceDescriptors(); for (NamespaceDescriptor n : ns) { System.out.println(n.getName()); } } @Test public void createTables() throws Exception { //创建conf对象 Configuration conf = HBaseConfiguration.create(); //通过工厂创建连接对象 Connection conn = ConnectionFactory.createConnection(conf); Admin admin = conn.getAdmin(); //创建表名对象 TableName tbn = TableName.valueOf(&quot;ns2:t2&quot;); //创建表描述符对象 HTableDescriptor tbl = new HTableDescriptor(tbn); //在表描述符中添加列族创建列族描述符 HColumnDescriptor col = new HColumnDescriptor(&quot;f1&quot;); tbl.addFamily(col); admin.createTable(tbl); System.out.println(&quot;over&quot;); } @Test public void disableTable() throws Exception { Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); Admin admin = conn.getAdmin(); admin.disableTable(TableName.valueOf(&quot;ns2:t2&quot;)); } @Test public void dropTable() throws Exception { Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); Admin admin = conn.getAdmin(); admin.disableTable(TableName.valueOf(&quot;ns2:t2&quot;)); admin.deleteTable(TableName.valueOf(&quot;ns2:t2&quot;)); } @Test public void deleteData() throws IOException { Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); TableName tname = TableName.valueOf(&quot;ns1:t1&quot;); Table table = conn.getTable(tname); Delete del = new Delete(Bytes.toBytes(&quot;row0000001&quot;)); del.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;)); del.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;)); table.delete(del); System.out.println(&quot;over&quot;); } @Test public void scanall() throws IOException { Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); TableName tname = TableName.valueOf(&quot;ns1:t1&quot;); Table table = conn.getTable(tname); Scan scan = new Scan(); ResultScanner rs = table.getScanner(scan); Iterator&lt;Result&gt; it = rs.iterator(); while (it.hasNext()) { Result r = it.next();//理解这个r是对一整行的封装 byte[] value = r.getValue(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;)); System.out.println(Bytes.toString(value)); } } @Test public void scan() throws IOException { Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); TableName tname = TableName.valueOf(&quot;ns1:t1&quot;); Table table = conn.getTable(tname); Scan scan = new Scan(); scan.setStartRow(Bytes.toBytes(&quot;row0005000&quot;)); scan.setStopRow(Bytes.toBytes(&quot;row0008000&quot;)); ResultScanner rs = table.getScanner(scan); Iterator&lt;Result&gt; it = rs.iterator(); while (it.hasNext()) { Result r = it.next();//理解这个r是对一整行的封装 byte[] value = r.getValue(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;)); System.out.println(Bytes.toString(value)); } } //动态获取所有的列和列族，动态遍历，每个列和值的集合，由于强行getvalue转换成string类型，所以难免出现乱码情况 @Test public void scan2() throws IOException { Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); TableName tname = TableName.valueOf(&quot;ns1:t1&quot;); Table table = conn.getTable(tname); Scan scan = new Scan(); ResultScanner rs = table.getScanner(scan); Iterator&lt;Result&gt; it = rs.iterator(); while (it.hasNext()) { Result r = it.next();//理解这个r是对一整行的封装 Map&lt;byte[], byte[]&gt; map = r.getFamilyMap(Bytes.toBytes(&quot;f1&quot;)); for (Map.Entry&lt;byte[], byte[]&gt; entrySet : map.entrySet()) { String col = Bytes.toString(entrySet.getKey()); String val = Bytes.toString(entrySet.getValue()); System.out.println(col + &quot;:&quot; + val + &quot;,&quot;); } System.out.println(); } } @Test public void scan3() throws IOException { Configuration conf = HBaseConfiguration.create(); Connection conn = ConnectionFactory.createConnection(conf); TableName tname = TableName.valueOf(&quot;ns1:t1&quot;); Table table = conn.getTable(tname); Scan scan = new Scan(); ResultScanner rs = table.getScanner(scan); Iterator&lt;Result&gt; it = rs.iterator(); while (it.hasNext()) { Result r = it.next();//理解这个r是对一整行的封装 //得到一行的所有map，key=f1,value=Map&lt;Col,Map&lt;Timestamp,value&gt;&gt;这个结构 NavigableMap&lt;byte[], NavigableMap&lt;byte[], NavigableMap&lt;Long, byte[]&gt;&gt;&gt; map = r.getMap(); for (Map.Entry&lt;byte[], NavigableMap&lt;byte[], NavigableMap&lt;Long, byte[]&gt;&gt;&gt; entry : map.entrySet()) { //得到列族 String f = Bytes.toString(entry.getKey()); NavigableMap&lt;byte[], NavigableMap&lt;Long, byte[]&gt;&gt; colDataMap = entry.getValue(); for (Map.Entry&lt;byte[], NavigableMap&lt;Long, byte[]&gt;&gt; ets : colDataMap.entrySet()) { String c = Bytes.toString(ets.getKey()); Map&lt;Long, byte[]&gt; tsValueMap = ets.getValue(); for (Map.Entry&lt;Long, byte[]&gt; e : tsValueMap.entrySet()) { Long ts = e.getKey(); String value = Bytes.toString(e.getValue()); System.out.println(f + &quot;:&quot; + c + &quot;:&quot; + ts + &quot;=&quot; + value + &quot;,&quot;); } } } } } }]]></content>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper第二天]]></title>
    <url>%2F2018%2F11%2F15%2FZookeeper%E7%AC%AC%E4%BA%8C%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[leader推选过程(最小号选举法)1.所有节点在同一目录下创建临时序列节点。 2.节点下会生成/xxx/xx000000001等节点。 3.序号最小的节点就是leader，其余就是follower. 4.每个节点观察小于自己节点的主机。(注册观察者) 5.如果leader挂了，对应znode删除了。 6.观察者收到通知。 配置完全分布式zk集群1.挑选3台主机 s201 ~ s203 2.每台机器都安装zk tar 环境变量 3.配置zk配置文件 s201 ~ s203 [/soft/zk/conf/zoo.cfg] ... dataDir=/home/centos/zookeeper server.1=s201:2888:3888 server.2=s202:2888:3888 server.3=s203:2888:3888 4.在每台主机的/home/centos/zookeeper中添加myid,内容分别是1,2,3 [s201] $&gt;echo 1 &gt; /home/centos/zookeeper/myid [s202] $&gt;echo 2 &gt; /home/centos/zookeeper/myid [s203] $&gt;echo 3 &gt; /home/centos/zookeeper/myid 5.启动服务器集群 $&gt;zkServer.sh start ... 6.查看每台服务器的状态 $&gt;zkServer.sh status 7.修改zk的log目录 vi /soft/zk/conf/log4j.properties 修改如下： 8.创建log目录： xcall.sh &quot;mkdir /home/centos/zookeeper/log&quot; 部署细节1.在jn节点分别启动jn进程 $&gt;hadoop-daemon.sh start journalnode 2.启动jn之后，在两个NN之间进行disk元数据同步 a)如果是全新集群，先format文件系统,只需要在一个nn上执行。 [s201] $&gt;hadoop namenode -format b)如果将非HA集群转换成HA集群，复制原NN的metadata到另一个nn. 1.步骤一 [s201] $&gt;scp -r /home/centos/hadoop/dfs centos@s206:/home/centos/hadoop/ 2.步骤二 在新的nn(未格式化的nn)上运行一下命令，实现待命状态引导。 [s206] $&gt;hdfs namenode -bootstrapStandby //需要s201为启动状态,提示是否格式化,选择N. 3)在一个NN上执行以下命令，完成edit日志到jn节点的传输。 $&gt;hdfs namenode -initializeSharedEdits #查看s202,s203是否有edit数据. 4)启动所有节点. [s201] $&gt;hadoop-daemon.sh start namenode //启动名称节点 $&gt;hadoop-daemons.sh start datanode //启动所有数据节点 [s206] $&gt;hadoop-daemon.sh start namenode //启动名称节点 HA管理$&gt;hdfs haadmin -transitionToActive nn1 //切成激活态 $&gt;hdfs haadmin -transitionToStandby nn1 //切成待命态 $&gt;hdfs haadmin -transitionToActive --forceactive nn2//强行激活 $&gt;hdfs haadmin -failover nn1 nn2 //模拟容灾演示,从nn1切换到nn2 完全0开始部署hadoop HDFS的HA集群，使用zk实现自动容灾1.停掉hadoop的所有进程 2.删除所有节点的日志和本地数据. 删除/home/centos/hadoop下的所有和 /home/centos/journal下的所有 3.改换hadoop符号连接为ha 4.登录每台JN节点主机，启动JN进程. [s202-s204] $&gt;hadoop-daemon.sh start journalnode 5.登录其中一个NN,格式化文件系统(s201) $&gt;hadoop namenode -format 6.复制201目录的下nn的元数据到s206 $&gt;scp -r ~/hadoop/* centos@s206:/home/centos/hadoop 7.在未格式化的NN(s206)节点上做standby引导. 7.1)需要保证201的NN启动 $&gt;hadoop-daemon.sh start namenode 7.2)登录到s206节点，做standby引导. $&gt;hdfs namenode -bootstrapStandby 7.3)登录201，将s201的edit日志初始化到JN节点。 $&gt;hdfs namenode -initializeSharedEdits 8.启动所有数据节点. $&gt;hadoop-daemons.sh start datanode 9.登录到206,启动NN $&gt;hadoop-daemon.sh start namenode 10.查看webui http://s201:50070/ http://s206:50070/ 11.自动容灾 11.1)介绍 自动容灾引入两个组件，zk quarum + zk容灾控制器(ZKFC)。 运行NN的主机还要运行ZKFC进程，主要负责: a.健康监控 b.session管理 c.选举 11.2部署容灾 a.停止所有进程 $&gt;stop-all.sh b.配置hdfs-site.xml，启用自动容灾. [hdfs-site.xml] &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; c.配置core-site.xml，指定zk的连接地址. &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;s201:2181,s202:2181,s203:2181&lt;/value&gt; &lt;/property&gt; d.分发以上两个文件到所有节点。 12.登录其中的一台NN(s201),在ZK中初始化HA状态 $&gt;hdfs zkfc -formatZK 13.启动hdfs进程. $&gt;start-dfs.sh 14.测试自动容在(206是活跃节点) $&gt;kill -9 配置RM的HA自动容灾1.配置yarn-site.xml &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;cluster1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &lt;value&gt;s201&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &lt;value&gt;s206&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt; &lt;value&gt;s201:8088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt; &lt;value&gt;s206:8088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;s201:2181,s202:2181,s203:2181&lt;/value&gt; &lt;/property&gt; 2.使用管理命令 //查看状态 $&gt;yarn rmadmin -getServiceState rm1 //切换状态到standby $&gt;yarn rmadmin -transitionToStandby rm1 3.启动yarn集群 $&gt;start-yarn.sh 4.hadoop没有启动两个resourcemanager,需要手动启动另外一个 $&gt;yarn-daemon.sh start resourcemanager 5.查看webui 6.做容灾模拟. kill -9 hive的注意事项如果配置hadoop HA之前，搭建了Hive的话，在HA之后，需要调整路径信息. 主要是修改mysql中的dbs,tbls等相关表。 Hbasehadoop数据库，分布式可伸缩大型数据存储。 用户对随机、实时读写数据。 十亿行 x 百万列。 版本化、非关系型数据库。 FeatureLinear and modular scalability. //线性模块化扩展方式。 Strictly consistent reads and writes. //严格一致性读写 Automatic and configurable sharding of tables //自动可配置表切割 Automatic failover support between RegionServers. //区域服务器之间自动容在 Convenient base classes for backing Hadoop MapReduce jobs with Apache HBase tables. // Easy to use Java API for client access. //java API Block cache and Bloom Filters for real-time queries //块缓存和布隆过滤器用于实时查询 Query predicate push down via server side Filters //通过服务器端过滤器实现查询预测 Thrift gateway and a REST-ful Web service that supports XML, Protobuf, and binary data encoding options // Extensible jruby-based (JIRB) shell // Support for exporting metrics via the Hadoop metrics subsystem to files or Ganglia; or via JMX //可视化 面向列数据库。 hbase存储机制面向列存储，table是按row排序。 搭建hbase集群0.选择安装的主机 s201 ~ s204 1.jdk 略 2.hadoop 略 3.tar 略 4.环境变量 略 5.验证安装是否成功 $&gt;hbase version 5.配置hbase模式 5.1)本地模式 [hbase/conf/hbase-env.sh] EXPORT JAVA_HOME=/soft/jdk [hbase/conf/hbase-site.xml] ... &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;file:/home/hadoop/HBase/HFiles&lt;/value&gt; &lt;/property&gt; 5.2)伪分布式 [hbase/conf/hbase-env.sh] EXPORT JAVA_HOME=/soft/jdk [hbase/conf/hbase-site.xml] &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://localhost:8030/hbase&lt;/value&gt; &lt;/property&gt; 5.3)完全分布式(必做) [hbase/conf/hbase-env.sh] export JAVA_HOME=/soft/jdk export HBASE_MANAGES_ZK=false [hbse-site.xml] &lt;!-- 使用完全分布式 --&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hbase数据在hdfs上的存放路径 --&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://s201:8020/hbase&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置zk地址 --&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;s201:2181,s202:2181,s203:2181&lt;/value&gt; &lt;/property&gt; &lt;!-- zk的本地目录 --&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/home/centos/zookeeper&lt;/value&gt; &lt;/property&gt; 6.配置regionservers [hbase/conf/regionservers] s202 s203 s204 7.启动hbase集群(s201) $&gt;start-hbase.sh 8.登录hbase的webui http://s201:16010]]></content>
  </entry>
  <entry>
    <title><![CDATA[Avro&protobuf]]></title>
    <url>%2F2018%2F11%2F11%2FAvro-protobuf%2F</url>
    <content type="text"><![CDATA[hive数据倾斜. $hive&gt;SET hive.optimize.skewjoin=true; $hive&gt;SET hive.skewjoin.key=100000; $hive&gt;SET hive.groupby.skewindata=true; CREATE TABLE mydb.doc(line string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’ ; select t.word,count(*) from (select explode(split(line,’ ‘)) word from doc ) t group by t.word ; java串行化串行化系统protobuf，协议缓冲区。 在Hadoop里面的代码很多是通过相关语言自动生成的。 hadoop底层的rpc都是自动生成的。 java也有串行化，但是hadoop不适用，因为相对来讲比protobuf和avro复杂而且性能差一些。 当吧对象写入磁盘文件或者用于网络传输的时候要把对象变成字节数组。 串行化从本质上来讲是数据格式，就是一种格式，吧对象变成字节数组，也就是说所有数据都要落实到字节数组上，都需要这样，不管是影音文件还是别的文件，通过串行化吧他边恒字节数组 关于Javabean:标准javabean(pojo,plain old java object) 任何一个Java类也可以叫javabean.广义上。 狭义上的javabean：就是普通古老的java对象pojo:plain old java object 也就是私有的属性，getset方法，空的构造函数。比如person,学生，人。都是javabean。现实生活中的名词，在开发中都被定义成一个类，也就是说的javabean。 下面的代码就是一段javabean class Person{ public Person(){ } private String name; public void setName(String name){ this.name=name; } publc String genName(){ return name; }} google protobuf1.下载google protobuf.配置环境 protoc-2.5.0-win32.zip 1&apos;.pom.xml &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.protobuf&lt;/groupId&gt; &lt;artifactId&gt;protobuf-java&lt;/artifactId&gt; &lt;version&gt;2.5.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 2.设计对象 ... 3.描述对象 package tutorial; option java_package = &quot;com.example.tutorial&quot;; option java_outer_classname = &quot;AddressBookProtos&quot;; //这个是一个javabean，在protobuf里面叫message message Person { required string name = 1; required int32 id = 2; optional string email = 3; //下面这个是一个phonetype的枚举类 enum PhoneType { MOBILE = 0; HOME = 1; WORK = 2; } //这个也是一个javabean。 message PhoneNumber { required string number = 1; optional PhoneType type = 2 [default = HOME]; } repeated PhoneNumber phone = 4; } message AddressBook { repeated Person person = 1; } 4.编译描述 cmd&gt;protoc --java_out . xxx.proto 5.导入源代码到项目中 ... 6.使用对象 public class TestProtoBuf { @Test public void write() throws Exception{ AddressBookProtos.Person john = AddressBookProtos.Person.newBuilder() .setId(12345) .setName(&quot;tomas&quot;) .setEmail(&quot;123@123.123&quot;) .addPhone(AddressBookProtos.Person.PhoneNumber.newBuilder() .setNumber(&quot;+351 999 999 999&quot;) .setType(AddressBookProtos.Person.PhoneType.HOME) .build()) .build(); john.writeTo(new FileOutputStream(&quot;d:/prototbuf.data&quot;)); } @Test public void read() throws Exception{ AddressBookProtos.Person john = AddressBookProtos.Person.parseFrom(new FileInputStream(&quot;d:/prototbuf.data&quot;)); System.out.println(john.getName()); } } 上一段是用Protobuf进行编译和反编译的过程。然后下面这段图片是用java进行编译和反编译的过程： xml&lt;?xml version=&quot;1.0&quot;?&gt; &lt;persons&gt; &lt;person id=&quot;&quot; name=&quot;&quot;&gt; &lt;age&gt;12&lt;/age&gt; &lt;/person&gt; &lt;/person&gt; json[{ &quot;id&quot; : 1, &quot;nmae&quot; : &quot;tom&quot;, &quot;age&quot; : 20 }, { &quot;id&quot; : 2, &quot;nmae&quot; : &quot;tomas&quot;, &quot;age&quot; : 30 } ] avro (doug cutting)1.数据串行化系统 2.自描述语言. 数据结构和数据都存在文件中。跨语言。 使用json格式存储数据。 3.可压缩 + 可切割。 4.使用avro a)定义schema b)编译schema，生成java类 { //名字空间是这个，然后类型，然后名字，然后是字段数组 &quot;namespace&quot;: &quot;tutorialspoint.com&quot;, &quot;type&quot;: &quot;record&quot;, &quot;name&quot;: &quot;emp&quot;, &quot;fields&quot;: [ {&quot;name&quot;: &quot;name&quot;, &quot;type&quot;: &quot;string&quot;}, {&quot;name&quot;: &quot;id&quot;, &quot;type&quot;: &quot;int&quot;}, {&quot;name&quot;: &quot;salary&quot;, &quot;type&quot;: &quot;int&quot;}, {&quot;name&quot;: &quot;age&quot;, &quot;type&quot;: &quot;int&quot;}, {&quot;name&quot;: &quot;address&quot;, &quot;type&quot;: &quot;string&quot;} ] } c)使用java类 cmd&gt;java -jar avro-tools-1.7.7.jar compile schema emp.avsc . d)单元测试 package com.it18zhang.avrodemo.test; import org.apache.avro.Schema; import org.apache.avro.file.DataFileReader; import org.apache.avro.file.DataFileWriter; import org.apache.avro.generic.GenericData; import org.apache.avro.generic.GenericRecord; import org.apache.avro.io.DatumWriter; import org.apache.avro.specific.SpecificDatumReader; import org.apache.avro.specific.SpecificDatumWriter; import org.junit.Test; import java.io.File; import java.io.IOException; import java.util.Iterator; /** * Created by Administrator on 2017/3/23. */ public class TestAvro { // @Test // public void write() throws Exception { // //创建writer对象 // SpecificDatumWriter empDatumWriter = new SpecificDatumWriter&lt;Employee&gt;(Employee.class); // //写入文件 // DataFileWriter&lt;Employee&gt; empFileWriter = new DataFileWriter&lt;Employee&gt;(empDatumWriter); // // //创建对象 // Employee e1 = new Employee(); // e1.setName(&quot;tomas&quot;); // e1.setAge(12); // // //串行化数据到磁盘 // empFileWriter.create(e1.getSchema(), new File(&quot;d:/avro/data/e1.avro&quot;)); // empFileWriter.append(e1); // empFileWriter.append(e1); // empFileWriter.append(e1); // empFileWriter.append(e1); // //关闭流 // empFileWriter.close(); // } // // @Test // public void read() throws Exception { // //创建writer对象 // SpecificDatumReader empDatumReader = new SpecificDatumReader&lt;Employee&gt;(Employee.class); // //写入文件 // DataFileReader&lt;Employee&gt; dataReader = new DataFileReader&lt;Employee&gt;(new File(&quot;d:/avro/data/e1.avro&quot;) ,empDatumReader); // Iterator&lt;Employee&gt; it = dataReader.iterator(); // while(it.hasNext()){ // System.out.println(it.next().getName()); // } // } /** * 直接使用schema文件进行读写，不需要编译 */ @Test public void writeInSchema() throws Exception { //指定定义的avsc文件。 Schema schema = new Schema.Parser().parse(new File(&quot;d:/avro/emp.avsc&quot;)); //创建GenericRecord,相当于Employee GenericRecord e1 = new GenericData.Record(schema); //设置javabean属性 e1.put(&quot;Name&quot;, &quot;ramu&quot;); // e1.put(&quot;id&quot;, 001); // e1.put(&quot;salary&quot;, 30000); e1.put(&quot;age&quot;, 25); // e1.put(&quot;address&quot;, &quot;chennai&quot;); // DatumWriter&lt;GenericRecord&gt; empDatumWriter = new SpecificDatumWriter&lt;GenericRecord&gt;(GenericRecord.class); DataFileWriter&lt;GenericRecord&gt; empFileWriter = new DataFileWriter&lt;GenericRecord&gt;(empDatumWriter); empFileWriter.create(schema,new File(&quot;d:/avro/data/e2.avro&quot;)) ; empFileWriter.append(e1); empFileWriter.append(e1); empFileWriter.append(e1); empFileWriter.append(e1); empFileWriter.close(); } } 看一下AVSC这个编译好的avro文件里面的是什么结构： 他其实是一个json格式的结构： 非编译模式 --------------- @Test public void writeInSchema() throws Exception { //指定定义的avsc文件。 Schema schema = new Schema.Parser().parse(new File(&quot;d:/avro/emp.avsc&quot;)); //创建GenericRecord,相当于Employee GenericRecord e1 = new GenericData.Record(schema); //设置javabean属性 e1.put(&quot;name&quot;, &quot;ramu&quot;); // e1.put(&quot;id&quot;, 001); // e1.put(&quot;salary&quot;, 30000); e1.put(&quot;age&quot;, 25); // e1.put(&quot;address&quot;, &quot;chennai&quot;); // DatumWriter w1 = new SpecificDatumWriter (schema); DataFileWriter w2 = new DataFileWriter(w1); w2.create(schema,new File(&quot;d:/avro/data/e2.avro&quot;)) ; w2.append(e1); w2.append(e1); w2.close(); } /** * 反串行avro数据 */ @Test public void readInSchema() throws Exception { //指定定义的avsc文件。 Schema schema = new Schema.Parser().parse(new File(&quot;d:/avro/emp.avsc&quot;)); GenericRecord e1 = new GenericData.Record(schema); DatumReader r1 = new SpecificDatumReader (schema); DataFileReader r2 = new DataFileReader(new File(&quot;d:/avro/data/e2.avro&quot;),r1); while(r2.hasNext()){ GenericRecord rec = (GenericRecord)r2.next(); System.out.println(rec.get(&quot;name&quot;)); } r2.close(); }]]></content>
      <categories>
        <category>Avro&amp;Protobuf</category>
      </categories>
      <tags>
        <tag>Avro</tag>
        <tag>Protobuf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[笔记重点总结]]></title>
    <url>%2F2018%2F11%2F11%2F%E7%AC%94%E8%AE%B0%E9%87%8D%E7%82%B9%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[在Hadoop中重点是，全排序，二次排序。在视频的最后有一天是讲解一个二次排序的实例，我没有细看。 在Hive中我认为重点的是关于查询的内容，因为在hive中一般都是加载数据然后查询，查询之后有一个自定义函数UDF的内容，这部分的内容看的不是很清晰。 在avro和Protoc里面主要讲解2中反编译的方法，主要是一个面向对象开发的一个过程，还是java编程不熟悉，然后代码跟着走了一遍但是肯定还是有问题，视频里面的老师是在一遍看外文的书籍，根据里面的代码模板来写的。在自己敲的过程中protoc的代码引入maven依赖包之后，还是没有能够将编译好的文件识别出来，不知道什么问题，但是avo就引入maven之后解决了这个问题。 在讲解zookeeper的时候我觉得在选举算法的时候他没有细致的讲解，然后这边主要是zookeeper部署在hadoop集群和使用API控制zookeeper。在集群部署高可用的时候是201202203部署zk202203204部署journalnode节点，（journalnode节点已经快忘记了）。 容灾管理器进程ZKFC在配置的时候后来跟着yarn启动了起来。下面图是整个的进程图： 这个zookeeper的地方看的比较快，对于每个目录做什么的比较忘记，然后在配置的时候没有细究配置的意义。就是跟着走了一遍。但是最后配置了起来了。配置hdfs高可用的时候比较费时费力，配置内容比较多，配置yarn的时候比较简单，配置一下即可。 在hbase安装的过程中，由于开始的时候设置了zookeeper的HA自动容灾，所以最开始的时候s206自动设置为active模式，要把s206设置为standby模式才能在s201里面设置s201为HMaster。 也就是说要杀死掉s206里面的namenode或者说通过转换吧他变成standby然后s201设置为active模式。不然会导致出错 几个端口2181 8080 50070 8020 16010 在讲解hbase 第二天的动态遍历的那个三层For循环并没有看明白，也就是说Java的基础还是很薄弱的。讲解hbase一个三层循环嵌套来scan的没有弄明白。hbase的时候讲解的重点在于协处理器的理解和那个电信的一个calllogs的rowkey的设计。利用了一个二级索引的方式。这个地方的原理很重要。然后最后有一个平时用于生产的工具叫phoenix的工具，可以用类sql的语句写出来，避免了自己设二次索引。里面有大量协处理器的封装，可以直接用sql语句。这边又讲了一个hive和hbase集成的一个问题，这个直接在hive中直接写语句即可。注意看一下phoenix和hive集成的区别。]]></content>
      <categories>
        <category>笔记总结</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>重点</tag>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[心情日记——2018.11.09]]></title>
    <url>%2F2018%2F11%2F09%2F%E4%BA%BA%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%8A%AA%E5%8A%9B%EF%BC%9F%E2%80%94%E2%80%942018-11-09%2F</url>
    <content type="text"><![CDATA[无意之间从知乎里面看见的题目是：人这一辈子为什么要努力？ 泪目，与诸君共勉。 奶奶五十岁才来的美国，身无分文，也没有一技之长，随身携带的只有长年辛勤劳作留下的一身病根。 那是九十年代初，她去了唐人街的扎花厂上班，每天工作十个小时以上，一年工作三百六十五天。 扎花按件算钱，她眼神虽然不好，却比谁扎得都快。 有人弹吉他磨出了茧，有人搬砖头磨出了茧，她被针头扎出了茧。 回国看我的时候，她给我带了费列罗巧克力，翘胡子薯片，Jif花生酱。 她给我买好看的小西装，给我买一斤几十元的黄螺。 她带我去动物园，游乐园，森林公园，带我去北京看长城，看天安门，看毛主席。 “奶奶，美国比北京还好吗？” 她用最朴实的语言向我描述纽约的繁华，告诉我美国好极了，一切都在等着我。 知乎上经常讨论富养女孩，我有一个男孩被富养的故事。 有一年，我的巧克力吃完了，薯片吃完了，花生酱吃完了，奶奶还是没有回来。 那是我第一个不愿意知道的生活真相，她中风了，瘫了半边身子。 奶奶移民美国时带去了三个未成年的儿子，二叔，三叔，四叔。 二叔和三叔在登陆美国的第二天就打工去了，年幼的四叔读了几年书后也离开了学校。 他们在亲戚家的餐馆打工，每天工作十二个小时，每周工作六天。 餐馆一年营业三百六十四天，只在感恩节那天歇业。 奶奶瘫痪后，叔叔们轮流回国，给我带好吃的，带我出去玩。 我喜欢打乒乓球，二叔给我买了乒乓球，乒乓球拍，乒乓球桌。 有一年流行四驱车，三叔给我买了一辆遥控越野车，助我碾压所有的小伙伴。 四叔年少风流，出门把妹的时候总不忘带上我，要把我培养成下一代情圣。 他们绝口不提在美国生存的艰辛，那是大人们的秘密，和我无关。 奶奶出国五年后，爸妈也去了美国。 怎一个落魄了得？夫妻俩连属于自己的房间都没有。 扎花已经被时代淘汰，只有重活可以干。我爸当过屠夫，货柜工人，货运司机。 细节不必赘述，无非就是 12小时 x 365天的陈词滥调。 后来，他们四兄弟聚首，开了一家小超市，一家餐馆，后来又开了第二家小超市，第二家餐馆。 钱赚得越来越多，老家伙们拼命工作的老毛病却没有得到丝毫缓解。 爸妈出国五年后，我也来了美国，看清了生活本来的面目。 我在纽约生活了几个月，带着半身不遂的奶奶看遍了世界之都的繁华。 在这之前，她几乎没有走出过唐人街的范围，以前对我描绘纽约时一半是转述，一半靠想象。 后来，我回到了父母的身边，结束了长达五年的骨肉分离。 我爸来车站接我，把我带到了一栋小别墅前，很得意地告诉我：“这是我们家，知道你要来，刚买的。” 我去过奶奶刚来美国时住过的那个阴暗破旧的小公寓楼，也去过爸妈栖身过的那个散发着霉味的地下室，我知道我在新家会有一张床，一个房间，但我没想到，我会有一个别墅，一个前院，一个后院，一整个铺垫好的未来。 人这一生为什么要努力？ 奶奶的答案是我，爸妈的答案是我，我的答案是他们，以及把身家性命托付于我的媳妇和孩子。 对于我来说，长大了，责任多了，自然而然地想要努力，不是为了赚很多很多钱，而是为了过上自己想要的生活，过上奶奶和父母不曾拥有过的生活。他们替我吃完了所有的苦，帮我走了九十九步，我自己只需再走一步，哪敢迟疑？ 从外曾祖父母算起，我们家族四代八十多口人已经在美国奋斗了半个多世纪。我们人人都在努力，我们一代好过一代。 如果你的人生起点不高，不曾有人为你走过人生百步中的任何一步，不打紧，你尽管努力，多出来的步数不会被浪费掉，总有你在乎的人用得着，而你迟早会遇到你在乎的人。 与诸君共勉。 顾宇的知乎回答索引]]></content>
      <categories>
        <category>心情记</category>
      </categories>
      <tags>
        <tag>心情记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive第二天]]></title>
    <url>%2F2018%2F11%2F08%2FHive%E7%AC%AC%E4%BA%8C%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[hive数据仓库,在线分析处理。 HiveQL,类似sql语言。 表,metadata-&gt;rdbms. hive处理的数据是hdfs. MR,聚合操作。 hive的2个重点一个是分区一个是桶表内部表,管理表,托管表hive,drop ,数据也删除 外部表hive表结构。 分区表目录. where 缩小查询范围。 bucket表文件。 hash clustered by &apos;&apos; join横向连接，也就是作外连接和右外连接 union竖向连接。只能指定相匹配的字段 select id ,name from a1 union select id ,cid from a2; hive union操作select id,name from customers union select id,orderno from orders ; $&gt;hive //hive --service cli $&gt;hive --servic hiveserver2 //启动hiveserver2，10000 [thriftServer] $&gt;hive --service beeline //beeline hive使用jdbc协议实现远程访问hive$hive&gt;CREATE TABLE t3(id int,name string,age int) PARTITIONED BY (Year INT, Month INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ; export$hive&gt;EXPORT TABLE customers TO &apos;/user/centos/tmp.txt&apos;; //导出表结构+数据到hdfs目录。 看一下导出来的东西，是一个目录，包括表结构和表内容。 /order全排序$hive&gt;select * from orders order by id asc ; sort,map端排序,本地有序。$hive&gt;select * from orders sort by id asc ; distribute by &amp; cluster by &amp; sort by类似于mysql的group by,进行分区操作。 //select cid , ... from orders distribute by cid sort by name ; //注意顺序. $hive&gt;select id,orderno,cid from orders distribute by cid sort by cid desc ; //cluster by ===&gt; distribute by cid sort by cid destribut by 和cluster by图解对比： 这个地方没有细讲，说主要应用还是group by 只要记住这个cluster是dirstribute by 和sort by 的组合极客 函数mysql&gt;select concat(&apos;tom&apos;,1000) ; $hive&gt;select current_database(),current_user() ; $hive&gt;tab //查看帮助 设置作业参数$hive&gt;set hive.exec.reducers.bytes.per.reducer=xxx //设置reducetask的字节数。 $hive&gt;set hive.exec.reducers.max=0 //设置reduce task的最大任务数 $hive&gt;set mapreduce.job.reduces=0 //设置reducetask个数。 动态分区-严格模式-非严格模式动态分区模式:strict-严格模式，插入时至少指定一个静态分区，nonstrict-非严格模式-可以不指定静态分区。 set hive.exec.dynamic.partition.mode=nonstrict //设置非严格模式 $hive&gt;INSERT [OVERWRITE] into TABLE employees PARTITION (country, state) SELECT ..., se.cnty, se.st FROM staged_employees se WHERE se.cnty = &apos;US&apos;; 本来有一个已经分区好了的表，然后再建立一个表，比如也是相同的字段，然后新表没有分区要插进去老表，让他自动分区，就要用到动态分区，动态分区分为严格模式和非严格模式。如下图先使用了严格模式的情况，出错，说要至少指定一个分区也就是 $hive&gt;INSERT [OVERWRITE] into TABLE employees PARTITION (country=&apos;US&apos;, state) SELECT ..., se.cnty, se.st FROM staged_employees; 但是严格模式还是要制定这个分区，所以用一下非严格模式先设置一下： set hive.exec.dynamic.partition.mode=nonstrict //设置非严格模式 然后我们开始插入： $hive&gt;INSERT [OVERWRITE] into TABLE employees PARTITION (country, state) SELECT ..., se.cnty, se.st FROM staged_employees; 这样就实现了非严格模式下的动态分区 hive事务处理在&gt;0.13.0之后支持行级事务。 #（这个地方老师没讲清楚自己没搞出来）1.所有事务自动提交。 2.只支持orc格式。 3.使用bucket表。 4.配置hive参数，使其支持事务。 $hive&gt;SET hive.support.concurrency = true;$hive&gt;SET hive.enforce.bucketing = true;$hive&gt;SET hive.exec.dynamic.partition.mode = nonstrict;$hive&gt;SET hive.txn.manager = org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;$hive&gt;SET hive.compactor.initiator.on = true;$hive&gt;SET hive.compactor.worker.threads = 1; 5.使用事务性操作 $&gt;CREATE TABLE tx(id int,name string,age int) CLUSTERED BY (id) INTO 3 BUCKETS ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; stored as orc TBLPROPERTIES (&apos;transactional&apos;=&apos;true&apos;); mysql支持这样的操作，左边选择的查询的列，并不出现在右边的分组当中这样可以查询，但是hive中不允许这样。 但是在hive当中只可以如下图查询： 聚合处理每个用户购买商品订单大于1的：这个按照cid分组的，查询每个分组里面的count(*)，也就是每个不同的cid 的 $hive&gt;select cid,count(*) c ,max(price) from orders group by cid having c &gt; 1 ; wordcount创建新表:stats(word string,c int) ; 将查询结果插入到指定表中。 按照下图这个切割方法，用一个split的函数切割，切割之后形成一个数组类型的东西 explode这个函数传入的需要是一个经过split切割之后的数组，传入之后再去展开得到如上图所示的东西。但是切出来还是需要聚合，查询每个单词的个数 对下面这个SQL语句进行讲解。先看括号里面的最里面的括号，select explode(split(line,’ ‘)) as word from doc这句话都能理解，就是先切割成数组，然后对这个数据进行explode炸裂成单词，然后as word就是别名叫word，然后再把整个查询的as t别名t，然后select t.word,count(*) c from …group by t.word order by c desc limit2;就是对t的word字段进行分组，对分组的t的字段word和计数总和进行查询，然后按照降序排序，然后limit 2取最高的2个数。 $hive&gt;select t.word,count(*) c from ((select explode(split(line, &apos; &apos;)) as word from doc) as t) group by t.word order by c desc limit 2 ; view:视图,虚表是一个虚拟的表，类似于对这个语句的封装，或者说起了一个别名，并不是真的表。 这个地方创建视图的时候不能 $hive&gt;create view v1 as select a.*,b.*from customers a left outer join default.tt b on a.id = b.cid ; 上图这样是错误的，因为在a和b里面有相同的字段，要把相同的字段修改为不同的字段，如下图： //创建视图 $hive&gt;create view v1 as select a.id aid,a.name ,b.id bid , b.order from customers a left outer join default.tt b on a.id = b.cid ; 可以在续表的基础上在查询，如下所示： //查看视图 $hive&gt;show tables ; $hive&gt;select * from v1 ; view这个在hdfs上没有哦，而是存放在了mysql里面 一个mysql里面的操作：\G 行转列select * from tbls \G：结果如下： Map端连接$hive&gt;set hive.auto.convert.join=true //设置自动转换连接,默认开启了。 //使用mapjoin连接暗示实现mapjoin $hive&gt;select /*+ mapjoin(customers) */ a.*,b.* from customers a left outer join orders b on a.id = b.cid ; 调优1.explain 使用explain查看查询计划 hive&gt;explain [extended] select count(*) from customers ; hive&gt;explain select t.name , count(*) from (select a.name ,b.id,b.orderno from customers a ,orders b where a.id = b.cid) t group by t.name ; //设置limit优化测，避免全部查询.但是树上说因为是随机取样，可能会出问题。 hive&gt;set hive.limit.optimize.enable=true //本地模式 $hive&gt;set mapred.job.tracker=local; // $hive&gt;set hive.exec.mode.local.auto=true //自动本地模式,主要用于测试不用于实战 //并行执行,同时执行不存在依赖关系的阶段。?? $hive&gt;set hive.exec.parallel=true //是自动设置好的 //严格模式, $hive&gt;set hive.mapred.mode=strict //手动设置之后。1.分区表必须指定分区进行查询，否则不让查询。 //2.order by时必须使用limit子句。 //3.不允许笛卡尔积. //设置MR的数量 hive&gt; set hive.exec.reducers.bytes.per.reducer=750000000; //设置reduce处理的字节数。 //JVM重用 $hive&gt;set mapreduce.job.jvm.numtasks=1 //-1没有限制，使用大量小文件。 //UDF //User define function,用户自定义函数 //current_database(),current_user(); //显式所有函数 $hive&gt;show functions; $hive&gt;select array(1,2,3) ; //显式指定函数帮助 $hive&gt;desc function current_database(); //表生成函数,多行函数。 $hive&gt;explode(str,exp); //按照exp切割str. 自定义函数 上图带括号的都是函数，不带括号的是命令 1.创建类，继承UDF package com.it18zhang.hivedemo.udf; import org.apache.hadoop.hive.ql.exec.Description; import org.apache.hadoop.hive.ql.exec.UDF; /** * 自定义hive函数 */ @Description(name = &quot;myadd&quot;, value = &quot;myadd(int a , int b) ==&gt; return a + b &quot;, extended = &quot;Example:\n&quot; + &quot; myadd(1,1) ==&gt; 2 \n&quot; + &quot; myadd(1,2,3) ==&gt; 6;&quot;) public class AddUDF extends UDF { public int evaluate(int a ,int b) { return a + b ; } public int evaluate(int a ,int b , int c) { return a + b + c; } } 2.打成jar包。 cmd&gt;cd {classes所在目录} cmd&gt;jar cvf HiveDemo.jar -C x/x/x/x/classes/ . 3.添加jar包到hive的类路径 //添加jar到类路径 $&gt;cp /mnt/hgfs/downloads/bigdata/data/HiveDemo.jar /soft/hive/lib 3.重进入hive $&gt;.... 4.创建临时函数 // CREATE TEMPORARY FUNCTION myadd AS &apos;com.it18zhang.hivedemo.udf.AddUDF&apos;; 5.在查询中使用自定义函数 $hive&gt;select myadd(1,2) ; 6.定义日期函数 1)定义类 public class ToCharUDF extends UDF { /** * 取出服务器的当前系统时间 2017/3/21 16:53:55 */ public String evaluate() { Date date = new Date(); SimpleDateFormat sdf = new SimpleDateFormat(); sdf.applyPattern(&quot;yyyy/MM/dd hh:mm:ss&quot;); return sdf.format(date) ; } public String evaluate(Date date) { SimpleDateFormat sdf = new SimpleDateFormat(); sdf.applyPattern(&quot;yyyy/MM/dd hh:mm:ss&quot;); return sdf.format(date) ; } public String evaluate(Date date,String frt) { SimpleDateFormat sdf = new SimpleDateFormat(); sdf.applyPattern(frt); return sdf.format(date) ; } } 2)导出jar包，通过命令添加到hive的类路径(不需要重进hive)。 $hive&gt;add jar /mnt/hgfs/downloads/bigdata/data/HiveDemo-1.0-SNAPSHOT.jar 3)注册函数 $hive&gt;CREATE TEMPORARY FUNCTION to_char AS &apos;com.it18zhang.hivedemo.udf.ToCharUDF&apos;; $hive&gt;CREATE TEMPORARY FUNCTION to_date AS &apos;com.it18zhang.hivedemo.udf.ToCharUDF&apos;; 定义Nvl函数(这个是在英文版的Hadoop权威指南上给的)package com.it18zhang.hivedemo.udf; import org.apache.hadoop.hive.ql.exec.UDFArgumentException; import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException; import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException; import org.apache.hadoop.hive.ql.metadata.HiveException; import org.apache.hadoop.hive.ql.udf.generic.GenericUDF; import org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils; import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector; /** * 自定义null值处理函数 */ public class Nvl extends GenericUDF { private GenericUDFUtils.ReturnObjectInspectorResolver returnOIResolver; private ObjectInspector[] argumentOIs; public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException { argumentOIs = arguments; //检查参数个数 if (arguments.length != 2) { throw new UDFArgumentLengthException( &quot;The operator &apos;NVL&apos; accepts 2 arguments.&quot;); } returnOIResolver = new GenericUDFUtils.ReturnObjectInspectorResolver(true); //检查参数类型 if (!(returnOIResolver.update(arguments[0]) &amp;&amp; returnOIResolver .update(arguments[1]))) { throw new UDFArgumentTypeException(2, &quot;The 1st and 2nd args of function NLV should have the same type, &quot; + &quot;but they are different: \&quot;&quot; + arguments[0].getTypeName() + &quot;\&quot; and \&quot;&quot; + arguments[1].getTypeName() + &quot;\&quot;&quot;); } return returnOIResolver.get(); } public Object evaluate(DeferredObject[] arguments) throws HiveException { Object retVal = returnOIResolver.convertIfNecessary(arguments[0].get(), argumentOIs[0]); if (retVal == null) { retVal = returnOIResolver.convertIfNecessary(arguments[1].get(), argumentOIs[1]); } return retVal; } public String getDisplayString(String[] children) { StringBuilder sb = new StringBuilder(); sb.append(&quot;if &quot;); sb.append(children[0]); sb.append(&quot; is null &quot;); sb.append(&quot;returns&quot;); sb.append(children[1]); return sb.toString(); } } 2)添加jar到类路径 ... 3)注册函数 $hive&gt;CREATE TEMPORARY FUNCTION nvl AS &apos;org.apache.hadoop.hive.ql.udf.generic.GenericUDFNvl&apos;; 一些课上用的PPT]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>join</tag>
        <tag>Hive</tag>
        <tag>内部表</tag>
        <tag>外部表</tag>
        <tag>托管表</tag>
        <tag>分区</tag>
        <tag>分桶</tag>
        <tag>union</tag>
        <tag>export</tag>
        <tag>order</tag>
        <tag>sort</tag>
        <tag>动态分区</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive第一天]]></title>
    <url>%2F2018%2F11%2F06%2FHive%E7%AC%AC%E4%B8%80%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[hive在hadoop处理结构化数据的数据仓库。 不是: 关系数据库 不是OLTP 实时查询和行级更新。 hive特点hive存储数据结构(schema)在数据库中,处理的数据进入hdfs. OLAP HQL / HiveQL hive安装1.下载hive2.1-tar.gz 2.tar开 $&gt;tar -xzvf hive-2.1.0.tar.gz -C /soft //tar开 $&gt;cd /soft/hive-2.1.0 // $&gt;ln -s hive-2.1.0 hive //符号连接 3.配置环境变量 [/etc/profile] HIVE_HOME=/soft/hive PATH=...:$HIVE_HOME/bin 4.验证hive安装成功 $&gt;hive --v 5.配置hive,使用win7的mysql存放hive的元数据. a)复制mysql驱动程序到hive的lib目录下。 ... b)配置hive-site.xml 复制hive-default.xml.template为hive-site.xml 修改连接信息为mysql链接地址，将${system:...字样替换成具体路径。 [hive/conf/hive-site.xml] &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;Username to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://192.168.231.1:3306/hive2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt; &lt;/property&gt; 6)在msyql中创建存放hive信息的数据库 mysql&gt;create database hive2 ; 6)初始化hive的元数据(表结构)到mysql中。 $&gt;cd /soft/hive/bin $&gt;schematool -dbType mysql -initSchema hive命令行操作1.创建hive的数据库$hive&gt;hive --version // $hive&gt;hive --help // $hive&gt;create database mydb2 ; // $hive&gt;show databases ; $hive&gt;use mydb2 ; $hive&gt;create table mydb2.t(id int,name string,age int); $hive&gt;drop table t ; $hive&gt;drop table mydb2.t ; $hive&gt;select * from mydb2.t ; //查看指定库的表 $hive&gt;exit ; //退出 $&gt;hive //hive --service cli $&gt;hive //hive --service cli 2.通过远程jdbc方式连接到hive数据仓库1.启动hiveserver2服务器，监听端口10000 $&gt;hive --service hiveserver2 &amp; 2.通过beeline命令行连接到hiveserver2 $&gt;beeline //进入beeline命令行(于hive --service beeline) $beeline&gt;!help //查看帮助 $beeline&gt;!quit //退出 $beeline&gt;!connect jdbc:hive2://localhost:10000/mydb2//连接到hibve数据 $beeline&gt;show databases ; $beeline&gt;use mydb2 ; $beeline&gt;show tables; //显式表 使用Hive-jdbc驱动程序采用jdbc方式访问远程数据仓库1.创建java模块 2.引入maven 3.添加hive-jdbc依赖 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.it18zhang&lt;/groupId&gt; &lt;artifactId&gt;HiveDemo&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt; &lt;version&gt;2.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; 4.App package com.it18zhang.hivedemo; import java.sql.Connection; import java.sql.DriverManager; import java.sql.ResultSet; import java.sql.Statement; /** * 使用jdbc方式连接到hive数据仓库，数据仓库需要开启hiveserver2服务。 */ public class App { public static void main(String[] args) throws Exception { Class.forName(&quot;org.apache.hive.jdbc.HiveDriver&quot;); Connection conn = DriverManager.getConnection(&quot;jdbc:hive2://192.168.231.201:10000/mydb2&quot;); Statement st = conn.createStatement(); ResultSet rs = st.executeQuery(&quot;select id , name ,age from t&quot;); while(rs.next()){ System.out.println(rs.getInt(1) + &quot;,&quot; + rs.getString(2)) ; } rs.close(); st.close(); conn.close(); } } hive中表1.managed table托管表。 删除表时，数据也删除了。 2.external table外部表。 删除表时，数据不删。 hive命令//创建表,external 外部表 $hive&gt;CREATE external TABLE IF NOT EXISTS t2(id int,name string,age int) COMMENT &apos;xx&apos; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; STORED AS TEXTFILE ; //查看表数据 $hive&gt;desc t2 ; $hive&gt;desc formatted t2 ; //加载数据到hive表 $hive&gt;load data local inpath &apos;/home/centos/customers.txt&apos; into table t2 ; //local上传文件 $hive&gt;load data inpath &apos;/user/centos/customers.txt&apos; [overwrite] into table t2 ; //移动文件 //复制表 mysql&gt;create table tt as select * from users ; //携带数据和表结构 mysql&gt;create table tt like users ; //不带数据，只有表结构 hive&gt;create table tt as select * from users ; hive&gt;create table tt like users ; //count()查询要转成mr $hive&gt;select count(*) from t2 ; $hive&gt;select id,name from t2 ; // $hive&gt;select * from t2 order by id desc ; //MR //启用/禁用表 $hive&gt;ALTER TABLE t2 ENABLE NO_DROP; //不允许删除 $hive&gt;ALTER TABLE t2 DISABLE NO_DROP; //允许删除 分区表优化手段之一，从目录的层面控制搜索数据的范围。 //创建分区表. $hive&gt;CREATE TABLE t3(id int,name string,age int) PARTITIONED BY (Year INT, Month INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ; //显式表的分区信息 $hive&gt;SHOW PARTITIONS t3; //添加分区,创建目录 $hive&gt; alter table t3 add partition (year=2014, month=12) partition (year=2014,month=11); //删除分区 hive&gt;ALTER TABLE employee_partitioned DROP IF EXISTS PARTITION (year=2014, month=11); //分区结构 hive&gt;/user/hive/warehouse/mydb2.db/t3/year=2014/month=11 hive&gt;/user/hive/warehouse/mydb2.db/t3/year=2014/month=12 //加载数据到分区表 hive&gt;load data local inpath &apos;/home/centos/customers.txt&apos; into table t3 partition(year=2014,month=11); //查询分区表 hive&gt;select * from t3 where year = 2014 and month =11; 分区对应的是文件夹是目录，分桶对应的是文件，按照id分桶的意思就是按照id哈希不同的哈希进入不同的桶 //创建桶表 $hive&gt;CREATE TABLE t4(id int,name string,age int) CLUSTERED BY (id) INTO 3 BUCKETS ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ; //加载数据不会进行分桶操作 $hive&gt;load data local inpath &apos;/home/centos/customers.txt&apos; into table t4 ; //查询t3表数据插入到t4中。t3是分区表还多了年份和月份的2个字段所以要投影查询不能直接insert into t4 select * from t3;这是错的 $hive&gt;insert into t4 select id,name,age from t3 ; //桶表的数量如何设置? //评估数据量，保证每个桶的数据量block的2倍大小。 //连接查询 $hive&gt;CREATE TABLE customers(id int,name string,age int) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ; $hive&gt;CREATE TABLE orders(id int,orderno string,price float,cid int) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ; //加载数据到表 //内连接查询 hive&gt;select a.*,b.* from customers a , orders b where a.id = b.cid ; //左外 hive&gt;select a.*,b.* from customers a left outer join orders b on a.id = b.cid ; hive&gt;select a.*,b.* from customers a right outer join orders b on a.id = b.cid ; hive&gt;select a.*,b.* from customers a full outer join orders b on a.id = b.cid ; //explode,炸裂,表生成函数。 //使用hive实现单词统计 //1.建表 $hive&gt;CREATE TABLE doc(line string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop第十一天]]></title>
    <url>%2F2018%2F11%2F01%2FHadoop%E7%AC%AC%E5%8D%81%E4%B8%80%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[数据[customers.txt] 1,tom,12 2,tom,13 3,tom,14 4,tom,15 [orders.txt] 1,no001,12.23,1 2,no001,12.23,1 3,no001,12.23,2 4,no001,12.23,2 5,no001,12.23,2 6,no001,12.23,3 7,no001,12.23,3 8,no001,12.23,3 9,no001,12.23,3 map端join1.创建Mapper package com.it18zhang.hdfs.mr.mapjoin; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.FSDataInputStream; import org.apache.hadoop.fs.FileSystem; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.NullWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Mapper; import java.io.BufferedReader; import java.io.IOException; import java.io.InputStreamReader; import java.util.HashMap; import java.util.Map; /** * join操作，map端连接。 */ public class MapJoinMapper extends Mapper&lt;LongWritable,Text,Text,NullWritable&gt; { private Map&lt;String,String&gt; allCustomers = new HashMap&lt;String,String&gt;(); //启动,初始化客户信息 protected void setup(Context context) throws IOException, InterruptedException { try { Configuration conf = context.getConfiguration(); FileSystem fs = FileSystem.get(conf); FSDataInputStream fis = fs.open(new Path(&quot;file:///d:/mr/mapjoin/customers.txt&quot;)); //得到缓冲区阅读器 BufferedReader br = new BufferedReader(new InputStreamReader(fis)); String line = null ; while((line = br.readLine()) != null){ //得到cid String cid = line.substring(0,line.indexOf(&quot;,&quot;)); allCustomers.put(cid,line); } } catch (Exception e) { e.printStackTrace(); } } protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { //订单信息 String line = value.toString(); //提取customer id String cid = line.substring(line.lastIndexOf(&quot;,&quot;) + 1); //订单信息 String orderInfo = line.substring(0,line.lastIndexOf(&quot;,&quot;)); //连接customer + &quot;,&quot; + order String customerInfo = allCustomers.get(cid); context.write(new Text(customerInfo + &quot;,&quot; + orderInfo),NullWritable.get()); } } 2.创建App package com.it18zhang.hdfs.mr.mapjoin; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.NullWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; /** * */ public class MapJoinApp { public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;); Job job = Job.getInstance(conf); //设置job的各种属性 job.setJobName(&quot;MapJoinApp&quot;); //作业名称 job.setJarByClass(MapJoinApp.class); //搜索类 //添加输入路径 FileInputFormat.addInputPath(job,new Path(args[0])); //设置输出路径 FileOutputFormat.setOutputPath(job,new Path(args[1])); //没有reduce job.setNumReduceTasks(0); job.setMapperClass(MapJoinMapper.class); //mapper类 job.setMapOutputKeyClass(Text.class); // job.setMapOutputValueClass(NullWritable.class); // job.waitForCompletion(true); } } join端连接1.自定义key package com.it18zhang.hdfs.mr.mapjoin.reducejoin; import org.apache.hadoop.io.WritableComparable; import java.io.DataInput; import java.io.DataOutput; import java.io.IOException; /** */ public class ComboKey2 implements WritableComparable&lt;ComboKey2&gt; { //0-customer 1-order private int type ; private int cid ; private int oid ; private String customerInfo = &quot;&quot; ; private String orderInfo = &quot;&quot; ; public int compareTo(ComboKey2 o) { int type0 = o.type ; int cid0= o.cid; int oid0 = o.oid; String customerInfo0 = o.customerInfo; String orderInfo0 = o.orderInfo ; //是否同一个customer的数据 if(cid == cid0){ //同一个客户的两个订单 if(type == type0){ return oid - oid0 ; } //一个Customer + 他的order else{ if(type ==0) return -1 ; else return 1 ; } } //cid不同 else{ return cid - cid0 ; } } public void write(DataOutput out) throws IOException { out.writeInt(type); out.writeInt(cid); out.writeInt(oid); out.writeUTF(customerInfo); out.writeUTF(orderInfo); } public void readFields(DataInput in) throws IOException { this.type = in.readInt(); this.cid = in.readInt(); this.oid = in.readInt(); this.customerInfo = in.readUTF(); this.orderInfo = in.readUTF(); } } 2.自定义分区类 public class CIDPartitioner extends Partitioner&lt;ComboKey2,NullWritable&gt;{ public int getPartition(ComboKey2 key, NullWritable nullWritable, int numPartitions) { return key.getCid() % numPartitions; } } 3.创建Mapper package com.it18zhang.hdfs.mr.mapjoin.reducejoin; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.NullWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.InputSplit; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.lib.input.FileSplit; import java.io.IOException; /** * mapper */ public class ReduceJoinMapper extends Mapper&lt;LongWritable,Text,ComboKey2,NullWritable&gt; { protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { // String line = value.toString() ; //判断是customer还是order FileSplit split = (FileSplit)context.getInputSplit(); String path = split.getPath().toString(); //客户信息 ComboKey2 key2 = new ComboKey2(); if(path.contains(&quot;customers&quot;)){ String cid = line.substring(0,line.indexOf(&quot;,&quot;)); String custInfo = line ; key2.setType(0); key2.setCid(Integer.parseInt(cid)); key2.setCustomerInfo(custInfo); } //order info else{ String cid = line.substring(line.lastIndexOf(&quot;,&quot;) + 1); String oid = line.substring(0, line.indexOf(&quot;,&quot;)); String oinfo = line.substring(0, line.lastIndexOf(&quot;,&quot;)); key2.setType(1); key2.setCid(Integer.parseInt(cid)); key2.setOid(Integer.parseInt(oid)); key2.setOrderInfo(oinfo); } context.write(key2,NullWritable.get()); } } 4.创建Reducer package com.it18zhang.hdfs.mr.mapjoin.reducejoin; import org.apache.hadoop.io.NullWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Reducer; import java.io.IOException; import java.util.Iterator; /** * ReduceJoinReducer,reducer端连接实现。 */ public class ReduceJoinReducer extends Reducer&lt;ComboKey2,NullWritable,Text,NullWritable&gt; { protected void reduce(ComboKey2 key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException { Iterator&lt;NullWritable&gt; it = values.iterator(); it.next(); int type = key.getType(); int cid = key.getCid() ; String cinfo = key.getCustomerInfo() ; while(it.hasNext()){ it.next(); String oinfo = key.getOrderInfo(); context.write(new Text(cinfo + &quot;,&quot; + oinfo),NullWritable.get()); } } } 5.创建排序对比器 package com.it18zhang.hdfs.mr.mapjoin.reducejoin; import com.it18zhang.hdfs.maxtemp.allsort.secondarysort.ComboKey; import org.apache.hadoop.io.WritableComparable; import org.apache.hadoop.io.WritableComparator; /** * 组合Key排序对比器 */ public class ComboKey2Comparator extends WritableComparator { protected ComboKey2Comparator() { super(ComboKey2.class, true); } public int compare(WritableComparable a, WritableComparable b) { ComboKey2 k1 = (ComboKey2) a; ComboKey2 k2 = (ComboKey2) b; return k1.compareTo(k2); } } 6.分组对比器 package com.it18zhang.hdfs.mr.mapjoin.reducejoin; import com.it18zhang.hdfs.maxtemp.allsort.secondarysort.ComboKey; import org.apache.hadoop.io.WritableComparable; import org.apache.hadoop.io.WritableComparator; /** * CID分组对比器 */ public class CIDGroupComparator extends WritableComparator{ protected CIDGroupComparator() { super(ComboKey2.class, true); } public int compare(WritableComparable a, WritableComparable b) { ComboKey2 k1 = (ComboKey2) a; ComboKey2 k2 = (ComboKey2) b; return k1.getCid() - k2.getCid(); } } 7.App package com.it18zhang.hdfs.mr.mapjoin.reducejoin; import com.it18zhang.hdfs.maxtemp.allsort.secondarysort.*; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.NullWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.input.TextInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; /** * */ public class ReduceJoinApp { public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;); Job job = Job.getInstance(conf); //设置job的各种属性 job.setJobName(&quot;ReduceJoinApp&quot;); //作业名称 job.setJarByClass(ReduceJoinApp.class); //搜索类 //添加输入路径 FileInputFormat.addInputPath(job,new Path(&quot;D:\\mr\\reducejoin&quot;)); //设置输出路径 FileOutputFormat.setOutputPath(job,new Path(&quot;D:\\mr\\reducejoin\\out&quot;)); job.setMapperClass(ReduceJoinMapper.class); //mapper类 job.setReducerClass(ReduceJoinReducer.class); //reducer类 //设置Map输出类型 job.setMapOutputKeyClass(ComboKey2.class); // job.setMapOutputValueClass(NullWritable.class); // //设置ReduceOutput类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); // //设置分区类 job.setPartitionerClass(CIDPartitioner.class); //设置分组对比器 job.setGroupingComparatorClass(CIDGroupComparator.class); //设置排序对比器 job.setSortComparatorClass(ComboKey2Comparator.class); job.setNumReduceTasks(2); //reduce个数 job.waitForCompletion(true); } } hive在hadoop处理结构化数据的数据仓库。 不是: 关系数据库 不是OLTP 实时查询和行级更新。 hive特点hive存储数据结构(schema)在数据库中,处理的数据进入hdfs. OLAP HQL / HiveQL hive安装1.下载hive2.1-tar.gz 2.tar开 $&gt;tar -xzvf hive-2.1.0.tar.gz -C /soft //tar开 $&gt;cd /soft/hive-2.1.0 // $&gt;ln -s hive-2.1.0 hive //符号连接 3.配置环境变量 [/etc/profile] HIVE_HOME=/soft/hive PATH=...:$HIVE_HOME/bin 4.验证hive安装成功 $&gt;hive --v 5.配置hive,使用win7的mysql存放hive的元数据. a)复制mysql驱动程序到hive的lib目录下。 ... b)配置hive-site.xml 复制hive-default.xml.template为hive-site.xml 修改连接信息为mysql链接地址，将${system:...字样替换成具体路径。 [hive/conf/hive-site.xml] &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;Username to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://192.168.231.1:3306/hive2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt; &lt;/property&gt; 6)在msyql中创建存放hive信息的数据库 mysql&gt;create database hive2 ; 6)初始化hive的元数据(表结构)到mysql中。 $&gt;cd /soft/hive/bin $&gt;schematool -dbType mysql -initSchema hive命令行操作1.创建hive的数据库 $hive&gt;hive --version // $hive&gt;hive --help // $hive&gt;create database mydb2 ; // $hive&gt;show databases ; $hive&gt;use mydb2 ; $hive&gt;create table mydb2.t(id int,name string,age int); $hive&gt;drop table t ; $hive&gt;drop table mydb2.t ; $hive&gt;select * from mydb2.t ; //查看指定库的表 $hive&gt;exit ; //退出 $&gt;hive //hive --service cli $&gt;hive //hive --service cli 通过远程jdbc方式连接到hive数据仓库1.启动hiveserver2服务器，监听端口10000 $&gt;hive --service hiveserver2 &amp; 2.通过beeline命令行连接到hiveserver2 $&gt;beeline //进入beeline命令行(于hive --service beeline) $beeline&gt;!help //查看帮助 $beeline&gt;!quit //退出 $beeline&gt;!connect jdbc:hive2://localhost:10000/mydb2//连接到hibve数据 $beeline&gt;show databases ; $beeline&gt;use mydb2 ; $beeline&gt;show tables; //显式表 使用Hive-jdbc驱动程序采用jdbc方式访问远程数据仓库1.创建java模块 2.引入maven 3.添加hive-jdbc依赖 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.it18zhang&lt;/groupId&gt; &lt;artifactId&gt;HiveDemo&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt; &lt;version&gt;2.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; 4.App package com.it18zhang.hivedemo; import java.sql.Connection; import java.sql.DriverManager; import java.sql.ResultSet; import java.sql.Statement; /** * 使用jdbc方式连接到hive数据仓库，数据仓库需要开启hiveserver2服务。 */ public class App { public static void main(String[] args) throws Exception { Class.forName(&quot;org.apache.hive.jdbc.HiveDriver&quot;); Connection conn = DriverManager.getConnection(&quot;jdbc:hive2://192.168.231.201:10000/mydb2&quot;); Statement st = conn.createStatement(); ResultSet rs = st.executeQuery(&quot;select id , name ,age from t&quot;); while(rs.next()){ System.out.println(rs.getInt(1) + &quot;,&quot; + rs.getString(2)) ; } rs.close(); st.close(); conn.close(); } } hive中表1.managed table 托管表。 删除表时，数据也删除了。 2.external table 外部表。 删除表时，数据不删。 hive命令//创建表,external 外部表 $hive&gt;CREATE external TABLE IF NOT EXISTS t2(id int,name string,age int) COMMENT &apos;xx&apos; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; STORED AS TEXTFILE ; //查看表数据 $hive&gt;desc t2 ; $hive&gt;desc formatted t2 ; //加载数据到hive表 $hive&gt;load data local inpath &apos;/home/centos/customers.txt&apos; into table t2 ; //local上传文件 $hive&gt;load data inpath &apos;/user/centos/customers.txt&apos; [overwrite] into table t2 ; //移动文件 //复制表 mysql&gt;create table tt as select * from users ; //携带数据和表结构 mysql&gt;create table tt like users ; //不带数据，只有表结构 hive&gt;create table tt as select * from users ; hive&gt;create table tt like users ; //count()查询要转成mr $hive&gt;select count(*) from t2 ; $hive&gt;select id,name from t2 ; // $hive&gt;select * from t2 order by id desc ; //MR //启用/禁用表 $hive&gt;ALTER TABLE t2 ENABLE NO_DROP; //不允许删除 $hive&gt;ALTER TABLE t2 DISABLE NO_DROP; //允许删除 //分区表,优化手段之一，从目录的层面控制搜索数据的范围。 //创建分区表. $hive&gt;CREATE TABLE t3(id int,name string,age int) PARTITIONED BY (Year INT, Month INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ; //显式表的分区信息 $hive&gt;SHOW PARTITIONS t3; //添加分区,创建目录 $hive&gt;alter table t3 add partition (year=2014, month=12); //删除分区 hive&gt;ALTER TABLE employee_partitioned DROP IF EXISTS PARTITION (year=2014, month=11); //分区结构 hive&gt;/user/hive/warehouse/mydb2.db/t3/year=2014/month=11 hive&gt;/user/hive/warehouse/mydb2.db/t3/year=2014/month=12 //加载数据到分区表 hive&gt;load data local inpath &apos;/home/centos/customers.txt&apos; into table t3 partition(year=2014,month=11); //创建桶表 $hive&gt;CREATE TABLE t4(id int,name string,age int) CLUSTERED BY (id) INTO 3 BUCKETS ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ; //加载数据不会进行分桶操作 $hive&gt;load data local inpath &apos;/home/centos/customers.txt&apos; into table t4 ; //查询t3表数据插入到t4中。 $hive&gt;insert into t4 select id,name,age from t3 ; //桶表的数量如何设置? //评估数据量，保证每个桶的数据量block的2倍大小。 //连接查询 $hive&gt;CREATE TABLE customers(id int,name string,age int) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ; $hive&gt;CREATE TABLE orders(id int,orderno string,price float,cid int) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ; //加载数据到表 //内连接查询 hive&gt;select a.*,b.* from customers a , orders b where a.id = b.cid ; //左外 hive&gt;select a.*,b.* from customers a left outer join orders b on a.id = b.cid ; hive&gt;select a.*,b.* from customers a right outer join orders b on a.id = b.cid ; hive&gt;select a.*,b.* from customers a full outer join orders b on a.id = b.cid ; //explode,炸裂,表生成函数。 //使用hive实现单词统计 //1.建表 $hive&gt;CREATE TABLE doc(line string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>join</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop第十天]]></title>
    <url>%2F2018%2F10%2F28%2FHadoop%E7%AC%AC%E5%8D%81%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[HA3个JournalNode,edit log 两个NN，active | standby 2NN ha的管理命令hdfs haadmin -getServiceState nn1 //查看服务 hdfs haadmin -transitionToActive nn1 //激活 hdfs haadmin -transitionToStandby nn2 //待命 hdfs haadmin -failover nn1 nn2 //对调 数据仓库OLAP //online analyze process,在线分析处理 //延迟性高. 在于后期海量数据的查询统计hive是延迟性比较高的操作实时性不好都是批量计算 数据库OLTP //online transaction process在线事务处理. //实时性好。延迟性很低 mysql针对事务性处理保证ACI特性。 HA3个JournalNode,edit log 两个NN，active | standby 2NN ha的管理命令hdfs haadmin -getServiceState nn1 //查看服务 hdfs haadmin -transitionToActive nn1 //激活 hdfs haadmin -transitionToStandby nn2 //待命 hdfs haadmin -failover nn1 nn2 //对调 数据仓库OLAP //online analyze process,在线分析处理 //延迟性高. 数据库OLTP //online transaction process在线事务处理. //实时性好。 jdbcjava database connection,java数据库连接。 java数据库连接设计套接字编程吗：面向接口编程，依托mysql驱动程序和mysql操作， jdbc就是socket，musql就是serversocket。 0 1.创建mysql数据库和表 create table users(id int primary key auto_increment , name varchar(20) , age int); 2.idea中创建jdbcDemo模块 事务:transaction,和数据库之间的一组操作。 特点. a //atomic,原子性,不可分割. c //consistent,不能破坏掉 i //isolate,隔离型. d //durable.永久性 truncate截断表，类似于delete操作，速度快，数据无法回滚。在Mysql里面操作 truncate table users ; sql语句1. 2. 3. Transactioncommit //提交 rollback //回滚 savePoint //保存点 插入10万条数据不用预处理语句import org.junit.Test; import java.sql.Connection; import java.sql.DriverManager; import java.sql.Statement; //测试增删改查基本功能 public class TestCRUD { @Test public void testStatement() throws Exception{ long start=System.currentTimeMillis(); //创建连接 String diverClass = &quot;com.mysql.jdbc.Driver&quot;; String url = &quot;jdbc:mysql://localhost:3306/big4&quot;; String username = &quot;root&quot;; String password = &quot;root&quot;; Class.forName(diverClass); Connection conn = DriverManager.getConnection(url, username, password); //关闭自动提交 conn.setAutoCommit(false); //创建语句对象 Statement st = conn.createStatement(); for(int i=0;i&lt;100000;i++){ String sql = &quot;insert into users(name,age) values(&apos;tomas&quot; + i + &quot;&apos;,&quot; + (i % 100) + &quot;)&quot;; st.execute(sql); } conn.commit(); st.close(); conn.close(); System.out.println(System.currentTimeMillis() - start); } } 使用预处理语句：import org.junit.Test; import java.sql.Connection; import java.sql.DriverManager; import java.sql.PreparedStatement; //测试增删改查基本功能 public class TestCRUD { @Test public void testPreparedStatement() throws Exception{ long start=System.currentTimeMillis(); //创建连接 String diverClass = &quot;com.mysql.jdbc.Driver&quot;; String url = &quot;jdbc:mysql://localhost:3306/big4&quot;; String username = &quot;root&quot;; String password = &quot;root&quot;; Class.forName(diverClass); Connection conn = DriverManager.getConnection(url, username, password); //关闭自动提交 conn.setAutoCommit(false); //创建语句对象 String sql = &quot;insert into users(name,age) value(?,?)&quot;; PreparedStatement ppst = conn.prepareStatement(sql); for(int i =0;i &lt; 10000;i++){ ppst.setString(1,&quot;tom&quot;+i); ppst.setInt(2,i%100); ppst.executeUpdate();//执行更新 } conn.commit(); ppst.close(); conn.close(); System.out.println(System.currentTimeMillis() - start); } } 上述代码加一个批处理：import org.junit.Test; import java.sql.Connection; import java.sql.DriverManager; import java.sql.PreparedStatement; //测试增删改查基本功能 public class TestCRUD { @Test public void testPreparedStatement() throws Exception { long start = System.currentTimeMillis(); //创建连接 String diverClass = &quot;com.mysql.jdbc.Driver&quot;; String url = &quot;jdbc:mysql://localhost:3306/big4&quot;; String username = &quot;root&quot;; String password = &quot;root&quot;; Class.forName(diverClass); Connection conn = DriverManager.getConnection(url, username, password); //关闭自动提交 conn.setAutoCommit(false); //创建语句对象 String sql = &quot;insert into users(name,age) value(?,?)&quot;; PreparedStatement ppst = conn.prepareStatement(sql); for (int i = 0; i &lt; 10000; i++) { ppst.setString(1, &quot;tom&quot; + i); ppst.setInt(2, i % 100); ppst.addBatch();//这个地方相当于一个缓冲区，相当于缓冲区里面放了2000个在传出去，减少了网络带宽速度会变快 if (i % 200 == 0) { ppst.executeUpdate();//执行更新 } } ppst.addBatch();//最后不够2000在进行一个批处理 conn.commit(); ppst.close(); conn.close(); System.out.println(System.currentTimeMillis() - start); } } 这边的一个通过自回环传输一百万次插入，传输量巨大。但是要是直接在Mysql端直接给个命令做一百万次插入，就很快。不用和服务器端交互很多的数据量。通过存储过程来写，存储过程就是一组sql语句。这些语句已经预先在数据库中存放了。无非就是循环了。要执行这些语句，就要发一条执行存储过程的指令，服务器已收到命令，就直接执行。 速度很快的。 100000条数据通过普通事务，预处理，批处理时间Statement //46698 PreparedStatent //43338 CallableStatement //14385 mysql存储过程msyql&gt;– 定义新的终止符,*不要带空格这个是注释*mysql&gt;delimiter // mysql&gt;– 创建存储过程mysql&gt;CREATE PROCEDURE simpleproc (OUT param1 INT) BEGIN SELECT COUNT(*) INTO param1 FROM users; – into 是赋值方式之一 END // mysql&gt;– 查看存储过程的状态mysql&gt;show procedure status // mysql&gt;– 查看指定存储过程创建语句mysql&gt;show create procedure simpleproc ; mysql&gt;– 调用存储过程,@a在命令中定义变量mysql&gt;call simpleproc(@a) mysql&gt;– 删除存储过程mysql&gt;show drop procedure simpleproc ; mysql&gt;– 定义加法存储过程,set赋值语句 :=mysql&gt;create procedure sp_add(in a int,in b int, out c int) begin set c := a + b ; end // java访问存储过程（调用的是上一步c=a+b这个过程）import org.junit.Test; import java.sql.*; /** * 测试基本操作 */ public class TestCRUD { /** * 存储过程 */ @Test public void testCallableStatement() throws Exception { long start = System.currentTimeMillis(); //创建连接 String driverClass = &quot;com.mysql.jdbc.Driver&quot;; String url = &quot;jdbc:mysql://localhost:3306/big4&quot;; String username = &quot;root&quot;; String password = &quot;root&quot;; Class.forName(driverClass); Connection conn = DriverManager.getConnection(url, username, password); //关闭自动提交 conn.setAutoCommit(false); //创建可调用语句，调用存储过程 CallableStatement cst = conn.prepareCall(&quot;{call sp_add(?,?,?)}&quot;); cst.setInt(1,2); //绑定参数 cst.setInt(2,3); //注册输出参数类型 cst.registerOutParameter(3,Types.INTEGER); cst.execute(); int sum = cst.getInt(3); System.out.println(sum); conn.commit(); conn.close(); System.out.println(System.currentTimeMillis() - start); } } 百万数据插入，存储过程的性能1.创建存储过程 mysql&gt;create procedure sp_batchinsert(in n int) begin DECLARE name0 varchar(20); -- 定义在begin内部 DECLARE age0 int; DECLARE i int default 0 ; while i &lt; n do set name0 := concat(&apos;tom&apos;,i) ; set age0 := i % 100 ; insert into users(name,age) values(name0,age0); set i := i + 1 ; end while ; end // 2.java代码 @Test public void testCallableStatement() throws Exception { long start = System.currentTimeMillis(); //创建连接 String driverClass = &quot;com.mysql.jdbc.Driver&quot;; String url = &quot;jdbc:mysql://localhost:3306/big4&quot;; String username = &quot;root&quot;; String password = &quot;root&quot;; Class.forName(driverClass); Connection conn = DriverManager.getConnection(url, username, password); //关闭自动提交 conn.setAutoCommit(false); //创建可调用语句，调用存储过程 CallableStatement cst = conn.prepareCall(&quot;{call sp_batchinsert(?)}&quot;); cst.setInt(1,1000000); //绑定参数 //注册输出参数类型 cst.execute(); conn.commit(); conn.close(); System.out.println(System.currentTimeMillis() - start); } mysql函数这个是创建函数的SQL参考手册： 1.函数和存储过程相似，只是多了返回值声明. 2.创建函数 mysql&gt;create function sf_add(a int ,b int) returns int begin return a + b ; end // 3.显式创建的函数 mysql&gt;show function status -- mysql&gt;show function status like &apos;%add%&apos; -- mysql&gt;select sf_add(1,2) -- 4.java调用函数 @Test public void testFunction() throws Exception { long start = System.currentTimeMillis(); //创建连接 String driverClass = &quot;com.mysql.jdbc.Driver&quot;; String url = &quot;jdbc:mysql://localhost:3306/big4&quot;; String username = &quot;root&quot;; String password = &quot;root&quot;; Class.forName(driverClass); Connection conn = DriverManager.getConnection(url, username, password); //关闭自动提交 conn.setAutoCommit(false); //创建可调用语句，调用存储过程 CallableStatement cst = conn.prepareCall(&quot;{? = call sf_add(?,?)}&quot;); cst.setInt(2,100);//设置第二个参数是100第三个参数是200，第一个参数是输出。 cst.setInt(3,200); cst.registerOutParameter(1,Types.INTEGER); //注册输出参数类型 cst.execute(); System.out.println(cst.getInt(1)); conn.commit(); conn.close(); System.out.println(System.currentTimeMillis() - start); } MVCCmultiple version concurrent control,多版本并发控制。 数据库的记录存储有多条，每一条都附加版本。所以可以实现隔离级别。是因为有MVCC多版本并发控制 事务的并发执行，容易出现的几个现象1.脏读 读未提交,一个事务读取了另外一个事务改写还没有提交的数据，如果另外一个 事务在稍后的时候回滚。 2.不可重复读 一个事务进行相同条件查询连续的两次或者两次以上，每次结果都不同。这个是对数据进行了更改 有其他事务做了update操作。 3.幻读 和(2)很像，其他事务做了insert操作.1时间查询到了10个，2时刻查询到了20个，是因为中间有做了插入操作 隔离级别（由于有MVCC）为了避免出现哪种并发现象的。 1 //read uncommitted ,读未提交 导致脏读导致不可重复读 2 //read committed ,读已提交 这里避免了脏读 4 //repeatable read ,可以重复读 这里避免了不可重复读。还是可以让他插入更新，但是读取的是更新之前的值 8 //serializable ,串行化(悲观锁) 这里避免了换读，串行化不支持并发了就已经。 演示mysql事务隔离级别1.开启mysql客户端 mysql&gt; 2.关闭自动提交 mysql&gt;set autocommit 0 ; 3.每次操作数据,都要开启事务，提交事务。 脏读现象[A] 1)mysql&gt;start transaction ; -- 开始事务 2)msyql&gt;update users set age = age + 1 where id = 1 ; -- 更新数据,没有提交 6)mysql&gt;rollback ; -- 回滚 7)mysql&gt;select * from users ; [B] 3)mysql&gt;set session transaction isolation level read uncommitted ; -- 读未提交 4)msyql&gt;start transaction ; -- 开始事务 5)mysql&gt;select * from users ; -- 13 避免脏读[A] 1)mysql&gt;start transaction ; -- 开始事务 2)msyql&gt;update users set age = age + 1 where id = 1 ; -- 更新数据,没有提交 6)mysql&gt;rollback ; -- 回滚 7)mysql&gt;select * from users ; [B] 3)mysql&gt;set session transaction isolation level read committed ; -- 读已提交 4)msyql&gt;start transaction ; -- 开始事务 5)mysql&gt;select * from users ; -- 13 测试不可重复读(隔离级别设置为读已提交不能避免不可重复读。)[A] 1)mysql&gt;commit ; 2)mysql&gt;set session transaction isolation level read committed ; -- 读已提交 3)mysql&gt;start transaction ; -- 开始事务 4)mysql&gt;select * from users ; -- 查询 9)mysql&gt;select * from users ; [B] 5)mysql&gt;commit; 6)mysql&gt;start transaction ; 7)mysql&gt;update users set age = 15 where id = 1 ; -- 更新 8)mysql&gt;commit; 测试避免不可重复读(隔离级别设置为读已提交不能避免不可重复读。)[A] 1)mysql&gt;commit ; 2)mysql&gt;set session transaction isolation level repeatable read ; -- 可以重复读 3)mysql&gt;start transaction ; -- 开始事务 4)mysql&gt;select * from users ; -- 查询 9)mysql&gt;select * from users ; [B] 5)mysql&gt;commit; 6)mysql&gt;start transaction ; 7)mysql&gt;update users set age = 15 where id = 1 ; -- 更新 8)mysql&gt;commit; 测试幻读(隔离级别设置为repeatable)[A] 1)mysql&gt;commit ; 2)mysql&gt;set session transaction isolation level serializable; -- 串行化 3)mysql&gt;start transaction ; -- 开始事务 4)mysql&gt;select * from users ; -- 查询 9)mysql&gt;select * from users ; [B] 5)mysql&gt;commit; 6)mysql&gt;start transaction ; 7)mysql&gt;insert into users(name,age) values(&apos;tomas&apos;,13); -- 更新 8)mysql&gt;commit; ANSI SQL美国国家标准结构SQL组 select * from users for update ; MySQL1.支持四种隔离级别。 2.默认隔离级别是可以重复读。 3.隔离级别是seriable,不支持并发写。 表级锁LOCK TABLE t WRITE; -- 加锁(表级锁,read) UNLOCK TABLES ; -- 解除自己所有的所有表级锁 表级锁只能通过命令来解锁。 编程实现脏读现象package com.it18zhang.jdbcdemo.test; import org.junit.Test; import java.sql.*; /** * 测试隔离级别 */ public class TestIsolationLevel { /** * 执行写，不提交 */ @Test public void testA() throws Exception{ //创建连接 String driverClass = &quot;com.mysql.jdbc.Driver&quot;; String url = &quot;jdbc:mysql://localhost:3306/big4&quot;; String username = &quot;root&quot;; String password = &quot;root&quot;; Class.forName(driverClass); Connection conn = DriverManager.getConnection(url, username, password); conn.setAutoCommit(false); Statement st = conn.createStatement(); st.execute(&quot;update users set age = 80 where id = 1&quot;); System.out.println(&quot;===============&quot;); conn.commit(); conn.close(); } /** * 查询，查到别人没有提交的数据 */ @Test public void testB() throws Exception{ //创建连接 String driverClass = &quot;com.mysql.jdbc.Driver&quot;; String url = &quot;jdbc:mysql://localhost:3306/big4&quot;; String username = &quot;root&quot;; String password = &quot;root&quot;; Class.forName(driverClass); Connection conn = DriverManager.getConnection(url, username, password); //设置隔离级别读未提交==&gt;导致脏读 /************************** 设置隔离级别 ***************************************/ conn.setTransactionIsolation(Connection.TRANSACTION_READ_UNCOMMITTED); conn.setAutoCommit(false); Statement st = conn.createStatement(); ResultSet rs = st.executeQuery(&quot;select age from users where id = 1&quot;); rs.next(); int age = rs.getInt(1) ; System.out.println(age); System.out.println(&quot;===============&quot;); conn.commit(); conn.close(); } 共享读锁独占写锁一个事务写操作，另一个塞住。行级别的锁 在独占锁这块，开启事务之后，资源存在征用的情况，如果A修改&gt;=2，b修改&lt;=3那么就出现先来先用的情况，没有提交事务之前，B一直要等待状态。然后事务是you原子性的，肯定就是非交集和交集的部分都不能更改。因为原子性。查询是由隔离级别控制。在查询的时候为了不让别人更新设置隔离级别为最高级别，就是8串行化，但是很难受，连接不关掉放到连接池里面，被下一个调用，还是串行化状态，不能写入，性能差。除了设置隔离级别，我们还可以加上一个select * from users for update。这样的话就代表仅仅是对这一个查询语句的时候执行独占写锁。这个时候不需要再设置隔离级别为8了。 SQL// insert into users(name,age,...) values(&apos;&apos;,12,..) ; -- insert update users set name = &apos;xxx&apos;,age = xxx ,... where id = xxx ; -- update delete from users where id = xxx -- delete -- 投影查询 projection. select id,name from users where ... order by limit xxx --select -- 查询时直接上独占写锁 select * from users for update ; 连接查询1.准备表[mysql.sql] drop table if exists customers; -- 删除表 drop table if exists orders ; -- 删除表 create table customers(id int primary key auto_increment , name varchar(20) , age int); -- 创建customers表 create table orders(id int primary key auto_increment , orderno varchar(20) , price float , cid int); -- 创建orders表 -- 插入数据 insert into customers(name,age) values(&apos;tom&apos;,12); insert into customers(name,age) values(&apos;tomas&apos;,13); insert into customers(name,age) values(&apos;tomasLee&apos;,14); insert into customers(name,age) values(&apos;tomason&apos;,15); -- 插入订单数据 insert into orders(orderno,price,cid) values(&apos;No001&apos;,12.25,1); insert into orders(orderno,price,cid) values(&apos;No002&apos;,12.30,1); insert into orders(orderno,price,cid) values(&apos;No003&apos;,12.25,2); insert into orders(orderno,price,cid) values(&apos;No004&apos;,12.25,2); insert into orders(orderno,price,cid) values(&apos;No005&apos;,12.25,2); insert into orders(orderno,price,cid) values(&apos;No006&apos;,12.25,3); insert into orders(orderno,price,cid) values(&apos;No007&apos;,12.25,3); insert into orders(orderno,price,cid) values(&apos;No008&apos;,12.25,3); insert into orders(orderno,price,cid) values(&apos;No009&apos;,12.25,3); insert into orders(orderno,price,cid) values(&apos;No0010&apos;,12.25,NULL); ---执行SQL文件 source d:/SQL/mysql.sql 2.查询--连接查询 mysql&gt;-- 笛卡尔积查询,无连接条件查询 mysql&gt;select a.*,b.* from customers a , orders b ; mysql&gt;-- 内连接,查询符合条件的记录. mysql&gt;select a.*,b.* from customers a , orders b where a.id = b.cid ; mysql&gt;-- 左外连接,查询符合条件的记录. mysql&gt;select a.*,b.* from customers a left outer join orders b on a.id = b.cid ; mysql&gt;-- 右外连接,查询符合条件的记录. mysql&gt;select a.*,b.* from customers a right outer join orders b on a.id = b.cid ; mysql&gt;-- 全外连接,查询符合条件的记录(mysql不支持全外链接) mysql&gt;select a.*,b.* from customers a full outer join orders b on a.id = b.cid ; 2.查询–分组 字段列表 表 条件 分组 组内条件 排序 分页 mysql&gt;select ... from ... where ... group by ... having ... order by ... limit .. mysql&gt;-- 去重查询 mysql&gt;select distinct price,cid from orders ; mysql&gt;-- 条件查询 mysql&gt;select price,cid from orders where price &gt; 12.27 ; mysql&gt;-- 聚集查询 mysql&gt;select max(price) from orders ; mysql&gt;select min(price) from orders ; mysql&gt;select avg(price) from orders ; mysql&gt;select sum(price) from orders ; mysql&gt;select count(id) from orders ; mysql&gt;-- 分组查询 mysql&gt;select max(price) from orders where cid is not null group by cid ; mysql&gt;-- 分组查询(组内过滤) mysql&gt;select cid ,orderno,max(price) as max_price,min(price) from orders where cid is not null group by cid having max_price &gt; 20 ; mysql&gt;-- 降序查询 mysql&gt;select cid ,orderno,max(price) as max_price,min(price) from orders where cid is not null group by cid having max_price &gt; 20 order by max_price desc; mysql&gt;-- 模糊查询 mysql&gt;select * from customers where name like &apos;toma%&apos; mysql&gt;select * from customers where name not like &apos;toma%&apos; mysql&gt;-- 范围查询 mysql&gt;select * from customers where id in (1,2,3) mysql&gt;select * from customers where id not in (1,2,3) mysql&gt;-- between 1 and 10,闭区间 mysql&gt;select * from customers where id between 1 and 3 ; mysql&gt;select * from customers where id &gt;= 1 and id &lt;= 3 ; mysql&gt;-- 嵌套子查询(查询没有订单的客户) mysql&gt;select * from customers where id not in (select distinct cid from orders where cid is not null); mysql&gt;-- 嵌套子查询(查询订单数量&gt;2的客户) mysql&gt;select * from customers where id in (select cid from orders group by cid having count(cid) &gt; 2); mysql&gt;select * from customers where id in ( select t.cid from (select cid,count(*) as c from orders group by cid having c &gt; 2) as t); mysql&gt;--向已有表中添加列 mysql&gt;--alter table orders add column area int; mysql&gt;--设置area字段的值 mysql&gt;--update orders set area = 2 where id in(1,3,,6,7); mysql&gt;--设置area字段的值 mysql&gt;--update orders set area = 1 where id not in(1,3,,6,7); mysql&gt;--设置area字段的值 mysql&gt;--update orders set area = 1 where id not in(1,3,,6,7); 稍微看一下下面这个SQL语句： mysq&gt;--select cid,area,count(price) from orders group by cid,area;//这句话的意思是对cid和area联合分组，什么意思：就是说要cid和area要都相同才能进入到同一个组里面去。 mysq&gt;--select cid,area,count(price),max(price),min(price) from orders group by cid,area order by cid asc, area desc ;//这句话的意思是对cid和area联合分组，什么意思：就是说要cid和area要都相同才能进入到同一个组里面去。然后这个cid升序area降序是在1里面对area降序，也就是说先对cid整体升序，然后对升序完之后相同的cid里面进行降序。 mysql&gt;-- 嵌套子查询(查询客户id,客户name,订单数量,最贵订单，最便宜订单，平均订单价格 where 订单数量&gt;2的客户) mysql&gt;select a.id,a.name,b.c,b.max,b.min,b.avg from customers a,((select cid,count(cid) c , max(price) max ,min(price) min,avg(price) avg from orders group by cid having c &gt; 2) as b) where a.id = b.cid ; hadoopMR 左外连接.]]></content>
      <categories>
        <category>Hadoop</category>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>jdbc加强</tag>
        <tag>事务操作</tag>
        <tag>批处理</tag>
        <tag>预处理</tag>
        <tag>自定义mysql存储过程</tag>
        <tag>mysql函数</tag>
        <tag>百万数据插入</tag>
        <tag>HA管理命令</tag>
        <tag>数据仓库</tag>
        <tag>数据库</tag>
        <tag>mysql事务隔离级别</tag>
        <tag>独占写锁</tag>
        <tag>共享读锁</tag>
        <tag>脏读</tag>
        <tag>不可重复读</tag>
        <tag>幻读</tag>
        <tag>串行化</tag>
        <tag>行级锁</tag>
        <tag>连接查询</tag>
        <tag>模糊查询</tag>
        <tag>外连接查询</tag>
        <tag>笛卡尔积查询</tag>
        <tag>分组查询</tag>
        <tag>嵌套子查询</tag>
        <tag>条件查询</tag>
        <tag>去重查询</tag>
        <tag>范围查询</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop第九天]]></title>
    <url>%2F2018%2F10%2F25%2FHadoop%E7%AC%AC%E4%B9%9D%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[复习：1.链式job编程 MR //Mapper+ / Reduce Mapper* 2.DBWritable 和数据库交互。 3.Sqoop 4.全排序 对reduce输出的所有结果进行排序。 5.二次排序 对value进行排序。 6.数据倾斜 1.reduce 2.自定义分区函数 数据结果错 + 二次job 3.重新设计key 数据结果错 + 二次job 机架感知机架有一块网卡。可以连接很多主机，机架上每个刀片服务器主机都连接到这个机架网卡上的。 比如一台刀片服务器上有2个进程，2个进程之间通信需要走机架上的交换机吗。不需要直接走自回环网络即可。走Localhost即可。所以2个进程之间不需要网卡交互数据。2个进程之间的距离就是0。每个进程所在的主机节点，连接他们之间的网络通路的跃点数就是他们之间的距离 如果b中绿色进程和a中红色进程需要交互就需要通过交换机，距离就是2.怎么算的，就是从每个服务器到交换机之间的距离相加，就是1+1； 如果是2个机架也要通过一个交换机连接起来。左边机架a中绿色和右边机架红色通信。距离就是4. 同一个机房通信最多就是4。也就是在通过一个交换机。 Hadoop都有机架感知能力：是他里面的一种策略，都需人为维护。在默认情况下所有节点都在同一个机架上。/default-rack。但是在临近节点数据量非常大的时候，也是不会使用的，就近原则只是一种策略，还有其他策略。 fault tolerance容错. 针对业务。 map或reduce任务失败，的这种错误。 fail over容灾. 针对硬件故障。 master / slave主(master,namenode)从(slave,datanode)结构. topology.node.switch.mapping.impl 客户端请求Namenode来读取datanodes的过程Namenode只存放文件系统的数据信息，而不存放各个节点的IP地址。存放块ID，块列表。 可靠性提供数据安全的能力。 可用性提供持续服务的能力。 默认的副本放置策略首选在本地机架的一个node存放副本,另一个副本在本地机架的另一个不同节点。 最后一个副本在不同机架的不同节点上。 hads oiv //image data metadata. 离线镜像查看器hads oev //edit 编辑日志 镜像文件存放的是元信息，元信息包括块，块ID，块列表，是不是上下级关系，副本数，权限，块大小。除了数据内容本身内容之外的。 通过实现接口改变配置实现一个机架感知。 自定义机架感知(优化hadoop集群一种方式)1.自定义实现类 package com.it18zhang.hdfs.rackaware; import org.apache.hadoop.net.DNSToSwitchMapping; import java.io.FileWriter; import java.io.IOException; import java.util.ArrayList; import java.util.List; /*机架感知实现类 吧203以下的机器设置为机架1，吧203以上的机架设置为机架2 */ public class MyRackAware implements DNSToSwitchMapping { public List&lt;String&gt; resolve(List&lt;String&gt; names) { ArrayList&lt;String&gt; list = new ArrayList&lt;String&gt;(); //true表示是不是追加模式 FileWriter fw = null; try { fw = new FileWriter(&quot;/home/centos/rackaware.txt&quot;, true); for (String str : names) { fw.write(str + &quot;\r\n&quot;); if (str.startsWith(&quot;192&quot;)) { //192.168.192.202 String ip = str.substring(str.lastIndexOf(&quot;.&quot;) + 1); if (Integer.parseInt(ip) &lt;= 203) { list.add(&quot;/rack1/&quot; + ip); } else { list.add(&quot;/rack2&quot; + ip); } } else if (str.startsWith(&quot;s&quot;)) { String ip = str.substring(1); if (Integer.parseInt(ip) &lt;= 203) { list.add(&quot;/rack1/&quot; + ip); } else { list.add(&quot;/rack2&quot; + ip); } } } } catch (IOException e) { e.printStackTrace(); } return list; } public void reloadCachedMappings() { } public void reloadCachedMappings(List&lt;String&gt; names) { } } 2.配置core-site.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://192.168.231.201/&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/centos/hadoop&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;topology.node.switch.mapping.impl&lt;/name&gt; &lt;value&gt;com.it18zhang.hdfs.rackaware.MyRackAware&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 3.导出jar包 4.复制jar到/soft/hadoop/shared/hadoop/common/lib（这个是hadoop的类路径） 5.分发jar.(可以不做) 实际上不需要分发，只在名称节点上运行。 6.重启名称节点 $&gt;hadoop-daemon.sh stop namenode $&gt;hadoop-daemon.sh start namenode 在s202上传一个文件，最后得出来确实和副本存放策略一致： 首选在本地机架的一个node存放副本,另一个副本在本地机架的另一个不同节点。 最后一个副本在不同机架的不同节点上。 关于HDFS下面这一段是关于HDFS的讲解，HDFS包括namenode和datanode，名称节点和数据节点。名称节点只是保存名称信息。那么如果在HDFS中文件移动的话怎么办，就是名称节点改变目录来移动即可。1kb和1G的速度是一样的。在HDFS中移动只是改变路径而已，块根本就没变，只能重新删掉在put才是真正移动了块。这一节就是通过实验验证了这个问题。 去IOEIBM // Oracle // EMC // HA1.NFS 网络共享存储设备。 2.QJM Quorum Journal Manager 3.两个名称节点 active //激活 standby //待命 active //激活deactive //钝化 SPOFsingle point of failure,单点故障。 事务是个特性a //atomic 原子性 c //consistent一致性 i //isolate 隔离型 d //durable 永久性· majority 大部分. HA高可用配置high availability,高可用. /home/centos/hadoop/dfs/data/current/BP-2100834435-192.168.231.201-1489328949370/current/finalized/subdir0/subdir0 两个名称节点，一个active(激活态)，一个是standby(slave待命),slave节点维护足够多状态以便于容灾。 和客户端交互的active节点,standby不交互. 两个节点都和JN守护进程构成组的进行通信。 数据节点配置两个名称节点，分别报告各自的信息。 同一时刻只能有一个激活态名称节点。 脑裂:两个节点都是激活态。 为防止脑裂，JNs只允许同一时刻只有一个节点向其写数据。容灾发生时，成为active节点的namenode接管 向jn的写入工作。 硬件资源名称节点: 硬件配置相同。 JN节点 : 轻量级进程，至少3个节点,允许挂掉的节点数 (n - 1) / 2. 不需要再运行辅助名称节点。 部署配置细节0.s201和s206具有完全一致的配置，尤其是ssh. 1.配置nameservice [hdfs-site.xml] &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;mycluster&lt;/value&gt; &lt;/property&gt; 2.dfs.ha.namenodes.[nameservice ID] [hdfs-site.xml] &lt;!-- myucluster下的名称节点两个id --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; 3.dfs.namenode.rpc-address.[nameservice ID].[name node ID] [hdfs-site.xml] 配置每个nn的rpc地址。 &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;s201:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;s206:8020&lt;/value&gt; &lt;/property&gt; 4.dfs.namenode.http-address.[nameservice ID].[name node ID] 配置webui端口 [hdfs-site.xml] &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;s201:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;s206:50070&lt;/value&gt; &lt;/property&gt; 5.dfs.namenode.shared.edits.dir 名称节点共享编辑目录. [hdfs-site.xml] &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://s202:8485;s203:8485;s204:8485/mycluster&lt;/value&gt; &lt;/property&gt; 6.dfs.client.failover.proxy.provider.[nameservice ID] java类，client使用它判断哪个节点是激活态。 [hdfs-site.xml] &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; 7.dfs.ha.fencing.methods 脚本列表或者java类，在容灾保护激活态的nn. [hdfs-site.xml] &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt; sshfence shell(/bin/true) &lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/home/centos/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt; 8.fs.defaultFS 配置hdfs文件系统名称服务。 [core-site.xml] &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://mycluster&lt;/value&gt; &lt;/property&gt; 9.dfs.journalnode.edits.dir 配置JN存放edit的本地路径。 [hdfs-site.xml] &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/home/centos/hadoop/journal&lt;/value&gt; &lt;/property&gt; 部署细节1.在jn节点分别启动jn进程 $&gt;hadoop-daemon.sh start journalnode 2.启动jn之后，在两个NN之间进行disk元数据同步 a)如果是全新集群，先format文件系统,只需要在一个nn上执行。 [s201] $&gt;hadoop namenode -format 格式化文件系统主要就是生成一个新的VERSION，里面内容就是生成一个名称空间ID集群ID时间块池ID b)如果将非HA集群转换成HA集群，复制原NN的metadata到另一个nn. 1.步骤一 [s201] $&gt;scp -r /home/centos/hadoop/dfs centos@s206:/home/centos/hadoop/ 2.步骤二 在新的nn(未格式化的nn)上运行一下命令，实现待命状态引导。 [s206] $&gt;hdfs namenode -bootstrapStandby //需要s201为启动状态,提示是否格式化,选择N. 3)在一个NN上执行以下命令，完成edit日志到jn节点的传输。 $&gt;hdfs namenode -initializeSharedEdits #查看s202,s203是否有edit数据. 4)启动所有节点. [s201] $&gt;hadoop-daemon.sh start namenode //启动名称节点 $&gt;hadoop-daemons.sh start datanode //启动所有数据节点 [s206] $&gt;hadoop-daemon.sh start namenode //启动名称节点 HA管理$&gt;hdfs haadmin -transitionToActive nn1 //切成激活态 $&gt;hdfs haadmin -transitionToStandby nn1 //切成待命态 $&gt;hdfs haadmin -transitionToActive --forceactive nn2//强行激活 $&gt;hdfs haadmin -failover nn1 nn2 //模拟容灾演示,从nn1切换到nn2]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>机架感知</tag>
        <tag>机架感知实现</tag>
        <tag>HA</tag>
        <tag>手动移动数据块</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop第八天]]></title>
    <url>%2F2018%2F10%2F22%2FHadoop%E7%AC%AC%E5%85%AB%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[二次排序链条化分组不能解决数据倾斜问题，已经进入到了同一个reduce对象里面了，分一个组出来就进入同一个reduce方法里面。每次分组只是决定调用不调用同一个reduce方法。已经晚了，数据已经进入到了同一个reduce了，分区可以决定数据倾斜问题。 reduce分为方法和对象，有的时候说reduce是指的是方法，有的时候指的是对象，要分开对待。 单词统计做统计肯定不能哈希分区，因为哈希同一个值进入到同一个reduce对象利民区去了 数据倾斜问题 随机分区 二次MR1. 2. 3. 4. 如果正常按照wordcount来处理会分为 reduce不设置就是1，map没机会设置数量。但是也可以设置map，但是意义，以为mr提交的时候map的个数取决于切片的个数。 切片的计算公式：min block、maxsplit、 blocksize取中间值等于blocksize。 现在文件如下，现在3个文件每个文件有一百万数据且有一百万个hello就对应了3个map。（这里没明白为什么3个切片也就是为啥3个map） 每个切片里，分区个数等于reduce个数，分区的个数取决于reduce个数，之前自己已经在WCAPP里面设置好了reduce个数。 我有3个map，4个reduce。那么每个map里面就有4个分区了。 要解决哈希分区带来的数据倾斜问题，可以通过随机分区，然后在进行二次mr来解决这个问题 组合combineor是对分区里面的预先聚合，下面的hello就不会发送一百万次，而是提前聚合了发送了hello 一百万次。这个样子。能解决目前的倾斜问题，但是不可能这么多数据，可能量很大 下面这种情况下，所有的hello根据哈希分区就进入到同一个分区，然后一百万个tom分4个分区 通过随机分区可以解决数据倾斜问题，但是会产生新的问题，就是说hello被分到4个不同的reduce里面，导致reduce1产生hello10reduce2产生hello13这种问题，所以通过二次mr来解决这个问题，让hello这个单词统计，进入到同一个reduce结果当中。所以也就是通过随即分区和二次mr解决 [1.txt]1000000 hello tom1 hello tom2 hello tom3 hello tom4 hello tom5 hello tom6 hello tom7 hello tom8 hello tom9 hello tom10 [2.txt]1000000 hello tom11 hello tom12 hello tom13 hello tom14 hello tom15 hello tom16 hello tom17 hello tom18 hello tom19 hello tom20 [3.txt]1000000 hello tom21 hello tom22 hello tom23 hello tom24 hello tom25 hello tom26 hello tom27 hello tom28 hello tom29 hello tom30 代码如下： //自定义分区函数 public class RandomPartitioner extends Partitioner&lt;Text,IntWritable&gt; { @Override public int getPartition(Text text, IntWritable intWritable, int numPartitions) { return new Random().nextInt(numPartitions); } } //解决数据倾斜问题： public class WCSkueApp { public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;); Job job = Job.getInstance(conf); // 设置作业的各种属性 job.setJobName(&quot;MaxTempApp&quot;); //作业名称 job.setJarByClass(WCSkueApp.class); //搜索类路径 job.setInputFormatClass(TextInputFormat.class);//设置输入格式类 //添加输入路径 FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew&quot;)); //设置输出路径 org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(&quot;e:/mr/skew/out&quot;)); //设置随机分区： job.setPartitionerClass(RandomPartitioner.class); job.setMapperClass(WCSkueMapper.class); //mapper类 job.setReducerClass(WCSkueReducer.class); //reduce类 job.setNumReduceTasks(4); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); job.setOutputKeyClass(Text.class); //设置输出类型 job.setOutputValueClass(IntWritable.class); job.waitForCompletion(true); } } public class WCSkueMapper extends Mapper &lt;LongWritable, Text, Text, IntWritable&gt;{ public WCSkueMapper(){ System.out.println(&quot;new WCMapper&quot;); } @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { String[] arr = value.toString().split(&quot; &quot;); Text keyout= new Text(); IntWritable valueout = new IntWritable(); for(String s:arr){ keyout.set(s); valueout.set(1); context.write(keyout,valueout); } } } public class WCSkueReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{ /** * reduce */ protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException { int count = 0 ; for(IntWritable iw : values){ count = count + iw.get() ; } context.write(key,new IntWritable(count)); } } //解决数据倾斜问题： public class WCSkueApp2 { public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;); Job job = Job.getInstance(conf); // 设置作业的各种属性 job.setJobName(&quot;MaxTempApp&quot;); //作业名称 job.setJarByClass(WCSkueApp2.class); //搜索类路径 job.setInputFormatClass(TextInputFormat.class);//设置输入格式类 //添加输入路径 FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00000&quot;)); FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00001&quot;)); FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00002&quot;)); FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00003&quot;)); //设置输出路径 org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(&quot;e:/mr/skew/out2&quot;)); job.setMapperClass(WCSkueMapper2.class); //mapper类 job.setReducerClass(WCSkueReducer2.class); //reduce类 job.setNumReduceTasks(4); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); job.setOutputKeyClass(Text.class); //设置输出类型 job.setOutputValueClass(IntWritable.class); job.waitForCompletion(true); } } public class WCSkueMapper2 extends Mapper &lt;LongWritable, Text, Text, IntWritable&gt;{ public WCSkueMapper2(){ System.out.println(&quot;new WCMapper&quot;); } protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { String[] arr = value.toString().split(&quot;\t&quot;); Text keyout= new Text(); IntWritable valueout = new IntWritable(); context.write(new Text(arr[0]),new IntWritable(Integer.parseInt(arr[1]))); } } public class WCSkueReducer2 extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{ /** * reduce */ protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException { int count = 0 ; for(IntWritable iw : values){ count = count + iw.get() ; } context.write(key,new IntWritable(count)); } } 上面这个就是通过2次mr解决数据倾斜问题 但是不用那么麻烦的去切分了直接使用Keyvalueinputformat即可实现：如下代码： //解决数据倾斜问题： public class WCSkueApp2 { public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;); Job job = Job.getInstance(conf); // 设置作业的各种属性 job.setJobName(&quot;MaxTempApp&quot;); //作业名称 job.setJarByClass(WCSkueApp2.class); //搜索类路径 job.setInputFormatClass(KeyValueTextInputFormat.class);//设置输入格式类 //添加输入路径 FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00000&quot;)); FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00001&quot;)); FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00002&quot;)); FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00003&quot;)); //设置输出路径 org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(&quot;e:/mr/skew/out2&quot;)); job.setMapperClass(WCSkueMapper2.class); //mapper类 job.setReducerClass(WCSkueReducer2.class); //reduce类 job.setNumReduceTasks(4); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); job.setOutputKeyClass(Text.class); //设置输出类型 job.setOutputValueClass(IntWritable.class); job.waitForCompletion(true); } } public class WCSkueMapper2 extends Mapper &lt;Text, Text, Text, IntWritable&gt;{ public WCSkueMapper2(){ System.out.println(&quot;new WCMapper&quot;); } protected void map(Text key, Text value, Context context) throws IOException, InterruptedException { context.write(key,new IntWritable(Integer.parseInt(value.toString()))); } } public class WCSkueReducer2 extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{ /** * reduce */ protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException { int count = 0 ; for(IntWritable iw : values){ count = count + iw.get() ; } context.write(key,new IntWritable(count)); } } 链条式编程 讲解上图：首先通过MapMapper1来切割，然后通过MapMapper1过滤掉法轮功，真个是一个链条，在map端，执行完之后开始聚合，在reduce端，在聚合的时候，每个单词都有各自的个数，对产生的结果再次过滤，过滤掉少于5的单词，当然可以放到同一个map里面，但是如果变成模块方法，易于维护。更加方便使用。m阶段可以有多个m但是R阶段只能有一个r。但是r阶段可以有多个m。 代码如下图：//链条式job任务 public class WCChainApp { public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;); Job job = Job.getInstance(conf); // 设置作业的各种属性 job.setJobName(&quot;MaxTempApp&quot;); //作业名称 job.setJarByClass(WCChainApp.class); //搜索类路径 job.setInputFormatClass(TextInputFormat.class);//设置输入格式类 //添加输入路径 FileInputFormat.addInputPath(job, new Path(&quot;e:/mr/skew&quot;)); //设置输出路径 org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(&quot;e:mr/skew/out&quot;)); //在map链条上添加一个mapper的环节 ChainMapper.addMapper(job,WCMapMapper1.class,LongWritable.class,Text.class,Text.class,IntWritable.class,conf); ChainMapper.addMapper(job,WCMapMapper2.class,Text.class,IntWritable.class,Text.class,IntWritable.class,conf); //在reduce链条上设置reduce ChainReducer.setReducer(job,WCReducer.class,Text.class,IntWritable.class,Text.class,IntWritable.class,conf); ChainReducer.addMapper(job,WCReduceMapper1.class,Text.class,IntWritable.class,Text.class,IntWritable.class,conf); job.setNumReduceTasks(3); job.waitForCompletion(true); } } public class WCMapMapper1 extends Mapper &lt;LongWritable, Text, Text, IntWritable&gt;{ protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { Text keyout= new Text(); IntWritable valueout = new IntWritable(); String[] arr = value.toString().split(&quot; &quot;); for(String s:arr){ keyout.set(s); valueout.set(1); context.write(keyout,valueout); } } } public class WCMapMapper2 extends Mapper &lt;Text, IntWritable, Text, IntWritable&gt;{ protected void map(Text key, IntWritable value, Context context) throws IOException, InterruptedException { if(!key.toString().equals(&quot;falungong&quot;)){ context.write(key , value); } } } //过滤单词个数 public class WCReduceMapper1 extends Mapper&lt;Text,IntWritable,Text,IntWritable&gt; { @Override protected void map(Text key, IntWritable value, Context context) throws IOException, InterruptedException { if(value.get() &gt; 5){ context.write(key,value); } } } /** * Reducer */ public class WCReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{ /** * reduce */ protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException { int count = 0 ; for(IntWritable iw : values){ count = count + iw.get() ; } context.write(key,new IntWritable(count)); } } FileInputFormat（读源码）获取切片集合。 子类都要重写方法isSplittable(); 负责创建RecordReader对象。 设置IO路径。 RecordReader（读源码）负责从InputSplit中读取KV对。 jdbc笔记模板：[写操作] Class.forName(&quot;com.mysql.jdbc.Driver&quot;); Connection conn = DriverMananger.getConnection(&quot;jdbc:mysql://localhost:3306/big4&quot;,&quot;root&quot;,&quot;root&quot;); //预处理语句 PreparedStatement ppst = conn.preparedStatement(&quot;insert into test(id,name,age) values(?,?,?)&quot;); //绑定参数 ppst.setInteger(1,1); ppst.setInteger(2,&quot;tom&quot;); ppst.setInteger(3,12); ppst.executeUpdate(); ppst.close(); conn.close(); [读操作] Class.forName(&quot;com.mysql.jdbc.Driver&quot;); Connection conn = DriverMananger.getConnection(&quot;jdbc:mysql://localhost:3306/big4&quot;,&quot;root&quot;,&quot;root&quot;); ppst = conn.preparedStatement(&quot;select id,name from test &quot;); //结果集 ResultSet rs = ppst.executeQuery(); while(rs.next()){ int id = rs.getInt(&quot;id&quot;); String name = rs.getInt(&quot;name&quot;); } rs.close(); conn.close(); 看一下下源码：看一下写和读入都把结果集和预处理语句都已经封装进去了，剩下的只要填空就好了。 以下是从数据库中读入的代码模板： 使用DBWritable向数据库从数据库中读取1.准备数据库create database big4 ; use big4 ; create table words(id int primary key auto_increment , name varchar(20) , txt varchar(255)); insert into words(name,txt) values(&apos;tomas&apos;,&apos;hello world tom&apos;); insert into words(txt) values(&apos;hello tom world&apos;); insert into words(txt) values(&apos;world hello tom&apos;); insert into words(txt) values(&apos;world tom hello&apos;); 2.编写hadoop MyDBWritable.import org.apache.hadoop.mapreduce.lib.db.DBWritable; import java.io.DataInput; import java.io.DataOutput; import java.io.IOException; import java.sql.PreparedStatement; import java.sql.ResultSet; import java.sql.SQLException; /** * MyDBWritable */ public class MyDBWritable implements DBWritable,Writable { private int id ; private String name ; private String txt ; public int getId() { return id; } public void setId(int id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public String getTxt() { return txt; } public void setTxt(String txt) { this.txt = txt; } public void write(DataOutput out) throws IOException { out.writeInt(id); out.writeUTF(name); out.writeUTF(txt); } public void readFields(DataInput in) throws IOException { id = in.readInt(); name = in.readUTF(); txt = in.readUTF(); } /** * 写入db */ public void write(PreparedStatement ppst) throws SQLException { ppst.setInt(1,id); ppst.setString(2,name); ppst.setString(3,txt); } /** * 从db读取 */ public void readFields(ResultSet rs) throws SQLException { id = rs.getInt(1); name = rs.getString(2); txt = rs.getString(3); } } 3.WcMapperpublic class WCMapper extends Mapper&lt;LongWritable,MyDBWritable,Text,IntWritable&gt; { protected void map(LongWritable key, MyDBWritable value, Context context) throws IOException, InterruptedException { System.out.println(key); String line = value.getTxt(); System.out.println(value.getId() + &quot;,&quot; + value.getName()); String[] arr = line.split(&quot; &quot;); for(String s : arr){ context.write(new Text(s),new IntWritable(1)); } } } 4.WCReducerprotected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException { int count = 0 ; for(IntWritable w : values){ count = count + w.get() ; } context.write(key,new IntWritable(count)); } 5.WCApppublic static void main(String[] args) throws Exception { Configuration conf = new Configuration(); Job job = Job.getInstance(conf); //设置job的各种属性 job.setJobName(&quot;MySQLApp&quot;); //作业名称 job.setJarByClass(WCApp.class); //搜索类 //配置数据库信息 String driverclass = &quot;com.mysql.jdbc.Driver&quot; ; String url = &quot;jdbc:mysql://localhost:3306/big4&quot; ; String username= &quot;root&quot; ; String password = &quot;root&quot; ; //设置数据库配置 DBConfiguration.configureDB(job.getConfiguration(),driverclass,url,username,password); //设置数据输入内容 DBInputFormat.setInput(job,MyDBWritable.class,&quot;select id,name,txt from words&quot;,&quot;select count(*) from words&quot;); //设置输出路径 FileOutputFormat.setOutputPath(job,new Path(&quot;e:/mr/sql/out&quot;)); //设置分区类 job.setMapperClass(WCMapper.class); //mapper类 job.setReducerClass(WCReducer.class); //reducer类 job.setNumReduceTasks(3); //reduce个数 job.setMapOutputKeyClass(Text.class); // job.setMapOutputValueClass(IntWritable.class); // job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); // job.waitForCompletion(true); } 6.pom.xml增加mysql驱动&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.17&lt;/version&gt; &lt;/dependency&gt; 7.将mr的统计结果写入mysql数据库a)准备表 create table stats(word varchar(50),c int); b)设置App的DBOutputFormat类 com.it18zhang.hdfs.mr.mysql.WCApp d) e) f) mysql分页查询如图所示吧： 使用DBWritable向数据库从数据库中写入public class MyDBWritalbe implements DBWritable,Writable { private int id=0; private String name=&quot;&quot;; private String txt=&quot;&quot;; private String word=&quot;&quot;; private int wordcount=0; public String getWord() { return word; } public void setWord(String word) { this.word = word; } public int getWordcount() { return wordcount; } public void setWordcount(int wordcount) { this.wordcount = wordcount; } public int getId() { return id; } public void setId(int id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public String getTxt() { return txt; } public void setTxt(String txt) { this.txt = txt; } public void write(DataOutput out) throws IOException { out.writeInt(id); out.writeUTF(name); out.writeUTF(txt); out.writeUTF(word); out.writeInt(wordcount); } public void readFields(DataInput in) throws IOException { id=in.readInt(); name=in.readUTF(); txt=in.readUTF(); word=in.readUTF(); wordcount=in.readInt(); } //向数据库中写入DB public void write(PreparedStatement ppst) throws SQLException { //要求定制字段列表的时候先单词后个数。 ppst.setString(1,word); ppst.setInt(2,wordcount); } //从DB中读出 public void readFields(ResultSet rs) throws SQLException { id=rs.getInt(1); name=rs.getString(2); txt=rs.getString(3); } } public class WCApp { // // public static void main(String[] args) throws Exception { // Configuration conf = new Configuration(); // // Job job = Job.getInstance(conf); // //// 设置作业的各种属性 // job.setJobName(&quot;MySQLApp&quot;); //作业名称 // job.setJarByClass(WCApp.class); //搜索类路径 // // //配置数据库信息 // String driverclass = &quot;com.mysql.jdbc.Driver&quot;; // String url = &quot;jdbc:mysql://localhost:3306/big4&quot;; // String usrname = &quot;root&quot;; // String password = &quot;root&quot;; // DBConfiguration.configureDB(conf,driverclass,url,usrname,password); // //设置数据输入内容 // DBInputFormat.setInput(job,DBWritable.class,&quot;select id,name,txt from words&quot;,&quot;select count(*) from words&quot;); // //设置输出路径 // FileOutputFormat.setOutputPath(job, new Path(&quot;d:/mr/sql/out&quot;)); // // job.setMapperClass(WCMapper.class); //mapper类 // job.setReducerClass(WCReducer.class); //reduce类 // // job.setNumReduceTasks(3); // // // job.setMapOutputKeyClass(Text.class); // job.setMapOutputValueClass(IntWritable.class); // // job.setOutputKeyClass(Text.class); //设置输出类型 // job.setOutputValueClass(IntWritable.class); // // job.waitForCompletion(true); // // } //} public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); Job job = Job.getInstance(conf); //设置job的各种属性 job.setJobName(&quot;MySQLApp&quot;); //作业名称 job.setJarByClass(WCApp.class); //搜索类 //配置数据库信息 String driverclass = &quot;com.mysql.jdbc.Driver&quot;; String url = &quot;jdbc:mysql://localhost:3306/big4&quot;; String username = &quot;root&quot;; String password = &quot;root&quot;; //设置数据库配置 DBConfiguration.configureDB(job.getConfiguration(), driverclass, url, username, password); //设置数据输入内容 DBInputFormat.setInput(job, MyDBWritalbe.class, &quot;select id,name,txt from words&quot;, &quot;select count(*) from words&quot;); //这里定制字段列表，先单词后个数 DBOutputFormat.setOutput(job,&quot;stats&quot;,&quot;word&quot;,&quot;c&quot;); //设置分区类 job.setMapperClass(WCMapper.class); //mapper类 job.setReducerClass(WCReducer.class); //reducer类 job.setNumReduceTasks(3); //reduce个数 job.setMapOutputKeyClass(Text.class); // job.setMapOutputValueClass(IntWritable.class); // job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); // job.waitForCompletion(true); } } public class WCMapper extends Mapper&lt;LongWritable, MyDBWritalbe, Text, IntWritable&gt; { @Override protected void map(LongWritable key, MyDBWritalbe value, Context context) throws IOException, InterruptedException { System.out.println(key); String line = value.getTxt(); System.out.println(value.getId() + &quot;,&quot; + value.getName()); String[] arr = line.split(&quot; &quot;); for (String s : arr) { context.write(new Text(s), new IntWritable(1)); } } } public class WCReducer extends Reducer&lt;Text, IntWritable, MyDBWritalbe, NullWritable&gt; { protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException { int count = 0; for (IntWritable w : values) { count = count + w.get(); } MyDBWritalbe keyout = new MyDBWritalbe(); keyout.setWord(key.toString()); keyout.setWordcount(count); context.write(keyout, NullWritable.get()); } } 在虚拟机中跑wordcount写入数据库问题：需要修改的地方：1.修改url 2.修改core-site.xml或者删除掉 3.在虚拟机中分发mysql-connector-java-5.1.7-bin.jar将之分发到每个节点的/soft/hadoop/share/hadoop/common/lib目录下。 4.在运行 hadoop jar HdfsDemo.jar com.it18zhang.hdfs.mr.mysql.WCApp 出现的问题：1.出现连接不上的报错： 百度了一下完美解决，是由于mysql没有对所有用户开启权限导致： https://blog.csdn.net/OldTogether/article/details/79612627?utm_source=blogxgwz1 2.出现一下问题： 原因是有一个s205的防火墙没有关掉导致的： https://blog.csdn.net/shirdrn/article/details/7280040 完]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>二次排序链条化</tag>
        <tag>数据倾斜</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop第七天]]></title>
    <url>%2F2018%2F10%2F17%2FHadoop%E7%AC%AC%E4%B8%83%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[多输入问题在IDEA里面代码： 首先明确一点InputInputformat也就是文本输入格式的输入是Longwritblele和Text，然后SequenceFileputFormat的输入是intwritble和text。 public class WCApp { public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;); Job job = Job.getInstance(conf); // 设置作业的各种属性 job.setJobName(&quot;WCAppMulti&quot;); //作业名称 job.setJarByClass(WCApp.class); //搜索类路径 //多个输入 MultipleInputs.addInputPath(job,new Path(&quot;file:///e:/mr/txt&quot;),TextInputFormat.class,WCTextMapper.class); MultipleInputs.addInputPath(job,new Path(&quot;file:///e:/mr/seq&quot;),SequenceFileInputFormat.class,WCSeqMapper.class); //设置输出 FileOutputFormat.setOutputPath(job,new Path(args[0])); job.setReducerClass(WCReducer.class);//reducer类 job.setNumReduceTasks(3);//reducer个数 job.setOutputKeyClass(Text.class); //设置输出类型 job.setOutputValueClass(IntWritable.class); job.waitForCompletion(true); } } ublic class WCReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{ /** * reduce */ protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException { int count = 0 ; for(IntWritable iw : values){ count = count + iw.get() ; } String tno = Thread.currentThread().getName(); System.out.println(tno + &quot;WCReducer:&quot; + key.toString() + &quot;=&quot; + count); context.write(key,new IntWritable(count)); } } public class WCTextMapper extends Mapper &lt;LongWritable, Text, Text, IntWritable&gt;{ @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { String[] arr = value.toString().split(&quot; &quot;); Text keyout= new Text(); IntWritable valueout = new IntWritable(); for(String s:arr){ keyout.set(s); valueout.set(1); context.write(keyout,valueout); } } } public class WCTextMapper extends Mapper &lt;LongWritable, Text, Text, IntWritable&gt;{ @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { String[] arr = value.toString().split(&quot; &quot;); Text keyout= new Text(); IntWritable valueout = new IntWritable(); for(String s:arr){ keyout.set(s); valueout.set(1); context.write(keyout,valueout); } } } 下图是配置本地的路径。要明确其中的arg[]也就是这个里配置的那个Program arguments的路径。因为只需要一个参数就可以了，因为在代码中已经写好了输入路径了。 计数器 日志目录：/soft/hadoop/logs/userlogs 用户打印的日志。Hadoop输出的日志都是在外层的文件之中。userlogs是自己的日志打印。 计数器是其他的内容都不做更改，在Mapper和Reducer里面添加这个语句即可 context.getCounter(“r”, “WCReducer.reduce”).increment(1); 然后扔到虚拟机里面去运行： hadoop jar HdfsDemo.jar com.it18zhang.hdfs.mr.WCApp hdfs://user/centos/data/ hdfs://user/centos/data/out 单独配置2nn到独立节点配置core-site文件 [hdfs-site.xml] &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;s206:50090&lt;/value&gt; &lt;/property&gt; 跟踪运行器信息 添加一个工具类： public class Util { public static String getInfo(Object o,String msg ){ return getHostname() + &quot;:&quot; + getPID() + &quot;:&quot; + getTID()+&quot;:&quot;+getObjInfo(o) +msg; } //得到主机名 public static String getHostname() { try { return InetAddress.getLocalHost().getHostName(); } catch (UnknownHostException e) { e.printStackTrace(); } return null; } //获得当前程序的所在的进程ID。 public static int getPID() { String info = ManagementFactory.getRuntimeMXBean().getName(); return Integer.parseInt(info.substring(0,info.indexOf(&quot;@&quot;))); } //返回当前线程ID， public static String getTID(){ return Thread.currentThread().getName(); } public static String getObjInfo(Object o){ String sname = o.getClass().getSimpleName(); return sname + &quot;@&quot;+o.hashCode(); } } 然后在map和reduce阶段添加： //每执行一次，计数器对这个组+1 context.getCounter(&quot;m&quot;,Util.getInfo(this,&quot;map&quot;)).increment(1); 效果如下图 全排序## 普通排序求最高年份温度 ## 代码如下： public class MaxTempApp { public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;); Job job = Job.getInstance(conf); // 设置作业的各种属性 job.setJobName(&quot;MaxTempApp&quot;); //作业名称 job.setJarByClass(MaxTempApp.class); //搜索类路径 job.setInputFormatClass(TextInputFormat.class);//设置输入格式类 //添加输入路径 FileInputFormat.addInputPath(job, new Path(args[0])); //设置输出路径 org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(args[1])); job.setMapperClass(MaxTempMapper.class); //mapper类 job.setReducerClass(MaxTempReducer.class); //reduce类 job.setNumReduceTasks(3); //reduce个数 job.setMapOutputKeyClass(IntWritable.class); job.setMapOutputValueClass(IntWritable.class); job.setOutputKeyClass(IntWritable.class); //设置输出类型 job.setOutputValueClass(IntWritable.class); job.waitForCompletion(true); } } public class MaxTempMapper extends Mapper&lt;LongWritable, Text, IntWritable, IntWritable&gt; { public MaxTempMapper() { System.out.println(&quot;new MaxTempMapper&quot;); } @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { String line = value.toString(); String arr[] = line.split(&quot; &quot;); context.write(new IntWritable(Integer.parseInt(arr[0])), new IntWritable((Integer.parseInt(arr[1])))); } } public class MaxTempReducer extends Reducer&lt;IntWritable, IntWritable, IntWritable, IntWritable&gt;{ /** * reduce */ protected void reduce(IntWritable key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException { int max =Integer.MIN_VALUE; for(IntWritable iw : values){ max = max &gt; iw.get() ? max : iw.get() ; } context.write(key,new IntWritable(max)); } } 全排序代码上面代码产生的会是3个文件，文件内部各自排序，但是不是全排序的文件。全排序2种办法： 1、设置分区数是1，但是数据倾斜 在每个分区里设置0-30，30-60，60-100即可,然后每个分区返回0,1,2进入分成不同的区代码如下图： public class YearPartitioner extends Partitioner&lt;IntWritable, IntWritable&gt; { public int getPartition(IntWritable year, IntWritable temp, int parts) { int y = year.get() - 1970; if (y &lt; 33) { return 0; } if (y &gt; 33 &amp;&amp; y &lt; 66) { return 1; } else { return 2; } } } public class MaxTempReducer extends Reducer&lt;IntWritable, IntWritable, IntWritable, IntWritable&gt;{ /** * reduce */ protected void reduce(IntWritable key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException { int max =Integer.MIN_VALUE; for(IntWritable iw : values){ max = max &gt; iw.get() ? max : iw.get() ; } context.write(key,new IntWritable(max)); } } public class MaxTempMapper extends Mapper&lt;LongWritable, Text, IntWritable, IntWritable&gt; { public MaxTempMapper() { System.out.println(&quot;new MaxTempMapper&quot;); } @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { String line = value.toString(); String arr[] = line.split(&quot; &quot;); context.write(new IntWritable(Integer.parseInt(arr[0])), new IntWritable((Integer.parseInt(arr[1])))); } } public class MaxTempApp { public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;); Job job = Job.getInstance(conf); // 设置作业的各种属性 job.setJobName(&quot;MaxTempApp&quot;); //作业名称 job.setJarByClass(MaxTempApp.class); //搜索类路径 job.setInputFormatClass(TextInputFormat.class);//设置输入格式类 job.setPartitionerClass(YearPartitioner.class); //添加输入路径 FileInputFormat.addInputPath(job, new Path(args[0])); //设置输出路径 org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(args[1])); job.setMapperClass(MaxTempMapper.class); //mapper类 job.setReducerClass(MaxTempReducer.class); //reduce类 job.setNumReduceTasks(3); //reduce个数 job.setMapOutputKeyClass(IntWritable.class); job.setMapOutputValueClass(IntWritable.class); job.setOutputKeyClass(IntWritable.class); //设置输出类型 job.setOutputValueClass(IntWritable.class); job.waitForCompletion(true); } } 其实无非就是加一个partitioner的这个类而已。 全排序采样器1.定义1个reduce 2.自定义分区函数。： 自行设置分解区间。 3.使用hadoop采样机制。 通过采样器生成分区文件，结合hadoop的TotalOrderPartitioner进行分区划分 TotalOrderPartitioner //全排序分区类,读取外部生成的分区文件确定区间。 使用时采样代码在最后端,否则会出现错误。 //分区文件设置，设置的job的配置对象，不要是之前的conf.这里面的getconfiguration()是对最开始new的conf的拷贝，TotalOrderPartitioner.setPartitionFile(job.getConfiguration(),new Path(“d:/mr/par.lst”)); 首先一段产生随机年份，温度的代码public class PrepareTempData { @Test public void makeData() throws IOException { FileWriter fw = new FileWriter(&quot;e:/mr/temp.txt&quot;); for(int i=0;i&lt;6000;i++){ int year=1970+ new Random().nextInt(100); int temp=-30 + new Random().nextInt(600); fw.write(&quot; &quot;+ year + &quot; &quot;+ temp + &quot;\r\n&quot; ); } fw.close(); } } 全排序采样器代码 public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS&quot;, &quot;file:///&quot;); Job job = Job.getInstance(conf); 设置作业的各种属性 job.setJobName(&quot;MaxTempApp&quot;); //作业名称 job.setJarByClass(MaxTempApp.class); //搜索类路径 job.setInputFormatClass(SequenceFileInputFormat.class);//设置输入格式类 //添加输入路径 FileInputFormat.addInputPath(job, new Path(args[0])); //设置输出路径 org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(args[1])); job.setMapperClass(MaxTempMapper.class); //mapper类 job.setReducerClass(MaxTempReducer.class); //reduce类 job.setMapOutputKeyClass(IntWritable.class); job.setMapOutputValueClass(IntWritable.class); job.setOutputKeyClass(IntWritable.class); //设置输出类型 job.setOutputValueClass(IntWritable.class); //设置 全排序分区类 job.setPartitionerClass(TotalOrderPartitioner.class); //将sample数据 写入分区文件 TotalOrderPartitioner.setPartitionFile(conf, new Path(&quot;e:/mr/par.lst&quot;)); //创建随机采样器对象 InputSampler.Sampler&lt;IntWritable, IntWritable&gt; sampler = new InputSampler.RandomSampler&lt;IntWritable, IntWritable&gt;(0.1, 10000, 10); job.setNumReduceTasks(3); //reduce个数 InputSampler.writePartitionFile(job, sampler); job.waitForCompletion(true); } } public class MaxTempMapper extends Mapper&lt;IntWritable, IntWritable, IntWritable, IntWritable&gt; { public MaxTempMapper() { System.out.println(&quot;new MaxTempMapper&quot;); } @Override protected void map(IntWritable key, IntWritable value, Context context) throws IOException, InterruptedException { context.write(key,value); } } public class MaxTempReducer extends Reducer&lt;IntWritable, IntWritable, IntWritable, IntWritable&gt;{ /** * reduce */ protected void reduce(IntWritable key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException { int max =Integer.MIN_VALUE; for(IntWritable iw : values){ max = max &gt; iw.get() ? max : iw.get() ; } context.write(key,new IntWritable(max)); } } public class YearPartitioner extends Partitioner&lt;IntWritable, IntWritable&gt; { public int getPartition(IntWritable year, IntWritable temp, int parts) { int y = year.get() - 1970; if (y &lt; 33) { return 0; } if (y &gt; 33 &amp;&amp; y &lt; 66) { return 1; } else { return 2; } } } 全排序官方笔记: 1.定义1个reduce 2.自定义分区函数. 自行设置分解区间。 3.使用hadoop采样机制。 通过采样器生成分区文件，结合hadoop的TotalOrderPartitioner进行分区划分。 TotalOrderPartitioner //全排序分区类,读取外部生成的分区文件确定区间。 使用时采样代码在最后端,否则会出现错误。 //分区文件设置，设置的job的配置对象，不要是之前的conf. TotalOrderPartitioner.setPartitionFile(job.getConfiguration(),new Path(&quot;d:/mr/par.lst&quot;)); 全排序和部分排序二次排序在面试比例很重的分区在map端，分组在reduce端。二次排序就是把value和key组合到一起，然后value就是nullwritable这样子。合在一起成为一个combokey。Combokey不但要可比较还要串行化，hadoop的串行化机制是writable ，串行就是写入到流离去，反串行就是读出来。IntWritable LongWritable这些都是经过串行化实现的类，Writable有很多子类。 value本身不能排序，为了能让他排序，吧value做到key里面去。吧年份和气温都做到key里面去，这个key是自定义的key,然后在combokey中定义里面的排序规则。自定义key,自定义对比器。 wirtable是串行化机制，java本身也有串行化机制，本质是对象输出流和输入流objectInputStream,objectOutputStream。需要写入和读取就可以了，但是java的串行化效率比较低而且复杂，不能够夸语言，所以hadoop有一个自己的串行化Wriable过程。 这端是reduce端，经过map端的combokey输出的combokey类是这个样子的，如果不经过处理直接聚合，就会发先，每一个combo都是一个新的都不相同，但是我们想让相同年份的进入到同一个组里面去。所以要重写分组。根据年份写分组，只要是同一个年就是同一组 reduce端里面有reduce对象和reduce方法。我们说的分几个区进入到几个reduce的意思是进入到几个reduce对象。 现在讲一下下图：下图说在没有分组的情况下，一个key对应好多value,也就是一个1978对应好多个12。好多个12进入到迭代器里面去，不断it.next是下一个v。但是key也改变，其实每次都是变化的， 讲一下二次排序和全排序：全排序就是整个年份-温度数据，分成几个区，让第一个区的最大值小于第二个区的最小值，然后这样子排下去，既解决了分布式的问题，又解决了数据倾斜的问题，但是value是不能排序的，因为mapreduce天生value就不能够排序。那么如何解决让温度也排序呢，就是要把年份和温度做成一个key，传入，然后再排序。就是二次排序，既实现了年份的排序，又实现了温度的排序，这里说的温度的排序是指同一个年份温度升序降序的问题。 二次排序代码如下： /** * 自定义组合key */ public class ComboKey implements WritableComparable&lt;ComboKey&gt; { private int year; private int temp; public int getYear() { return year; } public void setYear(int year) { this.year = year; } public int getTemp() { return temp; } public void setTemp(int temp) { this.temp = temp; } /** * 对key进行比较实现 */ public int compareTo(ComboKey o) { int y0 = o.getYear(); int t0 = o.getTemp(); //年份相同(升序) if (year == y0) { //气温降序 return -(temp - t0); // //这个地方为什么说temp-t0是升序排列，因为这个temp-t0是默认的。默认就是升序排列加一个负号就是降序排列。本身升序排列的就是temp-t0是大于0的 } else { return year - y0; } } /** * 串行化过程 */ public void write(DataOutput out) throws IOException { //年份 out.writeInt(year); //气温 out.writeInt(temp); } public void readFields(DataInput in) throws IOException { year = in.readInt(); temp = in.readInt(); } } /** *ComboKeyComparator */ public class ComboKeyComparator extends WritableComparator { protected ComboKeyComparator() { super(ComboKey.class, true); } public int compare(WritableComparable a, WritableComparable b) { ComboKey k1 = (ComboKey) a; ComboKey k2 = (ComboKey) b; return k1.compareTo(k2); } } public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS&quot;, &quot;file:///&quot;); Job job = Job.getInstance(conf); // 设置作业的各种属性 job.setJobName(&quot;SecondarySortApp&quot;); //作业名称 job.setJarByClass(MaxTempApp.class); //搜索类路径 job.setInputFormatClass(TextInputFormat.class);//设置输入格式类 //添加输入路径 FileInputFormat.addInputPath(job, new Path(args[0])); //设置输出路径 org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(args[1])); job.setMapperClass(MaxTempMapper.class); //mapper类 job.setReducerClass(MaxTempReducer.class); //reduce类 //设置map输出类型 job.setMapOutputKeyClass(ComboKey.class); job.setMapOutputValueClass(NullWritable.class); //设置Reduceoutput类型 job.setOutputKeyClass(IntWritable.class); //设置输出类型 job.setOutputValueClass(IntWritable.class); //设置分区类 job.setPartitionerClass(YearPartitioner.class); //设置分组对比器。 job.setGroupingComparatorClass(YearGroupComparator.class); //设置排序对比器，听说不写那个ComboKeyComparator不设置也可以 job.setSortComparatorClass(ComboKeyComparator.class); //reduce个数 job.setNumReduceTasks(3); job.waitForCompletion(true); } } public class MaxTempMapper extends Mapper&lt;LongWritable, Text, ComboKey, NullWritable&gt; { @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { String line = value.toString(); String[] arr = line.split(&quot; &quot;); ComboKey keyout = new ComboKey(); keyout.setYear(Integer.parseInt(arr[0])); keyout.setTemp(Integer.parseInt(arr[1])); context.write(keyout, NullWritable.get()); } } /** * Reducer */ public class MaxTempReducer extends Reducer&lt;ComboKey, NullWritable, IntWritable, IntWritable&gt;{ protected void reduce(ComboKey key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException { int year = key.getYear(); int temp = key.getTemp(); context.write(new IntWritable(year),new IntWritable(temp)); } } public class YearGroupComparator extends WritableComparator { protected YearGroupComparator() { super(ComboKey.class, true); } public int compare(WritableComparable a, WritableComparable b) { ComboKey k1 = (ComboKey) a ; ComboKey k2 = (ComboKey) b ; return k1.getYear() - k2.getYear() ; } } //自定义分区，在map端执行，是map中的一个阶段，mapkv进kv出，kv出去之后要有一个分区的过程。 //默认是哈希分区，这里边是修改了分区规则。 public class YearPartitioner extends Partitioner&lt;ComboKey,NullWritable&gt; { public int getPartition(ComboKey key, NullWritable nullWritable, int numPartitions) { int year = key.getYear(); return year % numPartitions; } }]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>多输入问题</tag>
        <tag>计数器使用</tag>
        <tag>跟踪运行信息</tag>
        <tag>产生随机数文件</tag>
        <tag>全排序</tag>
        <tag>全排序采样器</tag>
        <tag>二次排序</tag>
        <tag>倒排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop第六天之Yarn作业提交]]></title>
    <url>%2F2018%2F10%2F11%2FHadoop%E7%AC%AC%E5%85%AD%E5%A4%A9%E4%B9%8BYarn%E4%BD%9C%E4%B8%9A%E6%8F%90%E4%BA%A4%2F</url>
    <content type="text"><![CDATA[本地模式job提交流程mr.Job = new Job(); job.setxxx(); JobSubmitter.提交 LocalJobRunner.Job(); start(); hdfs writepacket, hdfs 切片计算方式getFormatMinSplitSize() = 1 //最小值(&gt;=1) 1 0 long minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job)); //最大值(&lt;= Long.Max , mapreduce.input.fileinputformat.split.maxsize=) long maxSize = getMaxSplitSize(job); //得到block大小 long blockSize = file.getBlockSize(); //minSplit maxSplit blockSize //Math.max(minSize, Math.min(maxSize, blockSize)); LF : Line feed,换行符private static final byte CR = ‘\r’;private static final byte LF = ‘\n’; 压缩1.Windows 源文件大小:82.8k 源文件类型:txt 压缩性能比较 | DeflateCodec GzipCodec BZip2Codec Lz4Codec SnappyCodec |结论 ------------|-------------------------------------------------------------------|---------------------- 压缩时间(ms)| 450 7 196 44 不支持 |Gzip &gt; Lz4 &gt; BZip2 &gt; Deflate ------------|-------------------------------------------------------------------|---------------------- 解压时间(ms)| 444 66 85 33 |lz4 &gt; gzip &gt; bzip2 &gt; Deflate ------------|-------------------------------------------------------------------|---------------------- 占用空间(k) | 19k 19k 17k 31k 不支持 |Bzip &gt; Deflate = Gzip &gt; Lz4 | | 2.CentOS 源文件大小:82.8k 源文件类型:txt | DeflateCodec GzipCodec BZip2Codec Lz4Codec LZO SnappyCodec |结论 ------------|---------------------------------------------------------------------------|---------------------- 压缩时间(ms)| 944 77 261 53 77 不支持 |Gzip &gt; Lz4 &gt; BZip2 &gt; Deflate ------------|---------------------------------------------------------------------------|---------------------- 解压时间(ms)| 67 66 106 52 73 |lz4 &gt; gzip &gt; Deflate&gt; lzo &gt; bzip2 ------------|---------------------------------------------------------------------------|---------------------- 占用空间(k) | 19k 19k 17k 31k 34k |Bzip &gt; Deflate = Gzip &gt; Lz4 &gt; lzo 远程调试1.设置服务器java vm的-agentlib:jdwp选项. [server] //windwos //set JAVA_OPTS=%JAVA_OPTS% -agentlib:jdwp=transport=dt_socket,address=8888,server=y,suspend=n //linux export HADOOP_CLIENT_OPTS=-agentlib:jdwp=transport=dt_socket,address=8888,server=y,suspend=y 2.在server启动java程序 hadoop jar HdfsDemo.jar com.it18zhang.hdfs.mr.compress.TestCompress 3.server会暂挂在8888. Listening ... 4.客户端通过远程调试连接到远程主机的8888. 5.客户端就可以调试了。 hadoop jarjava hadoop jar com.it18zhang.hdfs.mr.compress.TestCompress export HADOOP_CLIENT_OPTS=-agentlib:jdwp=transport=dt_socket,address=8888,server=y,suspend=y 在pom.xml中引入新的插件(maven-antrun-plugin),实现文件的复制.[pom.xml] &lt;project&gt; ... &lt;build&gt; &lt;plugins&gt; ... &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-antrun-plugin&lt;/artifactId&gt; &lt;version&gt;1.8&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;run&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;tasks&gt; &lt;echo&gt;---------开始复制jar包到共享目录下----------&lt;/echo&gt; &lt;delete file=&quot;D:\downloads\bigdata\data\HdfsDemo-1.0-SNAPSHOT.jar&quot;&gt;&lt;/delete&gt; &lt;copy file=&quot;target/HdfsDemo-1.0-SNAPSHOT.jar&quot; toFile=&quot;D:\downloads\bigdata\data\HdfsDemo.jar&quot;&gt; &lt;/copy&gt; &lt;/tasks&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; ... &lt;/project&gt; 在centos上使用yum安装snappy压缩库文件[google snappy] $&gt;sudo yum search snappy #查看是否有snappy库 $&gt;sudo yum install -y snappy.x86_64 #安装snappy压缩解压缩库 库文件windows :dll(dynamic linked library) linux :so(shared object) LZO1.在pom.xml引入lzo依赖 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.it18zhang&lt;/groupId&gt; &lt;artifactId&gt;HdfsDemo&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins &lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-antrun-plugin&lt;/artifactId&gt; &lt;version&gt;1.8&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;run&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;tasks&gt; &lt;echo&gt;---------开始复制jar包到共享目录下----------&lt;/echo&gt; &lt;delete file=&quot;D:\downloads\bigdata\data\HdfsDemo-1.0-SNAPSHOT.jar&quot;&gt;&lt;/delete&gt; &lt;copy file=&quot;target/HdfsDemo-1.0-SNAPSHOT.jar&quot; toFile=&quot;D:\downloads\bigdata\data\HdfsDemo.jar&quot;&gt; &lt;/copy&gt; &lt;/tasks&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.7.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-yarn-common&lt;/artifactId&gt; &lt;version&gt;2.7.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-yarn-client&lt;/artifactId&gt; &lt;version&gt;2.7.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-yarn-server-resourcemanager&lt;/artifactId&gt; &lt;version&gt;2.7.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.anarres.lzo&lt;/groupId&gt; &lt;artifactId&gt;lzo-hadoop&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; 2.在centos上安装lzo库 $&gt;sudo yum -y install lzo 3.使用mvn命令下载工件中的所有依赖 进入pom.xml所在目录，运行cmd： mvn -DoutputDirectory=./lib -DgroupId=com.it18zhang -DartifactId=HdfsDemo -Dversion=1.0-SNAPSHOT dependency:copy-dependencies 4.在lib下存放依赖所有的第三方jar 5.找出lzo-hadoop.jar + lzo-core.jar复制到hadoop的响应目录下。 $&gt;cp lzo-hadoop.jar lzo-core.jar /soft/hadoop/shared/hadoop/common/lib 6.执行远程程序即可。 修改maven使用aliyun镜像。[maven/conf/settings.xml] &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;settings xmlns=&quot;http://maven.apache.org/SETTINGS/1.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd&quot;&gt; &lt;pluginGroups&gt; &lt;/pluginGroups&gt; &lt;proxies&gt; &lt;/proxies&gt; &lt;servers&gt; &lt;server&gt; &lt;id&gt;releases&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;snapshots&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;Tomcat7&lt;/id&gt; &lt;username&gt;tomcat&lt;/username&gt; &lt;password&gt;tomcat&lt;/password&gt; &lt;/server&gt; &lt;/servers&gt; &lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;*&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;/mirror&gt; &lt;/mirrors&gt; &lt;/settings&gt; 文件格式:SequenceFile1.SequenceFile Key-Value对方式。 2.不是文本文件，是二进制文件。 3.可切割 因为有同步点。 reader.sync(pos); //定位到pos之后的第一个同步点。 writer.sync(); //写入同步点 4.压缩方式 不压缩 record压缩 //只压缩value 块压缩 //按照多个record形成一个block. 文件格式:MapFile1.Key-value 2.key按升序写入(可重复)。 3.mapFile对应一个目录，目录下有index和data文件,都是序列文件。 4.index文件划分key区间,用于快速定位。 自定义分区函数1.定义分区类 public class MyPartitioner extends Partitioner&lt;Text, IntWritable&gt;{ public int getPartition(Text text, IntWritable intWritable, int numPartitions) { return 0; } } 2.程序中配置使用分区类 job.setPartitionerClass(MyPartitioner.class); combinerd（合成） 继承了Reducer 任何的Reducer类都能被他使用 Map端的Reducer 预先化简1，为了减少网络带宽 将Map端发出的的数据进行聚合 并不是所有的都可以用combiner2,combiner 切片个数是四个，mapper就需要也是4个 下图是一个客户端的写入过程：首先形成一个管线，因为有很多节点，节点形成管线，现在本地构建2个队列，一个发送队列sendQueue，一个是确认队列ackQueue，客户端吧消息封装成Packet放到队列里面，而且结束后会放一个空包进去，发送的话，是以管线的方式发给A，A在发给B，B在发给C，在发送的过程中有一个校验的过程chunk(4+512)就是4个校验位+512个数据字节，一个Packet是64K+33个头的字节，在确认队列里面是一个编号seqNo如果在丢列中ABC中都回执，就吧seqNo在ackQueue中移除。发送如果失败，就吧管线重新建立，吧没法送的包返回到队列之前的一个容错机制。 输入格式定位文件，在输入格式中间有一个交互reader，在Map和Reduce都有这么一些方法，在Mapper和Reducer中有回调方法，就是本来使用的是子类的方法，但是子类中调用有父类的的run方法，然后run方法里面有reduce方法也就又调用了子类的方法，这叫做回调 。Mapper是通过reader读取下一条数据传进来的。Map和Reduce中间有一个shuffer的混洗过程，网络间分发数据的过程， Map里面先分区，分区个数等于Reduce的个数，所以有几个reduce就有几个分区，默认情况下，Map分区通过哈希算法来分区的，对K哈希。只要K相同，一定会被哈希到一个分区里面，分区也就是桶的概念，也就是只要K相同进入到同一个桶里面，只要是同一个分区就是进入到同一个reduce，默认情况下inputformat的格式，他的K就是偏移量。如果是图片格式的话就可能是别的。只有在文本输入的时候才是偏移量，当然也可以自己定义Inputformat格式。在mr计算之前切片就已经算好了，产生的切片文件和配置XML文件放到了临时目录下了。 看下图，首先客户端运行作业，第一步run job，第二部走ResourceManager，是Yarn层面，负责资源调度，所有集群的资源都属于ResourceManager，第二部get new application，首先得到应用的ID，第三部copyt job resources，复制切片信息和xml配置信息和jar包到HDFS，第四部提交给资源管理器，然后资源管理器start container，然后通过节点管理器启动一个MRAPPMaster（应用管理程序），第六步初始化Job，第七部检索切片信息，第八步请求资源管理器去分配一个资源列表，第九步启动节点管理器，通过节点管理器来启动一个JVM虚拟机，在虚拟机里面启动yarn子进程，第十部通过子进程读取切片信息，然后在运行MapTast或者ReduceTask 客户端提取一个作业，然后联系资源管理器，为了得到id,在完全分布式下客户端client叫job，一旦提交就叫做application了，先请求，然后得到了appid返回给了客户端，第三部将作业信息上传到HDFS的临时目录，存档切片信息和jar包，然后找到节点管理器，然后NM孵化一个MRAPPMaster，然后检索HDFS文件系统，获取到有多少个要执行的并发任务，拿到任务之后，要求资源管理器分配一个集合，哪些节点可以用。 然后MRAppmaster在联系其他的NM，让NM去开启Yarn子进程，子进程在去检索HDFS信息，在开启Map和Reduce hdfs 切片计算方式getFormatMinSplitSize() = 1 //最小值(&gt;=1) 1 0 long minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job)); //最大值(&lt;= Long.Max , mapreduce.input.fileinputformat.split.maxsize=) long maxSize = getMaxSplitSize(job); //得到block大小 long blockSize = file.getBlockSize(); //minSplit maxSplit blockSize //Math.max(minSize, Math.min(maxSize, blockSize)); 在计算切片的时候一般块大小不设置，是最好的，一般就是128M，采用本地优先策略，一个快大小是128m如果一旦把他边变1m那么切片大小就会变成1M，一个，然后128M的块大小会被分发128份切片分别传输给MR去计算，这样的话，就会增加网络间的传输，增加网络负载。所以128m是最合适的 CR=‘\r’回车符LF+’\n’换行符 windows系统里面是\r\n。回车带换行。linux系统只有一个\n 切片是定义大方向的，阅读器recorderreader是控制细节的。切片定义任务数量，开几个map但是不一定是每个map都有任务。 切片问题是物理设置，但是是逻辑读取。 打个比方说上图这个东西比如说第一个例子，前面的设置物理划分，根据字节数划分成了4个切片，也就是有4个Mapper,然后四个切片开始读取，其中第一个物理切割到t但是实际上，由于切片读取到换行符所以，第一个Mapper也就是读取到了hello world tom，然后第二个，第二个偏移量是13+13.偏移量是13，在物理读取13个直接，但是由于Mapper检测不是行首，所以直接从下一行读取也在读取一整行。这个就是每个切片128MB的缩影，吧128M变成了13而已。然后其实Textinputformat下一个环节是。recoderreader。这个会先检测是否是行首， 压缩问题package com.it18zhang.hdfs.mr.compress; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.io.IOUtils; import org.apache.hadoop.io.compress.*; import org.apache.hadoop.util.ReflectionUtils; import org.junit.Test; import java.io.FileInputStream; import java.io.FileOutputStream; /** * @Title:TestCompress * @Description: * @Auther: Eric Hunn * @Version:1.0 * @Create 2018/10/14 19:39 */ public class TestCompress { @Test public void deflateCompress() throws Exception { Class[] zipClasses = { DeflateCodec.class, GzipCodec.class, BZip2Codec.class, Lz4Codec.class, }; for(Class c : zipClasses){ zip(c); } } /*压缩测试 * * */ public void zip(Class codecClass) throws Exception { long start = System.currentTimeMillis(); //实例化对象 CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, new Configuration()); //创建文件输出流，得到默认拓展名 FileOutputStream fos = new FileOutputStream(&quot;e:/comp/b&quot; + codec.getDefaultExtension()); //得到压缩流 CompressionOutputStream zipOut = codec.createOutputStream(fos); IOUtils.copyBytes(new FileInputStream(&quot;e:/11111.txt/&quot;), zipOut, 1024); zipOut.close(); System.out.println(codecClass.getSimpleName() + &quot; ：&quot;); } } 压缩一般是在文件Map输出的时候压缩，Map输出的时候内存不够一般要往磁盘里面溢出，在溢出的时候可以让他压缩溢出，这样reduce的时候就要解压缩。上面代码测试过lz4不行，但是视频上是可以的，暂时不知道哪里出错了。 package com.it18zhang.hdfs.mr.compress; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.io.IOUtils; import org.apache.hadoop.io.compress.*; import org.apache.hadoop.util.ReflectionUtils; import org.junit.Test; import java.io.FileInputStream; import java.io.FileOutputStream; /** * @Title:TestCompress * @Description: * @Auther: Eric Hunn * @Version:1.0 * @Create 2018/10/14 19:39 */ public class TestCompress { @Test public void deflateCompress() throws Exception { Class[] zipClasses = { DeflateCodec.class, GzipCodec.class, BZip2Codec.class, // Lz4Codec.class, // SnappyCodec.class, }; for(Class c : zipClasses){ unzip(c); } } /*压缩测试 * * */ public void zip(Class codecClass) throws Exception { long start = System.currentTimeMillis(); //实例化对象 CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, new Configuration()); //创建文件输出流，得到默认拓展名 FileOutputStream fos = new FileOutputStream(&quot;e:/comp/b&quot; + codec.getDefaultExtension()); //得到压缩流 CompressionOutputStream zipOut = codec.createOutputStream(fos); IOUtils.copyBytes(new FileInputStream(&quot;e:/11111.txt/&quot;), zipOut, 1024); zipOut.close(); System.out.println(codecClass.getSimpleName() + &quot; ：&quot;); } public void unzip(Class codecClass) throws Exception { long start = System.currentTimeMillis(); //实例化对象 CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, new Configuration()); //创建文件输出流，得到默认拓展名 FileInputStream fis = new FileInputStream(&quot;e:/comp/b&quot; + codec.getDefaultExtension()); //得到压缩流 CompressionInputStream zipIn = codec.createInputStream(fis); IOUtils.copyBytes(zipIn,new FileOutputStream(&quot;e:/comp/b&quot; + codec.getDefaultExtension() + &quot;.txt&quot;), 1024); zipIn.close(); System.out.println(codecClass.getSimpleName() + &quot; ：&quot; + (System.currentTimeMillis() - start)); } } 在集群上运package com.it18zhang.hdfs.mr.compress; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.io.IOUtils; import org.apache.hadoop.io.compress.*; import org.apache.hadoop.util.ReflectionUtils; import java.io.FileInputStream; import java.io.FileOutputStream; /** * @Title:TestCompress * @Description: * @Auther: Eric Hunn * @Version:1.0 * @Create 2018/10/14 19:39 */ public class TestCompress { public static void main(String[] args) throws Exception { Class[] zipClasses = { DeflateCodec.class, GzipCodec.class, BZip2Codec.class, // Lz4Codec.class, // SnappyCodec.class, }; for(Class c : zipClasses){ zip(c); } System.out.println(&quot;==================================&quot;); for(Class c : zipClasses){ unzip(c); } } /*压缩测试 * * */ public static void zip(Class codecClass) throws Exception { long start = System.currentTimeMillis(); //实例化对象 CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, new Configuration()); //创建文件输出流，得到默认拓展名 FileOutputStream fos = new FileOutputStream(&quot;/home/centos/zip/b&quot; + codec.getDefaultExtension()); //得到压缩流 CompressionOutputStream zipOut = codec.createOutputStream(fos); IOUtils.copyBytes(new FileInputStream(&quot;/home/centos/zip/a.txt&quot;), zipOut, 1024); zipOut.close(); System.out.println(codecClass.getSimpleName() + &quot; ：&quot;); } public static void unzip(Class codecClass) throws Exception { long start = System.currentTimeMillis(); //实例化对象 CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, new Configuration()); //创建文件输出流，得到默认拓展名 FileInputStream fis = new FileInputStream(&quot;/home/centos/zip/b&quot; + codec.getDefaultExtension()); //得到压缩流 CompressionInputStream zipIn = codec.createInputStream(fis); IOUtils.copyBytes(zipIn,new FileOutputStream(&quot;/home/centos/zip/b&quot; + codec.getDefaultExtension() + &quot;.txt&quot;), 1024); zipIn.close(); System.out.println(codecClass.getSimpleName() + &quot; ：&quot; + (System.currentTimeMillis() - start)); } } 远程调试1.设置服务器java vm的-agentlib:jdwp选项. [server] //windwos //set JAVA_OPTS=%JAVA_OPTS% -agentlib:jdwp=transport=dt_socket,address=8888,server=y,suspend=n //linux 2.在server启动java程序 hadoop jar HdfsDemo.jar com.it18zhang.hdfs.mr.compress.TestCompress 3.server会暂挂在8888. Listening ... 4.客户端通过远程调试连接到远程主机的8888. 5.客户端就可以调试了。 通过MapFile来写入 /*写操作 * */ @Test public void save() throws Exception { Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS &quot;, &quot;file:///&quot;); FileSystem fs = FileSystem.get(conf); Path p = new Path(&quot;e:/seq/1.seq&quot;); MapFile.Writer writer = new MapFile.Writer(conf, fs, &quot;e:/map&quot;, IntWritable.class, Text.class); for (int i = 0; i &lt; 100000; i++) { writer.append(new IntWritable(i), new Text(&quot;tom&quot; + i)); } // for(int i =0 ;i &lt; 10 ; i++){// writer.append(new IntWritable(i),new Text(“tom” + i));// } writer.close(); } 通过MapFile来读取/*读取Mapfile文件 * */ @Test public void readMapfile() throws Exception { Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS &quot;, &quot;file:///&quot;); FileSystem fs = FileSystem.get(conf); Path p = new Path(&quot;e:/seq/1.seq&quot;); MapFile.Reader reader = new MapFile.Reader(fs, &quot;e:/map&quot;, conf); IntWritable key = new IntWritable(); Text value = new Text(); while (reader.next(key, value)) { System.out.println(key.get() + &quot;:&quot; + value.toString()); } reader.close(); } Map的分区是哈希分区 combiner的好处是：map端的reduce在map端做集合，减少了shuffle量。统计每个单词hello 1发了十万次，不如发一个hello1在map端做聚合发过去hello十万，即刻。就是在map端口预先reduce化简的过程。不是所有的作业都可以把reduce做成combiner。最大值最小值都可以combiner但是平均值就不行了。 第六天的最后两个视频没有往下写代码，因为太多太杂了。主要讲解MR计数器，数据倾斜，Mapfile 序列化读取，压缩这些。]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Yan作业提交</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop第五天01之hdfs写入剖析]]></title>
    <url>%2F2018%2F10%2F05%2FHadoop%E7%AC%AC%E4%BA%94%E5%A4%A901%E4%B9%8Bhdfs%E5%86%99%E5%85%A5%E5%89%96%E6%9E%90%2F</url>
    <content type="text"><![CDATA[一段HDFS写入流源码分析。 首先这个得到了一个抽象类，这个FileSystemm是一个抽象类。 这个抽象类有子类，左边有一个箭头，点击得到了一个这个抽象类的所有的子类，简单看下，因为Hadoop包类的hadfs这个，得到的子类是DistributedFileSystem这个分布式文件系统。 也可以吧鼠标放到fs上会显示返回的类 也可以在IDEA的右下角的类标签里面找到： 也就是说返回了一个DistributedFIleSystem, 然后文件系统fs.create调用了create方法返回了一个FSDataOutputStream流，这个是一个HDFS数据输出流 单机F5单部进入第一个： 看这个，进到下一步，调用checkClosed()，方法，这个方法其实是继承FSOutput流里面的东西。 看到这个里面out=DFSOutputStream。这个是被装饰的流。调用这个流的write方法 HDFS流是对DFS输出流的包装 进去这个是装饰模式。 在这个构造模式中也声明了字段。 下一步： 调用了Close方法因为是继承 都是FSoutput流的子类。一个检查的方法，判断是否数组越界。 下面这个for是个循环，循环写入，。 然后下一步，进入到write1方法。 里面的buf是一个缓冲区，count是一个成员常量 上图的buf看注释，是一个缓冲区用哦过来存储数据，在他被checksum校验和之前 校验和就是检查是否是比之前设置的最小块大小大，而且还必须是512字节的倍数 上图点击进入到一个校验和类的里面看注释，校验和用于数据传输之前。 上图这两行，通过一个算法来实现校验和。通过CRC32算法类似于哈希算法。 不管他，回到buf缓冲这个地方，单部进入 首先在缓冲区进行一个判定 拷贝本地数据到本地的一个buf，显示Localbuffer=4608,count=0.要拷贝的字节是4608。如果拷贝的内容没有缓冲区大就考数据内容，要是比缓冲区大的话，就拷贝缓冲区大小的内容。 在单部进入到上图。 返回到代码。进入到源代码中，如上图 在单部进入到这个Close里面：如下图： 这个close是out.close。out是调用父类的close方法，而且父类还是一个包装类，所以进入了包装的列的close方法。再进入到close()方法。如下图： out的本类HDFSOutputStream，然后这个又是一个装饰类，本质上是DFSOutputStream这个类的方法。 单部进入到Out里面，看一下调用的到底是谁的out。再单部进入到Out里面 看注释，关闭输出流并且 释放与之相关联的系统资源。上图最终进入到了DFSOutputstream的close()方法里面了。 接上图，看到在这个close(）方法里面有一个closeImp（)方法。调用自己的关闭实现方法还在这个类里面执行呢：继续在这个类里面往下走:如下图： 这个里面有一个flushbuffer()方法， 在单部进入到这个方法里面。如下图： 看到这个图里面this和当前类不太一样说明这两个东西也是继承关系。 清理缓冲区两个参数，看注释：强制任何输出流字节被校验和并且被写入底层的输出流。 再单步进入： 看到如上图的内容。然后到了writeChecksumChunks这个方法里面，单部进入到这个里面： 对给定的数据小块来生成校验和，并且根据小块和校验和给底层的数据输出流。 单部进入到这个sum.calculateChunckedSum方法里面。 下一步 上图吧数据写入了底层里面去了。 下一步 下一步 下一步 上图就是将底层的内容打成包，再去通过网络传输。在包里面加编号和序号，也就是加顺序。 往下不写了太多了。现在的视频位置是hdfs写入剖析2的开头。]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>hdfs写入</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop第四天之滚动日志-安全模式]]></title>
    <url>%2F2018%2F10%2F04%2FHadoop%E7%AC%AC%E5%9B%9B%E5%A4%A9%E4%B9%8B%E6%BB%9A%E5%8A%A8%E6%97%A5%E5%BF%97-%E5%AE%89%E5%85%A8%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Hadoop第四天之最小块设置-指定副本数]]></title>
    <url>%2F2018%2F10%2F04%2FHadoop%E7%AC%AC%E5%9B%9B%E5%A4%A9%E4%B9%8B%E6%9C%80%E5%B0%8F%E5%9D%97%E8%AE%BE%E7%BD%AE-%E6%8C%87%E5%AE%9A%E5%89%AF%E6%9C%AC%E6%95%B0%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[遇到未解决的问题]]></title>
    <url>%2F2018%2F10%2F01%2F%E9%81%87%E5%88%B0%E6%9C%AA%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[在eclipse中，ctrl+左键找到源码，为什么旁边在pakage explore中有有轮廓啊？ 点击两个箭头即可转换。 这个东西是在IDEA中怎么调出来的。 克隆centos之后有时候出现这种情况 IDEA的使用问题：这个是什么快捷键。 在增强虚拟机工具的时候按照视频的方法没有成功，然后通过挂载的方法实现了，但是hgfs文件夹和文件夹里面的内容都是root权限的，只能修改文件夹的所有者，但是不能修改里面文件的所有者。 有一个分组的问题： 分组就是说组合key出来本来是每一个组合key大多都是不同的key。相同年份不同温度也是不同的key，但是为了相同的年份的组合key能够进入到同一个reduce，所以要通过分组让他这个样子，但是为什么要这个样 经过分区已经说达到了将不同年份的 在mapreduce阶段有一个大表和小表连接 hive里面也有一个小表和小表连接就在map端口，然后小表和大表连接在reduce端口。 这个地方的问题没有搞明白。具体的hive在hive第二天07mr的忘记在哪里了 然后mr的最后一天hadoop第十一天的二次排序没有搞得特别明白。和hive阶段的自定义函数UDF没有特别清楚 在avro和rotobuf第一天 在讲解protobuf的时候写代码写到下图的时候听说这个com.example.tuorial是源码包里面的东西，我是没找到，我觉得可能是源码包里面的例子，里面包括的一些类，但是还是没有这个东西。 已经解决，见大坑 在一个spring项目里面有这样一个问题： dao层类dao一个方法，然后在servicce层里面没有new出一个dao类的对象，直接写一个属性Private Dao d;然后直接d.insert(); 这种方法为什么可以 这种是什么调用方法：]]></content>
      <categories>
        <category>问题</category>
      </categories>
      <tags>
        <tag>未解决的问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[haoop第三天之脚本分析，单个进程启动]]></title>
    <url>%2F2018%2F09%2F30%2Fhaoop%E7%AC%AC%E4%B8%89%E5%A4%A9%E4%B9%8B%E8%84%9A%E6%9C%AC%E5%88%86%E6%9E%90%EF%BC%8C%E5%8D%95%E4%B8%AA%E8%BF%9B%E7%A8%8B%E5%90%AF%E5%8A%A8%2F</url>
    <content type="text"><![CDATA[ssh权限问题1.~/.ssh/authorized_keys 644 2.$/.ssh 700 3.root 配置SSH生成密钥对 $&gt;ssh-keygen -t rsa -P &apos;&apos; -f ~/.ssh/id_rsa 添加认证文件 $&gt;cat ~/.ssh/id_rsa.pub &gt; ~/.ssh/authorized_keys 权限设置,文件和文件夹权限除了自己之外，别人不可写。 $&gt;chmod 700 ~/.ssh $&gt;chmod 644 ~/.ssh/authorized_keys scp远程复制. rsync远程同步,支持符号链接。 rsync -lr xxx xxx 完全分布式1.配置文件 [core-site.xml] fs.defaultFS=hdfs://s201:8020/ [hdfs-site.xml] replication=1 //伪分布 replication=3 //完全分布 [mapred-site.xml] mapreduce.framework.name=yarn [yarn-site.xml] rm.name=s201 [slaves] s202 s203 s204 2.分发文件 a)ssh openssh-server //sshd openssh-clients //ssh openssh //ssh-keygen b)scp/rsync 3.格式化文件系统 $&gt;hadoop namenode -format 4.启动hadoop所有进程 //start-dfs.sh + start-yarn.sh $&gt;start-all.sh 5.xcall.sh jps /usr/local/bin/jps /usr/local/bin/java 6.查看jps进程 $&gt;xcall.sh jps 7.关闭centos的防火墙 $&gt;sudo service firewalld stop // &lt;=6.5 start/stop/status/restart $&gt;sudo systemctl stop firewalld // 7.0 停止 start/stop/status/restart $&gt;sudo systemctl disable firewalld //关闭 $&gt;sudo systemctl enable firewalld //启用 7.最终通过webui http://s201:50070/ 符号连接1.修改符号连接的owner $&gt;chown -h centos:centos xxx //-h:针对连接本身，而不是所指文件. 2.修改符号链接 $&gt;ln -sfT index.html index //覆盖原有的连接。 hadoop模块common // hdfs // mapreduce // yarn // 进程[hdfs]start-dfs.sh NameNode NN DataNode DN SecondaryNamenode 2NN [yarn]start-yarn.sh ResourceMananger RM NodeManager NM 脚本分析sbin/start-all.sh -------------- libexec/hadoop-config.sh start-dfs.sh start-yarn.sh sbin/start-dfs.sh -------------- libexec/hadoop-config.sh sbin/hadoop-daemons.sh --config .. --hostname .. start namenode ... sbin/hadoop-daemons.sh --config .. --hostname .. start datanode ... sbin/hadoop-daemons.sh --config .. --hostname .. start sescondarynamenode ... sbin/hadoop-daemons.sh --config .. --hostname .. start zkfc ... // sbin/start-yarn.sh -------------- libexec/yarn-config.sh bin/yarn-daemon.sh start resourcemanager bin/yarn-daemons.sh start nodemanager sbin/hadoop-daemons.sh ---------------------- libexec/hadoop-config.sh slaves hadoop-daemon.sh sbin/hadoop-daemon.sh ----------------------- libexec/hadoop-config.sh bin/hdfs .... sbin/yarn-daemon.sh ----------------------- libexec/yarn-config.sh bin/yarn bin/hadoop ------------------------ hadoop verion //版本 hadoop fs //文件系统客户端. hadoop jar // hadoop classpath hadoop checknative bin/hdfs ------------------------ dfs // === hadoop fs classpath namenode -format secondarynamenode namenode journalnode zkfc datanode dfsadmin haadmin fsck balancer jmxget mover oiv oiv_legacy oev fetchdt getconf groups snapshotDiff lsSnapshottableDir portmap nfs3 cacheadmin crypto storagepolicies version hdfs常用命令$&gt;hdfs dfs -mkdir /user/centos/hadoop $&gt;hdfs dfs -ls -r /user/centos/hadoop $&gt;hdfs dfs -lsr /user/centos/hadoop $&gt;hdfs dfs -put index.html /user/centos/hadoop $&gt;hdfs dfs -get /user/centos/hadoop/index.html a.html $&gt;hdfs dfs -rm -r -f /user/centos/hadoop no route 关闭防火墙。 $&gt;su root $&gt;xcall.sh &quot;service firewalld stop&quot; $&gt;xcall.sh &quot;systemctl disable firewalld&quot; hdfs500G 1024G = 2T/4T 切割。 寻址时间:10ms左右 磁盘速率 : 100M /s 64M 128M //让寻址时间占用读取时间的1%. 1ms 1 / 100 size = 181260798 block-0 : 134217728 block-1 : 47043070 -------------------- b0.no : 1073741829 b1.no : 1073741830 HAhigh availability,高可用性。通常用几个9衡量。 99.999% SPOF:single point of failure,单点故障。 secondarynamenode找到所有的配置文件1.tar开hadoop-2.7.3.tar.gz hadoop-2.7.3\share\hadoop\common\hadoop-common-2.7.3.jar\core-default.xml hadoop-2.7.3\share\hadoop\hdfs\hadoop-hdfs-2.7.3.jar\hdfs-default.xml hadoop-2.7.3\share\hadoop\mapreduce\hadoop-mapreduce-client-core-2.7.3.jar\mapred-default.xml hadoop-2.7.3\share\hadoop\yarn\hadoop-yarn-common-2.7.3.jar\yarn-site.xml 本地模式[core-site.xml] fs.defaultFS=file:/// //默认值 配置hadoop临时目录1.配置[core-site.xml]文件 &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://s201/&lt;/value&gt; &lt;/property&gt; &lt;!--- 配置新的本地目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/centos/hadoop&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; //以下属性均由hadoop.tmp.dir决定,在hdfs-site.xml文件中配置。 dfs.namenode.name.dir=file://${hadoop.tmp.dir}/dfs/name dfs.datanode.data.dir=file://${hadoop.tmp.dir}/dfs/data dfs.datanode.data.dir=file://${hadoop.tmp.dir}/dfs/data dfs.namenode.checkpoint.dir=file://${hadoop.tmp.dir}/dfs/namesecondary dfs.namenode.checkpoint.dir=file://${hadoop.tmp.dir}/dfs/namesecondary 2.分发core-site.xml文件 $&gt;xsync core-site.xml 3.格式化文件系统,只对namenode的本地目录进行初始化。 $&gt;hadoop namenode -format //hdfs namenode -format 4.启动hadoop $&gt;start-dfs.sh 使用xcall.sh在所有节点上创建jps符号连接，指向/soft/jdk/bin/jps1.切换到root用户 $&gt;su root 2.创建符号连接 $&gt;xcall.sh &quot;ln -sfT /soft/jdk/bin/jps /usr/local/bin/jps&quot; 3.修改jps符号连接的owner $&gt;xcall.sh &quot;chown -h centos:centos /usr/local/bin/jps&quot; 4.查看所有主机上的java进程 $&gt;xcall.sh jps 在centos桌面版中安装eclipse1.下载eclipse linux版 eclipse-jee-mars-R-linux-gtk-x86_64.tar.gz 2.tar开到/soft下, $&gt;tar -xzvf eclipse-jee-mars-R-linux-gtk-x86_64.tar.gz -C /soft 3.启动eclipse $&gt;cd /soft/eclipse $&gt;./eclipse &amp; //后台启动 4.创建桌面快捷方式 $&gt;ln -s /soft/eclipse/eclipse ~/Desktop/eclipse 5. 收集hadoop的所有jar包使用hadoop客户端api访问hdfs1.创建java项目 2.导入hadoop类库 3. 4. 5. 网络拓扑1. 2. 3. 4. 作业1.使用hadoop API递归输出整个文件系统 2.]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>脚本分析</tag>
        <tag>单个进程启动</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop第二天之搭建]]></title>
    <url>%2F2018%2F09%2F29%2FHadoop%E7%AC%AC%E4%BA%8C%E5%A4%A9%E4%B9%8B%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[hadoop1.独立模式(standalone|local) nothing! 本地文件系统。 不需要启用单独进程。 2.pesudo(伪分布模式) 等同于完全分布式，只有一个节点。 SSH: //(Socket), //public + private //server : sshd ps -Af | grep sshd //clint : ssh //ssh-keygen:生成公私秘钥。 //authorized_keys 需要使用644 //ssh 192.168.231.201 yes [配置文件] core-site.xml //fs.defaultFS=hdfs://localhost/ hdfs-site.xml //replication=1 mapred-site.xml // yarn-site.xml // 3.full distributed(完全分布式) 让命令行提示符显式完整路径1.编辑profile文件，添加环境变量PS1 [/etc/profile] export PS1=&apos;[\u@\h `pwd`]\$&apos; 2.source $&gt;source /etc/profile 配置hadoop，使用符号连接的方式，让三种配置形态共存。1.创建三个配置目录,内容等同于hadoop目录 ${hadoop_home}/etc/local ${hadoop_home}/etc/pesudo ${hadoop_home}/etc/full 2.创建符号连接 $&gt;ln -s 3.对hdfs进行格式化 $&gt;hadoop namenode -format 4.修改hadoop配置文件，手动指定JAVA_HOME环境变量 [${hadoop_home}/etc/hadoop/hadoop-env.sh] ... export JAVA_HOME=/soft/jdk ... 5.启动hadoop的所有进程 $&gt;start-all.sh 6.启动完成后，出现以下进程 $&gt;jps 33702 NameNode 33792 DataNode 33954 SecondaryNameNode 29041 ResourceManager 34191 NodeManager 7.查看hdfs文件系统 $&gt;hdfs dfs -ls / 8.创建目录 $&gt;hdfs dfs -mkdir -p /user/centos/hadoop 9.通过webui查看hadoop的文件系统 http://localhost:50070/ 10.停止hadoop所有进程 $&gt;stop-all.sh 11.centos防火墙操作 [cnetos 6.5之前的版本] $&gt;sudo service firewalld stop //停止服务 $&gt;sudo service firewalld start //启动服务 $&gt;sudo service firewalld status //查看状态 [centos7] $&gt;sudo systemctl enable firewalld.service //&quot;开机启动&quot;启用 $&gt;sudo systemctl disable firewalld.service //&quot;开机自启&quot;禁用 $&gt;sudo systemctl start firewalld.service //启动防火墙 $&gt;sudo systemctl stop firewalld.service //停止防火墙 $&gt;sudo systemctl status firewalld.service //查看防火墙状态 [开机自启] $&gt;sudo chkconfig firewalld on //&quot;开启自启&quot;启用 $&gt;sudo chkconfig firewalld off //&quot;开启自启&quot;禁用 hadoop的端口50070 //namenode http port 50075 //datanode http port 50090 //2namenode http port 8020 //namenode rpc port 50010 //datanode rpc port hadoop四大模块common hdfs //namenode + datanode + secondarynamenode mapred yarn //resourcemanager + nodemanager 启动脚本1.start-all.sh //启动所有进程 2.stop-all.sh //停止所有进程 3.start-dfs.sh // 4.start-yarn.sh [hdfs] start-dfs.sh stop-dfs.sh NN DN 2NN [yarn] start-yarn.sh stop-yarn.sh RM NM 修改主机名1./etc/hostname s201 2./etc/hosts 127.0.0.1 localhost 192.168.231.201 s201 192.168.231.202 s202 192.168.231.203 s203 192.168.231.204 s204 完全分布式1.克隆3台client(centos7) 右键centos-7--&gt;管理-&gt;克隆-&gt; ... -&gt; 完整克隆 2.启动client 3.启用客户机共享文件夹。 4.修改hostname和ip地址文件 [/etc/hostname] s202 [/etc/sysconfig/network-scripts/ifcfg-ethxxxx] ... IPADDR=.. 5.重启网络服务 $&gt;sudo service network restart 6.修改/etc/resolv.conf文件 nameserver 192.168.231.2 7.重复以上3 ~ 6过程. 准备完全分布式主机的ssh1.删除所有主机上的/home/centos/.ssh/* 2.在s201主机上生成密钥对 $&gt;ssh-keygen -t rsa -P &apos;&apos; -f ~/.ssh/id_rsa 3.将s201的公钥文件id_rsa.pub远程复制到202 ~ 204主机上。 并放置/home/centos/.ssh/authorized_keys $&gt;scp id_rsa.pub centos@s201:/home/centos/.ssh/authorized_keys $&gt;scp id_rsa.pub centos@s202:/home/centos/.ssh/authorized_keys $&gt;scp id_rsa.pub centos@s203:/home/centos/.ssh/authorized_keys $&gt;scp id_rsa.pub centos@s204:/home/centos/.ssh/authorized_keys 4.配置完全分布式(${hadoop_home}/etc/hadoop/) [core-site.xml] &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://s201/&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; [hdfs-site.xml] &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; [mapred-site.xml] 不变 [yarn-site.xml] &lt;?xml version=&quot;1.0&quot;?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;s201&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; [slaves] s202 s203 s204 [hadoop-env.sh] ... export JAVA_HOME=/soft/jdk ... 5.分发配置 $&gt;cd /soft/hadoop/etc/ $&gt;scp -r full centos@s202:/soft/hadoop/etc/ $&gt;scp -r full centos@s203:/soft/hadoop/etc/ $&gt;scp -r full centos@s204:/soft/hadoop/etc/ 6.删除符号连接 $&gt;cd /soft/hadoop/etc $&gt;rm hadoop $&gt;ssh s202 rm /soft/hadoop/etc/hadoop $&gt;ssh s203 rm /soft/hadoop/etc/hadoop $&gt;ssh s204 rm /soft/hadoop/etc/hadoop 7.创建符号连接 $&gt;cd /soft/hadoop/etc/ $&gt;ln -s full hadoop $&gt;ssh s202 ln -s /soft/hadoop/etc/full /soft/hadoop/etc/hadoop $&gt;ssh s203 ln -s /soft/hadoop/etc/full /soft/hadoop/etc/hadoop $&gt;ssh s204 ln -s /soft/hadoop/etc/full /soft/hadoop/etc/hadoop 8.删除临时目录文件 $&gt;cd /tmp $&gt;rm -rf hadoop-centos $&gt;ssh s202 rm -rf /tmp/hadoop-centos $&gt;ssh s203 rm -rf /tmp/hadoop-centos $&gt;ssh s204 rm -rf /tmp/hadoop-centos 9.删除hadoop日志 $&gt;cd /soft/hadoop/logs $&gt;rm -rf * $&gt;ssh s202 rm -rf /soft/hadoop/logs/* $&gt;ssh s203 rm -rf /soft/hadoop/logs/* $&gt;ssh s204 rm -rf /soft/hadoop/logs/* 10.格式化文件系统 $&gt;hadoop namenode -format 11.启动hadoop进程 $&gt;start-all.sh rsync四个机器均安装rsync命令。 远程同步. $&gt;sudo yum install rsync 将root用户实现无密登录1.同 编写脚本1.xcall.sh 2.xsync.sh xsync.sh /home/etc/a.txt rsync -lr /home/etc/a.txt centos@s202:/home/etc netstat -anop 查看进程]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>搭建</tag>
        <tag>第二天</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop安装配置]]></title>
    <url>%2F2018%2F09%2F24%2FHadoop%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"></content>
      <categories>
        <category>-Hadoop</category>
      </categories>
      <tags>
        <tag>-Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux基础之Yum命令]]></title>
    <url>%2F2018%2F09%2F20%2FLinux%E5%9F%BA%E7%A1%80%E4%B9%8BYum%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[软件源Repository //仓库. URL //http:// .d //directory目录 xxxd //daemon 查看仓库文件/etc/yum.repos.d/xxx.repo curl传输url上的数据的。 [下载文件到指定目录] curl -o /etc/yum.repos.d/ali.repo http://mirrors.aliyun.com/repo/Centos-7.repo 更换centos的软件源1.下载源仓库文件,xxx.repo curl -o /etc/yum.repos.d/ali.repo http://mirrors.aliyun.com/repo/Centos-7.repo 2.将repo文件保存到/etc/yum.repos.d/目录中。 屏蔽软件仓库1.将/etc/yum.repos.d/xxx.repo文件删除或者更换扩展名即可。 修改centos能够使用sudo命令[/etc/sudoers] $&gt;su root $&gt;nano /etc/sudoers ... centos ALL 使用yum进行软件包安装卸载$&gt;yum list //列出所有软件包 $&gt;yum list installed //列出已经安装的软件包 $&gt;yum list installed | grep nano //列出已经安装的软件包 $&gt;yum search nano //在yum的软件源中搜索软件 $&gt;yum remove nano //卸载软件 $&gt;yum -y install nano //直接安装，不需要yes确认. $&gt;yum list installed | grep nano //查看是否安装了Nano $&gt;mkdir /home/centos/rpms $echo 以下命令只下载软件，不安装软件 $&gt;sudo yum install --downloadonly //只下载 --downloaddir=/home/centos/rpms //指定下载目录 wget //下载已经安装的软件 $&gt;sudo yum reinstall --downloadonly --downloaddir=/home/centos/rpms wget $&gt;sudo yum localinstall xxx.rpm //从本地rpm文件直接安装软件 $&gt;su root $&gt;yum search ifconfig $&gt;yum -y install net-tools //安装网络工具 #==========修改网络地址====================== //需要重启network服务 $&gt;sudo nano /etc/sysconfig/network-scripts/ifcfg-eth1677736 [/etc/sysconfig/network-scripts/ifcfg-eth1677736] ... IPADDR=192.168.231.201 GATEWAY=192.168.231.2 DNS=192.168.231.2 $&gt;service network restart //重启网络服务。 $&gt;sudo nano /etc/resolv.conf //修改该文件不需要重启network服务 [/etc/resolv.conf] nameserver 192.168.231.2 在没有nano时，使用自带的vi文本编辑器1.vi xx.txt 2.模式切换 esc //切换到命令模式,退出编辑模式 //:q! 不保存退出 //:wq 保存退出 //x 删除一个字符 //dd 删除一行 insert //切换到编辑模式,退出命令模式 //del backspace Which命令which命令用于查找并显示给定命令的绝对路径，环境变量PATH中保存了查找命令时需要遍历的目录。 which指令会在环境变量$PATH设置的目录里查找符合条件的文件。也就是说，使用which命令，就可以看到某个系统命令是否存在，以及执行的到底是哪一个位置的命令。 语法which(选项)(参数)]]></content>
      <tags>
        <tag>Linux基础</tag>
        <tag>Yum命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux基础第之循环命令]]></title>
    <url>%2F2018%2F09%2F19%2FLinux%E5%9F%BA%E7%A1%80%E7%AC%AC%E4%B9%8B%E5%BE%AA%E7%8E%AF%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[访问文件(夹)物理位置pwd命令是显示当前的逻辑位置。 物理位置是针对符号连接也是针对软连接的。 //进入/t的物理位置 $&gt;cd -P /t //显式当前目录的物理位置 $&gt;pwd -P 访问环境变量echo ${PATH} //ok echo $PATH //ok echo &quot;$PATH&quot; //ok echo &apos;$PATH&apos; //&apos;&apos;原样输出这个不行，显示“PATH” export定义环境变量,只在session中有效 (当前会话)$&gt;export name=${PATH}:tom 设置name为${Var1}的值，Var1没有设置为${Var2}的值。 $&gt;export name=${Var1:-${Var2}} 命令执行过程$? //命令的返回值存储变量,0:成功 1:失败。 $# //参数个数 $1 //第几个参数 $0 //当前脚本(命令)名称 $@ //取出所有参数 shift //参数左移 ${a/b/c} // 下面一个例子 向左移位解释： if 命令讲解语法: 中括号是可以选择的： if COMMANDS; then COMMANDS; [ elif COMMANDS; then COMMANDS; ]... [ else COMMANDS; ] fi if [ $# -lt 3 ]; then xx ; fi 3,5 使用for循环输出1 - 100个数看一下for的帮助文档，注意从冒号之后是开始的： for NAME [in WORDS ... ] ; do COMMANDS; done for x in a b c d ; do echo $x ; done ; 通过for循环打印一个三角形：首先看一下 然后这段是命令： while语法for: for NAME [in WORDS ... ] ; do COMMANDS; done for ((: for (( exp1; exp2; exp3 )); do COMMANDS; done 一个例子： #!/bin/bash ((i=0)) while ((i&lt;100)) ; do echo $i; i=$((i+1)) done]]></content>
      <tags>
        <tag>linux</tag>
        <tag>循环命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux基础之hostname]]></title>
    <url>%2F2018%2F09%2F19%2FLinux%E5%9F%BA%E7%A1%80%E4%B9%8Bhostname%2F</url>
    <content type="text"><![CDATA[命令嵌套使用$&gt;echo `cat b.txt` //命令解析,无法嵌套 $&gt;$(... $()) //支持命令的嵌套 创建用户0.用户和组之间，一个用户可以属于多个组。 但是有一个首要组。 1.adduser,等同于useradd 符号链接。 /usr/sbin/adduser --&gt; /usr/sbin/useradd. 2.useradd 输入新密码. 重复输入 $&gt;su root $&gt;useradd -m centos2 $&gt;su root $&gt;passwd centos2 -m, --create-home create the user&apos;s home directory -p, --password PASSWORD encrypted password of the new account 3.使用方法 $&gt;su root $&gt;userdel -r centos2 //用户所在组目录也会被删除.在删除用户时候要用exit退出要删除的用户,删除的时候可能会exit好多次，因为会来回su。]]></content>
      <tags>
        <tag>Linux基础</tag>
        <tag>基础命令</tag>
        <tag>hostname</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux基础之进程查看]]></title>
    <url>%2F2018%2F09%2F18%2FLinux%E5%9F%BA%E7%A1%80%E4%B9%8B%E8%BF%9B%E7%A8%8B%E6%9F%A5%E7%9C%8B%2F</url>
    <content type="text"><![CDATA[job 放到后台运行的进程.1.将程序放到后台运行,以&amp;结尾. $&gt;nano b.txt &amp; 2.查看后台运行的jobs数 $&gt;jobs 3.切换后台作业到前台来. $&gt;fg %n //n是job编号. 4.前台正在的进程，放到后台。 ctrl + z 5.让后作业运行 $&gt;bg %1 // 6.杀死作业 $&gt;kill %1 // man + 命令 ：查看该命令详细帮助 进程查看,prcess show $&gt;ps -Af |grep gnome //-A:所有进程 -f:所有列格式. $&gt;top //动态显示进程信息。含有cpu、内存的使用情况. //q,按照q退出。 cut剪切显示文件的每一行。 $&gt;cut -c 1-5 a.txt //从第一个字符开始,下标从1开始。 $&gt;ps -Af | cut -c 45-80 | more //吧PS里面获得的内容剪切显示，显示没一行的45-80，翻页查看 查看帮助$&gt;help //查看os内置的命令 $&gt;man ifcon fig //查看特定命令 $&gt;ifconfig --help $&gt;ifconfig -h $&gt;info ifconfig // 磁盘分区使用$&gt;fdisk -l /dev/sda 里面的中括号是可选的，尖括号是必须要写的 里面sad是磁盘，sda1是分区。sd1，sd2，sd3是磁盘的三个分区。 查看磁盘使用情况(disk free)$&gt;df -ah /home/centos //查看 dirname取出指定地址的上级目录. $&gt;dirname /a/b/c/d $&gt;/a/b/c basename取出当前地址的上级目录. $&gt;dirname /a/b/c/d $&gt;d 主机名$&gt;hostname //显式主机名 $&gt;修改主机名(sudo) [/etc/hostname] s200 关机重启命令$&gt;reboot //重启 $&gt;halt //停止,黑屏 //halt -p === poweroff //halt -r === reboot $&gt;poweroff //关机 $&gt;shutdown //shutdown now, 命令嵌套1.使用 $&gt;echo `cat b.txt` //命令解析,无法嵌套 $&gt;$(... $()) //支持命令的嵌套 2.]]></content>
      <tags>
        <tag>Linux基础</tag>
        <tag>进程查看</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux基础之符号连接]]></title>
    <url>%2F2018%2F09%2F18%2FLinux%E5%9F%BA%E7%A1%80%E4%B9%8B%E7%AC%A6%E5%8F%B7%E8%BF%9E%E6%8E%A5%2F</url>
    <content type="text"><![CDATA[权限 r //100 = 4 //文件 :读取内容， //文件夹:是查看文件夹的内容 w //文件 :写数据到文件 //文件夹:增删文件. //10 = 2 x //文件 : 运行程序 //文件夹: 进入该目录. // 1 = 1 权限控制涉及范围 U:user ,rwx r-x --- G:group , O:other , 修改文件的owner,change owner chown -R root:root a.txt //递归修改owner chmod -R 777 xxx //递归修改权限. -R :递归显示 -l :列表显示 通过递归改变整个文件夹里面的文件的权限和所有者和所在组 chown -R root:root tmp Linux文件夹 / //根目录 /bin //祖先 /sbin //祖先 /usr/bin //厂商 /usr/sbin //厂商 /usr/local/bin //用户 /usr/local/sbin //用户 /etc //配置目录 /mnt //挂载目录 /boot //引导目录 /dev //设备目录 /lib[64] //库目录 -:文件 d:目录 l:link 等价于windows快捷方式 b:block,块设备 c:charactor,字符文件 创建连接文件 1.硬链接两个完全相同文件，类似于实时备份。两个文件之间完全同步。删除时，只删一个。 目录不能使用硬链接。 ln a.txt alink //a.txt:目标文件, alink:连接名称. ln b.txt b_lnk //硬链接，修改连接文件，源文件也改变，但是删除连接文件源文件不被删除。 硬链接用的很少，大多使用符号连接 mv b_link b.txt 同一目录下改名字就用移动命令即可 2.符号连接-软连接相当于快捷方式. 可以对文件，也可以对文件夹创建符号连接。 符号连接存在的时候，可以删除目标文件。 $&gt;ln -s a.txt alink //a.txt: 目标文件 alink:连接名称(symbolic) blk存放的是路径的字节数大小。开始的时候只想本目录下的b.txt所以是5个字节，后来只想一个绝对路径就变成了28个字符。例子如下图所示 删除掉了链接指向的文件，就会变成红色。删除后在加上就回复正常变成了浅蓝色。 sudo 临时借用root的权限执行命令,只在当前命令下有效。命令结束后，还是原来用户。 1.配置当前用户具有sudo的执行权利 [/etc/sudoers] ... root ALL=(ALL) ALL centos ALL=(ALL) ALL ... $&gt;sudo chown -R centos:centos . 临时切换超级管理员权限 sudo 切换到另外一个用户 su]]></content>
      <tags>
        <tag>linux基础</tag>
        <tag>符号连接</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux基础之虚拟机增强工具的安装-文本模式的安装]]></title>
    <url>%2F2018%2F09%2F18%2FLinux%E5%9F%BA%E7%A1%80%E4%B9%8B%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%A2%9E%E5%BC%BA%E5%B7%A5%E5%85%B7%E7%9A%84%E5%AE%89%E8%A3%85-%E6%96%87%E6%9C%AC%E6%A8%A1%E5%BC%8F%E7%9A%84%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[虚拟机增强工具 1.原理 插入iso(C:\myprograms\vmwar11.0.4-x86_64\linux.iso)文件到光盘中。 2.vmware虚拟机菜单 -&gt; 重新安装vmware-tools 3.自动会将C:\myprograms\vmwar11.0.4-x86_64\linux.iso镜像文件插入光驱中，并直接打开。 4.复制VMwareTools-9.9.3-2759765.tar.gz文件到centos的桌面下。 5.tar开该文件. 鼠标右键点击桌面的tar.gz文件，选择 extract here. 6.进入桌面的vmware-tools-distrib目录下. $&gt;su root $&gt;cd /home/centos/Desktop/vmware-tools-distrib 7.执行安装脚本 $&gt;./vmware-install.pl 一路回车。 只到遇到Enjoy!!... 图片为在xshell里面执行。在Mini中执行虚拟机增强的方法： 在mini版的centos7下遇到了很多问题。又在大坑系列中也有这一部分。其实这部分遇到了很多问题，除了视频中的问题，按照如下所示可以完美解决： 为了实现文件夹的共享，要安装vmtools。现在的VMware是11版本。Centos是1151版本。 然后按照视频上讲解的来安装不上。最后先是 这样子，然后 之后按照如图所示挂载即可 本视频还有一个while循环实现99乘法表的例子（我没细看）：]]></content>
      <tags>
        <tag>-Linux基础 -虚拟机增强工具 -文本模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux基础基础—————踩过的大坑]]></title>
    <url>%2F2018%2F09%2F18%2F%E8%B8%A9%E8%BF%87%E7%9A%84%E5%A4%A7%E5%9D%91%2F</url>
    <content type="text"><![CDATA[问题：图形界面浏览器能访问外网，但是ping不通很奇怪的问题，配置都配置好好的跟着视频一步一步走下来的，按理说应该没问题。但是。。。。 解决：将网络中心里面改成这样子就行了。 具体的网站解释：https://blog.csdn.net/Arnold_lee_yc/article/details/74785995 不通CentOS版本的目录都不太一样。centos7和7.3的目录结构都不同。所以本教程使用的是7.1也就是1503版本的centos 切换用户之后出现这种情况： 解决办法：是在设置centos用户的权限的时候配置错了。修改回来即可 su root vi sudoers 视频里面的人他的ALL写成了小写的l。配置错误 安装VMtools工具为了实现文件夹的共享，要安装vmtools。现在的VMware是11版本。Centos是1151版本。我的理解是需要将那个文件夹挂载到虚拟机上。才能实现共享，可能和VMware的版本有关。网上说vmware14可以直接挂载不需要设置，这里我使用的是vmware11 然后按照视频上讲解的来安装不上。最后先是 这样子，然后 之后按照如图所示挂载即可 克隆机器后网卡设置克隆机器后会出现设置ifg-eno123142这个文件但是设置之后Ip还是没有修改的问题。实际上是因为克隆机器后ifg-eno123123的网卡名字变成了ifcfg-ens33这个这个名字，所以需要把ifg-eno12312这个网卡改名ifg-ens33然后把里面的name和device两个都改成ens33即可。 看这张图下面的网卡名字是ens33，在网卡配置中就要在name和device中写ens33 配置无密登陆在按照视频生成秘钥和公钥的时候，开始的时候删掉的其他机器的.ssh文件。所以导致传输公钥的时候，传输完了之后仍然需要密码。这是由于其他机器的.ssh文件的权限有问题，因为是删除了之后自己重新建立的。所以默认的权限是不对的。这里的网站讲解的非常详细 将ssh文件夹配置成700的属性即可。 https://blog.csdn.net/qq_26442553/article/details/79357498 多次format namenode会导致namenode的id和datanode的id有变化，需要重新格式化/tmp文件夹 在windows上跑mr之前视频上没有去说明在windows上配置hadoop，自己去CSDN里面查找才发现下面是CSDN里面原话： 1.hadoop官网上下载hadoop2.7.2.tar.gz 并且配置成环境变量 开始之前必须配置本地的hadoop环境 HADOOP_HOME=H:\source\hadoop\hadoop-2.7.2 PATH中增加 %HADOOP_HOME%\bin 配置完成后，通过cmd 执行hadoop 如果能够成功证明环境配置完成。 2.下载windows-hadoop-bin的压缩包(windows下运行MR 必备的) bin2.7.2 包我会提供出来 bin2.7.2 这个是windows-hadoop-bin 的压缩包，解压完了后用解压的bin包替换成hadoop-2.7.2 里面的bin。 注意是替换 3.将解压出来bin目录中的hadoop.dll也放入C:\Windows\System32(最好操作) 4.1901 这个是天气的测试数据包(后续解压在hadoop MR 的输入文件夹中) 5.windows运行时中可能出现的错误 No valid local directories in property: mapreduce.cluster.local.dir 如果出现这个错误，可以在代码中通过这个配置和在本地的hadoop目录下中建立data ======例如我的 H:\source\hadoop\hadoop-2.7.2\data==== Job job = new Job(); Configuration conf = job.getConfiguration(); conf.set(&quot;mapreduce.cluster.local.dir&quot;,&quot;H:\\source\\hadoop\\hadoop-2.7.2\\data&quot;); job=new Job(conf); 6.Winodws 运行出现 解决org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z 点击打开链接 大致流程，去官网下载src源码，将源码中nativeIo.java复制在本地项目中（新建源码中nativeIo.java的全路径,将其放入即可），修改源码中对windows的IO验证 Access方法 7.如果在集群运行时出现 No valid local directories in property: mapreduce.cluster.local.dir 查看是否采用的本地的localrunnal，也就是说mapred-site.xml和yarn-site.xml是否被加载.一般是这个原因，因为2.x版本上的defalut-site.xml已经配置该属性 8.集群提交时采用hadoop和yarn jar xxx.jar XXX agrs1 agr2 提交。加载的logj配置文件也可以作为参数传入。 9.提交集群时，如果在mapper或者reduce存在着构建一些引用包的对象，那么提交的xxx.jar必须是被打成依赖包，因为MR执行是将jar包分发其他的节点。所以不能像普通的java进程采用 java -cp x.jar 等方式。 10.IDEA 打依赖包是，不能通过Artifrt右边的put into 左边的项目。必须在选择时，采用jar-with-dependents的方式 两种的区别在于，第一种仅仅是将jar放入压缩的jar文件中。第二种是，采用依赖包的方式，才能识别具体的引用类。 11.本地还需要配置log4j的配置文件，查看具体的日志12. hadoop-2.7.2的tag.gz 包 13.windows-hadoop-bin 的包 具体的hadoop2.7.2tat.gz和windows下编译的bin包已经放到小号的百度网盘里面了。并且在chrome大数据技术2书签里面也有。 IDEA在IDEA里面有的时候会出现倒错包的情况，那么怎么自己导入又有提示呢? 在new Text()里面按alt=enter 里面有自动导包的提示。 关于Integer,parseInt例题： 设有下面两个赋值语句：a = Integer.parseInt(“123”);b = Integer.valueOf(“123”).intValue();下述说法正确的是（ d ）。A、a是整数类型变量，b是整数类对象。B、a是整数类对象，b是整数类型变量。C、a和b都是整数类对象并且值相等。D、a和b都是整数类型变量并且值相等。 详细解析：parseInt(String s )方法是类Integer的静态方法，它的作用就是将形参 s 转化为整数，比如：Interger.parseInt(“1”)=1;Integer.parseInt(“20”)=20;Integer.parseInt(“324”)=324;当然，s 表示的整数必须合法，不然是会抛异常的。valueOf(String s )也是Integer类的静态方法，它的作用是将形参 s 转化为Integer对象，什么是Integer对象，Integer就是基本数据类型int型包装类，就是将int包装成一个类，这样在很多场合下是必须的。如果理解不了，你就认为int是Integer的mini版，好用了很多，但也丢失了一些功能，好了，看代码：Interger.valueOf(“123”)=Integer(123)这时候Integer（123）就是整数123的对象表示形式，它再调用intValue()方法，就是将123的对象表示形式转化为基本数据123所以，选择D 在安装hive的时候出现了一个权限拒绝的错误，一切都是按照视频上来的为什么会出错：因为视频里面的有一些日志文件夹的创建，所在的按个父目录所属权是root的不让你创建，所以说要把他的所有者改掉。改成centos即可。 Hive创建表格报【Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException】引发的血案在成功启动Hive之后感慨这次终于没有出现Bug了，满怀信心地打了长长的创建表格的命令，结果现实再一次给了我一棒，报了以下的错误Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException，看了一下错误之后，先是楞了一下，接着我就发出感慨，自从踏上编程这条不归路之后，就没有一天不是在找Bug的路上就是在处理Bug，给自己贴了个标签：找Bug就跟吃饭一样的男人。抒发心中的感慨之后，该干活还是的干活。 第一步：查看配置文件 确保没有出错 第二步：上网找找同是天涯落人，看看有哪位哥们也跟我一样出现了这个问题，找来找去，就找到一个更换mysql-connector-java jar包的解决方案，怎么看都不像能解决问题，抱着死马当活马医的心态，还是试了一下，果然不出意料，没有解决问题，PASS! 第三步：运行 hive -hiveconf hive.root.logger=DEBUG,console打印hive启动过程的log。不打不知道，一打吓一跳，看到下面血淋淋的报错信息，心凉了半截，血案啊！！！由于太长我就截了一小部分出来 [main]: ERROR DataNucleus.Datastore: Error thrown executing CREATE TABLE `SERDE_PARAMS` ( `SERDE_ID` BIGINT NOT NULL, `PARAM_KEY` VARCHAR(256) BINARY NOT NULL, `PARAM_VALUE` VARCHAR(4000) BINARY NULL, CONSTRAINT `SERDE_PARAMS_PK` PRIMARY KEY (`SERDE_ID`,`PARAM_KEY`) ) ENGINE=INNODB : Specified key was too long; max key length is 767 bytes com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Specified key was too long; max key length is 767 bytes at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ... at org.apache.hadoop.util.RunJar.main(RunJar.java:212) main]: ERROR Datastore.Schema: An exception was thrown while adding/validating class(es) : Specified key was too long; max key length is 767 bytes com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Specified key was too long; max key length is 767 bytes at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ... at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.hadoop.util.RunJar.main(RunJar.java:212) ———————————————————————— [main]: DEBUG metastore.HiveMetaStore: admin role already exists InvalidObjectException(message:Role admin already exists.) at org.apache.hadoop.hive.metastore.ObjectStore.addRole(ObjectStore.java:3187) ... at java.lang.reflect.Method.invoke(Method.java:606) [main]: DEBUG metastore.HiveMetaStore: Failed while granting global privs to admin InvalidObjectException(message:All is already granted by admin) at org.apache.hadoop.hive.metastore.ObjectStore.grantPrivileges(ObjectStore.java:3912) ... at org.apache.hadoop.util.RunJar.main(RunJar.java:212) 第四步：继续搜索解决方法，然后找到了可能是字符集的原因，可以通过在mysql中将数据库的字符集改为latin1，执行以下命令： alter database hive character set latin1; 第五步：重启hive，继续看log，除了第一个exception消失了之外，其他依然存在，有些疑惑，认真看了一下，发现metastore.HiveMetaStore: admin role already exists，会不会是之前hive启动出错时留下的问题，那么把mysql中的hive数据删掉重新创建一个看看。 第六步：在mysql中执行以下命令：drop database hive;create database hive;alter database hive character set latin1; 第七步：重启hive，查看log，问题解决了！！！ 在编译源码包的问题自己粗浅的理解编译的问题：举个例子有一个用Protoc编译的包，叫example.protoc，然后在windows上安装protoc，然后在cmd里面用命令将它编译成一个Java类文件。编译命令如下图 在hbase安装的过程中，由于开始的时候设置了zookeeper的HA自动容灾，所以最开始的时候s206自动设置为active模式，要把s206设置为standby模式才能在s201里面设置s201为HMaster。 也就是说要杀死掉s206里面的namenode或者说通过转换吧他变成standby然后s201设置为active模式。不然会导致出错 在最开始打开集群的时候顺序很重要。先打开zookeeper，然后再打开hadoop集群，然后再打开hbase。才可以。 因为一旦没有打开zookeeper就先打开hadoop集群，在使用start-all.sh的时候就会导致resourcemanger。 完成之后是这个样子的: 在使用java api连接hbase的时候出现在IDEA里面卡死的情况，并没有什么其他的异常也不报错。原因是因为本地windows的hosts的名字不对，要修改和centos对应的hosts即可。 VMware一个50块钱买来的知识在使用VMware的时候，用360清理的时候360网络修复会自动将网络的IPV4的地址改为空： 如图将之修改为192.168.192.2和255.255.255.0即可。 坑爹的错误： 在上图的情下，看见下面的mybtis-spring是依赖于mybatis3.1.0，而上面有一个跟他一模一样的包，所以到下面这块，下面的这个版本的包就看不出来了。所以如果有的类仅仅是在下面这个版本下才有的，需要把上面的第一个包先删除掉。因为它只显示第一个依赖的包 关于360网络修复的问题，导致虚拟机无法Ping外网由于360网络修复的时候修复了一个是win的v8的网络设置，一个是他修改了vmware这个软件的网络配置。 解决办法：还原VMware的网络设置。 关于虚拟机源，和安装nc瑞士军刀的问题：首先说源：怎么安装阿里源关于阿里源：https://opsx.alibaba.com/mirror 阿里源安装办法： 然后那个淘宝店主有安装了一个自动帮助配置源的一个软件：他说如果nc不在这个里面的话就用这个命令：用yum install -y epel-releases，但是最后不是用这个软件解决的。 然后就是yum provides */nc用这个命令先找到有关于nc的东西，然后找到这个源里面的关于nc的安装包，先要找到这个安装包的名字，然后才好安装。 就是下面这样先找到。然后用yum install即可。]]></content>
      <tags>
        <tag>大坑</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux基础之网络配置-域名解析-光驱挂载]]></title>
    <url>%2F2018%2F09%2F17%2FLinux%E5%9F%BA%E7%A1%80%E4%B9%8B%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE-%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90-%E5%85%89%E9%A9%B1%E6%8C%82%E8%BD%BD%2F</url>
    <content type="text"><![CDATA[[客户端与宿主机之间的网络连通方式]1.桥接桥接(client完全等价于一台物理主机) 2.NAT(最多,默认模式)a.Net Address transform,网络地址转换. b.客户机能访问外网，可以访问局域网内的其他物理主机。 c.其他的局域网内物理主机不能访问客户机。 3.only host.a.和NAT非常像。 b.不能访问外网。 4.查看client机的网络连接模式a.右键选择Centos客户机。 b.点击&quot;设置&quot; c.网络适配器. 5.查看DHCP的分配网段a.vmware--&gt;编辑--&gt;虚拟网络编辑器 b.选中V8条目 c.下方显示的V8的详细信息。 d.点击DHCP的设置. e.查看分配网段. [修改静态IP]1.切换root用户$&gt;su root 2.编辑/etc/sysconfig/network-scripts/ifcfg-eno16777736a.备份文件 $&gt;cd /etc/sysconfig/network-scripts $&gt;cp ifcfg-eno16777736 ifcfg-eno16777736.bak b.进入/etc/sysconfig/network-scripts $&gt;cd /etc/sysconfig/network-scripts c.编辑ifcfg-eno16777736文件 $&gt;nano ifcfg-eno16777736 TYPE=Ethernet BOOTPROTO=none DEFROUTE=yes IPV4_FAILURE_FATAL=no IPV6INIT=no IPV6_AUTOCONF=no IPV6_DEFROUTE=no IPV6_PEERDNS=no IPV6_PEERROUTES=no IPV6_FAILURE_FATAL=no NAME=eno16777736 UUID=33f3ce5f-8b5c-41af-90ed-863736e09c29 DEVICE=eno16777736 ONBOOT=yes IPADDR=192.168.231.200 PREFIX=24 GATEWAY=192.168.231.2 DNS=192.168.231.2 注意:查看NAT网络的网关地址。 0)Client机的网卡的DNS和GATEWAY设置为虚拟网卡NAT的网关值。 1)vmware--&gt;编辑--&gt;虚拟网路编辑器 2)v8条目 3)点击&quot;NAT设置&quot;按钮 4)查看网关地址:192.168.231.2(通常为xxx.xxx.xxx.2) e.重启网络服务 $&gt;su root $&gt;service network restart f.解决通过ip能够访问网络，通过域名无法访问的问题。 1)编辑/etc/resolv.conf,添加名称服务器，内容是网关地址。 nameserver 192.168.231.2 2)保存退出 3)重启服务 $&gt;su root $&gt;service network restart 4)测试www.baidu.com $&gt;ping www.baidu.com service管理命令 1.查看服务的状态$&gt;service server_name status //语法 $&gt;service network status $&gt;service network start //启动 $&gt;service network stop //停止 $&gt;service network restart //重启 里面的lo网卡是自回环网络lopback（音似）还有一个就是局域网网卡 其中的ifcfg-lo就是自回环网络 打开里面的内容查看 mount挂载外设 1.右键client右下角的光盘图标 -&gt;设置 2.iso文件 选择一个iso镜像文件。 3.右键client右下角的光盘图标 -&gt;连接. 4.创建文件夹/mnt/cdrom $&gt;su root $&gt;mkdir cdrom 5.挂载光驱/dev/cdrom到/mnt/cdrom $&gt;mount /dev/cdrom /mnt/cdrom $&gt;find . /mnt/cdrom 卸载外设 1.从挂载的目录中出来,否则出现设备繁忙 $&gt;cd .. 2.使用umount进行卸载 $&gt;umount /mnt/cdrom 启用client和host之间共享目录的功能 1.右键点击vmware中的client机，选择设置 2.找到”选项” -&gt; “共享文件夹” 3.选择”总是启用” 4.在文件夹区域中添加要共享的目录 d:/downloads 5.确定. 6.重启客户机.]]></content>
      <tags>
        <tag>Linux基础</tag>
        <tag>网络配置</tag>
        <tag>域名解析</tag>
        <tag>光驱挂载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux基础之网络连接模式]]></title>
    <url>%2F2018%2F09%2F16%2FLinux%E5%9F%BA%E7%A1%80%E4%B9%8B%E7%BD%91%E7%BB%9C%E8%BF%9E%E6%8E%A5%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Linux基础之文件类型-权限]]></title>
    <url>%2F2018%2F09%2F16%2FLinux%E5%9F%BA%E7%A1%80%E4%B9%8B%E6%96%87%E4%BB%B6%E7%B1%BB%E5%9E%8B-%E6%9D%83%E9%99%90%2F</url>
    <content type="text"><![CDATA[目录和权限 [windows] 以磁盘分区物理结构作为文件系统结构 每个用户在/home下面都有一个自己的家目录比如centos的家目录在/home/centos但是root是管理员比较特殊root的目录在/root下~是home的子一级 ‘ [centos] 逻辑结构./ //文件系统的根. /bin //最初的命令(祖先)，binary文件,可执行文件 /sbin //super binary(重要性高) /usr/bin //厂商相关的命令,ubuntu /usr/sbin //厂商相关的命令,ubuntu /usr/local/bin //用户级别。 /usr/local/sbin [Linux文件类型] - //文件 d //目录 l //链接,类似于windows快捷方式. b //block,块文件。 c //字符文件 第一个字母是L的意思是link连接类的。 ifconfig的命令的目录在/sbin/ifconfig这里，但是实际上/sbin已经链接到了/usr/bin里面了。所以CentOS是没有bin和、usr/bin之分的，因为已经link过去了 [linux的权限]一共有9个位，每个成分是从0-7如果是777就是全部权限都付给他了 看这段截图。里面的-rw-rw-r–第一个-是文件类型第rw后面的-是代表0 $&gt;chmod //修改文件(夹)权限 $&gt;chmod g-w //去除group中write权. chmod //不受文件权限控制,只有owner和root才具有文件权限的修改权。 【read权限】 文件 :文件内容 文件夹 :文件夹的内容 【write权限】 ------------ 【execute权限】 ------------- 文件 :执行 文件夹 :进入目录 看下面这个例子： 看最下面的这个chmode 644 a.txt 6是用户的成分。4是组的成分，最后4，是others的成分。所以chmode 644最后是 -rw-r–r– 这里面的小细节格式 ： chmode 655 a.txt chmode g+w a.txt 文本输入格式他的偏移量是Longwritable,他的值是value是一行，在定义采样器的时候，是intwritable。要是做采样要对产生年份和温度采样。如果是文本输入格式就是文本偏移量和value。所以要把采样器采取的样本换成kv的值，而不是文本的偏移量和文本，所以对数据改造，改成序列文件，序列文件就可以读成kv了。要把输入文件改成序列文件，才可以采到他的值。 package com.it18zhang.hdfs.maxtemp; import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;import org.apache.hadoop.mapreduce.lib.partition.InputSampler;import org.apache.hadoop.mapreduce.lib.partition.TotalOrderPartitioner; public class MaxTempApp { public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS&quot;, &quot;file:///&quot;); Job job = Job.getInstance(conf); // 设置作业的各种属性 job.setJobName(“MaxTempApp”); //作业名称 job.setJarByClass(MaxTempApp.class); //搜索类路径 job.setInputFormatClass(SequenceFileInputFormat.class);//设置输入格式类 //添加输入路径 FileInputFormat.addInputPath(job, new Path(args[0])); //设置输出路径 org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(args[1])); job.setMapperClass(MaxTempMapper.class); //mapper类 job.setReducerClass(MaxTempReducer.class); //reduce类 job.setMapOutputKeyClass(IntWritable.class); job.setMapOutputValueClass(IntWritable.class); job.setOutputKeyClass(IntWritable.class); //设置输出类型 job.setOutputValueClass(IntWritable.class); //设置 全排序分区类 job.setPartitionerClass(TotalOrderPartitioner.class); //将sample数据 写入分区文件 TotalOrderPartitioner.setPartitionFile(conf, new Path(&quot;e:/mr/par.lst&quot;)); //创建随机采样器对象 InputSampler.Sampler&lt;IntWritable, IntWritable&gt; sampler = new InputSampler.RandomSampler&lt;IntWritable, IntWritable&gt;(0.1, 10000, 10); job.setNumReduceTasks(3); //reduce个数 InputSampler.writePartitionFile(job, sampler); job.waitForCompletion(true); } } public class MaxTempMapper extends Mapper&lt;IntWritable, IntWritable, IntWritable, IntWritable&gt; { public MaxTempMapper() { System.out.println(&quot;new MaxTempMapper&quot;); } @Override protected void map(IntWritable key, IntWritable value, Context context) throws IOException, InterruptedException { context.write(key,value); } } public class MaxTempReducer extends Reducer&lt;IntWritable, IntWritable, IntWritable, IntWritable&gt;{ /** * reduce */ protected void reduce(IntWritable key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException { int max =Integer.MIN_VALUE; for(IntWritable iw : values){ max = max &gt; iw.get() ? max : iw.get() ; } context.write(key,new IntWritable(max)); } } public class YearPartitioner extends Partitioner&lt;IntWritable, IntWritable&gt; { public int getPartition(IntWritable year, IntWritable temp, int parts) { int y = year.get() - 1970; if (y &lt; 33) { return 0; } if (y &gt; 33 &amp;&amp; y &lt; 66) { return 1; } else { return 2; } } } 上图在运行的时候要注意的问题： 1.Sequencen的输入是intwriteble和intwritable输入的模式 2.在MaxTempApp里面的创建随机采样器和设置reduce个数，全排序分区类，sampler写入分区文件是有顺序的，看源码可以找到顺序 2.5这个地方有一个设置分区个数的问题，如果是设置到了采样个数之后，那么就不需要采样了，采样的目的就是为了把分区设置2条分界线，要预先知道分区有几个，然后才能设置好分界线。 3.在后面用到conf的时候，已经不再是之前的conf了，是有修改了的所以要用job.configuration。如下图要变成这样。这边的job.configuration是做了一个拷贝，并不是操纵conf的。在前面写着有Job job = Job.getInstance(conf);在这句话里面，又new了一个新的conf。所以这个conf并不是原来的new的那个conf。所以要用job.configuration()这样的语句，如果用conf就找不到原来文件的位置了。]]></content>
      <tags>
        <tag>Linux基础</tag>
        <tag>文件类型权限</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux基础]]></title>
    <url>%2F2018%2F09%2F16%2FLinux%E5%9F%BA%E7%A1%80%E4%B9%8BCentOS%E5%91%BD%E4%BB%A4%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[CentOS Windows $&gt;ls cmd&gt;dir // $&gt;ls --help //查看命令帮助 $&gt;man ls //查看命令帮助 $&gt;clear cmd&gt;cls //清屏 $&gt;cd /home cmd&gt;cd d:/ //切换目录 $&gt;cd . cmd&gt;cd . //进入当前目录 $&gt;cd .. cmd&gt;cd .. //进入上级目录 $&gt;cd /home/centos cmd&gt;cd d:/xx/x/x //进入绝对路径 $&gt;pwd //显式当前目录 $&gt;whoami //显式当前用户名 $&gt;su root //切换用户,输入密码,swith user $&gt;passwd //修改当前用户的密码 $&gt;ifconfig cmd&gt;ipconfig //查看ip地址 $&gt;ping localhost cmd&gt;ping localhost //查看网络连通情况 $&gt;ping www.baidu.com cmd&gt;ping www.baidu.com //查看网络连通情况 $&gt;启动桌面版的网卡 on. $&gt;su centos // $&gt;cd /home/centos // $&gt;cd ~ //回到主目录 $&gt;cd - //回到上次的目录 $&gt;ll //别名,ls -l --autocolor... $&gt;alias //查看所有的别名 $&gt;ls -a -l -h //查看当前目录-h:人性化 -l:列表 -a:显式.开头 $&gt;mkdir ~/Downloads/a //创建目录 $&gt;touch ~/Downloads/a/1.txt //创建文件 $&gt;echo helloworld &gt; 1.txt //重定向输出(覆盖) $&gt;echo helloworld &gt;&gt; 1.txt //重定向输出(追加模式) $&gt;cat 1.txt cmd&gt;type a.txt //查看文件 $&gt;cp 1.txt 2.txt //复制文件 $&gt;rm 1.txt //删除文件 $&gt;rm -rf / //强行递归删除 $&gt;mv a.txt tmp/ //强行递归删除 [centos client中切换模式] ctrl + alt + f6 //切换到文本模式 ctrl + alt //切换鼠标 ctrl + alt + f1 //切换桌面模式. ctrl + alt + f5 //切换到新的文本模式 [nano文本编辑器,命令行模式] $&gt;nano a.txt //打开nano编辑器，编辑a.txt文件 $&gt;.... $&gt;ctrl + o //保存文件,提示后直接回车 $&gt;ctrl + x //退出文件 $&gt;ctrl + k //cut 文本 $&gt;ctrl + u //cut 文本 $&gt;more a.txt //分屏显式 q:退出 h:帮助 $&gt;more -5 a.txt //显式前5行内容 $&gt;tail a.txt //最后10行内容 $&gt;find . | more // | 是管道符，前面的命令的输出作为后面命令输入。 $&gt;find ~ $&gt;ls -aR ~ //递归显式主目录所有的文件.(a表示包含.开头的文件) $&gt;head a.txt //显式前10行 $&gt;head -n 10 a.txt //显式前10行 $&gt;head -10 a.txt //显式前10行 $&gt;tail a.txt $&gt;tail -n 20 a.txt $&gt;tail -20 a.txt $&gt;tail --lines=20 a.txt $&gt;wc -c -l -w a.txt //统计文本信息, //显式统计信息-c:字节 -l:line -w:word $&gt;hostname //查看主机名称 $&gt;uname -r //查看系统内核 $&gt;uname -a //查看系统内核 $&gt;uname -p //查看系统内核 $&gt;uname -m //查看系统内核 $&gt;file xxx.xx //查看文件类型 $&gt;gzip a.txt //原地压缩 $&gt;gzip -d a.txt //原地压缩 $&gt;gzip -dr tmp //递归操纵文件夹下的文件 $&gt;gunzip a.txt.gz //等价于gzip -d a.txt $&gt;tar -cvf my.tar 1.txt tmp //创建归档文件 $&gt;tar -vxf my.tar //解档文件 把多个文件保存到一个文件，也可以从归档文件恢复到单个文件 -c create创建 -f 归档文件名 -vf 列出所有文件在。。里面 -xf 从。。里面抽取所有文件。 —r 追加 -cf 创建 例子 tar -cf my.tar a.txt ：将a.txt归档压缩到my.tar tar -xvf my.tar a.txt: 抽取my.tar里面的a.txt tar -cf my.tar a.txt tmp/ :将a.txt和tmp/都压缩到my.tar里面 xargs 在&apos;xargs&apos;加单引号把他识别为一个命令。 $&gt;find . | grep txt | cp `xargs` temp //xargs是多行变单行，使用空格替换回车换行符. //`` : 是强制命令解析。 反引号’’问题 $&gt;ping `cat a.txt` //命令嵌套 $&gt;which echo //查看命令的文件路径]]></content>
      <tags>
        <tag>linux</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[京东实战项目之hive实战（1）]]></title>
    <url>%2F2018%2F09%2F14%2F%E4%BA%AC%E4%B8%9C%E5%AE%9E%E6%88%98%E9%A1%B9%E7%9B%AE%E4%B9%8Bhive%E5%AE%9E%E6%88%98%EF%BC%881%EF%BC%89%2F</url>
    <content type="text"><![CDATA[先熟悉一下Hive的基本语句： 首先里面的库有这么几个表， desc formatted ods_order; 看到表有几个信息 可以看到表的字段的信息。 同时可以看见表的是否是分区表partition information Location:可以看见路径，数据是在哪里的。 table Type: 是否是内部表 可以通过Hive看到进入到dfs里面的路径 可以看见里面文件的大小 通过explain 来看他的hive是怎么走的，看他的执行计划。 观察一个表的分区信息： 查看hadoop正在运行的任务 yarn application -list show functions;//显示hive里所有函数desc function extended add_months//显示里面具体的函数的用法 有时候desc formatted ods_order;里面显示的路径可能会是假的路径。在hadoop搭建的时候配置出问题，就会出现假的路径。 通过命令 desc extended ods_order partition(dt=20151010); //通过这个命令可以找到库表的实际路径。其中的dt是通过 show partition ado_order;来找到的。 以前的0.1之前有些扫描数据的时候默认是不开启mapreduce的。在select的时候是不开启mapreduce的。如果是少量的数据可以直接扫描出来的。但是表的数据非常大，如果不主动开启reduce执行，那么需要手动执行。通过上图来操作。设置是否开启mapreduce来执行。]]></content>
      <tags>
        <tag>hive</tag>
        <tag>实战项目</tag>
        <tag>基本hive语句</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java基础学习笔记]]></title>
    <url>%2F2018%2F09%2F12%2FJava%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Java中的总结合集（1）]]></title>
    <url>%2F2018%2F09%2F12%2FJava%E4%B8%AD%E7%9A%84%E6%80%BB%E7%BB%93%E5%90%88%E9%9B%86%EF%BC%881%EF%BC%89%2F</url>
    <content type="text"><![CDATA[关于构造函数构造函数本身是没有返回值的。但是不能加void，一旦加了void就变成了一个函数。 public class sadf { public static void main(String[] args) { Cat cat = new Cat(); cat.cry(); } } abstract class Animal{ public String name; public abstract void cry(); public void Animal() { System.out.println(&quot;new Animal&quot;); } } class Cat extends Animal{ public void cry() { System.out.println(&quot;喵喵12345 &#125; &#125;这样的函数运行之后是 喵喵 因为Animal有void变成了一个函数。还有一个默认的空构造。 应该去掉void。 public class sadf { public static void main(String[] args) { Cat cat = new Cat(); cat.cry(); } } abstract class Animal{ public String name; public abstract void cry(); public Animal() { System.out.println(&quot;new Animal&quot;); } } class Cat extends Animal{ public void cry() { System.out.println(&quot;喵喵1234567 &#125; &#125;***结果是： new Animal 喵喵 关于抽象abstractclass ABSTACT { public static void main(String[] args) { JiaFeiCat j = new JiaFeiCat(); } } abstract class Animal { abstract void cry(); public Animal() { System.out.println(&quot;我是动物&quot;); } } abstract class Cat extends Animal { public Cat() { System.out.println(&quot;我是猫&quot;); } final void catchMouse() { System.out.println(&quot;猫能抓老鼠&quot;); } } final class JiaFeiCat extends Cat { public JiaFeiCat() { System.out.println(&quot;我是加菲猫&quot;); } @Override void cry() { System.out.println(&quot;加菲猫会哭&quot;); } } class BosiCat extends Cat { public BosiCat() { System.out.println(&quot;我叫波斯猫&quot;); } @Override void cry() { System.out.println(&quot;波斯猫会叫&quot;); } } 打印结果： 我是动物 我是猫 我是加菲猫 运行成功abstact抽象类方法可以在子类的子类中继承即可，不用一定在第一代子类中继承。]]></content>
      <tags>
        <tag>Java基础问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop权威指南学习笔记（1）——关于JS在Hadoop里面函数]]></title>
    <url>%2F2018%2F09%2F11%2FHadoop%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%881%EF%BC%89%2F</url>
    <content type="text"><![CDATA[先看一段代码 package lianxi; import java.io.IOException; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapred.MapReduceBase; import org.apache.hadoop.mapred.OutputCollector; import org.apache.hadoop.mapred.Reporter; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.lib.map.WrappedMapper.Context; public class MaxTemperatureMapper extends MapReduceBase implements org.apache.hadoop.mapred.Mapper&lt;LongWritable, Text, Text, IntWritable&gt; { private static final int MISSING = 9999; public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { String line = value.toString(); String year = line.substring(15, 19); int airTemperature; if (line.charAt(87) == &apos;+&apos;) {// parseInt doesn&apos;t like leading plus signs airTemperature = Integer.parseInt(line.substring(88, 92)); } else { airTemperature = Integer.parseInt(line.substring(87, 92)); } String quality = line.substring(92, 93); if (airTemperature != MISSING &amp;&amp; quality.matches(&quot;[01459]&quot;)) { context.write(new Text(year), new IntWritable(airTemperature)); } } @Override public void map(LongWritable arg0, Text arg1, OutputCollector&lt;Text, IntWritable&gt; arg2, Reporter arg3) throws IOException { // TODO Auto-generated method stub } } 这个里面的涉及到的JS的函数： toString:toString() 方法可把一个逻辑值转换为字符串，并返回结果。 substring:substring() 方法用于提取字符串中介于两个指定下标之间的字符。 语法stringObject.substring(start,stop)返回值一个新的字符串，该字符串值包含 stringObject 的一个子字符串，其内容是从 start 处到 stop-1 处的所有字符，其长度为 stop 减 start。 说明substring() 方法返回的子串包括 start 处的字符，但不包括 stop 处的字符。 如果参数 start 与 stop 相等，那么该方法返回的就是一个空串（即长度为 0 的字符串）。如果 start 比 stop 大，那么该方法在提取子串之前会先交换这两个参数。 chartAt:charAt() 方法可返回指定位置的字符。 请注意，JavaScript 并没有一种有别于字符串类型的字符数据类型，所以返回的字符是长度为 1 的字符串 语法stringObject.charAt(index) 注释：字符串中第一个字符的下标是 0。如果参数 index 不在 0 与 string.length 之间，该方法将返回一个空字符串。 关于Integer 方法摘要 这里使用的函数是：paresInt(String int):将字符串参数作为有符号的十进制整数进行分析。 语法： parseInt(string, radix) parseInt(&quot;10&quot;); //返回 10 parseInt(&quot;19&quot;,10); //返回 19 (10+9) parseInt(&quot;11&quot;,2); //返回 3 (2+1) parseInt(&quot;17&quot;,8); //返回 15 (8+7) parseInt(&quot;1f&quot;,16); //返回 31 (16+15) parseInt(&quot;010&quot;); //未定：返回 10 或 8 matches() 方法用于检测字符串是否匹配给定的正则表达式。 调用此方法的 str.matches(regex) 形式与以下表达式产生的结果完全相同： 在字符串匹配给定的正则表达式时，返回 true。 public class Test { public static void main(String args[]) { String Str = new String(&quot;www.runoob.com&quot;); System.out.print(&quot;返回值 :&quot; ); System.out.println(Str.matches(&quot;(.*)runoob(.*)&quot;)); System.out.print(&quot;返回值 :&quot; ); System.out.println(Str.matches(&quot;(.*)google(.*)&quot;)); System.out.print(&quot;返回值 :&quot; ); System.out.println(Str.matches(&quot;www(.*)&quot;)); } } 以上程序执行结果为： 返回值 :true 返回值 :false 返回值 :true 关于random()方法讲解java Random.nextInt()方法public int nextInt(int n) 该方法的作用是生成一个随机的int值，该值介于[0,n)的区间，也就是0到n之间的随机int值，包含0而不包含n package org.xiaowu.random.demo; import java.util.Random; import org.junit.Test; public class RandomDemo { @Test public void Demo(){ Random rnd = new Random(); int code = rnd.nextInt(8999) + 1000; System.out.println(&quot;code:&quot;+code); } @Test public void Demo1(){ Random r = new Random(); int nextInt = r.nextInt(); Random r1 = new Random(10); int nextInt2 = r1.nextInt(); System.out.println(&quot;nextInt:&quot;+nextInt); System.out.println(&quot;nextInt2:&quot;+nextInt2); } /** * 生成[0,1.0)区间的小数 * */ @Test public void Demo2(){ Random r = new Random(); double d1 = r.nextDouble(); System.out.println(&quot;d1:&quot;+d1); } /** * 生成[0,5.0)区间的小数 * */ @Test public void Demo3(){ Random r = new Random(); double d2 = r.nextDouble()* 5; System.out.println(&quot;d1:&quot;+d2); } /** * 生成[1,2.5)区间的小数 * */ @Test public void Demo4(){ Random r = new Random(); double d3 = r.nextDouble() * 1.5 + 1; System.out.println(&quot;d1:&quot;+d3); } /** * 生成任意整数 * */ @Test public void Demo5(){ Random r = new Random(); int n1 = r.nextInt(); System.out.println(&quot;d1:&quot;+n1); } /** * 生成[0,10)区间的整数 * */ @Test public void Demo6(){ Random r = new Random(); int n2 = r.nextInt(10); int n3 = Math.abs(r.nextInt() % 10); System.out.println(&quot;n2:&quot;+n2); System.out.println(&quot;n3:&quot;+n3); } /** * 生成[0,10]区间的整数 * */ @Test public void Demo7(){ Random r = new Random(); int n3 = r.nextInt(11); int n4 = Math.abs(r.nextInt() % 11); System.out.println(&quot;n3:&quot;+n3); System.out.println(&quot;n4:&quot;+n4); } /** * 生成[-3,15)区间的整数 * */ @Test public void Demo8(){ Random r = new Random(); int n4 = r.nextInt(18) - 3; int n5 = Math.abs(r.nextInt() % 18) - 3; System.out.println(&quot;n4:&quot;+n4); System.out.println(&quot;n5:&quot;+n5); } } 关于nextInt()函数的一点儿说明：如果想得到30到200的(包含30和200)这个跨度的数在java中一般可以有如下方式获得 （1）int i = (int)(Math.random()*171) + 30; （2）Random r = new Random () ; r.nextInt (201) ; // 这个是0 - 200 （3）Random r = new Random () ; r.nextInt (171) + 30 ; // 这个是30 到 200. //如下为二维数组的一点儿东西 public class 数组的使用说明代码 { public static void main(String args[]){ int[] array=creatArray(10); printArray(array); } public static int[] creatArray(int length){ //构造含length个元素的数组的方法 int[] array =new int[length]; Random rad=new Random(); //产生随机数的方法（系统自己的） for(int i=0;i&lt;array.length;i++){ int value = rad.nextInt(100) + 200; //rad.nextInt(100) 意思是随机产生一个大于等于0小于100的数 ------即包含0不包含100 array[i]=value; } return array; } public static void printArray(int[] array){ for(int i=0;i&lt;array.length;i++) System.out.println(array[i]+&apos;\t&apos;); } } JavaScript indexOf() 方法JavaScript String 对象 定义和用法]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop权威指南</tag>
        <tag>MapReduce学习</tag>
        <tag>JS函数学习</tag>
      </tags>
  </entry>
</search>
