<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Hadoop第九天]]></title>
    <url>%2F2018%2F10%2F25%2FHadoop%E7%AC%AC%E4%B9%9D%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[复习：1.链式job编程 MR //Mapper+ / Reduce Mapper* 2.DBWritable 和数据库交互。 3.Sqoop 4.全排序 对reduce输出的所有结果进行排序。 5.二次排序 对value进行排序。 6.数据倾斜 1.reduce 2.自定义分区函数 数据结果错 + 二次job 3.重新设计key 数据结果错 + 二次job 机架感知机架有一块网卡。可以连接很多主机，机架上每个刀片服务器主机都连接到这个机架网卡上的。 比如一台刀片服务器上有2个进程，2个进程之间通信需要走机架上的交换机吗。不需要直接走自回环网络即可。走Localhost即可。所以2个进程之间不需要网卡交互数据。2个进程之间的距离就是0。每个进程所在的主机节点，连接他们之间的网络通路的跃点数就是他们之间的距离 如果b中绿色进程和a中红色进程需要交互就需要通过交换机，距离就是2.怎么算的，就是从每个服务器到交换机之间的距离相加，就是1+1； 如果是2个机架也要通过一个交换机连接起来。左边机架a中绿色和右边机架红色通信。距离就是4. 同一个机房通信最多就是4。也就是在通过一个交换机。 Hadoop都有机架感知能力：是他里面的一种策略，都需人为维护。在默认情况下所有节点都在同一个机架上。/default-rack。但是在临近节点数据量非常大的时候，也是不会使用的，就近原则只是一种策略，还有其他策略。 fault tolerance容错. 针对业务。 map或reduce任务失败，的这种错误。 fail over容灾. 针对硬件故障。 master / slave主(master,namenode)从(slave,datanode)结构. topology.node.switch.mapping.impl 客户端请求Namenode来读取datanodes的过程Namenode只存放文件系统的数据信息，而不存放各个节点的IP地址。存放块ID，块列表。 可靠性提供数据安全的能力。 可用性提供持续服务的能力。 默认的副本放置策略首选在本地机架的一个node存放副本,另一个副本在本地机架的另一个不同节点。 最后一个副本在不同机架的不同节点上。 hads oiv //image data metadata. 离线镜像查看器hads oev //edit 编辑日志 镜像文件存放的是元信息，元信息包括块，块ID，块列表，是不是上下级关系，副本数，权限，块大小。除了数据内容本身内容之外的。 通过实现接口改变配置实现一个机架感知。 自定义机架感知(优化hadoop集群一种方式)1.自定义实现类 package com.it18zhang.hdfs.rackaware; import org.apache.hadoop.net.DNSToSwitchMapping; import java.io.FileWriter; import java.io.IOException; import java.util.ArrayList; import java.util.List; /*机架感知实现类 吧203以下的机器设置为机架1，吧203以上的机架设置为机架2 */ public class MyRackAware implements DNSToSwitchMapping { public List&lt;String&gt; resolve(List&lt;String&gt; names) { ArrayList&lt;String&gt; list = new ArrayList&lt;String&gt;(); //true表示是不是追加模式 FileWriter fw = null; try { fw = new FileWriter(&quot;/home/centos/rackaware.txt&quot;, true); for (String str : names) { fw.write(str + &quot;\r\n&quot;); if (str.startsWith(&quot;192&quot;)) { //192.168.192.202 String ip = str.substring(str.lastIndexOf(&quot;.&quot;) + 1); if (Integer.parseInt(ip) &lt;= 203) { list.add(&quot;/rack1/&quot; + ip); } else { list.add(&quot;/rack2&quot; + ip); } } else if (str.startsWith(&quot;s&quot;)) { String ip = str.substring(1); if (Integer.parseInt(ip) &lt;= 203) { list.add(&quot;/rack1/&quot; + ip); } else { list.add(&quot;/rack2&quot; + ip); } } } } catch (IOException e) { e.printStackTrace(); } return list; } public void reloadCachedMappings() { } public void reloadCachedMappings(List&lt;String&gt; names) { } } 2.配置core-site.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://192.168.231.201/&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/centos/hadoop&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;topology.node.switch.mapping.impl&lt;/name&gt; &lt;value&gt;com.it18zhang.hdfs.rackaware.MyRackAware&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 3.导出jar包 4.复制jar到/soft/hadoop/shared/hadoop/common/lib（这个是hadoop的类路径） 5.分发jar.(可以不做) 实际上不需要分发，只在名称节点上运行。 6.重启名称节点 $&gt;hadoop-daemon.sh stop namenode $&gt;hadoop-daemon.sh start namenode 在s202上传一个文件，最后得出来确实和副本存放策略一致： 首选在本地机架的一个node存放副本,另一个副本在本地机架的另一个不同节点。 最后一个副本在不同机架的不同节点上。 关于HDFS下面这一段是关于HDFS的讲解，HDFS包括namenode和datanode，名称节点和数据节点。名称节点只是保存名称信息。那么如果在HDFS中文件移动的话怎么办，就是名称节点改变目录来移动即可。1kb和1G的速度是一样的。在HDFS中移动只是改变路径而已，块根本就没变，只能重新删掉在put才是真正移动了块。这一节就是通过实验验证了这个问题。 去IOEIBM // Oracle // EMC // HA1.NFS 网络共享存储设备。 2.QJM Quorum Journal Manager 3.两个名称节点 active //激活 standby //待命 active //激活deactive //钝化 SPOFsingle point of failure,单点故障。 事务是个特性a //atomic 原子性 c //consistent一致性 i //isolate 隔离型 d //durable 永久性· majority 大部分. HA高可用配置high availability,高可用. /home/centos/hadoop/dfs/data/current/BP-2100834435-192.168.231.201-1489328949370/current/finalized/subdir0/subdir0 两个名称节点，一个active(激活态)，一个是standby(slave待命),slave节点维护足够多状态以便于容灾。 和客户端交互的active节点,standby不交互. 两个节点都和JN守护进程构成组的进行通信。 数据节点配置两个名称节点，分别报告各自的信息。 同一时刻只能有一个激活态名称节点。 脑裂:两个节点都是激活态。 为防止脑裂，JNs只允许同一时刻只有一个节点向其写数据。容灾发生时，成为active节点的namenode接管 向jn的写入工作。 硬件资源名称节点: 硬件配置相同。 JN节点 : 轻量级进程，至少3个节点,允许挂掉的节点数 (n - 1) / 2. 不需要再运行辅助名称节点。 部署配置细节0.s201和s206具有完全一致的配置，尤其是ssh. 1.配置nameservice [hdfs-site.xml] &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;mycluster&lt;/value&gt; &lt;/property&gt; 2.dfs.ha.namenodes.[nameservice ID] [hdfs-site.xml] &lt;!-- myucluster下的名称节点两个id --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; 3.dfs.namenode.rpc-address.[nameservice ID].[name node ID] [hdfs-site.xml] 配置每个nn的rpc地址。 &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;s201:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;s206:8020&lt;/value&gt; &lt;/property&gt; 4.dfs.namenode.http-address.[nameservice ID].[name node ID] 配置webui端口 [hdfs-site.xml] &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;s201:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;s206:50070&lt;/value&gt; &lt;/property&gt; 5.dfs.namenode.shared.edits.dir 名称节点共享编辑目录. [hdfs-site.xml] &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://s202:8485;s203:8485;s204:8485/mycluster&lt;/value&gt; &lt;/property&gt; 6.dfs.client.failover.proxy.provider.[nameservice ID] java类，client使用它判断哪个节点是激活态。 [hdfs-site.xml] &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; 7.dfs.ha.fencing.methods 脚本列表或者java类，在容灾保护激活态的nn. [hdfs-site.xml] &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt; sshfence shell(/bin/true) &lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/home/centos/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt; 8.fs.defaultFS 配置hdfs文件系统名称服务。 [core-site.xml] &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://mycluster&lt;/value&gt; &lt;/property&gt; 9.dfs.journalnode.edits.dir 配置JN存放edit的本地路径。 [hdfs-site.xml] &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/home/centos/hadoop/journal&lt;/value&gt; &lt;/property&gt; 部署细节1.在jn节点分别启动jn进程 $&gt;hadoop-daemon.sh start journalnode 2.启动jn之后，在两个NN之间进行disk元数据同步 a)如果是全新集群，先format文件系统,只需要在一个nn上执行。 [s201] $&gt;hadoop namenode -format 格式化文件系统主要就是生成一个新的VERSION，里面内容就是生成一个名称空间ID集群ID时间块池ID b)如果将非HA集群转换成HA集群，复制原NN的metadata到另一个nn. 1.步骤一 [s201] $&gt;scp -r /home/centos/hadoop/dfs centos@s206:/home/centos/hadoop/ 2.步骤二 在新的nn(未格式化的nn)上运行一下命令，实现待命状态引导。 [s206] $&gt;hdfs namenode -bootstrapStandby //需要s201为启动状态,提示是否格式化,选择N. 3)在一个NN上执行以下命令，完成edit日志到jn节点的传输。 $&gt;hdfs namenode -initializeSharedEdits #查看s202,s203是否有edit数据. 4)启动所有节点. [s201] $&gt;hadoop-daemon.sh start namenode //启动名称节点 $&gt;hadoop-daemons.sh start datanode //启动所有数据节点 [s206] $&gt;hadoop-daemon.sh start namenode //启动名称节点 HA管理$&gt;hdfs haadmin -transitionToActive nn1 //切成激活态 $&gt;hdfs haadmin -transitionToStandby nn1 //切成待命态 $&gt;hdfs haadmin -transitionToActive --forceactive nn2//强行激活 $&gt;hdfs haadmin -failover nn1 nn2 //模拟容灾演示,从nn1切换到nn2]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>机架感知</tag>
        <tag>机架感知实现</tag>
        <tag>HA</tag>
        <tag>手动移动数据块</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop第八天]]></title>
    <url>%2F2018%2F10%2F22%2FHadoop%E7%AC%AC%E5%85%AB%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[二次排序链条化分组不能解决数据倾斜问题，已经进入到了同一个reduce对象里面了，分一个组出来就进入同一个reduce方法里面。每次分组只是决定调用不调用同一个reduce方法。已经晚了，数据已经进入到了同一个reduce了，分区可以决定数据倾斜问题。 reduce分为方法和对象，有的时候说reduce是指的是方法，有的时候指的是对象，要分开对待。 单词统计做统计肯定不能哈希分区，因为哈希同一个值进入到同一个reduce对象利民区去了 数据倾斜问题 随机分区 二次MR1. 2. 3. 4. 如果正常按照wordcount来处理会分为 reduce不设置就是1，map没机会设置数量。但是也可以设置map，但是意义，以为mr提交的时候map的个数取决于切片的个数。 切片的计算公式：min block、maxsplit、 blocksize取中间值等于blocksize。 现在文件如下，现在3个文件每个文件有一百万数据且有一百万个hello就对应了3个map。（这里没明白为什么3个切片也就是为啥3个map） 每个切片里，分区个数等于reduce个数，分区的个数取决于reduce个数，之前自己已经在WCAPP里面设置好了reduce个数。 我有3个map，4个reduce。那么每个map里面就有4个分区了。 要解决哈希分区带来的数据倾斜问题，可以通过随机分区，然后在进行二次mr来解决这个问题 组合combineor是对分区里面的预先聚合，下面的hello就不会发送一百万次，而是提前聚合了发送了hello 一百万次。这个样子。能解决目前的倾斜问题，但是不可能这么多数据，可能量很大 下面这种情况下，所有的hello根据哈希分区就进入到同一个分区，然后一百万个tom分4个分区 通过随机分区可以解决数据倾斜问题，但是会产生新的问题，就是说hello被分到4个不同的reduce里面，导致reduce1产生hello10reduce2产生hello13这种问题，所以通过二次mr来解决这个问题，让hello这个单词统计，进入到同一个reduce结果当中。所以也就是通过随即分区和二次mr解决 [1.txt]1000000 hello tom1 hello tom2 hello tom3 hello tom4 hello tom5 hello tom6 hello tom7 hello tom8 hello tom9 hello tom10 [2.txt]1000000 hello tom11 hello tom12 hello tom13 hello tom14 hello tom15 hello tom16 hello tom17 hello tom18 hello tom19 hello tom20 [3.txt]1000000 hello tom21 hello tom22 hello tom23 hello tom24 hello tom25 hello tom26 hello tom27 hello tom28 hello tom29 hello tom30 代码如下： //自定义分区函数 public class RandomPartitioner extends Partitioner&lt;Text,IntWritable&gt; { @Override public int getPartition(Text text, IntWritable intWritable, int numPartitions) { return new Random().nextInt(numPartitions); } } //解决数据倾斜问题： public class WCSkueApp { public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;); Job job = Job.getInstance(conf); // 设置作业的各种属性 job.setJobName(&quot;MaxTempApp&quot;); //作业名称 job.setJarByClass(WCSkueApp.class); //搜索类路径 job.setInputFormatClass(TextInputFormat.class);//设置输入格式类 //添加输入路径 FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew&quot;)); //设置输出路径 org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(&quot;e:/mr/skew/out&quot;)); //设置随机分区： job.setPartitionerClass(RandomPartitioner.class); job.setMapperClass(WCSkueMapper.class); //mapper类 job.setReducerClass(WCSkueReducer.class); //reduce类 job.setNumReduceTasks(4); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); job.setOutputKeyClass(Text.class); //设置输出类型 job.setOutputValueClass(IntWritable.class); job.waitForCompletion(true); } } public class WCSkueMapper extends Mapper &lt;LongWritable, Text, Text, IntWritable&gt;{ public WCSkueMapper(){ System.out.println(&quot;new WCMapper&quot;); } @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { String[] arr = value.toString().split(&quot; &quot;); Text keyout= new Text(); IntWritable valueout = new IntWritable(); for(String s:arr){ keyout.set(s); valueout.set(1); context.write(keyout,valueout); } } } public class WCSkueReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{ /** * reduce */ protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException { int count = 0 ; for(IntWritable iw : values){ count = count + iw.get() ; } context.write(key,new IntWritable(count)); } } //解决数据倾斜问题： public class WCSkueApp2 { public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;); Job job = Job.getInstance(conf); // 设置作业的各种属性 job.setJobName(&quot;MaxTempApp&quot;); //作业名称 job.setJarByClass(WCSkueApp2.class); //搜索类路径 job.setInputFormatClass(TextInputFormat.class);//设置输入格式类 //添加输入路径 FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00000&quot;)); FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00001&quot;)); FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00002&quot;)); FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00003&quot;)); //设置输出路径 org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(&quot;e:/mr/skew/out2&quot;)); job.setMapperClass(WCSkueMapper2.class); //mapper类 job.setReducerClass(WCSkueReducer2.class); //reduce类 job.setNumReduceTasks(4); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); job.setOutputKeyClass(Text.class); //设置输出类型 job.setOutputValueClass(IntWritable.class); job.waitForCompletion(true); } } public class WCSkueMapper2 extends Mapper &lt;LongWritable, Text, Text, IntWritable&gt;{ public WCSkueMapper2(){ System.out.println(&quot;new WCMapper&quot;); } protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { String[] arr = value.toString().split(&quot;\t&quot;); Text keyout= new Text(); IntWritable valueout = new IntWritable(); context.write(new Text(arr[0]),new IntWritable(Integer.parseInt(arr[1]))); } } public class WCSkueReducer2 extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{ /** * reduce */ protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException { int count = 0 ; for(IntWritable iw : values){ count = count + iw.get() ; } context.write(key,new IntWritable(count)); } } 上面这个就是通过2次mr解决数据倾斜问题 但是不用那么麻烦的去切分了直接使用Keyvalueinputformat即可实现：如下代码： //解决数据倾斜问题： public class WCSkueApp2 { public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;); Job job = Job.getInstance(conf); // 设置作业的各种属性 job.setJobName(&quot;MaxTempApp&quot;); //作业名称 job.setJarByClass(WCSkueApp2.class); //搜索类路径 job.setInputFormatClass(KeyValueTextInputFormat.class);//设置输入格式类 //添加输入路径 FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00000&quot;)); FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00001&quot;)); FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00002&quot;)); FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00003&quot;)); //设置输出路径 org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(&quot;e:/mr/skew/out2&quot;)); job.setMapperClass(WCSkueMapper2.class); //mapper类 job.setReducerClass(WCSkueReducer2.class); //reduce类 job.setNumReduceTasks(4); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); job.setOutputKeyClass(Text.class); //设置输出类型 job.setOutputValueClass(IntWritable.class); job.waitForCompletion(true); } } public class WCSkueMapper2 extends Mapper &lt;Text, Text, Text, IntWritable&gt;{ public WCSkueMapper2(){ System.out.println(&quot;new WCMapper&quot;); } protected void map(Text key, Text value, Context context) throws IOException, InterruptedException { context.write(key,new IntWritable(Integer.parseInt(value.toString()))); } } public class WCSkueReducer2 extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{ /** * reduce */ protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException { int count = 0 ; for(IntWritable iw : values){ count = count + iw.get() ; } context.write(key,new IntWritable(count)); } } 链条式编程 讲解上图：首先通过MapMapper1来切割，然后通过MapMapper1过滤掉法轮功，真个是一个链条，在map端，执行完之后开始聚合，在reduce端，在聚合的时候，每个单词都有各自的个数，对产生的结果再次过滤，过滤掉少于5的单词，当然可以放到同一个map里面，但是如果变成模块方法，易于维护。更加方便使用。m阶段可以有多个m但是R阶段只能有一个r。但是r阶段可以有多个m。 代码如下图：//链条式job任务 public class WCChainApp { public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;); Job job = Job.getInstance(conf); // 设置作业的各种属性 job.setJobName(&quot;MaxTempApp&quot;); //作业名称 job.setJarByClass(WCChainApp.class); //搜索类路径 job.setInputFormatClass(TextInputFormat.class);//设置输入格式类 //添加输入路径 FileInputFormat.addInputPath(job, new Path(&quot;e:/mr/skew&quot;)); //设置输出路径 org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(&quot;e:mr/skew/out&quot;)); //在map链条上添加一个mapper的环节 ChainMapper.addMapper(job,WCMapMapper1.class,LongWritable.class,Text.class,Text.class,IntWritable.class,conf); ChainMapper.addMapper(job,WCMapMapper2.class,Text.class,IntWritable.class,Text.class,IntWritable.class,conf); //在reduce链条上设置reduce ChainReducer.setReducer(job,WCReducer.class,Text.class,IntWritable.class,Text.class,IntWritable.class,conf); ChainReducer.addMapper(job,WCReduceMapper1.class,Text.class,IntWritable.class,Text.class,IntWritable.class,conf); job.setNumReduceTasks(3); job.waitForCompletion(true); } } public class WCMapMapper1 extends Mapper &lt;LongWritable, Text, Text, IntWritable&gt;{ protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { Text keyout= new Text(); IntWritable valueout = new IntWritable(); String[] arr = value.toString().split(&quot; &quot;); for(String s:arr){ keyout.set(s); valueout.set(1); context.write(keyout,valueout); } } } public class WCMapMapper2 extends Mapper &lt;Text, IntWritable, Text, IntWritable&gt;{ protected void map(Text key, IntWritable value, Context context) throws IOException, InterruptedException { if(!key.toString().equals(&quot;falungong&quot;)){ context.write(key , value); } } } //过滤单词个数 public class WCReduceMapper1 extends Mapper&lt;Text,IntWritable,Text,IntWritable&gt; { @Override protected void map(Text key, IntWritable value, Context context) throws IOException, InterruptedException { if(value.get() &gt; 5){ context.write(key,value); } } } /** * Reducer */ public class WCReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{ /** * reduce */ protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException { int count = 0 ; for(IntWritable iw : values){ count = count + iw.get() ; } context.write(key,new IntWritable(count)); } } FileInputFormat（读源码）获取切片集合。 子类都要重写方法isSplittable(); 负责创建RecordReader对象。 设置IO路径。 RecordReader（读源码）负责从InputSplit中读取KV对。 jdbc笔记模板：[写操作] Class.forName(&quot;com.mysql.jdbc.Driver&quot;); Connection conn = DriverMananger.getConnection(&quot;jdbc:mysql://localhost:3306/big4&quot;,&quot;root&quot;,&quot;root&quot;); //预处理语句 PreparedStatement ppst = conn.preparedStatement(&quot;insert into test(id,name,age) values(?,?,?)&quot;); //绑定参数 ppst.setInteger(1,1); ppst.setInteger(2,&quot;tom&quot;); ppst.setInteger(3,12); ppst.executeUpdate(); ppst.close(); conn.close(); [读操作] Class.forName(&quot;com.mysql.jdbc.Driver&quot;); Connection conn = DriverMananger.getConnection(&quot;jdbc:mysql://localhost:3306/big4&quot;,&quot;root&quot;,&quot;root&quot;); ppst = conn.preparedStatement(&quot;select id,name from test &quot;); //结果集 ResultSet rs = ppst.executeQuery(); while(rs.next()){ int id = rs.getInt(&quot;id&quot;); String name = rs.getInt(&quot;name&quot;); } rs.close(); conn.close(); 看一下下源码：看一下写和读入都把结果集和预处理语句都已经封装进去了，剩下的只要填空就好了。 以下是从数据库中读入的代码模板： 使用DBWritable向数据库从数据库中读取1.准备数据库create database big4 ; use big4 ; create table words(id int primary key auto_increment , name varchar(20) , txt varchar(255)); insert into words(name,txt) values(&apos;tomas&apos;,&apos;hello world tom&apos;); insert into words(txt) values(&apos;hello tom world&apos;); insert into words(txt) values(&apos;world hello tom&apos;); insert into words(txt) values(&apos;world tom hello&apos;); 2.编写hadoop MyDBWritable.import org.apache.hadoop.mapreduce.lib.db.DBWritable; import java.io.DataInput; import java.io.DataOutput; import java.io.IOException; import java.sql.PreparedStatement; import java.sql.ResultSet; import java.sql.SQLException; /** * MyDBWritable */ public class MyDBWritable implements DBWritable,Writable { private int id ; private String name ; private String txt ; public int getId() { return id; } public void setId(int id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public String getTxt() { return txt; } public void setTxt(String txt) { this.txt = txt; } public void write(DataOutput out) throws IOException { out.writeInt(id); out.writeUTF(name); out.writeUTF(txt); } public void readFields(DataInput in) throws IOException { id = in.readInt(); name = in.readUTF(); txt = in.readUTF(); } /** * 写入db */ public void write(PreparedStatement ppst) throws SQLException { ppst.setInt(1,id); ppst.setString(2,name); ppst.setString(3,txt); } /** * 从db读取 */ public void readFields(ResultSet rs) throws SQLException { id = rs.getInt(1); name = rs.getString(2); txt = rs.getString(3); } } 3.WcMapperpublic class WCMapper extends Mapper&lt;LongWritable,MyDBWritable,Text,IntWritable&gt; { protected void map(LongWritable key, MyDBWritable value, Context context) throws IOException, InterruptedException { System.out.println(key); String line = value.getTxt(); System.out.println(value.getId() + &quot;,&quot; + value.getName()); String[] arr = line.split(&quot; &quot;); for(String s : arr){ context.write(new Text(s),new IntWritable(1)); } } } 4.WCReducerprotected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException { int count = 0 ; for(IntWritable w : values){ count = count + w.get() ; } context.write(key,new IntWritable(count)); } 5.WCApppublic static void main(String[] args) throws Exception { Configuration conf = new Configuration(); Job job = Job.getInstance(conf); //设置job的各种属性 job.setJobName(&quot;MySQLApp&quot;); //作业名称 job.setJarByClass(WCApp.class); //搜索类 //配置数据库信息 String driverclass = &quot;com.mysql.jdbc.Driver&quot; ; String url = &quot;jdbc:mysql://localhost:3306/big4&quot; ; String username= &quot;root&quot; ; String password = &quot;root&quot; ; //设置数据库配置 DBConfiguration.configureDB(job.getConfiguration(),driverclass,url,username,password); //设置数据输入内容 DBInputFormat.setInput(job,MyDBWritable.class,&quot;select id,name,txt from words&quot;,&quot;select count(*) from words&quot;); //设置输出路径 FileOutputFormat.setOutputPath(job,new Path(&quot;e:/mr/sql/out&quot;)); //设置分区类 job.setMapperClass(WCMapper.class); //mapper类 job.setReducerClass(WCReducer.class); //reducer类 job.setNumReduceTasks(3); //reduce个数 job.setMapOutputKeyClass(Text.class); // job.setMapOutputValueClass(IntWritable.class); // job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); // job.waitForCompletion(true); } 6.pom.xml增加mysql驱动&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.17&lt;/version&gt; &lt;/dependency&gt; 7.将mr的统计结果写入mysql数据库a)准备表 create table stats(word varchar(50),c int); b)设置App的DBOutputFormat类 com.it18zhang.hdfs.mr.mysql.WCApp d) e) f) mysql分页查询如图所示吧： 使用DBWritable向数据库从数据库中写入public class MyDBWritalbe implements DBWritable,Writable { private int id=0; private String name=&quot;&quot;; private String txt=&quot;&quot;; private String word=&quot;&quot;; private int wordcount=0; public String getWord() { return word; } public void setWord(String word) { this.word = word; } public int getWordcount() { return wordcount; } public void setWordcount(int wordcount) { this.wordcount = wordcount; } public int getId() { return id; } public void setId(int id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public String getTxt() { return txt; } public void setTxt(String txt) { this.txt = txt; } public void write(DataOutput out) throws IOException { out.writeInt(id); out.writeUTF(name); out.writeUTF(txt); out.writeUTF(word); out.writeInt(wordcount); } public void readFields(DataInput in) throws IOException { id=in.readInt(); name=in.readUTF(); txt=in.readUTF(); word=in.readUTF(); wordcount=in.readInt(); } //向数据库中写入DB public void write(PreparedStatement ppst) throws SQLException { //要求定制字段列表的时候先单词后个数。 ppst.setString(1,word); ppst.setInt(2,wordcount); } //从DB中读出 public void readFields(ResultSet rs) throws SQLException { id=rs.getInt(1); name=rs.getString(2); txt=rs.getString(3); } } public class WCApp { // // public static void main(String[] args) throws Exception { // Configuration conf = new Configuration(); // // Job job = Job.getInstance(conf); // //// 设置作业的各种属性 // job.setJobName(&quot;MySQLApp&quot;); //作业名称 // job.setJarByClass(WCApp.class); //搜索类路径 // // //配置数据库信息 // String driverclass = &quot;com.mysql.jdbc.Driver&quot;; // String url = &quot;jdbc:mysql://localhost:3306/big4&quot;; // String usrname = &quot;root&quot;; // String password = &quot;root&quot;; // DBConfiguration.configureDB(conf,driverclass,url,usrname,password); // //设置数据输入内容 // DBInputFormat.setInput(job,DBWritable.class,&quot;select id,name,txt from words&quot;,&quot;select count(*) from words&quot;); // //设置输出路径 // FileOutputFormat.setOutputPath(job, new Path(&quot;d:/mr/sql/out&quot;)); // // job.setMapperClass(WCMapper.class); //mapper类 // job.setReducerClass(WCReducer.class); //reduce类 // // job.setNumReduceTasks(3); // // // job.setMapOutputKeyClass(Text.class); // job.setMapOutputValueClass(IntWritable.class); // // job.setOutputKeyClass(Text.class); //设置输出类型 // job.setOutputValueClass(IntWritable.class); // // job.waitForCompletion(true); // // } //} public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); Job job = Job.getInstance(conf); //设置job的各种属性 job.setJobName(&quot;MySQLApp&quot;); //作业名称 job.setJarByClass(WCApp.class); //搜索类 //配置数据库信息 String driverclass = &quot;com.mysql.jdbc.Driver&quot;; String url = &quot;jdbc:mysql://localhost:3306/big4&quot;; String username = &quot;root&quot;; String password = &quot;root&quot;; //设置数据库配置 DBConfiguration.configureDB(job.getConfiguration(), driverclass, url, username, password); //设置数据输入内容 DBInputFormat.setInput(job, MyDBWritalbe.class, &quot;select id,name,txt from words&quot;, &quot;select count(*) from words&quot;); //这里定制字段列表，先单词后个数 DBOutputFormat.setOutput(job,&quot;stats&quot;,&quot;word&quot;,&quot;c&quot;); //设置分区类 job.setMapperClass(WCMapper.class); //mapper类 job.setReducerClass(WCReducer.class); //reducer类 job.setNumReduceTasks(3); //reduce个数 job.setMapOutputKeyClass(Text.class); // job.setMapOutputValueClass(IntWritable.class); // job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); // job.waitForCompletion(true); } } public class WCMapper extends Mapper&lt;LongWritable, MyDBWritalbe, Text, IntWritable&gt; { @Override protected void map(LongWritable key, MyDBWritalbe value, Context context) throws IOException, InterruptedException { System.out.println(key); String line = value.getTxt(); System.out.println(value.getId() + &quot;,&quot; + value.getName()); String[] arr = line.split(&quot; &quot;); for (String s : arr) { context.write(new Text(s), new IntWritable(1)); } } } public class WCReducer extends Reducer&lt;Text, IntWritable, MyDBWritalbe, NullWritable&gt; { protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException { int count = 0; for (IntWritable w : values) { count = count + w.get(); } MyDBWritalbe keyout = new MyDBWritalbe(); keyout.setWord(key.toString()); keyout.setWordcount(count); context.write(keyout, NullWritable.get()); } } 在虚拟机中跑wordcount写入数据库问题：需要修改的地方：1.修改url 2.修改core-site.xml或者删除掉 3.在虚拟机中分发mysql-connector-java-5.1.7-bin.jar将之分发到每个节点的/soft/hadoop/share/hadoop/common/lib目录下。 4.在运行 hadoop jar HdfsDemo.jar com.it18zhang.hdfs.mr.mysql.WCApp 出现的问题：1.出现连接不上的报错： 百度了一下完美解决，是由于mysql没有对所有用户开启权限导致： https://blog.csdn.net/OldTogether/article/details/79612627?utm_source=blogxgwz1 2.出现一下问题： 原因是有一个s205的防火墙没有关掉导致的： https://blog.csdn.net/shirdrn/article/details/7280040 完]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>二次排序链条化</tag>
        <tag>数据倾斜</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop第七天]]></title>
    <url>%2F2018%2F10%2F17%2FHadoop%E7%AC%AC%E4%B8%83%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[多输入问题在IDEA里面代码： 首先明确一点InputInputformat也就是文本输入格式的输入是Longwritblele和Text，然后SequenceFileputFormat的输入是intwritble和text。 public class WCApp { public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;); Job job = Job.getInstance(conf); // 设置作业的各种属性 job.setJobName(&quot;WCAppMulti&quot;); //作业名称 job.setJarByClass(WCApp.class); //搜索类路径 //多个输入 MultipleInputs.addInputPath(job,new Path(&quot;file:///e:/mr/txt&quot;),TextInputFormat.class,WCTextMapper.class); MultipleInputs.addInputPath(job,new Path(&quot;file:///e:/mr/seq&quot;),SequenceFileInputFormat.class,WCSeqMapper.class); //设置输出 FileOutputFormat.setOutputPath(job,new Path(args[0])); job.setReducerClass(WCReducer.class);//reducer类 job.setNumReduceTasks(3);//reducer个数 job.setOutputKeyClass(Text.class); //设置输出类型 job.setOutputValueClass(IntWritable.class); job.waitForCompletion(true); } } ublic class WCReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{ /** * reduce */ protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException { int count = 0 ; for(IntWritable iw : values){ count = count + iw.get() ; } String tno = Thread.currentThread().getName(); System.out.println(tno + &quot;WCReducer:&quot; + key.toString() + &quot;=&quot; + count); context.write(key,new IntWritable(count)); } } public class WCTextMapper extends Mapper &lt;LongWritable, Text, Text, IntWritable&gt;{ @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { String[] arr = value.toString().split(&quot; &quot;); Text keyout= new Text(); IntWritable valueout = new IntWritable(); for(String s:arr){ keyout.set(s); valueout.set(1); context.write(keyout,valueout); } } } public class WCTextMapper extends Mapper &lt;LongWritable, Text, Text, IntWritable&gt;{ @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { String[] arr = value.toString().split(&quot; &quot;); Text keyout= new Text(); IntWritable valueout = new IntWritable(); for(String s:arr){ keyout.set(s); valueout.set(1); context.write(keyout,valueout); } } } 下图是配置本地的路径。要明确其中的arg[]也就是这个里配置的那个Program arguments的路径。因为只需要一个参数就可以了，因为在代码中已经写好了输入路径了。 计数器 日志目录：/soft/hadoop/logs/userlogs 用户打印的日志。Hadoop输出的日志都是在外层的文件之中。userlogs是自己的日志打印。 计数器是其他的内容都不做更改，在Mapper和Reducer里面添加这个语句即可 context.getCounter(“r”, “WCReducer.reduce”).increment(1); 然后扔到虚拟机里面去运行： hadoop jar HdfsDemo.jar com.it18zhang.hdfs.mr.WCApp hdfs://user/centos/data/ hdfs://user/centos/data/out 单独配置2nn到独立节点配置core-site文件 [hdfs-site.xml] &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;s206:50090&lt;/value&gt; &lt;/property&gt; 跟踪运行器信息 添加一个工具类： public class Util { public static String getInfo(Object o,String msg ){ return getHostname() + &quot;:&quot; + getPID() + &quot;:&quot; + getTID()+&quot;:&quot;+getObjInfo(o) +msg; } //得到主机名 public static String getHostname() { try { return InetAddress.getLocalHost().getHostName(); } catch (UnknownHostException e) { e.printStackTrace(); } return null; } //获得当前程序的所在的进程ID。 public static int getPID() { String info = ManagementFactory.getRuntimeMXBean().getName(); return Integer.parseInt(info.substring(0,info.indexOf(&quot;@&quot;))); } //返回当前线程ID， public static String getTID(){ return Thread.currentThread().getName(); } public static String getObjInfo(Object o){ String sname = o.getClass().getSimpleName(); return sname + &quot;@&quot;+o.hashCode(); } } 然后在map和reduce阶段添加： //每执行一次，计数器对这个组+1 context.getCounter(&quot;m&quot;,Util.getInfo(this,&quot;map&quot;)).increment(1); 效果如下图 全排序## 普通排序求最高年份温度 ## 代码如下： public class MaxTempApp { public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;); Job job = Job.getInstance(conf); // 设置作业的各种属性 job.setJobName(&quot;MaxTempApp&quot;); //作业名称 job.setJarByClass(MaxTempApp.class); //搜索类路径 job.setInputFormatClass(TextInputFormat.class);//设置输入格式类 //添加输入路径 FileInputFormat.addInputPath(job, new Path(args[0])); //设置输出路径 org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(args[1])); job.setMapperClass(MaxTempMapper.class); //mapper类 job.setReducerClass(MaxTempReducer.class); //reduce类 job.setNumReduceTasks(3); //reduce个数 job.setMapOutputKeyClass(IntWritable.class); job.setMapOutputValueClass(IntWritable.class); job.setOutputKeyClass(IntWritable.class); //设置输出类型 job.setOutputValueClass(IntWritable.class); job.waitForCompletion(true); } } public class MaxTempMapper extends Mapper&lt;LongWritable, Text, IntWritable, IntWritable&gt; { public MaxTempMapper() { System.out.println(&quot;new MaxTempMapper&quot;); } @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { String line = value.toString(); String arr[] = line.split(&quot; &quot;); context.write(new IntWritable(Integer.parseInt(arr[0])), new IntWritable((Integer.parseInt(arr[1])))); } } public class MaxTempReducer extends Reducer&lt;IntWritable, IntWritable, IntWritable, IntWritable&gt;{ /** * reduce */ protected void reduce(IntWritable key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException { int max =Integer.MIN_VALUE; for(IntWritable iw : values){ max = max &gt; iw.get() ? max : iw.get() ; } context.write(key,new IntWritable(max)); } } 全排序代码上面代码产生的会是3个文件，文件内部各自排序，但是不是全排序的文件。全排序2种办法： 1、设置分区数是1，但是数据倾斜 在每个分区里设置0-30，30-60，60-100即可,然后每个分区返回0,1,2进入分成不同的区代码如下图： public class YearPartitioner extends Partitioner&lt;IntWritable, IntWritable&gt; { public int getPartition(IntWritable year, IntWritable temp, int parts) { int y = year.get() - 1970; if (y &lt; 33) { return 0; } if (y &gt; 33 &amp;&amp; y &lt; 66) { return 1; } else { return 2; } } } public class MaxTempReducer extends Reducer&lt;IntWritable, IntWritable, IntWritable, IntWritable&gt;{ /** * reduce */ protected void reduce(IntWritable key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException { int max =Integer.MIN_VALUE; for(IntWritable iw : values){ max = max &gt; iw.get() ? max : iw.get() ; } context.write(key,new IntWritable(max)); } } public class MaxTempMapper extends Mapper&lt;LongWritable, Text, IntWritable, IntWritable&gt; { public MaxTempMapper() { System.out.println(&quot;new MaxTempMapper&quot;); } @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { String line = value.toString(); String arr[] = line.split(&quot; &quot;); context.write(new IntWritable(Integer.parseInt(arr[0])), new IntWritable((Integer.parseInt(arr[1])))); } } public class MaxTempApp { public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;); Job job = Job.getInstance(conf); // 设置作业的各种属性 job.setJobName(&quot;MaxTempApp&quot;); //作业名称 job.setJarByClass(MaxTempApp.class); //搜索类路径 job.setInputFormatClass(TextInputFormat.class);//设置输入格式类 job.setPartitionerClass(YearPartitioner.class); //添加输入路径 FileInputFormat.addInputPath(job, new Path(args[0])); //设置输出路径 org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(args[1])); job.setMapperClass(MaxTempMapper.class); //mapper类 job.setReducerClass(MaxTempReducer.class); //reduce类 job.setNumReduceTasks(3); //reduce个数 job.setMapOutputKeyClass(IntWritable.class); job.setMapOutputValueClass(IntWritable.class); job.setOutputKeyClass(IntWritable.class); //设置输出类型 job.setOutputValueClass(IntWritable.class); job.waitForCompletion(true); } } 其实无非就是加一个partitioner的这个类而已。 全排序采样器1.定义1个reduce 2.自定义分区函数。： 自行设置分解区间。 3.使用hadoop采样机制。 通过采样器生成分区文件，结合hadoop的TotalOrderPartitioner进行分区划分 TotalOrderPartitioner //全排序分区类,读取外部生成的分区文件确定区间。 使用时采样代码在最后端,否则会出现错误。 //分区文件设置，设置的job的配置对象，不要是之前的conf.这里面的getconfiguration()是对最开始new的conf的拷贝，TotalOrderPartitioner.setPartitionFile(job.getConfiguration(),new Path(“d:/mr/par.lst”)); 首先一段产生随机年份，温度的代码public class PrepareTempData { @Test public void makeData() throws IOException { FileWriter fw = new FileWriter(&quot;e:/mr/temp.txt&quot;); for(int i=0;i&lt;6000;i++){ int year=1970+ new Random().nextInt(100); int temp=-30 + new Random().nextInt(600); fw.write(&quot; &quot;+ year + &quot; &quot;+ temp + &quot;\r\n&quot; ); } fw.close(); } } 全排序采样器代码 public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS&quot;, &quot;file:///&quot;); Job job = Job.getInstance(conf); 设置作业的各种属性 job.setJobName(&quot;MaxTempApp&quot;); //作业名称 job.setJarByClass(MaxTempApp.class); //搜索类路径 job.setInputFormatClass(SequenceFileInputFormat.class);//设置输入格式类 //添加输入路径 FileInputFormat.addInputPath(job, new Path(args[0])); //设置输出路径 org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(args[1])); job.setMapperClass(MaxTempMapper.class); //mapper类 job.setReducerClass(MaxTempReducer.class); //reduce类 job.setMapOutputKeyClass(IntWritable.class); job.setMapOutputValueClass(IntWritable.class); job.setOutputKeyClass(IntWritable.class); //设置输出类型 job.setOutputValueClass(IntWritable.class); //设置 全排序分区类 job.setPartitionerClass(TotalOrderPartitioner.class); //将sample数据 写入分区文件 TotalOrderPartitioner.setPartitionFile(conf, new Path(&quot;e:/mr/par.lst&quot;)); //创建随机采样器对象 InputSampler.Sampler&lt;IntWritable, IntWritable&gt; sampler = new InputSampler.RandomSampler&lt;IntWritable, IntWritable&gt;(0.1, 10000, 10); job.setNumReduceTasks(3); //reduce个数 InputSampler.writePartitionFile(job, sampler); job.waitForCompletion(true); } } public class MaxTempMapper extends Mapper&lt;IntWritable, IntWritable, IntWritable, IntWritable&gt; { public MaxTempMapper() { System.out.println(&quot;new MaxTempMapper&quot;); } @Override protected void map(IntWritable key, IntWritable value, Context context) throws IOException, InterruptedException { context.write(key,value); } } public class MaxTempReducer extends Reducer&lt;IntWritable, IntWritable, IntWritable, IntWritable&gt;{ /** * reduce */ protected void reduce(IntWritable key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException { int max =Integer.MIN_VALUE; for(IntWritable iw : values){ max = max &gt; iw.get() ? max : iw.get() ; } context.write(key,new IntWritable(max)); } } public class YearPartitioner extends Partitioner&lt;IntWritable, IntWritable&gt; { public int getPartition(IntWritable year, IntWritable temp, int parts) { int y = year.get() - 1970; if (y &lt; 33) { return 0; } if (y &gt; 33 &amp;&amp; y &lt; 66) { return 1; } else { return 2; } } } 全排序官方笔记: 1.定义1个reduce 2.自定义分区函数. 自行设置分解区间。 3.使用hadoop采样机制。 通过采样器生成分区文件，结合hadoop的TotalOrderPartitioner进行分区划分。 TotalOrderPartitioner //全排序分区类,读取外部生成的分区文件确定区间。 使用时采样代码在最后端,否则会出现错误。 //分区文件设置，设置的job的配置对象，不要是之前的conf. TotalOrderPartitioner.setPartitionFile(job.getConfiguration(),new Path(&quot;d:/mr/par.lst&quot;)); ## 全排序和部分排序二次排序在面试比例很重的 ##分区在map端，分组在reduce端。二次排序就是把value和key组合到一起，然后value就是nullwritable这样子。合在一起成为一个combokey。Combokey不但要可比较还要串行化，hadoop的串行化机制是writable ，串行就是写入到流离去，反串行就是读出来。IntWritable LongWritable这些都是经过串行化实现的类，Writable有很多子类。 二次排序代码如下： /** * 自定义组合key */ public class ComboKey implements WritableComparable&lt;ComboKey&gt; { private int year; private int temp; public int getYear() { return year; } public void setYear(int year) { this.year = year; } public int getTemp() { return temp; } public void setTemp(int temp) { this.temp = temp; } /** * 对key进行比较实现 */ public int compareTo(ComboKey o) { int y0 = o.getYear(); int t0 = o.getTemp(); //年份相同(升序) if (year == y0) { //气温降序 return -(temp - t0); } else { return year - y0; } } /** * 串行化过程 */ public void write(DataOutput out) throws IOException { //年份 out.writeInt(year); //气温 out.writeInt(temp); } public void readFields(DataInput in) throws IOException { year = in.readInt(); temp = in.readInt(); } } /** *ComboKeyComparator */ public class ComboKeyComparator extends WritableComparator { protected ComboKeyComparator() { super(ComboKey.class, true); } public int compare(WritableComparable a, WritableComparable b) { ComboKey k1 = (ComboKey) a; ComboKey k2 = (ComboKey) b; return k1.compareTo(k2); } } public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS&quot;, &quot;file:///&quot;); Job job = Job.getInstance(conf); // 设置作业的各种属性 job.setJobName(&quot;SecondarySortApp&quot;); //作业名称 job.setJarByClass(MaxTempApp.class); //搜索类路径 job.setInputFormatClass(TextInputFormat.class);//设置输入格式类 //添加输入路径 FileInputFormat.addInputPath(job, new Path(args[0])); //设置输出路径 org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(args[1])); job.setMapperClass(MaxTempMapper.class); //mapper类 job.setReducerClass(MaxTempReducer.class); //reduce类 //设置map输出类型 job.setMapOutputKeyClass(ComboKey.class); job.setMapOutputValueClass(NullWritable.class); //设置Reduceoutput类型 job.setOutputKeyClass(IntWritable.class); //设置输出类型 job.setOutputValueClass(IntWritable.class); //设置分区类 job.setPartitionerClass(YearPartitioner.class); //设置分组对比器。 job.setGroupingComparatorClass(YearGroupComparator.class); //设置排序对比器，听说不写那个ComboKeyComparator不设置也可以 job.setSortComparatorClass(ComboKeyComparator.class); //reduce个数 job.setNumReduceTasks(3); job.waitForCompletion(true); } } public class MaxTempMapper extends Mapper&lt;LongWritable, Text, ComboKey, NullWritable&gt; { @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { String line = value.toString(); String[] arr = line.split(&quot; &quot;); ComboKey keyout = new ComboKey(); keyout.setYear(Integer.parseInt(arr[0])); keyout.setTemp(Integer.parseInt(arr[1])); context.write(keyout, NullWritable.get()); } } /** * Reducer */ public class MaxTempReducer extends Reducer&lt;ComboKey, NullWritable, IntWritable, IntWritable&gt;{ protected void reduce(ComboKey key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException { int year = key.getYear(); int temp = key.getTemp(); context.write(new IntWritable(year),new IntWritable(temp)); } } public class YearGroupComparator extends WritableComparator { protected YearGroupComparator() { super(ComboKey.class, true); } public int compare(WritableComparable a, WritableComparable b) { ComboKey k1 = (ComboKey) a ; ComboKey k2 = (ComboKey) b ; return k1.getYear() - k2.getYear() ; } } public class YearPartitioner extends Partitioner&lt;ComboKey,NullWritable&gt; { public int getPartition(ComboKey key, NullWritable nullWritable, int numPartitions) { int year = key.getYear(); return year % numPartitions; } }]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>多输入问题</tag>
        <tag>计数器使用</tag>
        <tag>跟踪运行信息</tag>
        <tag>产生随机数文件</tag>
        <tag>全排序</tag>
        <tag>全排序采样器</tag>
        <tag>二次排序</tag>
        <tag>倒排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop第六天之Yarn作业提交]]></title>
    <url>%2F2018%2F10%2F11%2FHadoop%E7%AC%AC%E5%85%AD%E5%A4%A9%E4%B9%8BYarn%E4%BD%9C%E4%B8%9A%E6%8F%90%E4%BA%A4%2F</url>
    <content type="text"><![CDATA[本地模式job提交流程mr.Job = new Job(); job.setxxx(); JobSubmitter.提交 LocalJobRunner.Job(); start(); hdfs writepacket, hdfs 切片计算方式getFormatMinSplitSize() = 1 //最小值(&gt;=1) 1 0 long minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job)); //最大值(&lt;= Long.Max , mapreduce.input.fileinputformat.split.maxsize=) long maxSize = getMaxSplitSize(job); //得到block大小 long blockSize = file.getBlockSize(); //minSplit maxSplit blockSize //Math.max(minSize, Math.min(maxSize, blockSize)); LF : Line feed,换行符private static final byte CR = ‘\r’;private static final byte LF = ‘\n’; 压缩1.Windows 源文件大小:82.8k 源文件类型:txt 压缩性能比较 | DeflateCodec GzipCodec BZip2Codec Lz4Codec SnappyCodec |结论 ------------|-------------------------------------------------------------------|---------------------- 压缩时间(ms)| 450 7 196 44 不支持 |Gzip &gt; Lz4 &gt; BZip2 &gt; Deflate ------------|-------------------------------------------------------------------|---------------------- 解压时间(ms)| 444 66 85 33 |lz4 &gt; gzip &gt; bzip2 &gt; Deflate ------------|-------------------------------------------------------------------|---------------------- 占用空间(k) | 19k 19k 17k 31k 不支持 |Bzip &gt; Deflate = Gzip &gt; Lz4 | | 2.CentOS 源文件大小:82.8k 源文件类型:txt | DeflateCodec GzipCodec BZip2Codec Lz4Codec LZO SnappyCodec |结论 ------------|---------------------------------------------------------------------------|---------------------- 压缩时间(ms)| 944 77 261 53 77 不支持 |Gzip &gt; Lz4 &gt; BZip2 &gt; Deflate ------------|---------------------------------------------------------------------------|---------------------- 解压时间(ms)| 67 66 106 52 73 |lz4 &gt; gzip &gt; Deflate&gt; lzo &gt; bzip2 ------------|---------------------------------------------------------------------------|---------------------- 占用空间(k) | 19k 19k 17k 31k 34k |Bzip &gt; Deflate = Gzip &gt; Lz4 &gt; lzo 远程调试1.设置服务器java vm的-agentlib:jdwp选项. [server] //windwos //set JAVA_OPTS=%JAVA_OPTS% -agentlib:jdwp=transport=dt_socket,address=8888,server=y,suspend=n //linux export HADOOP_CLIENT_OPTS=-agentlib:jdwp=transport=dt_socket,address=8888,server=y,suspend=y 2.在server启动java程序 hadoop jar HdfsDemo.jar com.it18zhang.hdfs.mr.compress.TestCompress 3.server会暂挂在8888. Listening ... 4.客户端通过远程调试连接到远程主机的8888. 5.客户端就可以调试了。 hadoop jarjava hadoop jar com.it18zhang.hdfs.mr.compress.TestCompress export HADOOP_CLIENT_OPTS=-agentlib:jdwp=transport=dt_socket,address=8888,server=y,suspend=y 在pom.xml中引入新的插件(maven-antrun-plugin),实现文件的复制.[pom.xml] &lt;project&gt; ... &lt;build&gt; &lt;plugins&gt; ... &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-antrun-plugin&lt;/artifactId&gt; &lt;version&gt;1.8&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;run&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;tasks&gt; &lt;echo&gt;---------开始复制jar包到共享目录下----------&lt;/echo&gt; &lt;delete file=&quot;D:\downloads\bigdata\data\HdfsDemo-1.0-SNAPSHOT.jar&quot;&gt;&lt;/delete&gt; &lt;copy file=&quot;target/HdfsDemo-1.0-SNAPSHOT.jar&quot; toFile=&quot;D:\downloads\bigdata\data\HdfsDemo.jar&quot;&gt; &lt;/copy&gt; &lt;/tasks&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; ... &lt;/project&gt; 在centos上使用yum安装snappy压缩库文件[google snappy] $&gt;sudo yum search snappy #查看是否有snappy库 $&gt;sudo yum install -y snappy.x86_64 #安装snappy压缩解压缩库 库文件windows :dll(dynamic linked library) linux :so(shared object) LZO1.在pom.xml引入lzo依赖 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.it18zhang&lt;/groupId&gt; &lt;artifactId&gt;HdfsDemo&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins &lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-antrun-plugin&lt;/artifactId&gt; &lt;version&gt;1.8&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;run&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;tasks&gt; &lt;echo&gt;---------开始复制jar包到共享目录下----------&lt;/echo&gt; &lt;delete file=&quot;D:\downloads\bigdata\data\HdfsDemo-1.0-SNAPSHOT.jar&quot;&gt;&lt;/delete&gt; &lt;copy file=&quot;target/HdfsDemo-1.0-SNAPSHOT.jar&quot; toFile=&quot;D:\downloads\bigdata\data\HdfsDemo.jar&quot;&gt; &lt;/copy&gt; &lt;/tasks&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.7.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-yarn-common&lt;/artifactId&gt; &lt;version&gt;2.7.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-yarn-client&lt;/artifactId&gt; &lt;version&gt;2.7.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-yarn-server-resourcemanager&lt;/artifactId&gt; &lt;version&gt;2.7.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.anarres.lzo&lt;/groupId&gt; &lt;artifactId&gt;lzo-hadoop&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; 2.在centos上安装lzo库 $&gt;sudo yum -y install lzo 3.使用mvn命令下载工件中的所有依赖 进入pom.xml所在目录，运行cmd： mvn -DoutputDirectory=./lib -DgroupId=com.it18zhang -DartifactId=HdfsDemo -Dversion=1.0-SNAPSHOT dependency:copy-dependencies 4.在lib下存放依赖所有的第三方jar 5.找出lzo-hadoop.jar + lzo-core.jar复制到hadoop的响应目录下。 $&gt;cp lzo-hadoop.jar lzo-core.jar /soft/hadoop/shared/hadoop/common/lib 6.执行远程程序即可。 修改maven使用aliyun镜像。[maven/conf/settings.xml] &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;settings xmlns=&quot;http://maven.apache.org/SETTINGS/1.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd&quot;&gt; &lt;pluginGroups&gt; &lt;/pluginGroups&gt; &lt;proxies&gt; &lt;/proxies&gt; &lt;servers&gt; &lt;server&gt; &lt;id&gt;releases&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;snapshots&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;Tomcat7&lt;/id&gt; &lt;username&gt;tomcat&lt;/username&gt; &lt;password&gt;tomcat&lt;/password&gt; &lt;/server&gt; &lt;/servers&gt; &lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;*&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;/mirror&gt; &lt;/mirrors&gt; &lt;/settings&gt; 文件格式:SequenceFile1.SequenceFile Key-Value对方式。 2.不是文本文件，是二进制文件。 3.可切割 因为有同步点。 reader.sync(pos); //定位到pos之后的第一个同步点。 writer.sync(); //写入同步点 4.压缩方式 不压缩 record压缩 //只压缩value 块压缩 //按照多个record形成一个block. 文件格式:MapFile1.Key-value 2.key按升序写入(可重复)。 3.mapFile对应一个目录，目录下有index和data文件,都是序列文件。 4.index文件划分key区间,用于快速定位。 自定义分区函数1.定义分区类 public class MyPartitioner extends Partitioner&lt;Text, IntWritable&gt;{ public int getPartition(Text text, IntWritable intWritable, int numPartitions) { return 0; } } 2.程序中配置使用分区类 job.setPartitionerClass(MyPartitioner.class); combinerd（合成） 继承了Reducer 任何的Reducer类都能被他使用 Map端的Reducer 预先化简1，为了减少网络带宽 将Map端发出的的数据进行聚合 并不是所有的都可以用combiner2,combiner 切片个数是四个，mapper就需要也是4个 下图是一个客户端的写入过程：首先形成一个管线，因为有很多节点，节点形成管线，现在本地构建2个队列，一个发送队列sendQueue，一个是确认队列ackQueue，客户端吧消息封装成Packet放到队列里面，而且结束后会放一个空包进去，发送的话，是以管线的方式发给A，A在发给B，B在发给C，在发送的过程中有一个校验的过程chunk(4+512)就是4个校验位+512个数据字节，一个Packet是64K+33个头的字节，在确认队列里面是一个编号seqNo如果在丢列中ABC中都回执，就吧seqNo在ackQueue中移除。发送如果失败，就吧管线重新建立，吧没法送的包返回到队列之前的一个容错机制。 输入格式定位文件，在输入格式中间有一个交互reader，在Map和Reduce都有这么一些方法，在Mapper和Reducer中有回调方法，就是本来使用的是子类的方法，但是子类中调用有父类的的run方法，然后run方法里面有reduce方法也就又调用了子类的方法，这叫做回调 。Mapper是通过reader读取下一条数据传进来的。Map和Reduce中间有一个shuffer的混洗过程，网络间分发数据的过程， Map里面先分区，分区个数等于Reduce的个数，所以有几个reduce就有几个分区，默认情况下，Map分区通过哈希算法来分区的，对K哈希。只要K相同，一定会被哈希到一个分区里面，分区也就是桶的概念，也就是只要K相同进入到同一个桶里面，只要是同一个分区就是进入到同一个reduce，默认情况下inputformat的格式，他的K就是偏移量。如果是图片格式的话就可能是别的。只有在文本输入的时候才是偏移量，当然也可以自己定义Inputformat格式。在mr计算之前切片就已经算好了，产生的切片文件和配置XML文件放到了临时目录下了。 看下图，首先客户端运行作业，第一步run job，第二部走ResourceManager，是Yarn层面，负责资源调度，所有集群的资源都属于ResourceManager，第二部get new application，首先得到应用的ID，第三部copyt job resources，复制切片信息和xml配置信息和jar包到HDFS，第四部提交给资源管理器，然后资源管理器start container，然后通过节点管理器启动一个MRAPPMaster（应用管理程序），第六步初始化Job，第七部检索切片信息，第八步请求资源管理器去分配一个资源列表，第九步启动节点管理器，通过节点管理器来启动一个JVM虚拟机，在虚拟机里面启动yarn子进程，第十部通过子进程读取切片信息，然后在运行MapTast或者ReduceTask 客户端提取一个作业，然后联系资源管理器，为了得到id,在完全分布式下客户端client叫job，一旦提交就叫做application了，先请求，然后得到了appid返回给了客户端，第三部将作业信息上传到HDFS的临时目录，存档切片信息和jar包，然后找到节点管理器，然后NM孵化一个MRAPPMaster，然后检索HDFS文件系统，获取到有多少个要执行的并发任务，拿到任务之后，要求资源管理器分配一个集合，哪些节点可以用。 然后MRAppmaster在联系其他的NM，让NM去开启Yarn子进程，子进程在去检索HDFS信息，在开启Map和Reduce hdfs 切片计算方式getFormatMinSplitSize() = 1 //最小值(&gt;=1) 1 0 long minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job)); //最大值(&lt;= Long.Max , mapreduce.input.fileinputformat.split.maxsize=) long maxSize = getMaxSplitSize(job); //得到block大小 long blockSize = file.getBlockSize(); //minSplit maxSplit blockSize //Math.max(minSize, Math.min(maxSize, blockSize)); 在计算切片的时候一般块大小不设置，是最好的，一般就是128M，采用本地优先策略，一个快大小是128m如果一旦把他边变1m那么切片大小就会变成1M，一个，然后128M的块大小会被分发128份切片分别传输给MR去计算，这样的话，就会增加网络间的传输，增加网络负载。所以128m是最合适的 CR=‘\r’回车符LF+’\n’换行符 windows系统里面是\r\n。回车带换行。linux系统只有一个\n 切片是定义大方向的，阅读器recorderreader是控制细节的。切片定义任务数量，开几个map但是不一定是每个map都有任务。 切片问题是物理设置，但是是逻辑读取。 打个比方说上图这个东西比如说第一个例子，前面的设置物理划分，根据字节数划分成了4个切片，也就是有4个Mapper,然后四个切片开始读取，其中第一个物理切割到t但是实际上，由于切片读取到换行符所以，第一个Mapper也就是读取到了hello world tom，然后第二个，第二个偏移量是13+13.偏移量是13，在物理读取13个直接，但是由于Mapper检测不是行首，所以直接从下一行读取也在读取一整行。这个就是每个切片128MB的缩影，吧128M变成了13而已。然后其实Textinputformat下一个环节是。recoderreader。这个会先检测是否是行首， 压缩问题package com.it18zhang.hdfs.mr.compress; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.io.IOUtils; import org.apache.hadoop.io.compress.*; import org.apache.hadoop.util.ReflectionUtils; import org.junit.Test; import java.io.FileInputStream; import java.io.FileOutputStream; /** * @Title:TestCompress * @Description: * @Auther: Eric Hunn * @Version:1.0 * @Create 2018/10/14 19:39 */ public class TestCompress { @Test public void deflateCompress() throws Exception { Class[] zipClasses = { DeflateCodec.class, GzipCodec.class, BZip2Codec.class, Lz4Codec.class, }; for(Class c : zipClasses){ zip(c); } } /*压缩测试 * * */ public void zip(Class codecClass) throws Exception { long start = System.currentTimeMillis(); //实例化对象 CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, new Configuration()); //创建文件输出流，得到默认拓展名 FileOutputStream fos = new FileOutputStream(&quot;e:/comp/b&quot; + codec.getDefaultExtension()); //得到压缩流 CompressionOutputStream zipOut = codec.createOutputStream(fos); IOUtils.copyBytes(new FileInputStream(&quot;e:/11111.txt/&quot;), zipOut, 1024); zipOut.close(); System.out.println(codecClass.getSimpleName() + &quot; ：&quot;); } } 压缩一般是在文件Map输出的时候压缩，Map输出的时候内存不够一般要往磁盘里面溢出，在溢出的时候可以让他压缩溢出，这样reduce的时候就要解压缩。上面代码测试过lz4不行，但是视频上是可以的，暂时不知道哪里出错了。 package com.it18zhang.hdfs.mr.compress; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.io.IOUtils; import org.apache.hadoop.io.compress.*; import org.apache.hadoop.util.ReflectionUtils; import org.junit.Test; import java.io.FileInputStream; import java.io.FileOutputStream; /** * @Title:TestCompress * @Description: * @Auther: Eric Hunn * @Version:1.0 * @Create 2018/10/14 19:39 */ public class TestCompress { @Test public void deflateCompress() throws Exception { Class[] zipClasses = { DeflateCodec.class, GzipCodec.class, BZip2Codec.class, // Lz4Codec.class, // SnappyCodec.class, }; for(Class c : zipClasses){ unzip(c); } } /*压缩测试 * * */ public void zip(Class codecClass) throws Exception { long start = System.currentTimeMillis(); //实例化对象 CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, new Configuration()); //创建文件输出流，得到默认拓展名 FileOutputStream fos = new FileOutputStream(&quot;e:/comp/b&quot; + codec.getDefaultExtension()); //得到压缩流 CompressionOutputStream zipOut = codec.createOutputStream(fos); IOUtils.copyBytes(new FileInputStream(&quot;e:/11111.txt/&quot;), zipOut, 1024); zipOut.close(); System.out.println(codecClass.getSimpleName() + &quot; ：&quot;); } public void unzip(Class codecClass) throws Exception { long start = System.currentTimeMillis(); //实例化对象 CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, new Configuration()); //创建文件输出流，得到默认拓展名 FileInputStream fis = new FileInputStream(&quot;e:/comp/b&quot; + codec.getDefaultExtension()); //得到压缩流 CompressionInputStream zipIn = codec.createInputStream(fis); IOUtils.copyBytes(zipIn,new FileOutputStream(&quot;e:/comp/b&quot; + codec.getDefaultExtension() + &quot;.txt&quot;), 1024); zipIn.close(); System.out.println(codecClass.getSimpleName() + &quot; ：&quot; + (System.currentTimeMillis() - start)); } } 在集群上运package com.it18zhang.hdfs.mr.compress; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.io.IOUtils; import org.apache.hadoop.io.compress.*; import org.apache.hadoop.util.ReflectionUtils; import java.io.FileInputStream; import java.io.FileOutputStream; /** * @Title:TestCompress * @Description: * @Auther: Eric Hunn * @Version:1.0 * @Create 2018/10/14 19:39 */ public class TestCompress { public static void main(String[] args) throws Exception { Class[] zipClasses = { DeflateCodec.class, GzipCodec.class, BZip2Codec.class, // Lz4Codec.class, // SnappyCodec.class, }; for(Class c : zipClasses){ zip(c); } System.out.println(&quot;==================================&quot;); for(Class c : zipClasses){ unzip(c); } } /*压缩测试 * * */ public static void zip(Class codecClass) throws Exception { long start = System.currentTimeMillis(); //实例化对象 CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, new Configuration()); //创建文件输出流，得到默认拓展名 FileOutputStream fos = new FileOutputStream(&quot;/home/centos/zip/b&quot; + codec.getDefaultExtension()); //得到压缩流 CompressionOutputStream zipOut = codec.createOutputStream(fos); IOUtils.copyBytes(new FileInputStream(&quot;/home/centos/zip/a.txt&quot;), zipOut, 1024); zipOut.close(); System.out.println(codecClass.getSimpleName() + &quot; ：&quot;); } public static void unzip(Class codecClass) throws Exception { long start = System.currentTimeMillis(); //实例化对象 CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, new Configuration()); //创建文件输出流，得到默认拓展名 FileInputStream fis = new FileInputStream(&quot;/home/centos/zip/b&quot; + codec.getDefaultExtension()); //得到压缩流 CompressionInputStream zipIn = codec.createInputStream(fis); IOUtils.copyBytes(zipIn,new FileOutputStream(&quot;/home/centos/zip/b&quot; + codec.getDefaultExtension() + &quot;.txt&quot;), 1024); zipIn.close(); System.out.println(codecClass.getSimpleName() + &quot; ：&quot; + (System.currentTimeMillis() - start)); } } 远程调试1.设置服务器java vm的-agentlib:jdwp选项. [server] //windwos //set JAVA_OPTS=%JAVA_OPTS% -agentlib:jdwp=transport=dt_socket,address=8888,server=y,suspend=n //linux 2.在server启动java程序 hadoop jar HdfsDemo.jar com.it18zhang.hdfs.mr.compress.TestCompress 3.server会暂挂在8888. Listening ... 4.客户端通过远程调试连接到远程主机的8888. 5.客户端就可以调试了。 通过MapFile来写入 /*写操作 * */ @Test public void save() throws Exception { Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS &quot;, &quot;file:///&quot;); FileSystem fs = FileSystem.get(conf); Path p = new Path(&quot;e:/seq/1.seq&quot;); MapFile.Writer writer = new MapFile.Writer(conf, fs, &quot;e:/map&quot;, IntWritable.class, Text.class); for (int i = 0; i &lt; 100000; i++) { writer.append(new IntWritable(i), new Text(&quot;tom&quot; + i)); } // for(int i =0 ;i &lt; 10 ; i++){// writer.append(new IntWritable(i),new Text(“tom” + i));// } writer.close(); } 通过MapFile来读取/*读取Mapfile文件 * */ @Test public void readMapfile() throws Exception { Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS &quot;, &quot;file:///&quot;); FileSystem fs = FileSystem.get(conf); Path p = new Path(&quot;e:/seq/1.seq&quot;); MapFile.Reader reader = new MapFile.Reader(fs, &quot;e:/map&quot;, conf); IntWritable key = new IntWritable(); Text value = new Text(); while (reader.next(key, value)) { System.out.println(key.get() + &quot;:&quot; + value.toString()); } reader.close(); } Map的分区是哈希分区 combiner的好处是：map端的reduce在map端做集合，减少了shuffle量。统计每个单词hello 1发了十万次，不如发一个hello1在map端做聚合发过去hello十万，即刻。就是在map端口预先reduce化简的过程。不是所有的作业都可以把reduce做成combiner。最大值最小值都可以combiner但是平均值就不行了。 第六天的最后两个视频没有往下写代码，因为太多太杂了。主要讲解MR计数器，数据倾斜，Mapfile 序列化读取，压缩这些。]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Yan作业提交</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop第五天01之hdfs写入剖析]]></title>
    <url>%2F2018%2F10%2F05%2FHadoop%E7%AC%AC%E4%BA%94%E5%A4%A901%E4%B9%8Bhdfs%E5%86%99%E5%85%A5%E5%89%96%E6%9E%90%2F</url>
    <content type="text"><![CDATA[一段HDFS写入流源码分析。 首先这个得到了一个抽象类，这个FileSystemm是一个抽象类。 这个抽象类有子类，左边有一个箭头，点击得到了一个这个抽象类的所有的子类，简单看下，因为Hadoop包类的hadfs这个，得到的子类是DistributedFileSystem这个分布式文件系统。 也可以吧鼠标放到fs上会显示返回的类 也可以在IDEA的右下角的类标签里面找到： 也就是说返回了一个DistributedFIleSystem, 然后文件系统fs.create调用了create方法返回了一个FSDataOutputStream流，这个是一个HDFS数据输出流 单机F5单部进入第一个： 看这个，进到下一步，调用checkClosed()，方法，这个方法其实是继承FSOutput流里面的东西。 看到这个里面out=DFSOutputStream。这个是被装饰的流。调用这个流的write方法 HDFS流是对DFS输出流的包装 进去这个是装饰模式。 在这个构造模式中也声明了字段。 下一步： 调用了Close方法因为是继承 都是FSoutput流的子类。一个检查的方法，判断是否数组越界。 下面这个for是个循环，循环写入，。 然后下一步，进入到write1方法。 里面的buf是一个缓冲区，count是一个成员常量 上图的buf看注释，是一个缓冲区用哦过来存储数据，在他被checksum校验和之前 校验和就是检查是否是比之前设置的最小块大小大，而且还必须是512字节的倍数 上图点击进入到一个校验和类的里面看注释，校验和用于数据传输之前。 上图这两行，通过一个算法来实现校验和。通过CRC32算法类似于哈希算法。 不管他，回到buf缓冲这个地方，单部进入 首先在缓冲区进行一个判定 拷贝本地数据到本地的一个buf，显示Localbuffer=4608,count=0.要拷贝的字节是4608。如果拷贝的内容没有缓冲区大就考数据内容，要是比缓冲区大的话，就拷贝缓冲区大小的内容。 在单部进入到上图。 返回到代码。进入到源代码中，如上图 在单部进入到这个Close里面：如下图： 这个close是out.close。out是调用父类的close方法，而且父类还是一个包装类，所以进入了包装的列的close方法。再进入到close()方法。如下图： out的本类HDFSOutputStream，然后这个又是一个装饰类，本质上是DFSOutputStream这个类的方法。 单部进入到Out里面，看一下调用的到底是谁的out。再单部进入到Out里面 看注释，关闭输出流并且 释放与之相关联的系统资源。上图最终进入到了DFSOutputstream的close()方法里面了。 接上图，看到在这个close(）方法里面有一个closeImp（)方法。调用自己的关闭实现方法还在这个类里面执行呢：继续在这个类里面往下走:如下图： 这个里面有一个flushbuffer()方法， 在单部进入到这个方法里面。如下图： 看到这个图里面this和当前类不太一样说明这两个东西也是继承关系。 清理缓冲区两个参数，看注释：强制任何输出流字节被校验和并且被写入底层的输出流。 再单步进入： 看到如上图的内容。然后到了writeChecksumChunks这个方法里面，单部进入到这个里面： 对给定的数据小块来生成校验和，并且根据小块和校验和给底层的数据输出流。 单部进入到这个sum.calculateChunckedSum方法里面。 下一步 上图吧数据写入了底层里面去了。 下一步 下一步 下一步 上图就是将底层的内容打成包，再去通过网络传输。在包里面加编号和序号，也就是加顺序。 往下不写了太多了。现在的视频位置是hdfs写入剖析2的开头。]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>hdfs写入</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop第四天之滚动日志-安全模式]]></title>
    <url>%2F2018%2F10%2F04%2FHadoop%E7%AC%AC%E5%9B%9B%E5%A4%A9%E4%B9%8B%E6%BB%9A%E5%8A%A8%E6%97%A5%E5%BF%97-%E5%AE%89%E5%85%A8%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Hadoop第四天之最小块设置-指定副本数]]></title>
    <url>%2F2018%2F10%2F04%2FHadoop%E7%AC%AC%E5%9B%9B%E5%A4%A9%E4%B9%8B%E6%9C%80%E5%B0%8F%E5%9D%97%E8%AE%BE%E7%BD%AE-%E6%8C%87%E5%AE%9A%E5%89%AF%E6%9C%AC%E6%95%B0%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[遇到未解决的问题]]></title>
    <url>%2F2018%2F10%2F01%2F%E9%81%87%E5%88%B0%E6%9C%AA%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[在eclipse中，ctrl+左键找到源码，为什么旁边在pakage explore中有有轮廓啊？ 点击两个箭头即可转换。 这个东西是在IDEA中怎么调出来的。 克隆centos之后有时候出现这种情况 IDEA的使用问题：这个是什么快捷键。 在增强虚拟机工具的时候按照视频的方法没有成功，然后通过挂载的方法实现了，但是hgfs文件夹和文件夹里面的内容都是root权限的，只能修改文件夹的所有者，但是不能修改里面文件的所有者。]]></content>
      <categories>
        <category>问题</category>
      </categories>
      <tags>
        <tag>未解决的问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[haoop第三天之脚本分析，单个进程启动]]></title>
    <url>%2F2018%2F09%2F30%2Fhaoop%E7%AC%AC%E4%B8%89%E5%A4%A9%E4%B9%8B%E8%84%9A%E6%9C%AC%E5%88%86%E6%9E%90%EF%BC%8C%E5%8D%95%E4%B8%AA%E8%BF%9B%E7%A8%8B%E5%90%AF%E5%8A%A8%2F</url>
    <content type="text"><![CDATA[ssh权限问题1.~/.ssh/authorized_keys 644 2.$/.ssh 700 3.root 配置SSH生成密钥对 $&gt;ssh-keygen -t rsa -P &apos;&apos; -f ~/.ssh/id_rsa 添加认证文件 $&gt;cat ~/.ssh/id_rsa.pub &gt; ~/.ssh/authorized_keys 权限设置,文件和文件夹权限除了自己之外，别人不可写。 $&gt;chmod 700 ~/.ssh $&gt;chmod 644 ~/.ssh/authorized_keys scp远程复制. rsync远程同步,支持符号链接。 rsync -lr xxx xxx 完全分布式1.配置文件 [core-site.xml] fs.defaultFS=hdfs://s201:8020/ [hdfs-site.xml] replication=1 //伪分布 replication=3 //完全分布 [mapred-site.xml] mapreduce.framework.name=yarn [yarn-site.xml] rm.name=s201 [slaves] s202 s203 s204 2.分发文件 a)ssh openssh-server //sshd openssh-clients //ssh openssh //ssh-keygen b)scp/rsync 3.格式化文件系统 $&gt;hadoop namenode -format 4.启动hadoop所有进程 //start-dfs.sh + start-yarn.sh $&gt;start-all.sh 5.xcall.sh jps /usr/local/bin/jps /usr/local/bin/java 6.查看jps进程 $&gt;xcall.sh jps 7.关闭centos的防火墙 $&gt;sudo service firewalld stop // &lt;=6.5 start/stop/status/restart $&gt;sudo systemctl stop firewalld // 7.0 停止 start/stop/status/restart $&gt;sudo systemctl disable firewalld //关闭 $&gt;sudo systemctl enable firewalld //启用 7.最终通过webui http://s201:50070/ 符号连接1.修改符号连接的owner $&gt;chown -h centos:centos xxx //-h:针对连接本身，而不是所指文件. 2.修改符号链接 $&gt;ln -sfT index.html index //覆盖原有的连接。 hadoop模块common // hdfs // mapreduce // yarn // 进程[hdfs]start-dfs.sh NameNode NN DataNode DN SecondaryNamenode 2NN [yarn]start-yarn.sh ResourceMananger RM NodeManager NM 脚本分析sbin/start-all.sh -------------- libexec/hadoop-config.sh start-dfs.sh start-yarn.sh sbin/start-dfs.sh -------------- libexec/hadoop-config.sh sbin/hadoop-daemons.sh --config .. --hostname .. start namenode ... sbin/hadoop-daemons.sh --config .. --hostname .. start datanode ... sbin/hadoop-daemons.sh --config .. --hostname .. start sescondarynamenode ... sbin/hadoop-daemons.sh --config .. --hostname .. start zkfc ... // sbin/start-yarn.sh -------------- libexec/yarn-config.sh bin/yarn-daemon.sh start resourcemanager bin/yarn-daemons.sh start nodemanager sbin/hadoop-daemons.sh ---------------------- libexec/hadoop-config.sh slaves hadoop-daemon.sh sbin/hadoop-daemon.sh ----------------------- libexec/hadoop-config.sh bin/hdfs .... sbin/yarn-daemon.sh ----------------------- libexec/yarn-config.sh bin/yarn bin/hadoop ------------------------ hadoop verion //版本 hadoop fs //文件系统客户端. hadoop jar // hadoop classpath hadoop checknative bin/hdfs ------------------------ dfs // === hadoop fs classpath namenode -format secondarynamenode namenode journalnode zkfc datanode dfsadmin haadmin fsck balancer jmxget mover oiv oiv_legacy oev fetchdt getconf groups snapshotDiff lsSnapshottableDir portmap nfs3 cacheadmin crypto storagepolicies version hdfs常用命令$&gt;hdfs dfs -mkdir /user/centos/hadoop $&gt;hdfs dfs -ls -r /user/centos/hadoop $&gt;hdfs dfs -lsr /user/centos/hadoop $&gt;hdfs dfs -put index.html /user/centos/hadoop $&gt;hdfs dfs -get /user/centos/hadoop/index.html a.html $&gt;hdfs dfs -rm -r -f /user/centos/hadoop no route 关闭防火墙。 $&gt;su root $&gt;xcall.sh &quot;service firewalld stop&quot; $&gt;xcall.sh &quot;systemctl disable firewalld&quot; hdfs500G 1024G = 2T/4T 切割。 寻址时间:10ms左右 磁盘速率 : 100M /s 64M 128M //让寻址时间占用读取时间的1%. 1ms 1 / 100 size = 181260798 block-0 : 134217728 block-1 : 47043070 -------------------- b0.no : 1073741829 b1.no : 1073741830 HAhigh availability,高可用性。通常用几个9衡量。 99.999% SPOF:single point of failure,单点故障。 secondarynamenode找到所有的配置文件1.tar开hadoop-2.7.3.tar.gz hadoop-2.7.3\share\hadoop\common\hadoop-common-2.7.3.jar\core-default.xml hadoop-2.7.3\share\hadoop\hdfs\hadoop-hdfs-2.7.3.jar\hdfs-default.xml hadoop-2.7.3\share\hadoop\mapreduce\hadoop-mapreduce-client-core-2.7.3.jar\mapred-default.xml hadoop-2.7.3\share\hadoop\yarn\hadoop-yarn-common-2.7.3.jar\yarn-site.xml 本地模式[core-site.xml] fs.defaultFS=file:/// //默认值 配置hadoop临时目录1.配置[core-site.xml]文件 &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://s201/&lt;/value&gt; &lt;/property&gt; &lt;!--- 配置新的本地目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/centos/hadoop&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; //以下属性均由hadoop.tmp.dir决定,在hdfs-site.xml文件中配置。 dfs.namenode.name.dir=file://${hadoop.tmp.dir}/dfs/name dfs.datanode.data.dir=file://${hadoop.tmp.dir}/dfs/data dfs.datanode.data.dir=file://${hadoop.tmp.dir}/dfs/data dfs.namenode.checkpoint.dir=file://${hadoop.tmp.dir}/dfs/namesecondary dfs.namenode.checkpoint.dir=file://${hadoop.tmp.dir}/dfs/namesecondary 2.分发core-site.xml文件 $&gt;xsync core-site.xml 3.格式化文件系统,只对namenode的本地目录进行初始化。 $&gt;hadoop namenode -format //hdfs namenode -format 4.启动hadoop $&gt;start-dfs.sh 使用xcall.sh在所有节点上创建jps符号连接，指向/soft/jdk/bin/jps1.切换到root用户 $&gt;su root 2.创建符号连接 $&gt;xcall.sh &quot;ln -sfT /soft/jdk/bin/jps /usr/local/bin/jps&quot; 3.修改jps符号连接的owner $&gt;xcall.sh &quot;chown -h centos:centos /usr/local/bin/jps&quot; 4.查看所有主机上的java进程 $&gt;xcall.sh jps 在centos桌面版中安装eclipse1.下载eclipse linux版 eclipse-jee-mars-R-linux-gtk-x86_64.tar.gz 2.tar开到/soft下, $&gt;tar -xzvf eclipse-jee-mars-R-linux-gtk-x86_64.tar.gz -C /soft 3.启动eclipse $&gt;cd /soft/eclipse $&gt;./eclipse &amp; //后台启动 4.创建桌面快捷方式 $&gt;ln -s /soft/eclipse/eclipse ~/Desktop/eclipse 5. 收集hadoop的所有jar包使用hadoop客户端api访问hdfs1.创建java项目 2.导入hadoop类库 3. 4. 5. 网络拓扑1. 2. 3. 4. 作业1.使用hadoop API递归输出整个文件系统 2.]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hadoop第二天之搭建]]></title>
    <url>%2F2018%2F09%2F29%2FHadoop%E7%AC%AC%E4%BA%8C%E5%A4%A9%E4%B9%8B%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[hadoop1.独立模式(standalone|local) nothing! 本地文件系统。 不需要启用单独进程。 2.pesudo(伪分布模式) 等同于完全分布式，只有一个节点。 SSH: //(Socket), //public + private //server : sshd ps -Af | grep sshd //clint : ssh //ssh-keygen:生成公私秘钥。 //authorized_keys 需要使用644 //ssh 192.168.231.201 yes [配置文件] core-site.xml //fs.defaultFS=hdfs://localhost/ hdfs-site.xml //replication=1 mapred-site.xml // yarn-site.xml // 3.full distributed(完全分布式) 让命令行提示符显式完整路径1.编辑profile文件，添加环境变量PS1 [/etc/profile] export PS1=&apos;[\u@\h `pwd`]\$&apos; 2.source $&gt;source /etc/profile 配置hadoop，使用符号连接的方式，让三种配置形态共存。1.创建三个配置目录,内容等同于hadoop目录 ${hadoop_home}/etc/local ${hadoop_home}/etc/pesudo ${hadoop_home}/etc/full 2.创建符号连接 $&gt;ln -s 3.对hdfs进行格式化 $&gt;hadoop namenode -format 4.修改hadoop配置文件，手动指定JAVA_HOME环境变量 [${hadoop_home}/etc/hadoop/hadoop-env.sh] ... export JAVA_HOME=/soft/jdk ... 5.启动hadoop的所有进程 $&gt;start-all.sh 6.启动完成后，出现以下进程 $&gt;jps 33702 NameNode 33792 DataNode 33954 SecondaryNameNode 29041 ResourceManager 34191 NodeManager 7.查看hdfs文件系统 $&gt;hdfs dfs -ls / 8.创建目录 $&gt;hdfs dfs -mkdir -p /user/centos/hadoop 9.通过webui查看hadoop的文件系统 http://localhost:50070/ 10.停止hadoop所有进程 $&gt;stop-all.sh 11.centos防火墙操作 [cnetos 6.5之前的版本] $&gt;sudo service firewalld stop //停止服务 $&gt;sudo service firewalld start //启动服务 $&gt;sudo service firewalld status //查看状态 [centos7] $&gt;sudo systemctl enable firewalld.service //&quot;开机启动&quot;启用 $&gt;sudo systemctl disable firewalld.service //&quot;开机自启&quot;禁用 $&gt;sudo systemctl start firewalld.service //启动防火墙 $&gt;sudo systemctl stop firewalld.service //停止防火墙 $&gt;sudo systemctl status firewalld.service //查看防火墙状态 [开机自启] $&gt;sudo chkconfig firewalld on //&quot;开启自启&quot;启用 $&gt;sudo chkconfig firewalld off //&quot;开启自启&quot;禁用 hadoop的端口50070 //namenode http port 50075 //datanode http port 50090 //2namenode http port 8020 //namenode rpc port 50010 //datanode rpc port hadoop四大模块common hdfs //namenode + datanode + secondarynamenode mapred yarn //resourcemanager + nodemanager 启动脚本1.start-all.sh //启动所有进程 2.stop-all.sh //停止所有进程 3.start-dfs.sh // 4.start-yarn.sh [hdfs] start-dfs.sh stop-dfs.sh NN DN 2NN [yarn] start-yarn.sh stop-yarn.sh RM NM 修改主机名1./etc/hostname s201 2./etc/hosts 127.0.0.1 localhost 192.168.231.201 s201 192.168.231.202 s202 192.168.231.203 s203 192.168.231.204 s204 完全分布式1.克隆3台client(centos7) 右键centos-7--&gt;管理-&gt;克隆-&gt; ... -&gt; 完整克隆 2.启动client 3.启用客户机共享文件夹。 4.修改hostname和ip地址文件 [/etc/hostname] s202 [/etc/sysconfig/network-scripts/ifcfg-ethxxxx] ... IPADDR=.. 5.重启网络服务 $&gt;sudo service network restart 6.修改/etc/resolv.conf文件 nameserver 192.168.231.2 7.重复以上3 ~ 6过程. 准备完全分布式主机的ssh1.删除所有主机上的/home/centos/.ssh/* 2.在s201主机上生成密钥对 $&gt;ssh-keygen -t rsa -P &apos;&apos; -f ~/.ssh/id_rsa 3.将s201的公钥文件id_rsa.pub远程复制到202 ~ 204主机上。 并放置/home/centos/.ssh/authorized_keys $&gt;scp id_rsa.pub centos@s201:/home/centos/.ssh/authorized_keys $&gt;scp id_rsa.pub centos@s202:/home/centos/.ssh/authorized_keys $&gt;scp id_rsa.pub centos@s203:/home/centos/.ssh/authorized_keys $&gt;scp id_rsa.pub centos@s204:/home/centos/.ssh/authorized_keys 4.配置完全分布式(${hadoop_home}/etc/hadoop/) [core-site.xml] &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://s201/&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; [hdfs-site.xml] &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; [mapred-site.xml] 不变 [yarn-site.xml] &lt;?xml version=&quot;1.0&quot;?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;s201&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; [slaves] s202 s203 s204 [hadoop-env.sh] ... export JAVA_HOME=/soft/jdk ... 5.分发配置 $&gt;cd /soft/hadoop/etc/ $&gt;scp -r full centos@s202:/soft/hadoop/etc/ $&gt;scp -r full centos@s203:/soft/hadoop/etc/ $&gt;scp -r full centos@s204:/soft/hadoop/etc/ 6.删除符号连接 $&gt;cd /soft/hadoop/etc $&gt;rm hadoop $&gt;ssh s202 rm /soft/hadoop/etc/hadoop $&gt;ssh s203 rm /soft/hadoop/etc/hadoop $&gt;ssh s204 rm /soft/hadoop/etc/hadoop 7.创建符号连接 $&gt;cd /soft/hadoop/etc/ $&gt;ln -s full hadoop $&gt;ssh s202 ln -s /soft/hadoop/etc/full /soft/hadoop/etc/hadoop $&gt;ssh s203 ln -s /soft/hadoop/etc/full /soft/hadoop/etc/hadoop $&gt;ssh s204 ln -s /soft/hadoop/etc/full /soft/hadoop/etc/hadoop 8.删除临时目录文件 $&gt;cd /tmp $&gt;rm -rf hadoop-centos $&gt;ssh s202 rm -rf /tmp/hadoop-centos $&gt;ssh s203 rm -rf /tmp/hadoop-centos $&gt;ssh s204 rm -rf /tmp/hadoop-centos 9.删除hadoop日志 $&gt;cd /soft/hadoop/logs $&gt;rm -rf * $&gt;ssh s202 rm -rf /soft/hadoop/logs/* $&gt;ssh s203 rm -rf /soft/hadoop/logs/* $&gt;ssh s204 rm -rf /soft/hadoop/logs/* 10.格式化文件系统 $&gt;hadoop namenode -format 11.启动hadoop进程 $&gt;start-all.sh rsync四个机器均安装rsync命令。 远程同步. $&gt;sudo yum install rsync 将root用户实现无密登录1.同 编写脚本1.xcall.sh 2.xsync.sh xsync.sh /home/etc/a.txt rsync -lr /home/etc/a.txt centos@s202:/home/etc netstat -anop 查看进程]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>搭建</tag>
        <tag>第二天</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop安装配置]]></title>
    <url>%2F2018%2F09%2F24%2FHadoop%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"></content>
      <categories>
        <category>-Hadoop</category>
      </categories>
      <tags>
        <tag>-hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux基础之Yum命令]]></title>
    <url>%2F2018%2F09%2F20%2FLinux%E5%9F%BA%E7%A1%80%E4%B9%8BYum%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[软件源Repository //仓库. URL //http:// .d //directory目录 xxxd //daemon 查看仓库文件/etc/yum.repos.d/xxx.repo curl传输url上的数据的。 [下载文件到指定目录] curl -o /etc/yum.repos.d/ali.repo http://mirrors.aliyun.com/repo/Centos-7.repo 更换centos的软件源1.下载源仓库文件,xxx.repo curl -o /etc/yum.repos.d/ali.repo http://mirrors.aliyun.com/repo/Centos-7.repo 2.将repo文件保存到/etc/yum.repos.d/目录中。 屏蔽软件仓库1.将/etc/yum.repos.d/xxx.repo文件删除或者更换扩展名即可。 修改centos能够使用sudo命令[/etc/sudoers] $&gt;su root $&gt;nano /etc/sudoers ... centos ALL 使用yum进行软件包安装卸载$&gt;yum list //列出所有软件包 $&gt;yum list installed //列出已经安装的软件包 $&gt;yum list installed | grep nano //列出已经安装的软件包 $&gt;yum search nano //在yum的软件源中搜索软件 $&gt;yum remove nano //卸载软件 $&gt;yum -y install nano //直接安装，不需要yes确认. $&gt;yum list installed | grep nano //查看是否安装了Nano $&gt;mkdir /home/centos/rpms $echo 以下命令只下载软件，不安装软件 $&gt;sudo yum install --downloadonly //只下载 --downloaddir=/home/centos/rpms //指定下载目录 wget //下载已经安装的软件 $&gt;sudo yum reinstall --downloadonly --downloaddir=/home/centos/rpms wget $&gt;sudo yum localinstall xxx.rpm //从本地rpm文件直接安装软件 $&gt;su root $&gt;yum search ifconfig $&gt;yum -y install net-tools //安装网络工具 #==========修改网络地址====================== //需要重启network服务 $&gt;sudo nano /etc/sysconfig/network-scripts/ifcfg-eth1677736 [/etc/sysconfig/network-scripts/ifcfg-eth1677736] ... IPADDR=192.168.231.201 GATEWAY=192.168.231.2 DNS=192.168.231.2 $&gt;service network restart //重启网络服务。 $&gt;sudo nano /etc/resolv.conf //修改该文件不需要重启network服务 [/etc/resolv.conf] nameserver 192.168.231.2 在没有nano时，使用自带的vi文本编辑器1.vi xx.txt 2.模式切换 esc //切换到命令模式,退出编辑模式 //:q! 不保存退出 //:wq 保存退出 //x 删除一个字符 //dd 删除一行 insert //切换到编辑模式,退出命令模式 //del backspace Which命令which命令用于查找并显示给定命令的绝对路径，环境变量PATH中保存了查找命令时需要遍历的目录。 which指令会在环境变量$PATH设置的目录里查找符合条件的文件。也就是说，使用which命令，就可以看到某个系统命令是否存在，以及执行的到底是哪一个位置的命令。 语法which(选项)(参数)]]></content>
      <tags>
        <tag>Linux基础</tag>
        <tag>Yum</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux基础第之循环命令]]></title>
    <url>%2F2018%2F09%2F19%2FLinux%E5%9F%BA%E7%A1%80%E7%AC%AC%E4%B9%8B%E5%BE%AA%E7%8E%AF%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[访问文件(夹)物理位置pwd命令是显示当前的逻辑位置。 物理位置是针对符号连接也是针对软连接的。 //进入/t的物理位置 $&gt;cd -P /t //显式当前目录的物理位置 $&gt;pwd -P 访问环境变量echo ${PATH} //ok echo $PATH //ok echo &quot;$PATH&quot; //ok echo &apos;$PATH&apos; //&apos;&apos;原样输出这个不行，显示“PATH” export定义环境变量,只在session中有效 (当前会话)$&gt;export name=${PATH}:tom 设置name为${Var1}的值，Var1没有设置为${Var2}的值。 $&gt;export name=${Var1:-${Var2}} 命令执行过程$? //命令的返回值存储变量,0:成功 1:失败。 $# //参数个数 $1 //第几个参数 $0 //当前脚本(命令)名称 $@ //取出所有参数 shift //参数左移 ${a/b/c} // 下面一个例子 向左移位解释： if 命令讲解语法: 中括号是可以选择的： if COMMANDS; then COMMANDS; [ elif COMMANDS; then COMMANDS; ]... [ else COMMANDS; ] fi if [ $# -lt 3 ]; then xx ; fi 3,5 使用for循环输出1 - 100个数看一下for的帮助文档，注意从冒号之后是开始的： for NAME [in WORDS ... ] ; do COMMANDS; done for x in a b c d ; do echo $x ; done ; 通过for循环打印一个三角形：首先看一下 然后这段是命令： while语法for: for NAME [in WORDS ... ] ; do COMMANDS; done for ((: for (( exp1; exp2; exp3 )); do COMMANDS; done 一个例子： #!/bin/bash ((i=0)) while ((i&lt;100)) ; do echo $i; i=$((i+1)) done]]></content>
      <tags>
        <tag>linux</tag>
        <tag>循环命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux基础之hostname]]></title>
    <url>%2F2018%2F09%2F19%2FLinux%E5%9F%BA%E7%A1%80%E4%B9%8Bhostname%2F</url>
    <content type="text"><![CDATA[命令嵌套使用$&gt;echo `cat b.txt` //命令解析,无法嵌套 $&gt;$(... $()) //支持命令的嵌套 创建用户0.用户和组之间，一个用户可以属于多个组。 但是有一个首要组。 1.adduser,等同于useradd 符号链接。 /usr/sbin/adduser --&gt; /usr/sbin/useradd. 2.useradd 输入新密码. 重复输入 $&gt;su root $&gt;useradd -m centos2 $&gt;su root $&gt;passwd centos2 -m, --create-home create the user&apos;s home directory -p, --password PASSWORD encrypted password of the new account 3.使用方法 $&gt;su root $&gt;userdel -r centos2 //用户所在组目录也会被删除.在删除用户时候要用exit退出要删除的用户,删除的时候可能会exit好多次，因为会来回su。]]></content>
      <tags>
        <tag>Linux基础</tag>
        <tag>基础命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux基础之进程查看]]></title>
    <url>%2F2018%2F09%2F18%2FLinux%E5%9F%BA%E7%A1%80%E4%B9%8B%E8%BF%9B%E7%A8%8B%E6%9F%A5%E7%9C%8B%2F</url>
    <content type="text"><![CDATA[job 放到后台运行的进程.1.将程序放到后台运行,以&amp;结尾. $&gt;nano b.txt &amp; 2.查看后台运行的jobs数 $&gt;jobs 3.切换后台作业到前台来. $&gt;fg %n //n是job编号. 4.前台正在的进程，放到后台。 ctrl + z 5.让后作业运行 $&gt;bg %1 // 6.杀死作业 $&gt;kill %1 // man + 命令 ：查看该命令详细帮助 进程查看,prcess show $&gt;ps -Af |grep gnome //-A:所有进程 -f:所有列格式. $&gt;top //动态显示进程信息。含有cpu、内存的使用情况. //q,按照q退出。 cut剪切显示文件的每一行。 $&gt;cut -c 1-5 a.txt //从第一个字符开始,下标从1开始。 $&gt;ps -Af | cut -c 45-80 | more //吧PS里面获得的内容剪切显示，显示没一行的45-80，翻页查看 查看帮助$&gt;help //查看os内置的命令 $&gt;man ifcon fig //查看特定命令 $&gt;ifconfig --help $&gt;ifconfig -h $&gt;info ifconfig // 磁盘分区使用$&gt;fdisk -l /dev/sda 里面的中括号是可选的，尖括号是必须要写的 里面sad是磁盘，sda1是分区。sd1，sd2，sd3是磁盘的三个分区。 查看磁盘使用情况(disk free)$&gt;df -ah /home/centos //查看 dirname取出指定地址的上级目录. $&gt;dirname /a/b/c/d $&gt;/a/b/c basename取出当前地址的上级目录. $&gt;dirname /a/b/c/d $&gt;d 主机名$&gt;hostname //显式主机名 $&gt;修改主机名(sudo) [/etc/hostname] s200 关机重启命令$&gt;reboot //重启 $&gt;halt //停止,黑屏 //halt -p === poweroff //halt -r === reboot $&gt;poweroff //关机 $&gt;shutdown //shutdown now, 命令嵌套1.使用 $&gt;echo `cat b.txt` //命令解析,无法嵌套 $&gt;$(... $()) //支持命令的嵌套 2.]]></content>
      <tags>
        <tag>Linux基础</tag>
        <tag>进程查看</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux基础之符号连接]]></title>
    <url>%2F2018%2F09%2F18%2FLinux%E5%9F%BA%E7%A1%80%E4%B9%8B%E7%AC%A6%E5%8F%B7%E8%BF%9E%E6%8E%A5%2F</url>
    <content type="text"><![CDATA[权限 r //100 = 4 //文件 :读取内容， //文件夹:是查看文件夹的内容 w //文件 :写数据到文件 //文件夹:增删文件. //10 = 2 x //文件 : 运行程序 //文件夹: 进入该目录. // 1 = 1 权限控制涉及范围 U:user ,rwx r-x --- G:group , O:other , 修改文件的owner,change owner chown -R root:root a.txt //递归修改owner chmod -R 777 xxx //递归修改权限. -R :递归显示 -l :列表显示 通过递归改变整个文件夹里面的文件的权限和所有者和所在组 chown -R root:root tmp Linux文件夹 / //根目录 /bin //祖先 /sbin //祖先 /usr/bin //厂商 /usr/sbin //厂商 /usr/local/bin //用户 /usr/local/sbin //用户 /etc //配置目录 /mnt //挂载目录 /boot //引导目录 /dev //设备目录 /lib[64] //库目录 -:文件 d:目录 l:link 等价于windows快捷方式 b:block,块设备 c:charactor,字符文件 创建连接文件 1.硬链接两个完全相同文件，类似于实时备份。两个文件之间完全同步。删除时，只删一个。 目录不能使用硬链接。 ln a.txt alink //a.txt:目标文件, alink:连接名称. ln b.txt b_lnk //硬链接，修改连接文件，源文件也改变，但是删除连接文件源文件不被删除。 硬链接用的很少，大多使用符号连接 mv b_link b.txt 同一目录下改名字就用移动命令即可 2.符号连接-软连接相当于快捷方式. 可以对文件，也可以对文件夹创建符号连接。 符号连接存在的时候，可以删除目标文件。 $&gt;ln -s a.txt alink //a.txt: 目标文件 alink:连接名称(symbolic) blk存放的是路径的字节数大小。开始的时候只想本目录下的b.txt所以是5个字节，后来只想一个绝对路径就变成了28个字符。例子如下图所示 删除掉了链接指向的文件，就会变成红色。删除后在加上就回复正常变成了浅蓝色。 sudo 临时借用root的权限执行命令,只在当前命令下有效。命令结束后，还是原来用户。 1.配置当前用户具有sudo的执行权利 [/etc/sudoers] ... root ALL=(ALL) ALL centos ALL=(ALL) ALL ... $&gt;sudo chown -R centos:centos . 临时切换超级管理员权限 sudo 切换到另外一个用户 su]]></content>
      <tags>
        <tag>linux基础</tag>
        <tag>符号连接</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux基础之虚拟机增强工具的安装-文本模式的安装]]></title>
    <url>%2F2018%2F09%2F18%2FLinux%E5%9F%BA%E7%A1%80%E4%B9%8B%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%A2%9E%E5%BC%BA%E5%B7%A5%E5%85%B7%E7%9A%84%E5%AE%89%E8%A3%85-%E6%96%87%E6%9C%AC%E6%A8%A1%E5%BC%8F%E7%9A%84%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[虚拟机增强工具 1.原理 插入iso(C:\myprograms\vmwar11.0.4-x86_64\linux.iso)文件到光盘中。 2.vmware虚拟机菜单 -&gt; 重新安装vmware-tools 3.自动会将C:\myprograms\vmwar11.0.4-x86_64\linux.iso镜像文件插入光驱中，并直接打开。 4.复制VMwareTools-9.9.3-2759765.tar.gz文件到centos的桌面下。 5.tar开该文件. 鼠标右键点击桌面的tar.gz文件，选择 extract here. 6.进入桌面的vmware-tools-distrib目录下. $&gt;su root $&gt;cd /home/centos/Desktop/vmware-tools-distrib 7.执行安装脚本 $&gt;./vmware-install.pl 一路回车。 只到遇到Enjoy!!... 图片为在xshell里面执行。在Mini中执行虚拟机增强的方法： 在mini版的centos7下遇到了很多问题。又在大坑系列中也有这一部分。其实这部分遇到了很多问题，除了视频中的问题，按照如下所示可以完美解决： 为了实现文件夹的共享，要安装vmtools。现在的VMware是11版本。Centos是1151版本。 然后按照视频上讲解的来安装不上。最后先是 这样子，然后 之后按照如图所示挂载即可 本视频还有一个while循环实现99乘法表的例子（我没细看）：]]></content>
  </entry>
  <entry>
    <title><![CDATA[Linux基础基础—————踩过的大坑]]></title>
    <url>%2F2018%2F09%2F18%2FLinux%E5%9F%BA%E7%A1%80%E5%9F%BA%E7%A1%80%E2%80%94%E2%80%94%E2%80%94%E8%B8%A9%E8%BF%87%E7%9A%84%E5%A4%A7%E5%9D%91%2F</url>
    <content type="text"><![CDATA[问题：图形界面浏览器能访问外网，但是ping不通很奇怪的问题，配置都配置好好的跟着视频一步一步走下来的，按理说应该没问题。但是。。。。 解决：将网络中心里面改成这样子就行了。 具体的网站解释：https://blog.csdn.net/Arnold_lee_yc/article/details/74785995 不通CentOS版本的目录都不太一样。centos7和7.3的目录结构都不同。所以本教程使用的是7.1也就是1503版本的centos 切换用户之后出现这种情况： 解决办法：是在设置centos用户的权限的时候配置错了。修改回来即可 su root vi sudoers 视频里面的人他的ALL写成了小写的l。配置错误 安装VMtools工具为了实现文件夹的共享，要安装vmtools。现在的VMware是11版本。Centos是1151版本。我的理解是需要将那个文件夹挂载到虚拟机上。才能实现共享，可能和VMware的版本有关。网上说vmware14可以直接挂载不需要设置，这里我使用的是vmware11 然后按照视频上讲解的来安装不上。最后先是 这样子，然后 之后按照如图所示挂载即可 克隆机器后网卡设置克隆机器后会出现设置ifg-eno123142这个文件但是设置之后Ip还是没有修改的问题。实际上是因为克隆机器后ifg-eno123123的网卡名字变成了ifcfg-ens33这个这个名字，所以需要把ifg-eno12312这个网卡改名ifg-ens33然后把里面的name和device两个都改成ens33即可。 看这张图下面的网卡名字是ens33，在网卡配置中就要在name和device中写ens33 配置无密登陆在按照视频生成秘钥和公钥的时候，开始的时候删掉的其他机器的.ssh文件。所以导致传输公钥的时候，传输完了之后仍然需要密码。这是由于其他机器的.ssh文件的权限有问题，因为是删除了之后自己重新建立的。所以默认的权限是不对的。这里的网站讲解的非常详细 将ssh文件夹配置成700的属性即可。 https://blog.csdn.net/qq_26442553/article/details/79357498 多次format namenode会导致namenode的id和datanode的id有变化，需要重新格式化/tmp文件夹 在windows上跑mr之前视频上没有去说明在windows上配置hadoop，自己去CSDN里面查找才发现下面是CSDN里面原话： 1.hadoop官网上下载hadoop2.7.2.tar.gz 并且配置成环境变量 开始之前必须配置本地的hadoop环境 HADOOP_HOME=H:\source\hadoop\hadoop-2.7.2 PATH中增加 %HADOOP_HOME%\bin 配置完成后，通过cmd 执行hadoop 如果能够成功证明环境配置完成。 2.下载windows-hadoop-bin的压缩包(windows下运行MR 必备的) bin2.7.2 包我会提供出来 bin2.7.2 这个是windows-hadoop-bin 的压缩包，解压完了后用解压的bin包替换成hadoop-2.7.2 里面的bin。 注意是替换 3.将解压出来bin目录中的hadoop.dll也放入C:\Windows\System32(最好操作) 4.1901 这个是天气的测试数据包(后续解压在hadoop MR 的输入文件夹中) 5.windows运行时中可能出现的错误 No valid local directories in property: mapreduce.cluster.local.dir 如果出现这个错误，可以在代码中通过这个配置和在本地的hadoop目录下中建立data ======例如我的 H:\source\hadoop\hadoop-2.7.2\data==== Job job = new Job(); Configuration conf = job.getConfiguration(); conf.set(&quot;mapreduce.cluster.local.dir&quot;,&quot;H:\\source\\hadoop\\hadoop-2.7.2\\data&quot;); job=new Job(conf); 6.Winodws 运行出现 解决org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z 点击打开链接 大致流程，去官网下载src源码，将源码中nativeIo.java复制在本地项目中（新建源码中nativeIo.java的全路径,将其放入即可），修改源码中对windows的IO验证 Access方法 7.如果在集群运行时出现 No valid local directories in property: mapreduce.cluster.local.dir 查看是否采用的本地的localrunnal，也就是说mapred-site.xml和yarn-site.xml是否被加载.一般是这个原因，因为2.x版本上的defalut-site.xml已经配置该属性 8.集群提交时采用hadoop和yarn jar xxx.jar XXX agrs1 agr2 提交。加载的logj配置文件也可以作为参数传入。 9.提交集群时，如果在mapper或者reduce存在着构建一些引用包的对象，那么提交的xxx.jar必须是被打成依赖包，因为MR执行是将jar包分发其他的节点。所以不能像普通的java进程采用 java -cp x.jar 等方式。 10.IDEA 打依赖包是，不能通过Artifrt右边的put into 左边的项目。必须在选择时，采用jar-with-dependents的方式 两种的区别在于，第一种仅仅是将jar放入压缩的jar文件中。第二种是，采用依赖包的方式，才能识别具体的引用类。 11.本地还需要配置log4j的配置文件，查看具体的日志12. hadoop-2.7.2的tag.gz 包 13.windows-hadoop-bin 的包 具体的hadoop2.7.2tat.gz和windows下编译的bin包已经放到小号的百度网盘里面了。并且在chrome大数据技术2书签里面也有。]]></content>
      <tags>
        <tag>Linux基础</tag>
        <tag>大坑</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux基础之网络配置-域名解析-光驱挂载]]></title>
    <url>%2F2018%2F09%2F17%2FLinux%E5%9F%BA%E7%A1%80%E4%B9%8B%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE-%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90-%E5%85%89%E9%A9%B1%E6%8C%82%E8%BD%BD%2F</url>
    <content type="text"><![CDATA[[客户端与宿主机之间的网络连通方式]1.桥接桥接(client完全等价于一台物理主机) 2.NAT(最多,默认模式)a.Net Address transform,网络地址转换. b.客户机能访问外网，可以访问局域网内的其他物理主机。 c.其他的局域网内物理主机不能访问客户机。 3.only host.a.和NAT非常像。 b.不能访问外网。 4.查看client机的网络连接模式a.右键选择Centos客户机。 b.点击&quot;设置&quot; c.网络适配器. 5.查看DHCP的分配网段a.vmware--&gt;编辑--&gt;虚拟网络编辑器 b.选中V8条目 c.下方显示的V8的详细信息。 d.点击DHCP的设置. e.查看分配网段. [修改静态IP]1.切换root用户$&gt;su root 2.编辑/etc/sysconfig/network-scripts/ifcfg-eno16777736a.备份文件 $&gt;cd /etc/sysconfig/network-scripts $&gt;cp ifcfg-eno16777736 ifcfg-eno16777736.bak b.进入/etc/sysconfig/network-scripts $&gt;cd /etc/sysconfig/network-scripts c.编辑ifcfg-eno16777736文件 $&gt;nano ifcfg-eno16777736 TYPE=Ethernet BOOTPROTO=none DEFROUTE=yes IPV4_FAILURE_FATAL=no IPV6INIT=no IPV6_AUTOCONF=no IPV6_DEFROUTE=no IPV6_PEERDNS=no IPV6_PEERROUTES=no IPV6_FAILURE_FATAL=no NAME=eno16777736 UUID=33f3ce5f-8b5c-41af-90ed-863736e09c29 DEVICE=eno16777736 ONBOOT=yes IPADDR=192.168.231.200 PREFIX=24 GATEWAY=192.168.231.2 DNS=192.168.231.2 注意:查看NAT网络的网关地址。 0)Client机的网卡的DNS和GATEWAY设置为虚拟网卡NAT的网关值。 1)vmware--&gt;编辑--&gt;虚拟网路编辑器 2)v8条目 3)点击&quot;NAT设置&quot;按钮 4)查看网关地址:192.168.231.2(通常为xxx.xxx.xxx.2) e.重启网络服务 $&gt;su root $&gt;service network restart f.解决通过ip能够访问网络，通过域名无法访问的问题。 1)编辑/etc/resolv.conf,添加名称服务器，内容是网关地址。 nameserver 192.168.231.2 2)保存退出 3)重启服务 $&gt;su root $&gt;service network restart 4)测试www.baidu.com $&gt;ping www.baidu.com service管理命令 1.查看服务的状态$&gt;service server_name status //语法 $&gt;service network status $&gt;service network start //启动 $&gt;service network stop //停止 $&gt;service network restart //重启 里面的lo网卡是自回环网络lopback（音似）还有一个就是局域网网卡 其中的ifcfg-lo就是自回环网络 打开里面的内容查看 mount挂载外设 1.右键client右下角的光盘图标 -&gt;设置 2.iso文件 选择一个iso镜像文件。 3.右键client右下角的光盘图标 -&gt;连接. 4.创建文件夹/mnt/cdrom $&gt;su root $&gt;mkdir cdrom 5.挂载光驱/dev/cdrom到/mnt/cdrom $&gt;mount /dev/cdrom /mnt/cdrom $&gt;find . /mnt/cdrom 卸载外设 1.从挂载的目录中出来,否则出现设备繁忙 $&gt;cd .. 2.使用umount进行卸载 $&gt;umount /mnt/cdrom 启用client和host之间共享目录的功能 1.右键点击vmware中的client机，选择设置 2.找到”选项” -&gt; “共享文件夹” 3.选择”总是启用” 4.在文件夹区域中添加要共享的目录 d:/downloads 5.确定. 6.重启客户机.]]></content>
  </entry>
  <entry>
    <title><![CDATA[Linux基础之网络连接模式]]></title>
    <url>%2F2018%2F09%2F16%2FLinux%E5%9F%BA%E7%A1%80%E4%B9%8B%E7%BD%91%E7%BB%9C%E8%BF%9E%E6%8E%A5%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Linux基础之文件类型-权限]]></title>
    <url>%2F2018%2F09%2F16%2FLinux%E5%9F%BA%E7%A1%80%E4%B9%8B%E6%96%87%E4%BB%B6%E7%B1%BB%E5%9E%8B-%E6%9D%83%E9%99%90%2F</url>
    <content type="text"><![CDATA[目录和权限 [windows] 以磁盘分区物理结构作为文件系统结构 每个用户在/home下面都有一个自己的家目录比如centos的家目录在/home/centos但是root是管理员比较特殊root的目录在/root下~是home的子一级 ‘ [centos] 逻辑结构./ //文件系统的根. /bin //最初的命令(祖先)，binary文件,可执行文件 /sbin //super binary(重要性高) /usr/bin //厂商相关的命令,ubuntu /usr/sbin //厂商相关的命令,ubuntu /usr/local/bin //用户级别。 /usr/local/sbin [Linux文件类型] - //文件 d //目录 l //链接,类似于windows快捷方式. b //block,块文件。 c //字符文件 第一个字母是L的意思是link连接类的。 ifconfig的命令的目录在/sbin/ifconfig这里，但是实际上/sbin已经链接到了/usr/bin里面了。所以CentOS是没有bin和、usr/bin之分的，因为已经link过去了 [linux的权限]一共有9个位，每个成分是从0-7如果是777就是全部权限都付给他了 看这段截图。里面的-rw-rw-r–第一个-是文件类型第rw后面的-是代表0 $&gt;chmod //修改文件(夹)权限 $&gt;chmod g-w //去除group中write权. chmod //不受文件权限控制,只有owner和root才具有文件权限的修改权。 【read权限】 文件 :文件内容 文件夹 :文件夹的内容 【write权限】 ------------ 【execute权限】 ------------- 文件 :执行 文件夹 :进入目录 看下面这个例子： 看最下面的这个chmode 644 a.txt 6是用户的成分。4是组的成分，最后4，是others的成分。所以chmode 644最后是 -rw-r–r– 这里面的小细节格式 ： chmode 655 a.txt chmode g+w a.txt 文本输入格式他的偏移量是Longwritable,他的值是value是一行，在定义采样器的时候，是intwritable。要是做采样要对产生年份和温度采样。如果是文本输入格式就是文本偏移量和value。所以要把采样器采取的样本换成kv的值，而不是文本的偏移量和文本，所以对数据改造，改成序列文件，序列文件就可以读成kv了。要把输入文件改成序列文件，才可以采到他的值。 package com.it18zhang.hdfs.maxtemp; import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;import org.apache.hadoop.mapreduce.lib.partition.InputSampler;import org.apache.hadoop.mapreduce.lib.partition.TotalOrderPartitioner; public class MaxTempApp { public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS&quot;, &quot;file:///&quot;); Job job = Job.getInstance(conf); // 设置作业的各种属性 job.setJobName(“MaxTempApp”); //作业名称 job.setJarByClass(MaxTempApp.class); //搜索类路径 job.setInputFormatClass(SequenceFileInputFormat.class);//设置输入格式类 //添加输入路径 FileInputFormat.addInputPath(job, new Path(args[0])); //设置输出路径 org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(args[1])); job.setMapperClass(MaxTempMapper.class); //mapper类 job.setReducerClass(MaxTempReducer.class); //reduce类 job.setMapOutputKeyClass(IntWritable.class); job.setMapOutputValueClass(IntWritable.class); job.setOutputKeyClass(IntWritable.class); //设置输出类型 job.setOutputValueClass(IntWritable.class); //设置 全排序分区类 job.setPartitionerClass(TotalOrderPartitioner.class); //将sample数据 写入分区文件 TotalOrderPartitioner.setPartitionFile(conf, new Path(&quot;e:/mr/par.lst&quot;)); //创建随机采样器对象 InputSampler.Sampler&lt;IntWritable, IntWritable&gt; sampler = new InputSampler.RandomSampler&lt;IntWritable, IntWritable&gt;(0.1, 10000, 10); job.setNumReduceTasks(3); //reduce个数 InputSampler.writePartitionFile(job, sampler); job.waitForCompletion(true); } } public class MaxTempMapper extends Mapper&lt;IntWritable, IntWritable, IntWritable, IntWritable&gt; { public MaxTempMapper() { System.out.println(&quot;new MaxTempMapper&quot;); } @Override protected void map(IntWritable key, IntWritable value, Context context) throws IOException, InterruptedException { context.write(key,value); } } public class MaxTempReducer extends Reducer&lt;IntWritable, IntWritable, IntWritable, IntWritable&gt;{ /** * reduce */ protected void reduce(IntWritable key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException { int max =Integer.MIN_VALUE; for(IntWritable iw : values){ max = max &gt; iw.get() ? max : iw.get() ; } context.write(key,new IntWritable(max)); } } public class YearPartitioner extends Partitioner&lt;IntWritable, IntWritable&gt; { public int getPartition(IntWritable year, IntWritable temp, int parts) { int y = year.get() - 1970; if (y &lt; 33) { return 0; } if (y &gt; 33 &amp;&amp; y &lt; 66) { return 1; } else { return 2; } } } 上图在运行的时候要注意的问题： 1.Sequencen的输入是intwriteble和intwritable输入的模式 2.在MaxTempApp里面的创建随机采样器和设置reduce个数，全排序分区类，sampler写入分区文件是有顺序的，看源码可以找到顺序 2.5这个地方有一个设置分区个数的问题，如果是设置到了采样个数之后，那么就不需要采样了，采样的目的就是为了把分区设置2条分界线，要预先知道分区有几个，然后才能设置好分界线。 3.在后面用到conf的时候，已经不再是之前的conf了，是有修改了的所以要用job.configuration。如下图要变成这样。这边的job.configuration是做了一个拷贝，并不是操纵conf的。在前面写着有Job job = Job.getInstance(conf);在这句话里面，又new了一个新的conf。所以这个conf并不是原来的new的那个conf。所以要用job.configuration()这样的语句，如果用conf就找不到原来文件的位置了。]]></content>
      <tags>
        <tag>Linux基础</tag>
        <tag>文件类型权限</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux基础]]></title>
    <url>%2F2018%2F09%2F16%2FLinux%E5%9F%BA%E7%A1%80%E4%B9%8BCentOS%E5%91%BD%E4%BB%A4%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[CentOS Windows $&gt;ls cmd&gt;dir // $&gt;ls --help //查看命令帮助 $&gt;man ls //查看命令帮助 $&gt;clear cmd&gt;cls //清屏 $&gt;cd /home cmd&gt;cd d:/ //切换目录 $&gt;cd . cmd&gt;cd . //进入当前目录 $&gt;cd .. cmd&gt;cd .. //进入上级目录 $&gt;cd /home/centos cmd&gt;cd d:/xx/x/x //进入绝对路径 $&gt;pwd //显式当前目录 $&gt;whoami //显式当前用户名 $&gt;su root //切换用户,输入密码,swith user $&gt;passwd //修改当前用户的密码 $&gt;ifconfig cmd&gt;ipconfig //查看ip地址 $&gt;ping localhost cmd&gt;ping localhost //查看网络连通情况 $&gt;ping www.baidu.com cmd&gt;ping www.baidu.com //查看网络连通情况 $&gt;启动桌面版的网卡 on. $&gt;su centos // $&gt;cd /home/centos // $&gt;cd ~ //回到主目录 $&gt;cd - //回到上次的目录 $&gt;ll //别名,ls -l --autocolor... $&gt;alias //查看所有的别名 $&gt;ls -a -l -h //查看当前目录-h:人性化 -l:列表 -a:显式.开头 $&gt;mkdir ~/Downloads/a //创建目录 $&gt;touch ~/Downloads/a/1.txt //创建文件 $&gt;echo helloworld &gt; 1.txt //重定向输出(覆盖) $&gt;echo helloworld &gt;&gt; 1.txt //重定向输出(追加模式) $&gt;cat 1.txt cmd&gt;type a.txt //查看文件 $&gt;cp 1.txt 2.txt //复制文件 $&gt;rm 1.txt //删除文件 $&gt;rm -rf / //强行递归删除 $&gt;mv a.txt tmp/ //强行递归删除 [centos client中切换模式] ctrl + alt + f6 //切换到文本模式 ctrl + alt //切换鼠标 ctrl + alt + f1 //切换桌面模式. ctrl + alt + f5 //切换到新的文本模式 [nano文本编辑器,命令行模式] $&gt;nano a.txt //打开nano编辑器，编辑a.txt文件 $&gt;.... $&gt;ctrl + o //保存文件,提示后直接回车 $&gt;ctrl + x //退出文件 $&gt;ctrl + k //cut 文本 $&gt;ctrl + u //cut 文本 $&gt;more a.txt //分屏显式 q:退出 h:帮助 $&gt;more -5 a.txt //显式前5行内容 $&gt;tail a.txt //最后10行内容 $&gt;find . | more // | 是管道符，前面的命令的输出作为后面命令输入。 $&gt;find ~ $&gt;ls -aR ~ //递归显式主目录所有的文件.(a表示包含.开头的文件) $&gt;head a.txt //显式前10行 $&gt;head -n 10 a.txt //显式前10行 $&gt;head -10 a.txt //显式前10行 $&gt;tail a.txt $&gt;tail -n 20 a.txt $&gt;tail -20 a.txt $&gt;tail --lines=20 a.txt $&gt;wc -c -l -w a.txt //统计文本信息, //显式统计信息-c:字节 -l:line -w:word $&gt;hostname //查看主机名称 $&gt;uname -r //查看系统内核 $&gt;uname -a //查看系统内核 $&gt;uname -p //查看系统内核 $&gt;uname -m //查看系统内核 $&gt;file xxx.xx //查看文件类型 $&gt;gzip a.txt //原地压缩 $&gt;gzip -d a.txt //原地压缩 $&gt;gzip -dr tmp //递归操纵文件夹下的文件 $&gt;gunzip a.txt.gz //等价于gzip -d a.txt $&gt;tar -cvf my.tar 1.txt tmp //创建归档文件 $&gt;tar -vxf my.tar //解档文件 把多个文件保存到一个文件，也可以从归档文件恢复到单个文件 -c create创建 -f 归档文件名 -vf 列出所有文件在。。里面 -xf 从。。里面抽取所有文件。 —r 追加 -cf 创建 例子 tar -cf my.tar a.txt ：将a.txt归档压缩到my.tar tar -xvf my.tar a.txt: 抽取my.tar里面的a.txt tar -cf my.tar a.txt tmp/ :将a.txt和tmp/都压缩到my.tar里面 xargs 在&apos;xargs&apos;加单引号把他识别为一个命令。 $&gt;find . | grep txt | cp `xargs` temp //xargs是多行变单行，使用空格替换回车换行符. //`` : 是强制命令解析。 反引号’’问题 $&gt;ping `cat a.txt` //命令嵌套 $&gt;which echo //查看命令的文件路径]]></content>
      <tags>
        <tag>linux</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[京东实战项目之hive实战（1）]]></title>
    <url>%2F2018%2F09%2F14%2F%E4%BA%AC%E4%B8%9C%E5%AE%9E%E6%88%98%E9%A1%B9%E7%9B%AE%E4%B9%8Bhive%E5%AE%9E%E6%88%98%EF%BC%881%EF%BC%89%2F</url>
    <content type="text"><![CDATA[先熟悉一下Hive的基本语句： 首先里面的库有这么几个表， desc formatted ods_order; 看到表有几个信息 可以看到表的字段的信息。 同时可以看见表的是否是分区表partition information Location:可以看见路径，数据是在哪里的。 table Type: 是否是内部表 可以通过Hive看到进入到dfs里面的路径 可以看见里面文件的大小 通过explain 来看他的hive是怎么走的，看他的执行计划。 观察一个表的分区信息： 查看hadoop正在运行的任务 yarn application -list show functions;//显示hive里所有函数desc function extended add_months//显示里面具体的函数的用法 有时候desc formatted ods_order;里面显示的路径可能会是假的路径。在hadoop搭建的时候配置出问题，就会出现假的路径。 通过命令 desc extended ods_order partition(dt=20151010); //通过这个命令可以找到库表的实际路径。其中的dt是通过 show partition ado_order;来找到的。 以前的0.1之前有些扫描数据的时候默认是不开启mapreduce的。在select的时候是不开启mapreduce的。如果是少量的数据可以直接扫描出来的。但是表的数据非常大，如果不主动开启reduce执行，那么需要手动执行。通过上图来操作。设置是否开启mapreduce来执行。]]></content>
      <tags>
        <tag>hive</tag>
        <tag>实战项目</tag>
        <tag>基本hive语句</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java基础学习笔记]]></title>
    <url>%2F2018%2F09%2F12%2FJava%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Java中的总结合集（1）]]></title>
    <url>%2F2018%2F09%2F12%2FJava%E4%B8%AD%E7%9A%84%E6%80%BB%E7%BB%93%E5%90%88%E9%9B%86%EF%BC%881%EF%BC%89%2F</url>
    <content type="text"><![CDATA[关于构造函数构造函数本身是没有返回值的。但是不能加void，一旦加了void就变成了一个函数。 public class sadf { public static void main(String[] args) { Cat cat = new Cat(); cat.cry(); } } abstract class Animal{ public String name; public abstract void cry(); public void Animal() { System.out.println(&quot;new Animal&quot;); } } class Cat extends Animal{ public void cry() { System.out.println(&quot;喵喵12345 &#125; &#125;这样的函数运行之后是 喵喵 因为Animal有void变成了一个函数。还有一个默认的空构造。 应该去掉void。 public class sadf { public static void main(String[] args) { Cat cat = new Cat(); cat.cry(); } } abstract class Animal{ public String name; public abstract void cry(); public Animal() { System.out.println(&quot;new Animal&quot;); } } class Cat extends Animal{ public void cry() { System.out.println(&quot;喵喵1234567 &#125; &#125;***结果是： new Animal 喵喵 关于抽象abstractclass ABSTACT { public static void main(String[] args) { JiaFeiCat j = new JiaFeiCat(); } } abstract class Animal { abstract void cry(); public Animal() { System.out.println(&quot;我是动物&quot;); } } abstract class Cat extends Animal { public Cat() { System.out.println(&quot;我是猫&quot;); } final void catchMouse() { System.out.println(&quot;猫能抓老鼠&quot;); } } final class JiaFeiCat extends Cat { public JiaFeiCat() { System.out.println(&quot;我是加菲猫&quot;); } @Override void cry() { System.out.println(&quot;加菲猫会哭&quot;); } } class BosiCat extends Cat { public BosiCat() { System.out.println(&quot;我叫波斯猫&quot;); } @Override void cry() { System.out.println(&quot;波斯猫会叫&quot;); } } 打印结果： 我是动物 我是猫 我是加菲猫 运行成功abstact抽象类方法可以在子类的子类中继承即可，不用一定在第一代子类中继承。]]></content>
      <tags>
        <tag>Java基础问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop权威指南学习笔记（1）——关于JS在Hadoop里面函数]]></title>
    <url>%2F2018%2F09%2F11%2FHadoop%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%881%EF%BC%89%2F</url>
    <content type="text"><![CDATA[先看一段代码 package lianxi; import java.io.IOException; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapred.MapReduceBase; import org.apache.hadoop.mapred.OutputCollector; import org.apache.hadoop.mapred.Reporter; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.lib.map.WrappedMapper.Context; public class MaxTemperatureMapper extends MapReduceBase implements org.apache.hadoop.mapred.Mapper&lt;LongWritable, Text, Text, IntWritable&gt; { private static final int MISSING = 9999; public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { String line = value.toString(); String year = line.substring(15, 19); int airTemperature; if (line.charAt(87) == &apos;+&apos;) {// parseInt doesn&apos;t like leading plus signs airTemperature = Integer.parseInt(line.substring(88, 92)); } else { airTemperature = Integer.parseInt(line.substring(87, 92)); } String quality = line.substring(92, 93); if (airTemperature != MISSING &amp;&amp; quality.matches(&quot;[01459]&quot;)) { context.write(new Text(year), new IntWritable(airTemperature)); } } @Override public void map(LongWritable arg0, Text arg1, OutputCollector&lt;Text, IntWritable&gt; arg2, Reporter arg3) throws IOException { // TODO Auto-generated method stub } } 这个里面的涉及到的JS的函数： toString:toString() 方法可把一个逻辑值转换为字符串，并返回结果。 substring:substring() 方法用于提取字符串中介于两个指定下标之间的字符。 语法stringObject.substring(start,stop)返回值一个新的字符串，该字符串值包含 stringObject 的一个子字符串，其内容是从 start 处到 stop-1 处的所有字符，其长度为 stop 减 start。 说明substring() 方法返回的子串包括 start 处的字符，但不包括 stop 处的字符。 如果参数 start 与 stop 相等，那么该方法返回的就是一个空串（即长度为 0 的字符串）。如果 start 比 stop 大，那么该方法在提取子串之前会先交换这两个参数。 chartAt:charAt() 方法可返回指定位置的字符。 请注意，JavaScript 并没有一种有别于字符串类型的字符数据类型，所以返回的字符是长度为 1 的字符串 语法stringObject.charAt(index) 注释：字符串中第一个字符的下标是 0。如果参数 index 不在 0 与 string.length 之间，该方法将返回一个空字符串。 关于Integer 方法摘要 这里使用的函数是：paresInt(String int):将字符串参数作为有符号的十进制整数进行分析。 语法： parseInt(string, radix) parseInt(&quot;10&quot;); //返回 10 parseInt(&quot;19&quot;,10); //返回 19 (10+9) parseInt(&quot;11&quot;,2); //返回 3 (2+1) parseInt(&quot;17&quot;,8); //返回 15 (8+7) parseInt(&quot;1f&quot;,16); //返回 31 (16+15) parseInt(&quot;010&quot;); //未定：返回 10 或 8 matches() 方法用于检测字符串是否匹配给定的正则表达式。 调用此方法的 str.matches(regex) 形式与以下表达式产生的结果完全相同： 在字符串匹配给定的正则表达式时，返回 true。 public class Test { public static void main(String args[]) { String Str = new String(&quot;www.runoob.com&quot;); System.out.print(&quot;返回值 :&quot; ); System.out.println(Str.matches(&quot;(.*)runoob(.*)&quot;)); System.out.print(&quot;返回值 :&quot; ); System.out.println(Str.matches(&quot;(.*)google(.*)&quot;)); System.out.print(&quot;返回值 :&quot; ); System.out.println(Str.matches(&quot;www(.*)&quot;)); } } 以上程序执行结果为： 返回值 :true 返回值 :false 返回值 :true 关于random()方法讲解java Random.nextInt()方法public int nextInt(int n) 该方法的作用是生成一个随机的int值，该值介于[0,n)的区间，也就是0到n之间的随机int值，包含0而不包含n package org.xiaowu.random.demo; import java.util.Random; import org.junit.Test; public class RandomDemo { @Test public void Demo(){ Random rnd = new Random(); int code = rnd.nextInt(8999) + 1000; System.out.println(&quot;code:&quot;+code); } @Test public void Demo1(){ Random r = new Random(); int nextInt = r.nextInt(); Random r1 = new Random(10); int nextInt2 = r1.nextInt(); System.out.println(&quot;nextInt:&quot;+nextInt); System.out.println(&quot;nextInt2:&quot;+nextInt2); } /** * 生成[0,1.0)区间的小数 * */ @Test public void Demo2(){ Random r = new Random(); double d1 = r.nextDouble(); System.out.println(&quot;d1:&quot;+d1); } /** * 生成[0,5.0)区间的小数 * */ @Test public void Demo3(){ Random r = new Random(); double d2 = r.nextDouble()* 5; System.out.println(&quot;d1:&quot;+d2); } /** * 生成[1,2.5)区间的小数 * */ @Test public void Demo4(){ Random r = new Random(); double d3 = r.nextDouble() * 1.5 + 1; System.out.println(&quot;d1:&quot;+d3); } /** * 生成任意整数 * */ @Test public void Demo5(){ Random r = new Random(); int n1 = r.nextInt(); System.out.println(&quot;d1:&quot;+n1); } /** * 生成[0,10)区间的整数 * */ @Test public void Demo6(){ Random r = new Random(); int n2 = r.nextInt(10); int n3 = Math.abs(r.nextInt() % 10); System.out.println(&quot;n2:&quot;+n2); System.out.println(&quot;n3:&quot;+n3); } /** * 生成[0,10]区间的整数 * */ @Test public void Demo7(){ Random r = new Random(); int n3 = r.nextInt(11); int n4 = Math.abs(r.nextInt() % 11); System.out.println(&quot;n3:&quot;+n3); System.out.println(&quot;n4:&quot;+n4); } /** * 生成[-3,15)区间的整数 * */ @Test public void Demo8(){ Random r = new Random(); int n4 = r.nextInt(18) - 3; int n5 = Math.abs(r.nextInt() % 18) - 3; System.out.println(&quot;n4:&quot;+n4); System.out.println(&quot;n5:&quot;+n5); } } 关于nextInt()函数的一点儿说明：如果想得到30到200的(包含30和200)这个跨度的数在java中一般可以有如下方式获得 （1）int i = (int)(Math.random()*171) + 30; （2）Random r = new Random () ; r.nextInt (201) ; // 这个是0 - 200 （3）Random r = new Random () ; r.nextInt (171) + 30 ; // 这个是30 到 200. //如下为二维数组的一点儿东西 public class 数组的使用说明代码 { public static void main(String args[]){ int[] array=creatArray(10); printArray(array); } public static int[] creatArray(int length){ //构造含length个元素的数组的方法 int[] array =new int[length]; Random rad=new Random(); //产生随机数的方法（系统自己的） for(int i=0;i&lt;array.length;i++){ int value = rad.nextInt(100) + 200; //rad.nextInt(100) 意思是随机产生一个大于等于0小于100的数 ------即包含0不包含100 array[i]=value; } return array; } public static void printArray(int[] array){ for(int i=0;i&lt;array.length;i++) System.out.println(array[i]+&apos;\t&apos;); } } JavaScript indexOf() 方法JavaScript String 对象 定义和用法]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop权威指南</tag>
        <tag>MapReduce学习</tag>
        <tag>JS函数学习</tag>
      </tags>
  </entry>
</search>
