<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />





  <link rel="alternate" href="/atom.xml" title="无心是一首歌" type="application/atom+xml" />






<meta name="description" content="大数据|动漫迷|科技控">
<meta property="og:type" content="website">
<meta property="og:title" content="无心是一首歌">
<meta property="og:url" content="http://erichunn.github.io/page/5/index.html">
<meta property="og:site_name" content="无心是一首歌">
<meta property="og:description" content="大数据|动漫迷|科技控">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="无心是一首歌">
<meta name="twitter:description" content="大数据|动漫迷|科技控">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: 'undefined',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://erichunn.github.io/page/5/"/>





  <title>无心是一首歌</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?02fe19ac065353167cab1b453f2909ec";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">无心是一首歌</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-首页">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-关于我">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于我
          </a>
        </li>
      
        
        <li class="menu-item menu-item-标签">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-分类">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-归档">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-站点地图">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br />
            
            站点地图
          </a>
        </li>
      
        
        <li class="menu-item menu-item-腾讯公益">
          <a href="/404/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br />
            
            腾讯公益
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/11/09/人为什么要努力？——2018-11-09/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/09/人为什么要努力？——2018-11-09/" itemprop="url">心情日记——2018.11.09</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-11-09T22:38:08+08:00">
                2018-11-09
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/心情记/" itemprop="url" rel="index">
                    <span itemprop="name">心情记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>无意之间从知乎里面看见的题目是：人这一辈子为什么要努力？</p>
<p>泪目，与诸君共勉。</p>
<pre><code>奶奶五十岁才来的美国，身无分文，也没有一技之长，随身携带的只有长年辛勤劳作留下的一身病根。

那是九十年代初，她去了唐人街的扎花厂上班，每天工作十个小时以上，一年工作三百六十五天。

扎花按件算钱，她眼神虽然不好，却比谁扎得都快。

有人弹吉他磨出了茧，有人搬砖头磨出了茧，她被针头扎出了茧。

回国看我的时候，她给我带了费列罗巧克力，翘胡子薯片，Jif花生酱。

她给我买好看的小西装，给我买一斤几十元的黄螺。

她带我去动物园，游乐园，森林公园，带我去北京看长城，看天安门，看毛主席。

“奶奶，美国比北京还好吗？”

她用最朴实的语言向我描述纽约的繁华，告诉我美国好极了，一切都在等着我。

知乎上经常讨论富养女孩，我有一个男孩被富养的故事。

有一年，我的巧克力吃完了，薯片吃完了，花生酱吃完了，奶奶还是没有回来。

那是我第一个不愿意知道的生活真相，她中风了，瘫了半边身子。



奶奶移民美国时带去了三个未成年的儿子，二叔，三叔，四叔。

二叔和三叔在登陆美国的第二天就打工去了，年幼的四叔读了几年书后也离开了学校。

他们在亲戚家的餐馆打工，每天工作十二个小时，每周工作六天。

餐馆一年营业三百六十四天，只在感恩节那天歇业。

奶奶瘫痪后，叔叔们轮流回国，给我带好吃的，带我出去玩。

我喜欢打乒乓球，二叔给我买了乒乓球，乒乓球拍，乒乓球桌。

有一年流行四驱车，三叔给我买了一辆遥控越野车，助我碾压所有的小伙伴。

四叔年少风流，出门把妹的时候总不忘带上我，要把我培养成下一代情圣。

他们绝口不提在美国生存的艰辛，那是大人们的秘密，和我无关。



奶奶出国五年后，爸妈也去了美国。

怎一个落魄了得？夫妻俩连属于自己的房间都没有。

扎花已经被时代淘汰，只有重活可以干。我爸当过屠夫，货柜工人，货运司机。

细节不必赘述，无非就是 12小时 x 365天的陈词滥调。

后来，他们四兄弟聚首，开了一家小超市，一家餐馆，后来又开了第二家小超市，第二家餐馆。

钱赚得越来越多，老家伙们拼命工作的老毛病却没有得到丝毫缓解。



爸妈出国五年后，我也来了美国，看清了生活本来的面目。

我在纽约生活了几个月，带着半身不遂的奶奶看遍了世界之都的繁华。

在这之前，她几乎没有走出过唐人街的范围，以前对我描绘纽约时一半是转述，一半靠想象。

后来，我回到了父母的身边，结束了长达五年的骨肉分离。

我爸来车站接我，把我带到了一栋小别墅前，很得意地告诉我：“这是我们家，知道你要来，刚买的。”

我去过奶奶刚来美国时住过的那个阴暗破旧的小公寓楼，也去过爸妈栖身过的那个散发着霉味的地下室，我知道我在新家会有一张床，一个房间，但我没想到，我会有一个别墅，一个前院，一个后院，一整个铺垫好的未来。



人这一生为什么要努力？

奶奶的答案是我，爸妈的答案是我，我的答案是他们，以及把身家性命托付于我的媳妇和孩子。

对于我来说，长大了，责任多了，自然而然地想要努力，不是为了赚很多很多钱，而是为了过上自己想要的生活，过上奶奶和父母不曾拥有过的生活。他们替我吃完了所有的苦，帮我走了九十九步，我自己只需再走一步，哪敢迟疑？

从外曾祖父母算起，我们家族四代八十多口人已经在美国奋斗了半个多世纪。我们人人都在努力，我们一代好过一代。



如果你的人生起点不高，不曾有人为你走过人生百步中的任何一步，不打紧，你尽管努力，多出来的步数不会被浪费掉，总有你在乎的人用得着，而你迟早会遇到你在乎的人。

与诸君共勉。

顾宇的知乎回答索引
</code></pre>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/11/08/Hive第二天/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/08/Hive第二天/" itemprop="url">Hive第二天</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-11-08T16:17:04+08:00">
                2018-11-08
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hive/" itemprop="url" rel="index">
                    <span itemprop="name">Hive</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="hive"><a href="#hive" class="headerlink" title="hive"></a>hive</h2><pre><code>数据仓库,在线分析处理。
HiveQL,类似sql语言。
表,metadata-&gt;rdbms.
hive处理的数据是hdfs.
MR,聚合操作。
</code></pre><h1 id="hive的2个重点一个是分区一个是桶表"><a href="#hive的2个重点一个是分区一个是桶表" class="headerlink" title="hive的2个重点一个是分区一个是桶表"></a>hive的2个重点一个是分区一个是桶表</h1><h1 id="内部表-管理表-托管表"><a href="#内部表-管理表-托管表" class="headerlink" title="内部表,管理表,托管表"></a>内部表,管理表,托管表</h1><pre><code>hive,drop ,数据也删除
</code></pre><h1 id="外部表"><a href="#外部表" class="headerlink" title="外部表"></a>外部表</h1><pre><code>hive表结构。
</code></pre><h1 id="分区表"><a href="#分区表" class="headerlink" title="分区表"></a>分区表</h1><pre><code>目录.
where 缩小查询范围。
</code></pre><h1 id="bucket表"><a href="#bucket表" class="headerlink" title="bucket表"></a>bucket表</h1><pre><code>文件。
hash
clustered by &apos;&apos;
</code></pre><h1 id="join"><a href="#join" class="headerlink" title="join"></a>join</h1><pre><code>横向连接，也就是作外连接和右外连接
</code></pre><h1 id="union"><a href="#union" class="headerlink" title="union"></a>union</h1><pre><code>竖向连接。只能指定相匹配的字段

select id ,name from a1 union select id ,cid from a2;
</code></pre><h1 id="hive-union操作"><a href="#hive-union操作" class="headerlink" title="hive union操作"></a>hive union操作</h1><pre><code>select id,name from customers union select id,orderno from orders ;
$&gt;hive                            //hive --service cli 
$&gt;hive --servic hiveserver2        //启动hiveserver2，10000 [thriftServer]
$&gt;hive --service beeline        //beeline    
</code></pre><h1 id="hive使用jdbc协议实现远程访问"><a href="#hive使用jdbc协议实现远程访问" class="headerlink" title="hive使用jdbc协议实现远程访问"></a>hive使用jdbc协议实现远程访问</h1><h1 id="hive-1"><a href="#hive-1" class="headerlink" title="hive"></a>hive</h1><pre><code>$hive&gt;CREATE TABLE t3(id int,name string,age int) PARTITIONED BY (Year INT, Month INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;
</code></pre><h1 id="export"><a href="#export" class="headerlink" title="export"></a>export</h1><pre><code>$hive&gt;EXPORT TABLE customers TO &apos;/user/centos/tmp.txt&apos;;        //导出表结构+数据到hdfs目录。
看一下导出来的东西，是一个目录，包括表结构和表内容。
</code></pre><p><img src="https://i.imgur.com/pFsQ5fZ.png" alt=""></p>
<h1 id="order全排序"><a href="#order全排序" class="headerlink" title="/order全排序"></a>/order全排序</h1><pre><code>$hive&gt;select * from orders order by id asc ;
</code></pre><h1 id="sort-map端排序-本地有序。"><a href="#sort-map端排序-本地有序。" class="headerlink" title="sort,map端排序,本地有序。"></a>sort,map端排序,本地有序。</h1><pre><code>$hive&gt;select * from orders sort by id asc ;
</code></pre><h1 id="distribute-by-amp-cluster-by-amp-sort-by"><a href="#distribute-by-amp-cluster-by-amp-sort-by" class="headerlink" title="distribute by &amp; cluster by  &amp; sort by"></a>distribute by &amp; cluster by  &amp; sort by</h1><p>类似于mysql的group by,进行分区操作。</p>
<pre><code>//select cid , ... from orders distribute by cid sort by name ;            //注意顺序.
$hive&gt;select id,orderno,cid from orders distribute by cid sort by cid desc ;

//cluster by ===&gt;  distribute by cid sort by cid
</code></pre><p><strong>destribut by 和cluster by图解对比：</strong></p>
<p><img src="https://i.imgur.com/BQ7NHQU.png" alt=""></p>
<p>这个地方没有细讲，说主要应用还是group by 只要记住这个cluster是dirstribute by 和sort by 的组合极客</p>
<h2 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h2><pre><code>mysql&gt;select concat(&apos;tom&apos;,1000) ;
$hive&gt;select current_database(),current_user() ;
$hive&gt;tab                                //查看帮助
</code></pre><h2 id="设置作业参数"><a href="#设置作业参数" class="headerlink" title="设置作业参数"></a>设置作业参数</h2><pre><code>$hive&gt;set hive.exec.reducers.bytes.per.reducer=xxx            //设置reducetask的字节数。
$hive&gt;set hive.exec.reducers.max=0                            //设置reduce task的最大任务数
$hive&gt;set mapreduce.job.reduces=0                            //设置reducetask个数。
</code></pre><h2 id="动态分区-严格模式-非严格模式"><a href="#动态分区-严格模式-非严格模式" class="headerlink" title="动态分区-严格模式-非严格模式"></a>动态分区-严格模式-非严格模式</h2><pre><code>动态分区模式:strict-严格模式，插入时至少指定一个静态分区，nonstrict-非严格模式-可以不指定静态分区。
set hive.exec.dynamic.partition.mode=nonstrict            //设置非严格模式
$hive&gt;INSERT [OVERWRITE] into TABLE employees PARTITION (country, state) SELECT ..., se.cnty, se.st FROM staged_employees se WHERE se.cnty = &apos;US&apos;;
</code></pre><p><img src="https://i.imgur.com/8zgzuuO.png" alt=""></p>
<p>本来有一个已经分区好了的表，然后再建立一个表，比如也是相同的字段，然后新表没有分区要插进去老表，让他自动分区，就要用到动态分区，动态分区分为严格模式和非严格模式。如下图先使用了严格模式的情况，出错，说要至少指定一个分区也就是</p>
<pre><code>$hive&gt;INSERT [OVERWRITE] into TABLE employees PARTITION (country=&apos;US&apos;, state) SELECT ..., se.cnty, se.st FROM staged_employees;
</code></pre><p>但是严格模式还是要制定这个分区，所以用一下非严格模式先设置一下：</p>
<pre><code>set hive.exec.dynamic.partition.mode=nonstrict            //设置非严格模式
</code></pre><p>然后我们开始插入：</p>
<pre><code>$hive&gt;INSERT [OVERWRITE] into TABLE employees PARTITION (country, state) SELECT ..., se.cnty, se.st FROM staged_employees;
</code></pre><p>这样就实现了非严格模式下的动态分区</p>
<h1 id="hive事务处理在-gt-0-13-0之后支持行级事务。-（这个地方老师没讲清楚自己没搞出来）"><a href="#hive事务处理在-gt-0-13-0之后支持行级事务。-（这个地方老师没讲清楚自己没搞出来）" class="headerlink" title="hive事务处理在&gt;0.13.0之后支持行级事务。 #（这个地方老师没讲清楚自己没搞出来）"></a>hive事务处理在&gt;0.13.0之后支持行级事务。 #（这个地方老师没讲清楚自己没搞出来）</h1><pre><code>1.所有事务自动提交。
2.只支持orc格式。
3.使用bucket表。
4.配置hive参数，使其支持事务。
</code></pre><p>$hive&gt;SET hive.support.concurrency = true;<br>$hive&gt;SET hive.enforce.bucketing = true;<br>$hive&gt;SET hive.exec.dynamic.partition.mode = nonstrict;<br>$hive&gt;SET hive.txn.manager = org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;<br>$hive&gt;SET hive.compactor.initiator.on = true;<br>$hive&gt;SET hive.compactor.worker.threads = 1;</p>
<pre><code>5.使用事务性操作
    $&gt;CREATE TABLE tx(id int,name string,age int) CLUSTERED BY (id) INTO 3 BUCKETS ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; stored as orc TBLPROPERTIES (&apos;transactional&apos;=&apos;true&apos;);
</code></pre><p><img src="https://i.imgur.com/UWJd7M5.png" alt=""></p>
<hr>
<p>mysql支持这样的操作，左边选择的查询的列，并不出现在右边的分组当中这样可以查询，但是hive中不允许这样。</p>
<p><img src="https://i.imgur.com/vpfbmjD.png" alt=""></p>
<p>但是在hive当中只可以如下图查询：</p>
<p><img src="https://i.imgur.com/oOqn35d.png" alt=""></p>
<h2 id="聚合处理"><a href="#聚合处理" class="headerlink" title="聚合处理"></a>聚合处理</h2><pre><code>每个用户购买商品订单大于1的：这个按照cid分组的，查询每个分组里面的count(*)，也就是每个不同的cid 的

$hive&gt;select cid,count(*) c ,max(price) from orders group by cid having c &gt; 1 ; 
</code></pre><h1 id="wordcount"><a href="#wordcount" class="headerlink" title="wordcount"></a>wordcount</h1><p>创建新表:stats(word string,c int) ;<br>    将查询结果插入到指定表中。</p>
<p><img src="https://i.imgur.com/wxA99my.png" alt=""></p>
<p>按照下图这个切割方法，用一个split的函数切割，切割之后形成一个数组类型的东西</p>
<p><img src="https://i.imgur.com/NVQuyss.png" alt=""></p>
<p>explode这个函数传入的需要是一个经过split切割之后的数组，传入之后再去展开得到如上图所示的东西。但是切出来还是需要聚合，查询每个单词的个数</p>
<p>对下面这个SQL语句进行讲解。先看括号里面的最里面的括号，select explode(split(line,’ ‘)) as word from doc这句话都能理解，就是先切割成数组，然后对这个数据进行explode炸裂成单词，然后as word就是别名叫word，然后再把整个查询的as t别名t，然后select t.word,count(*) c from …group by t.word order by c desc limit2;就是对t的word字段进行分组，对分组的t的字段word和计数总和进行查询，然后按照降序排序，然后limit 2取最高的2个数。</p>
<pre><code>$hive&gt;select t.word,count(*) c from ((select explode(split(line, &apos; &apos;)) as word from doc) as t) group by t.word order by c desc limit 2 ;
</code></pre><h1 id="view-视图-虚表"><a href="#view-视图-虚表" class="headerlink" title="view:视图,虚表"></a>view:视图,虚表</h1><p>是一个虚拟的表，类似于对这个语句的封装，或者说起了一个别名，并不是真的表。</p>
<p>这个地方创建视图的时候不能</p>
<pre><code>$hive&gt;create view v1 as select a.*,b.*from customers a left outer join default.tt b on a.id = b.cid ;
</code></pre><p>上图这样是错误的，因为在a和b里面有相同的字段，要把相同的字段修改为不同的字段，如下图：</p>
<pre><code>//创建视图
$hive&gt;create view v1 as select a.id aid,a.name ,b.id bid , b.order from customers a left outer join default.tt b on a.id = b.cid ;
</code></pre><p>可以在续表的基础上在查询，如下所示：</p>
<pre><code>//查看视图
$hive&gt;show tables ;
$hive&gt;select * from v1 ;
</code></pre><p>view这个在hdfs上没有哦，而是存放在了mysql里面</p>
<h2 id="一个mysql里面的操作：-G-行转列"><a href="#一个mysql里面的操作：-G-行转列" class="headerlink" title="一个mysql里面的操作：\G 行转列"></a>一个mysql里面的操作：\G 行转列</h2><p>select * from tbls \G：结果如下：</p>
<p><img src="https://i.imgur.com/AbE1mvx.png" alt=""></p>
<h2 id="Map端连接"><a href="#Map端连接" class="headerlink" title="Map端连接"></a>Map端连接</h2><pre><code>$hive&gt;set hive.auto.convert.join=true            //设置自动转换连接,默认开启了。

//使用mapjoin连接暗示实现mapjoin
$hive&gt;select /*+ mapjoin(customers) */ a.*,b.* from customers a left outer join orders b on a.id = b.cid ;
</code></pre><h2 id="调优"><a href="#调优" class="headerlink" title="调优"></a>调优</h2><pre><code>1.explain
    使用explain查看查询计划
    hive&gt;explain [extended] select count(*) from customers ;
    hive&gt;explain select t.name , count(*) from (select a.name ,b.id,b.orderno from customers a ,orders b where a.id = b.cid) t group by t.name ;

    //设置limit优化测，避免全部查询.但是树上说因为是随机取样，可能会出问题。
    hive&gt;set hive.limit.optimize.enable=true

    //本地模式
    $hive&gt;set mapred.job.tracker=local;            //
    $hive&gt;set hive.exec.mode.local.auto=true    //自动本地模式,主要用于测试不用于实战


    //并行执行,同时执行不存在依赖关系的阶段。??
    $hive&gt;set hive.exec.parallel=true            //是自动设置好的

    //严格模式,
    $hive&gt;set hive.mapred.mode=strict            //手动设置之后。1.分区表必须指定分区进行查询，否则不让查询。
                                                //2.order by时必须使用limit子句。
                                                //3.不允许笛卡尔积.


    //设置MR的数量
    hive&gt; set hive.exec.reducers.bytes.per.reducer=750000000;    //设置reduce处理的字节数。

    //JVM重用
    $hive&gt;set mapreduce.job.jvm.numtasks=1        //-1没有限制，使用大量小文件。


    //UDF
    //User define function,用户自定义函数
    //current_database(),current_user();

    //显式所有函数
    $hive&gt;show functions;
    $hive&gt;select array(1,2,3) ;

    //显式指定函数帮助
    $hive&gt;desc function current_database();

    //表生成函数,多行函数。
    $hive&gt;explode(str,exp);            //按照exp切割str.
</code></pre><h2 id="自定义函数"><a href="#自定义函数" class="headerlink" title="自定义函数"></a>自定义函数</h2><p><img src="https://i.imgur.com/lvT1nkj.png" alt=""></p>
<p>上图带括号的都是函数，不带括号的是命令</p>
<pre><code>1.创建类，继承UDF
    package com.it18zhang.hivedemo.udf;

    import org.apache.hadoop.hive.ql.exec.Description;
    import org.apache.hadoop.hive.ql.exec.UDF;

    /**
     * 自定义hive函数
     */
    @Description(name = &quot;myadd&quot;,
            value = &quot;myadd(int a , int b) ==&gt; return a + b &quot;,
            extended = &quot;Example:\n&quot;
                    + &quot; myadd(1,1) ==&gt; 2 \n&quot;
                    + &quot; myadd(1,2,3) ==&gt; 6;&quot;)
    public class AddUDF extends UDF {

        public int evaluate(int a ,int b) {
            return a + b ;
        }

        public int evaluate(int a ,int b , int c) {
            return a + b + c;
        }
    }
2.打成jar包。
    cmd&gt;cd {classes所在目录}
    cmd&gt;jar cvf HiveDemo.jar -C x/x/x/x/classes/ .
3.添加jar包到hive的类路径
    //添加jar到类路径
    $&gt;cp /mnt/hgfs/downloads/bigdata/data/HiveDemo.jar /soft/hive/lib

3.重进入hive
    $&gt;....

4.创建临时函数
    //
    CREATE TEMPORARY FUNCTION myadd AS &apos;com.it18zhang.hivedemo.udf.AddUDF&apos;;

5.在查询中使用自定义函数
    $hive&gt;select myadd(1,2)  ;

6.定义日期函数
    1)定义类
    public class ToCharUDF extends UDF {
        /**
         * 取出服务器的当前系统时间 2017/3/21 16:53:55
         */
        public String evaluate() {
            Date date = new Date();
            SimpleDateFormat sdf = new SimpleDateFormat();
            sdf.applyPattern(&quot;yyyy/MM/dd hh:mm:ss&quot;);
            return sdf.format(date) ;
        }
        public String evaluate(Date date) {
            SimpleDateFormat sdf = new SimpleDateFormat();
            sdf.applyPattern(&quot;yyyy/MM/dd hh:mm:ss&quot;);
            return sdf.format(date) ;
        }

        public String evaluate(Date date,String frt) {
            SimpleDateFormat sdf = new SimpleDateFormat();
            sdf.applyPattern(frt);
            return sdf.format(date) ;
        }
    }

    2)导出jar包，通过命令添加到hive的类路径(不需要重进hive)。
        $hive&gt;add jar /mnt/hgfs/downloads/bigdata/data/HiveDemo-1.0-SNAPSHOT.jar

    3)注册函数
        $hive&gt;CREATE TEMPORARY FUNCTION to_char AS &apos;com.it18zhang.hivedemo.udf.ToCharUDF&apos;;
        $hive&gt;CREATE TEMPORARY FUNCTION to_date AS &apos;com.it18zhang.hivedemo.udf.ToCharUDF&apos;;
</code></pre><h2 id="定义Nvl函数-这个是在英文版的Hadoop权威指南上给的"><a href="#定义Nvl函数-这个是在英文版的Hadoop权威指南上给的" class="headerlink" title="定义Nvl函数(这个是在英文版的Hadoop权威指南上给的)"></a>定义Nvl函数(这个是在英文版的Hadoop权威指南上给的)</h2><pre><code>package com.it18zhang.hivedemo.udf;

import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;
import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;
import org.apache.hadoop.hive.ql.metadata.HiveException;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;

/**
 * 自定义null值处理函数
 */
public class Nvl extends GenericUDF {
    private GenericUDFUtils.ReturnObjectInspectorResolver returnOIResolver;
    private ObjectInspector[] argumentOIs;

    public ObjectInspector initialize(ObjectInspector[] arguments)
            throws UDFArgumentException {
        argumentOIs = arguments;
        //检查参数个数
        if (arguments.length != 2) {
            throw new UDFArgumentLengthException(
                    &quot;The operator &apos;NVL&apos; accepts 2 arguments.&quot;);
        }
        returnOIResolver = new GenericUDFUtils.ReturnObjectInspectorResolver(true);
        //检查参数类型
        if (!(returnOIResolver.update(arguments[0]) &amp;&amp; returnOIResolver
                .update(arguments[1]))) {
            throw new UDFArgumentTypeException(2,
                    &quot;The 1st and 2nd args of function NLV should have the same type, &quot;
                            + &quot;but they are different: \&quot;&quot; + arguments[0].getTypeName()
                            + &quot;\&quot; and \&quot;&quot; + arguments[1].getTypeName() + &quot;\&quot;&quot;);
        }
        return returnOIResolver.get();
    }

    public Object evaluate(DeferredObject[] arguments) throws HiveException {
        Object retVal = returnOIResolver.convertIfNecessary(arguments[0].get(), argumentOIs[0]);
        if (retVal == null) {
            retVal = returnOIResolver.convertIfNecessary(arguments[1].get(),
                    argumentOIs[1]);
        }
        return retVal;
    }

    public String getDisplayString(String[] children) {
        StringBuilder sb = new StringBuilder();
        sb.append(&quot;if &quot;);
        sb.append(children[0]);
        sb.append(&quot; is null &quot;);
        sb.append(&quot;returns&quot;);
        sb.append(children[1]);
        return sb.toString();
    }
}

2)添加jar到类路径
    ...
3)注册函数
    $hive&gt;CREATE TEMPORARY FUNCTION nvl AS &apos;org.apache.hadoop.hive.ql.udf.generic.GenericUDFNvl&apos;;
</code></pre><h1 id="一些课上用的PPT"><a href="#一些课上用的PPT" class="headerlink" title="一些课上用的PPT"></a>一些课上用的PPT</h1><p><img src="https://i.imgur.com/6OyT9i3.png" alt=""><br><img src="https://i.imgur.com/CLZNHSV.png" alt=""><br><img src="https://i.imgur.com/Rx5Hslt.png" alt=""><br><img src="https://i.imgur.com/fb0PfG0.png" alt=""><br><img src="https://i.imgur.com/P8kwdax.png" alt=""><br><img src="https://i.imgur.com/avoDPKV.png" alt=""><br><img src="https://i.imgur.com/OwHiTIn.png" alt=""><br><img src="https://i.imgur.com/x4qXG7v.png" alt=""><br><img src="https://i.imgur.com/mIQbSR1.png" alt=""><br><img src="https://i.imgur.com/2bMpEMd.png" alt=""><br><img src="https://i.imgur.com/bSF4lMN.png" alt=""><br><img src="https://i.imgur.com/C3IMpOQ.png" alt=""><br><img src="https://i.imgur.com/6xpn8Ul.png" alt=""></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/11/06/Hive第一天/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/06/Hive第一天/" itemprop="url">Hive第一天</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-11-06T16:35:55+08:00">
                2018-11-06
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hive/" itemprop="url" rel="index">
                    <span itemprop="name">Hive</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="hive"><a href="#hive" class="headerlink" title="hive"></a>hive</h2><pre><code>在hadoop处理结构化数据的数据仓库。
不是:    关系数据库
        不是OLTP
        实时查询和行级更新。
</code></pre><h2 id="hive特点"><a href="#hive特点" class="headerlink" title="hive特点"></a>hive特点</h2><pre><code>hive存储数据结构(schema)在数据库中,处理的数据进入hdfs.
OLAP
HQL / HiveQL
</code></pre><h2 id="hive安装"><a href="#hive安装" class="headerlink" title="hive安装"></a>hive安装</h2><pre><code>1.下载hive2.1-tar.gz
2.tar开
    $&gt;tar -xzvf hive-2.1.0.tar.gz -C /soft    //tar开
    $&gt;cd /soft/hive-2.1.0                    //
    $&gt;ln -s hive-2.1.0 hive                    //符号连接

3.配置环境变量
    [/etc/profile]
    HIVE_HOME=/soft/hive
    PATH=...:$HIVE_HOME/bin

4.验证hive安装成功
    $&gt;hive --v

5.配置hive,使用win7的mysql存放hive的元数据.
    a)复制mysql驱动程序到hive的lib目录下。
        ...
    b)配置hive-site.xml
        复制hive-default.xml.template为hive-site.xml
        修改连接信息为mysql链接地址，将${system:...字样替换成具体路径。
        [hive/conf/hive-site.xml]
        &lt;property&gt;
            &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;
            &lt;value&gt;root&lt;/value&gt;
            &lt;description&gt;password to use against metastore database&lt;/description&gt;
        &lt;/property&gt;
        &lt;property&gt;
            &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;
            &lt;value&gt;root&lt;/value&gt;
            &lt;description&gt;Username to use against metastore database&lt;/description&gt;
        &lt;/property&gt;
        &lt;property&gt;
            &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;
            &lt;value&gt;jdbc:mysql://192.168.231.1:3306/hive2&lt;/value&gt;
        &lt;/property&gt;
        &lt;property&gt;
            &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;
            &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;
            &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;
        &lt;/property&gt;

    6)在msyql中创建存放hive信息的数据库
        mysql&gt;create database hive2 ;

    6)初始化hive的元数据(表结构)到mysql中。
        $&gt;cd /soft/hive/bin
        $&gt;schematool -dbType mysql -initSchema
</code></pre><h1 id="hive命令行操作"><a href="#hive命令行操作" class="headerlink" title="hive命令行操作"></a>hive命令行操作</h1><h2 id="1-创建hive的数据库"><a href="#1-创建hive的数据库" class="headerlink" title="1.创建hive的数据库"></a>1.创建hive的数据库</h2><pre><code>$hive&gt;hive --version                //
$hive&gt;hive --help                    //

$hive&gt;create database mydb2 ;                //
$hive&gt;show databases ;
$hive&gt;use mydb2 ;
$hive&gt;create table mydb2.t(id int,name string,age int);
$hive&gt;drop table t ;
$hive&gt;drop table mydb2.t ;
$hive&gt;select * from mydb2.t ;        //查看指定库的表
$hive&gt;exit ;                        //退出

$&gt;hive                                //hive --service cli
$&gt;hive                                //hive --service cli
</code></pre><h2 id="2-通过远程jdbc方式连接到hive数据仓库"><a href="#2-通过远程jdbc方式连接到hive数据仓库" class="headerlink" title="2.通过远程jdbc方式连接到hive数据仓库"></a>2.通过远程jdbc方式连接到hive数据仓库</h2><pre><code>1.启动hiveserver2服务器，监听端口10000
    $&gt;hive --service hiveserver2 &amp;

2.通过beeline命令行连接到hiveserver2
    $&gt;beeline                                            //进入beeline命令行(于hive --service beeline)
    $beeline&gt;!help                                        //查看帮助
    $beeline&gt;!quit                                        //退出
    $beeline&gt;!connect jdbc:hive2://localhost:10000/mydb2//连接到hibve数据
    $beeline&gt;show databases ;
    $beeline&gt;use mydb2 ;
    $beeline&gt;show tables;                                //显式表
</code></pre><h2 id="使用Hive-jdbc驱动程序采用jdbc方式访问远程数据仓库"><a href="#使用Hive-jdbc驱动程序采用jdbc方式访问远程数据仓库" class="headerlink" title="使用Hive-jdbc驱动程序采用jdbc方式访问远程数据仓库"></a>使用Hive-jdbc驱动程序采用jdbc方式访问远程数据仓库</h2><pre><code>1.创建java模块
2.引入maven
3.添加hive-jdbc依赖
    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
    &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;
             xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
             xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
        &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

        &lt;groupId&gt;com.it18zhang&lt;/groupId&gt;
        &lt;artifactId&gt;HiveDemo&lt;/artifactId&gt;
        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;

        &lt;dependencies&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;
                &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt;
                &lt;version&gt;2.1.0&lt;/version&gt;
            &lt;/dependency&gt;
        &lt;/dependencies&gt;
    &lt;/project&gt;

4.App
    package com.it18zhang.hivedemo;

    import java.sql.Connection;
    import java.sql.DriverManager;
    import java.sql.ResultSet;
    import java.sql.Statement;

    /**
     * 使用jdbc方式连接到hive数据仓库，数据仓库需要开启hiveserver2服务。
     */
    public class App {
        public static void main(String[] args) throws  Exception {
            Class.forName(&quot;org.apache.hive.jdbc.HiveDriver&quot;);
            Connection conn = DriverManager.getConnection(&quot;jdbc:hive2://192.168.231.201:10000/mydb2&quot;);
            Statement st = conn.createStatement();
            ResultSet rs = st.executeQuery(&quot;select id , name ,age from t&quot;);
            while(rs.next()){
                System.out.println(rs.getInt(1) + &quot;,&quot; + rs.getString(2)) ;
            }
            rs.close();
            st.close();
            conn.close();
        }
    }
</code></pre><h1 id="hive中表"><a href="#hive中表" class="headerlink" title="hive中表"></a>hive中表</h1><h2 id="1-managed-table"><a href="#1-managed-table" class="headerlink" title="1.managed table"></a>1.managed table</h2><pre><code>托管表。
删除表时，数据也删除了。
</code></pre><h2 id="2-external-table"><a href="#2-external-table" class="headerlink" title="2.external table"></a>2.external table</h2><pre><code>外部表。
删除表时，数据不删。
</code></pre><h1 id="hive命令"><a href="#hive命令" class="headerlink" title="hive命令"></a>hive命令</h1><pre><code>//创建表,external 外部表
$hive&gt;CREATE external TABLE IF NOT EXISTS t2(id int,name string,age int)
COMMENT &apos;xx&apos; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; STORED AS TEXTFILE ; 

//查看表数据
$hive&gt;desc t2 ;
$hive&gt;desc formatted t2 ;

//加载数据到hive表
$hive&gt;load data local inpath &apos;/home/centos/customers.txt&apos; into table t2 ;    //local上传文件
$hive&gt;load data inpath &apos;/user/centos/customers.txt&apos; [overwrite] into table t2 ;    //移动文件

//复制表
mysql&gt;create table tt as select * from users ;        //携带数据和表结构
mysql&gt;create table tt like users ;            //不带数据，只有表结构

hive&gt;create table tt as select * from users ;    
hive&gt;create table tt like users ;    


//count()查询要转成mr
$hive&gt;select count(*) from t2 ;
$hive&gt;select id,name from t2 ;


//
$hive&gt;select * from t2 order by id desc ;                //MR

//启用/禁用表
$hive&gt;ALTER TABLE t2 ENABLE NO_DROP;    //不允许删除
$hive&gt;ALTER TABLE t2 DISABLE NO_DROP;    //允许删除
</code></pre><h1 id="分区表"><a href="#分区表" class="headerlink" title="分区表"></a>分区表</h1><pre><code>优化手段之一，从目录的层面控制搜索数据的范围。
//创建分区表.
$hive&gt;CREATE TABLE t3(id int,name string,age int) PARTITIONED BY (Year INT, Month INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;

//显式表的分区信息
$hive&gt;SHOW PARTITIONS t3;

//添加分区,创建目录
$hive&gt; alter table t3 add partition (year=2014, month=12) partition (year=2014,month=11);

//删除分区
hive&gt;ALTER TABLE employee_partitioned DROP IF EXISTS PARTITION (year=2014, month=11);

//分区结构
hive&gt;/user/hive/warehouse/mydb2.db/t3/year=2014/month=11
hive&gt;/user/hive/warehouse/mydb2.db/t3/year=2014/month=12


//加载数据到分区表
hive&gt;load data local inpath &apos;/home/centos/customers.txt&apos; into table t3 partition(year=2014,month=11);

//查询分区表
hive&gt;select * from t3 where year = 2014 and month =11;
</code></pre><p><img src="https://i.imgur.com/Ad6JAI7.png" alt=""></p>
<p>分区对应的是文件夹是目录，分桶对应的是文件，按照id分桶的意思就是按照id哈希不同的哈希进入不同的桶</p>
<pre><code>//创建桶表
$hive&gt;CREATE TABLE t4(id int,name string,age int) CLUSTERED BY (id) INTO 3 BUCKETS ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;

//加载数据不会进行分桶操作
$hive&gt;load data local inpath &apos;/home/centos/customers.txt&apos; into table t4 ;

//查询t3表数据插入到t4中。t3是分区表还多了年份和月份的2个字段所以要投影查询不能直接insert into t4 select * from t3;这是错的
$hive&gt;insert into t4 select id,name,age from t3 ;
</code></pre><p><img src="https://i.imgur.com/5Rpz8su.png" alt=""></p>
<pre><code>//桶表的数量如何设置?
//评估数据量，保证每个桶的数据量block的2倍大小。


//连接查询
$hive&gt;CREATE TABLE customers(id int,name string,age int) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;
$hive&gt;CREATE TABLE orders(id int,orderno string,price float,cid int) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;

//加载数据到表
//内连接查询
hive&gt;select a.*,b.* from customers a , orders b where a.id = b.cid ;
//左外
hive&gt;select a.*,b.* from customers a left outer join orders b on a.id = b.cid ;
hive&gt;select a.*,b.* from customers a right outer join orders b on a.id = b.cid ;
hive&gt;select a.*,b.* from customers a full outer join orders b on a.id = b.cid ;

//explode,炸裂,表生成函数。
//使用hive实现单词统计
//1.建表
$hive&gt;CREATE TABLE doc(line string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;
</code></pre>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/11/01/Hadoop第十一天/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/01/Hadoop第十一天/" itemprop="url">Hadoop第十一天</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-11-01T18:11:38+08:00">
                2018-11-01
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hadoop/" itemprop="url" rel="index">
                    <span itemprop="name">Hadoop</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h2><pre><code>[customers.txt]
1,tom,12
2,tom,13
3,tom,14
4,tom,15

[orders.txt]
1,no001,12.23,1
2,no001,12.23,1
3,no001,12.23,2
4,no001,12.23,2
5,no001,12.23,2
6,no001,12.23,3
7,no001,12.23,3
8,no001,12.23,3
9,no001,12.23,3
</code></pre><h2 id="map端join"><a href="#map端join" class="headerlink" title="map端join"></a>map端join</h2><pre><code>1.创建Mapper
    package com.it18zhang.hdfs.mr.mapjoin;

    import org.apache.hadoop.conf.Configuration;
    import org.apache.hadoop.fs.FSDataInputStream;
    import org.apache.hadoop.fs.FileSystem;
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.io.LongWritable;
    import org.apache.hadoop.io.NullWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Mapper;

    import java.io.BufferedReader;
    import java.io.IOException;
    import java.io.InputStreamReader;
    import java.util.HashMap;
    import java.util.Map;

    /**
     * join操作，map端连接。
     */
    public class MapJoinMapper extends Mapper&lt;LongWritable,Text,Text,NullWritable&gt; {

        private Map&lt;String,String&gt; allCustomers = new HashMap&lt;String,String&gt;();

        //启动,初始化客户信息
        protected void setup(Context context) throws IOException, InterruptedException {
            try {
                Configuration conf = context.getConfiguration();
                FileSystem fs = FileSystem.get(conf);
                FSDataInputStream fis = fs.open(new Path(&quot;file:///d:/mr/mapjoin/customers.txt&quot;));
                //得到缓冲区阅读器
                BufferedReader br = new BufferedReader(new InputStreamReader(fis));
                String line = null ;
                while((line = br.readLine()) != null){
                    //得到cid
                    String cid = line.substring(0,line.indexOf(&quot;,&quot;));
                    allCustomers.put(cid,line);
                }
            } catch (Exception e) {
                e.printStackTrace();
            }
        }

        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            //订单信息
            String line = value.toString();
            //提取customer id
            String cid = line.substring(line.lastIndexOf(&quot;,&quot;) + 1);
            //订单信息
            String orderInfo = line.substring(0,line.lastIndexOf(&quot;,&quot;));

            //连接customer + &quot;,&quot; + order
            String customerInfo = allCustomers.get(cid);
            context.write(new Text(customerInfo + &quot;,&quot; + orderInfo),NullWritable.get());
        }

    }

2.创建App
    package com.it18zhang.hdfs.mr.mapjoin;

    import org.apache.hadoop.conf.Configuration;
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.io.NullWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Job;
    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

    /**
     *
     */
    public class MapJoinApp {
        public static void main(String[] args) throws Exception {

            Configuration conf = new Configuration();
            conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;);
            Job job = Job.getInstance(conf);

            //设置job的各种属性
            job.setJobName(&quot;MapJoinApp&quot;);                        //作业名称
            job.setJarByClass(MapJoinApp.class);                 //搜索类

            //添加输入路径
            FileInputFormat.addInputPath(job,new Path(args[0]));
            //设置输出路径
            FileOutputFormat.setOutputPath(job,new Path(args[1]));

            //没有reduce

            job.setNumReduceTasks(0);

            job.setMapperClass(MapJoinMapper.class);             //mapper类

            job.setMapOutputKeyClass(Text.class);           //
            job.setMapOutputValueClass(NullWritable.class);  //

            job.waitForCompletion(true);
        }
    }
</code></pre><h2 id="join端连接"><a href="#join端连接" class="headerlink" title="join端连接"></a>join端连接</h2><pre><code>1.自定义key
    package com.it18zhang.hdfs.mr.mapjoin.reducejoin;

    import org.apache.hadoop.io.WritableComparable;

    import java.io.DataInput;
    import java.io.DataOutput;
    import java.io.IOException;

    /**
     */
    public class ComboKey2 implements WritableComparable&lt;ComboKey2&gt; {
        //0-customer 1-order
        private int type ;
        private int cid ;
        private int oid ;
        private String customerInfo = &quot;&quot; ;
        private String orderInfo = &quot;&quot; ;


        public int compareTo(ComboKey2 o) {
            int type0 = o.type ;
            int cid0= o.cid;
            int oid0 = o.oid;
            String customerInfo0 = o.customerInfo;
            String orderInfo0 = o.orderInfo ;
            //是否同一个customer的数据
            if(cid == cid0){
                //同一个客户的两个订单
                if(type == type0){
                    return oid - oid0 ;
                }
                //一个Customer + 他的order
                else{
                    if(type ==0)
                        return -1 ;
                    else
                        return 1 ;
                }
            }
            //cid不同
            else{
                return cid - cid0 ;
            }
        }

        public void write(DataOutput out) throws IOException {
            out.writeInt(type);
            out.writeInt(cid);
            out.writeInt(oid);
            out.writeUTF(customerInfo);
            out.writeUTF(orderInfo);
        }

        public void readFields(DataInput in) throws IOException {
            this.type = in.readInt();
            this.cid = in.readInt();
            this.oid = in.readInt();
            this.customerInfo = in.readUTF();
            this.orderInfo = in.readUTF();
        }
    }

2.自定义分区类
    public class CIDPartitioner extends Partitioner&lt;ComboKey2,NullWritable&gt;{

        public int getPartition(ComboKey2 key, NullWritable nullWritable, int numPartitions) {
            return key.getCid() % numPartitions;
        }
    }
3.创建Mapper
    package com.it18zhang.hdfs.mr.mapjoin.reducejoin;

    import org.apache.hadoop.io.LongWritable;
    import org.apache.hadoop.io.NullWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.InputSplit;
    import org.apache.hadoop.mapreduce.Mapper;
    import org.apache.hadoop.mapreduce.lib.input.FileSplit;

    import java.io.IOException;

    /**
     * mapper
     */
    public class ReduceJoinMapper extends Mapper&lt;LongWritable,Text,ComboKey2,NullWritable&gt; {

        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            //
            String line = value.toString() ;

            //判断是customer还是order
            FileSplit split = (FileSplit)context.getInputSplit();
            String path = split.getPath().toString();
            //客户信息
            ComboKey2 key2 = new ComboKey2();
            if(path.contains(&quot;customers&quot;)){
                String cid = line.substring(0,line.indexOf(&quot;,&quot;));
                String custInfo = line ;
                key2.setType(0);
                key2.setCid(Integer.parseInt(cid));
                key2.setCustomerInfo(custInfo);
            }
            //order info
            else{
                String cid = line.substring(line.lastIndexOf(&quot;,&quot;) + 1);
                String oid = line.substring(0, line.indexOf(&quot;,&quot;));
                String oinfo = line.substring(0, line.lastIndexOf(&quot;,&quot;));
                key2.setType(1);
                key2.setCid(Integer.parseInt(cid));
                key2.setOid(Integer.parseInt(oid));
                key2.setOrderInfo(oinfo);
            }
            context.write(key2,NullWritable.get());
        }
    }

4.创建Reducer
    package com.it18zhang.hdfs.mr.mapjoin.reducejoin;

    import org.apache.hadoop.io.NullWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Reducer;

    import java.io.IOException;
    import java.util.Iterator;

    /**
     * ReduceJoinReducer,reducer端连接实现。
     */
    public class ReduceJoinReducer extends Reducer&lt;ComboKey2,NullWritable,Text,NullWritable&gt; {

        protected void reduce(ComboKey2 key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException {
            Iterator&lt;NullWritable&gt; it = values.iterator();
            it.next();
            int type = key.getType();
            int cid = key.getCid() ;
            String cinfo = key.getCustomerInfo() ;
            while(it.hasNext()){
                it.next();
                String oinfo = key.getOrderInfo();
                context.write(new Text(cinfo + &quot;,&quot; + oinfo),NullWritable.get());
            }
        }
    }

5.创建排序对比器
    package com.it18zhang.hdfs.mr.mapjoin.reducejoin;

    import com.it18zhang.hdfs.maxtemp.allsort.secondarysort.ComboKey;
    import org.apache.hadoop.io.WritableComparable;
    import org.apache.hadoop.io.WritableComparator;

    /**
     * 组合Key排序对比器
     */
    public class ComboKey2Comparator extends WritableComparator {
        protected ComboKey2Comparator() {
            super(ComboKey2.class, true);
        }

        public int compare(WritableComparable a, WritableComparable b) {
            ComboKey2 k1 = (ComboKey2) a;
            ComboKey2 k2 = (ComboKey2) b;
            return k1.compareTo(k2);
        }
    }

6.分组对比器
    package com.it18zhang.hdfs.mr.mapjoin.reducejoin;

    import com.it18zhang.hdfs.maxtemp.allsort.secondarysort.ComboKey;
    import org.apache.hadoop.io.WritableComparable;
    import org.apache.hadoop.io.WritableComparator;

    /**
     * CID分组对比器
     */
    public class CIDGroupComparator extends WritableComparator{

        protected CIDGroupComparator() {
            super(ComboKey2.class, true);
        }

        public int compare(WritableComparable a, WritableComparable b) {
            ComboKey2 k1 = (ComboKey2) a;
            ComboKey2 k2 = (ComboKey2) b;
            return k1.getCid() - k2.getCid();
        }
    }

7.App
    package com.it18zhang.hdfs.mr.mapjoin.reducejoin;

    import com.it18zhang.hdfs.maxtemp.allsort.secondarysort.*;
    import org.apache.hadoop.conf.Configuration;
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.io.IntWritable;
    import org.apache.hadoop.io.NullWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Job;
    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
    import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

    /**
     *
     */
    public class ReduceJoinApp {
        public static void main(String[] args) throws Exception {

            Configuration conf = new Configuration();
            conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;);

            Job job = Job.getInstance(conf);

            //设置job的各种属性
            job.setJobName(&quot;ReduceJoinApp&quot;);                        //作业名称
            job.setJarByClass(ReduceJoinApp.class);                 //搜索类

            //添加输入路径
            FileInputFormat.addInputPath(job,new Path(&quot;D:\\mr\\reducejoin&quot;));
            //设置输出路径
            FileOutputFormat.setOutputPath(job,new Path(&quot;D:\\mr\\reducejoin\\out&quot;));

            job.setMapperClass(ReduceJoinMapper.class);             //mapper类
            job.setReducerClass(ReduceJoinReducer.class);           //reducer类

            //设置Map输出类型
            job.setMapOutputKeyClass(ComboKey2.class);            //
            job.setMapOutputValueClass(NullWritable.class);      //

            //设置ReduceOutput类型
            job.setOutputKeyClass(Text.class);
            job.setOutputValueClass(NullWritable.class);         //

            //设置分区类
            job.setPartitionerClass(CIDPartitioner.class);
            //设置分组对比器
            job.setGroupingComparatorClass(CIDGroupComparator.class);
            //设置排序对比器
            job.setSortComparatorClass(ComboKey2Comparator.class);
            job.setNumReduceTasks(2);                           //reduce个数
            job.waitForCompletion(true);
        }
    }
</code></pre><h2 id="hive"><a href="#hive" class="headerlink" title="hive"></a>hive</h2><pre><code>在hadoop处理结构化数据的数据仓库。
不是:    关系数据库
        不是OLTP
        实时查询和行级更新。
</code></pre><h2 id="hive特点"><a href="#hive特点" class="headerlink" title="hive特点"></a>hive特点</h2><pre><code>hive存储数据结构(schema)在数据库中,处理的数据进入hdfs.
OLAP
HQL / HiveQL
</code></pre><h2 id="hive安装"><a href="#hive安装" class="headerlink" title="hive安装"></a>hive安装</h2><pre><code>1.下载hive2.1-tar.gz
2.tar开
    $&gt;tar -xzvf hive-2.1.0.tar.gz -C /soft    //tar开
    $&gt;cd /soft/hive-2.1.0                    //
    $&gt;ln -s hive-2.1.0 hive                    //符号连接

3.配置环境变量
    [/etc/profile]
    HIVE_HOME=/soft/hive
    PATH=...:$HIVE_HOME/bin

4.验证hive安装成功
    $&gt;hive --v

5.配置hive,使用win7的mysql存放hive的元数据.
    a)复制mysql驱动程序到hive的lib目录下。
        ...
    b)配置hive-site.xml
        复制hive-default.xml.template为hive-site.xml
        修改连接信息为mysql链接地址，将${system:...字样替换成具体路径。
        [hive/conf/hive-site.xml]
        &lt;property&gt;
            &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;
            &lt;value&gt;root&lt;/value&gt;
            &lt;description&gt;password to use against metastore database&lt;/description&gt;
        &lt;/property&gt;
        &lt;property&gt;
            &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;
            &lt;value&gt;root&lt;/value&gt;
            &lt;description&gt;Username to use against metastore database&lt;/description&gt;
        &lt;/property&gt;
        &lt;property&gt;
            &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;
            &lt;value&gt;jdbc:mysql://192.168.231.1:3306/hive2&lt;/value&gt;
        &lt;/property&gt;
        &lt;property&gt;
            &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;
            &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;
            &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;
        &lt;/property&gt;

    6)在msyql中创建存放hive信息的数据库
        mysql&gt;create database hive2 ;

    6)初始化hive的元数据(表结构)到mysql中。
        $&gt;cd /soft/hive/bin
        $&gt;schematool -dbType mysql -initSchema
</code></pre><h2 id="hive命令行操作"><a href="#hive命令行操作" class="headerlink" title="hive命令行操作"></a>hive命令行操作</h2><pre><code>1.创建hive的数据库

    $hive&gt;hive --version                //
    $hive&gt;hive --help                    //

    $hive&gt;create database mydb2 ;                //
    $hive&gt;show databases ;
    $hive&gt;use mydb2 ;
    $hive&gt;create table mydb2.t(id int,name string,age int);
    $hive&gt;drop table t ;
    $hive&gt;drop table mydb2.t ;
    $hive&gt;select * from mydb2.t ;        //查看指定库的表
    $hive&gt;exit ;                        //退出

    $&gt;hive                                //hive --service cli
    $&gt;hive                                //hive --service cli
</code></pre><h2 id="通过远程jdbc方式连接到hive数据仓库"><a href="#通过远程jdbc方式连接到hive数据仓库" class="headerlink" title="通过远程jdbc方式连接到hive数据仓库"></a>通过远程jdbc方式连接到hive数据仓库</h2><pre><code>1.启动hiveserver2服务器，监听端口10000
    $&gt;hive --service hiveserver2 &amp;

2.通过beeline命令行连接到hiveserver2
    $&gt;beeline                                            //进入beeline命令行(于hive --service beeline)
    $beeline&gt;!help                                        //查看帮助
    $beeline&gt;!quit                                        //退出
    $beeline&gt;!connect jdbc:hive2://localhost:10000/mydb2//连接到hibve数据
    $beeline&gt;show databases ;
    $beeline&gt;use mydb2 ;
    $beeline&gt;show tables;                                //显式表
</code></pre><h2 id="使用Hive-jdbc驱动程序采用jdbc方式访问远程数据仓库"><a href="#使用Hive-jdbc驱动程序采用jdbc方式访问远程数据仓库" class="headerlink" title="使用Hive-jdbc驱动程序采用jdbc方式访问远程数据仓库"></a>使用Hive-jdbc驱动程序采用jdbc方式访问远程数据仓库</h2><pre><code>1.创建java模块
2.引入maven
3.添加hive-jdbc依赖
    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
    &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;
             xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
             xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
        &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

        &lt;groupId&gt;com.it18zhang&lt;/groupId&gt;
        &lt;artifactId&gt;HiveDemo&lt;/artifactId&gt;
        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;

        &lt;dependencies&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;
                &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt;
                &lt;version&gt;2.1.0&lt;/version&gt;
            &lt;/dependency&gt;
        &lt;/dependencies&gt;
    &lt;/project&gt;

4.App
    package com.it18zhang.hivedemo;

    import java.sql.Connection;
    import java.sql.DriverManager;
    import java.sql.ResultSet;
    import java.sql.Statement;

    /**
     * 使用jdbc方式连接到hive数据仓库，数据仓库需要开启hiveserver2服务。
     */
    public class App {
        public static void main(String[] args) throws  Exception {
            Class.forName(&quot;org.apache.hive.jdbc.HiveDriver&quot;);
            Connection conn = DriverManager.getConnection(&quot;jdbc:hive2://192.168.231.201:10000/mydb2&quot;);
            Statement st = conn.createStatement();
            ResultSet rs = st.executeQuery(&quot;select id , name ,age from t&quot;);
            while(rs.next()){
                System.out.println(rs.getInt(1) + &quot;,&quot; + rs.getString(2)) ;
            }
            rs.close();
            st.close();
            conn.close();
        }
    }
</code></pre><h2 id="hive中表"><a href="#hive中表" class="headerlink" title="hive中表"></a>hive中表</h2><pre><code>1.managed table
    托管表。
    删除表时，数据也删除了。

2.external table
    外部表。
    删除表时，数据不删。
</code></pre><h2 id="hive命令"><a href="#hive命令" class="headerlink" title="hive命令"></a>hive命令</h2><pre><code>//创建表,external 外部表
$hive&gt;CREATE external TABLE IF NOT EXISTS t2(id int,name string,age int)
COMMENT &apos;xx&apos; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; STORED AS TEXTFILE ; 

//查看表数据
$hive&gt;desc t2 ;
$hive&gt;desc formatted t2 ;

//加载数据到hive表
$hive&gt;load data local inpath &apos;/home/centos/customers.txt&apos; into table t2 ;    //local上传文件
$hive&gt;load data inpath &apos;/user/centos/customers.txt&apos; [overwrite] into table t2 ;    //移动文件

//复制表
mysql&gt;create table tt as select * from users ;        //携带数据和表结构
mysql&gt;create table tt like users ;            //不带数据，只有表结构

hive&gt;create table tt as select * from users ;    
hive&gt;create table tt like users ;    


//count()查询要转成mr
$hive&gt;select count(*) from t2 ;
$hive&gt;select id,name from t2 ;


//
$hive&gt;select * from t2 order by id desc ;                //MR

//启用/禁用表
$hive&gt;ALTER TABLE t2 ENABLE NO_DROP;    //不允许删除
$hive&gt;ALTER TABLE t2 DISABLE NO_DROP;    //允许删除


//分区表,优化手段之一，从目录的层面控制搜索数据的范围。
//创建分区表.
$hive&gt;CREATE TABLE t3(id int,name string,age int) PARTITIONED BY (Year INT, Month INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;

//显式表的分区信息
$hive&gt;SHOW PARTITIONS t3;

//添加分区,创建目录
$hive&gt;alter table t3 add partition (year=2014, month=12);

//删除分区
hive&gt;ALTER TABLE employee_partitioned DROP IF EXISTS PARTITION (year=2014, month=11);

//分区结构
hive&gt;/user/hive/warehouse/mydb2.db/t3/year=2014/month=11
hive&gt;/user/hive/warehouse/mydb2.db/t3/year=2014/month=12


//加载数据到分区表
hive&gt;load data local inpath &apos;/home/centos/customers.txt&apos; into table t3 partition(year=2014,month=11);

//创建桶表
$hive&gt;CREATE TABLE t4(id int,name string,age int) CLUSTERED BY (id) INTO 3 BUCKETS ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;

//加载数据不会进行分桶操作
$hive&gt;load data local inpath &apos;/home/centos/customers.txt&apos; into table t4 ;

//查询t3表数据插入到t4中。
$hive&gt;insert into t4 select id,name,age from t3 ;

//桶表的数量如何设置?
//评估数据量，保证每个桶的数据量block的2倍大小。


//连接查询
$hive&gt;CREATE TABLE customers(id int,name string,age int) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;
$hive&gt;CREATE TABLE orders(id int,orderno string,price float,cid int) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;

//加载数据到表
//内连接查询
hive&gt;select a.*,b.* from customers a , orders b where a.id = b.cid ;
//左外
hive&gt;select a.*,b.* from customers a left outer join orders b on a.id = b.cid ;
hive&gt;select a.*,b.* from customers a right outer join orders b on a.id = b.cid ;
hive&gt;select a.*,b.* from customers a full outer join orders b on a.id = b.cid ;

//explode,炸裂,表生成函数。
//使用hive实现单词统计
//1.建表
$hive&gt;CREATE TABLE doc(line string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;
</code></pre>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/10/28/Hadoop第十天/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/28/Hadoop第十天/" itemprop="url">Hadoop第十天</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-10-28T10:31:16+08:00">
                2018-10-28
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hadoop/" itemprop="url" rel="index">
                    <span itemprop="name">Hadoop</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hadoop/Mysql/" itemprop="url" rel="index">
                    <span itemprop="name">Mysql</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="HA"><a href="#HA" class="headerlink" title="HA"></a>HA</h2><pre><code>3个JournalNode,edit log
两个NN，active | standby
2NN
</code></pre><h2 id="ha的管理命令"><a href="#ha的管理命令" class="headerlink" title="ha的管理命令"></a>ha的管理命令</h2><pre><code>hdfs haadmin -getServiceState nn1        //查看服务
hdfs haadmin -transitionToActive nn1    //激活
hdfs haadmin -transitionToStandby nn2    //待命
hdfs haadmin -failover nn1 nn2            //对调
</code></pre><h2 id="数据仓库"><a href="#数据仓库" class="headerlink" title="数据仓库"></a>数据仓库</h2><pre><code>OLAP        //online analyze process,在线分析处理
            //延迟性高.
在于后期海量数据的查询统计hive是延迟性比较高的操作实时性不好都是批量计算
</code></pre><h2 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h2><pre><code>OLTP        //online transaction process在线事务处理.
            //实时性好。延迟性很低
        mysql针对事务性处理保证ACI特性。        
</code></pre><h2 id="HA-1"><a href="#HA-1" class="headerlink" title="HA"></a>HA</h2><pre><code>3个JournalNode,edit log
两个NN，active | standby
2NN
</code></pre><h2 id="ha的管理命令-1"><a href="#ha的管理命令-1" class="headerlink" title="ha的管理命令"></a>ha的管理命令</h2><pre><code>hdfs haadmin -getServiceState nn1        //查看服务
hdfs haadmin -transitionToActive nn1    //激活
hdfs haadmin -transitionToStandby nn2    //待命
hdfs haadmin -failover nn1 nn2            //对调
</code></pre><h2 id="数据仓库-1"><a href="#数据仓库-1" class="headerlink" title="数据仓库"></a>数据仓库</h2><pre><code>OLAP        //online analyze process,在线分析处理
            //延迟性高.
</code></pre><h2 id="数据库-1"><a href="#数据库-1" class="headerlink" title="数据库"></a>数据库</h2><pre><code>OLTP        //online transaction process在线事务处理.
            //实时性好。
</code></pre><h2 id="jdbc"><a href="#jdbc" class="headerlink" title="jdbc"></a>jdbc</h2><pre><code>java database connection,java数据库连接。
</code></pre><p>java数据库连接设计套接字编程吗：面向接口编程，依托mysql驱动程序和mysql操作， jdbc就是socket，musql就是serversocket。</p>
<p><img src="https://i.imgur.com/5s8oEhk.png" alt=""><br>0</p>
<pre><code>1.创建mysql数据库和表
     create table users(id int primary key auto_increment , name varchar(20) , age int);
2.idea中创建jdbcDemo模块
</code></pre><h2 id="事务-transaction"><a href="#事务-transaction" class="headerlink" title="事务:transaction,"></a>事务:transaction,</h2><pre><code>和数据库之间的一组操作。
特点.
a        //atomic,原子性,不可分割.
c        //consistent,不能破坏掉
i        //isolate,隔离型.
d        //durable.永久性
</code></pre><h2 id="truncate"><a href="#truncate" class="headerlink" title="truncate"></a>truncate</h2><pre><code>截断表，类似于delete操作，速度快，数据无法回滚。在Mysql里面操作
truncate table users ;
</code></pre><h2 id="sql语句"><a href="#sql语句" class="headerlink" title="sql语句"></a>sql语句</h2><pre><code>1.
2.
3.
</code></pre><h2 id="Transaction"><a href="#Transaction" class="headerlink" title="Transaction"></a>Transaction</h2><pre><code>commit            //提交
rollback        //回滚
savePoint        //保存点
</code></pre><h1 id="插入10万条数据不用预处理语句"><a href="#插入10万条数据不用预处理语句" class="headerlink" title="插入10万条数据不用预处理语句"></a>插入10万条数据不用预处理语句</h1><pre><code>import org.junit.Test;
import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.Statement;

//测试增删改查基本功能
public class TestCRUD {
    @Test
    public void testStatement() throws Exception{
        long start=System.currentTimeMillis();
        //创建连接
        String diverClass = &quot;com.mysql.jdbc.Driver&quot;;
        String url = &quot;jdbc:mysql://localhost:3306/big4&quot;;
        String username = &quot;root&quot;;
        String password = &quot;root&quot;;
        Class.forName(diverClass);
        Connection conn = DriverManager.getConnection(url, username, password);
        //关闭自动提交
        conn.setAutoCommit(false);

        //创建语句对象
        Statement st = conn.createStatement();
        for(int i=0;i&lt;100000;i++){
            String sql = &quot;insert into users(name,age) values(&apos;tomas&quot; + i + &quot;&apos;,&quot; + (i % 100) + &quot;)&quot;;
            st.execute(sql);
        }
        conn.commit();
        st.close();
        conn.close();
        System.out.println(System.currentTimeMillis() - start);
    }
}
</code></pre><h1 id="使用预处理语句："><a href="#使用预处理语句：" class="headerlink" title="使用预处理语句："></a>使用预处理语句：</h1><pre><code>import org.junit.Test;
import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.PreparedStatement;

//测试增删改查基本功能
public class TestCRUD {
    @Test
    public void testPreparedStatement() throws Exception{
        long start=System.currentTimeMillis();
        //创建连接
        String diverClass = &quot;com.mysql.jdbc.Driver&quot;;
        String url = &quot;jdbc:mysql://localhost:3306/big4&quot;;
        String username = &quot;root&quot;;
        String password = &quot;root&quot;;
        Class.forName(diverClass);
        Connection conn = DriverManager.getConnection(url, username, password);
        //关闭自动提交
        conn.setAutoCommit(false);

        //创建语句对象
        String sql = &quot;insert into users(name,age) value(?,?)&quot;;
        PreparedStatement ppst = conn.prepareStatement(sql);
        for(int i =0;i &lt; 10000;i++){
                ppst.setString(1,&quot;tom&quot;+i);
                ppst.setInt(2,i%100);
                ppst.executeUpdate();//执行更新
             }
        conn.commit();
        ppst.close();
        conn.close();
        System.out.println(System.currentTimeMillis() - start);
    }
}
</code></pre><h2 id="上述代码加一个批处理："><a href="#上述代码加一个批处理：" class="headerlink" title="上述代码加一个批处理："></a>上述代码加一个批处理：</h2><pre><code>import org.junit.Test;
import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.PreparedStatement;

//测试增删改查基本功能
public class TestCRUD {
    @Test
    public void testPreparedStatement() throws Exception {
        long start = System.currentTimeMillis();
        //创建连接
        String diverClass = &quot;com.mysql.jdbc.Driver&quot;;
        String url = &quot;jdbc:mysql://localhost:3306/big4&quot;;
        String username = &quot;root&quot;;
        String password = &quot;root&quot;;
        Class.forName(diverClass);
        Connection conn = DriverManager.getConnection(url, username, password);
        //关闭自动提交
        conn.setAutoCommit(false);

        //创建语句对象
        String sql = &quot;insert into users(name,age) value(?,?)&quot;;
        PreparedStatement ppst = conn.prepareStatement(sql);
        for (int i = 0; i &lt; 10000; i++) {
            ppst.setString(1, &quot;tom&quot; + i);
            ppst.setInt(2, i % 100);
            ppst.addBatch();//这个地方相当于一个缓冲区，相当于缓冲区里面放了2000个在传出去，减少了网络带宽速度会变快
            if (i % 200 == 0) {
                ppst.executeUpdate();//执行更新
            }
        }
        ppst.addBatch();//最后不够2000在进行一个批处理
        conn.commit();
        ppst.close();
        conn.close();
        System.out.println(System.currentTimeMillis() - start);
    }
}
</code></pre><p>这边的一个通过自回环传输一百万次插入，传输量巨大。<br>但是要是直接在Mysql端直接给个命令做一百万次插入，就很快。不用和服务器端交互很多的数据量。通过存储过程来写，存储过程就是一组sql语句。这些语句已经预先在数据库中存放了。无非就是循环了。要执行这些语句，就要发一条执行存储过程的指令，服务器已收到命令，就直接执行。 速度很快的。</p>
<p><img src="https://i.imgur.com/7TubkBX.png" alt=""></p>
<h1 id="100000条数据通过普通事务，预处理，批处理时间"><a href="#100000条数据通过普通事务，预处理，批处理时间" class="headerlink" title="100000条数据通过普通事务，预处理，批处理时间"></a>100000条数据通过普通事务，预处理，批处理时间</h1><pre><code>Statement                //46698
PreparedStatent            //43338
CallableStatement        //14385
</code></pre><p><img src="https://i.imgur.com/YJmcnEF.png" alt=""></p>
<h1 id="mysql存储过程"><a href="#mysql存储过程" class="headerlink" title="mysql存储过程"></a>mysql存储过程</h1><p>msyql&gt;– 定义新的终止符,<strong>*</strong>不要带空格这个是注释<strong>*</strong><br>mysql&gt;delimiter //</p>
<p>mysql&gt;– 创建存储过程<br>mysql&gt;CREATE PROCEDURE simpleproc (OUT param1 INT)<br>        BEGIN<br>        SELECT COUNT(*) INTO param1 FROM users;        – into 是赋值方式之一<br>        END<br>        //</p>
<p>mysql&gt;– 查看存储过程的状态<br>mysql&gt;show procedure status //</p>
<p>mysql&gt;– 查看指定存储过程创建语句<br>mysql&gt;show create procedure simpleproc ;</p>
<p>mysql&gt;– 调用存储过程,@a在命令中定义变量<br>mysql&gt;call simpleproc(@a)</p>
<p>mysql&gt;– 删除存储过程<br>mysql&gt;show drop procedure simpleproc ;</p>
<p>mysql&gt;– 定义加法存储过程,set赋值语句 :=<br>mysql&gt;create procedure sp_add(in a int,in b int, out c int)<br>            begin<br>            set c := a + b ;<br>        end<br>        //</p>
<h2 id="java访问存储过程（调用的是上一步c-a-b这个过程）"><a href="#java访问存储过程（调用的是上一步c-a-b这个过程）" class="headerlink" title="java访问存储过程（调用的是上一步c=a+b这个过程）"></a>java访问存储过程（调用的是上一步c=a+b这个过程）</h2><pre><code>import org.junit.Test;
import java.sql.*;

/**
 * 测试基本操作
 */
public class TestCRUD {
    /**
     * 存储过程
     */
    @Test
    public void testCallableStatement() throws Exception {
        long start = System.currentTimeMillis();
        //创建连接
        String driverClass = &quot;com.mysql.jdbc.Driver&quot;;
        String url = &quot;jdbc:mysql://localhost:3306/big4&quot;;
        String username = &quot;root&quot;;
        String password = &quot;root&quot;;
        Class.forName(driverClass);
        Connection conn = DriverManager.getConnection(url, username, password);
        //关闭自动提交
        conn.setAutoCommit(false);

        //创建可调用语句，调用存储过程
        CallableStatement cst = conn.prepareCall(&quot;{call sp_add(?,?,?)}&quot;);
        cst.setInt(1,2);        //绑定参数
        cst.setInt(2,3);
        //注册输出参数类型
        cst.registerOutParameter(3,Types.INTEGER);
        cst.execute();
        int sum = cst.getInt(3);
        System.out.println(sum);
        conn.commit();
        conn.close();
        System.out.println(System.currentTimeMillis() - start);
    }
}
</code></pre><h2 id="百万数据插入，存储过程的性能"><a href="#百万数据插入，存储过程的性能" class="headerlink" title="百万数据插入，存储过程的性能"></a>百万数据插入，存储过程的性能</h2><pre><code>1.创建存储过程
    mysql&gt;create procedure sp_batchinsert(in n int)
        begin
        DECLARE name0 varchar(20);    -- 定义在begin内部
        DECLARE age0 int;
        DECLARE i int default 0 ;
            while i &lt; n do
                set name0 := concat(&apos;tom&apos;,i) ;
                set age0 := i % 100 ;
                insert into users(name,age) values(name0,age0);
                set i := i + 1 ;
            end while ;
        end 

        //
2.java代码
    @Test
    public void testCallableStatement() throws Exception {
        long start = System.currentTimeMillis();
        //创建连接
        String driverClass = &quot;com.mysql.jdbc.Driver&quot;;
        String url = &quot;jdbc:mysql://localhost:3306/big4&quot;;
        String username = &quot;root&quot;;
        String password = &quot;root&quot;;
        Class.forName(driverClass);
        Connection conn = DriverManager.getConnection(url, username, password);
        //关闭自动提交
        conn.setAutoCommit(false);

        //创建可调用语句，调用存储过程
        CallableStatement cst = conn.prepareCall(&quot;{call sp_batchinsert(?)}&quot;);
        cst.setInt(1,1000000);        //绑定参数
        //注册输出参数类型
        cst.execute();
        conn.commit();
        conn.close();
        System.out.println(System.currentTimeMillis() - start);
    }
</code></pre><h1 id="mysql函数"><a href="#mysql函数" class="headerlink" title="mysql函数"></a>mysql函数</h1><p>这个是创建函数的SQL参考手册：</p>
<p><img src="https://i.imgur.com/1gvxZzF.png" alt=""></p>
<pre><code>1.函数和存储过程相似，只是多了返回值声明.
2.创建函数
    mysql&gt;create function sf_add(a int ,b int) returns int
        begin
            return a + b ;
        end
        //

3.显式创建的函数
    mysql&gt;show function status                -- 
    mysql&gt;show function status like &apos;%add%&apos;    -- 
    mysql&gt;select sf_add(1,2)                --

4.java调用函数
    @Test
    public void testFunction() throws Exception {
        long start = System.currentTimeMillis();
        //创建连接
        String driverClass = &quot;com.mysql.jdbc.Driver&quot;;
        String url = &quot;jdbc:mysql://localhost:3306/big4&quot;;
        String username = &quot;root&quot;;
        String password = &quot;root&quot;;
        Class.forName(driverClass);
        Connection conn = DriverManager.getConnection(url, username, password);
        //关闭自动提交
        conn.setAutoCommit(false);

        //创建可调用语句，调用存储过程
        CallableStatement cst = conn.prepareCall(&quot;{? = call sf_add(?,?)}&quot;);
        cst.setInt(2,100);//设置第二个参数是100第三个参数是200，第一个参数是输出。
        cst.setInt(3,200);
        cst.registerOutParameter(1,Types.INTEGER);
        //注册输出参数类型
        cst.execute();
        System.out.println(cst.getInt(1));
        conn.commit();
        conn.close();
        System.out.println(System.currentTimeMillis() - start);
    }
</code></pre><h2 id="MVCC"><a href="#MVCC" class="headerlink" title="MVCC"></a>MVCC</h2><pre><code>multiple version concurrent control,多版本并发控制。
数据库的记录存储有多条，每一条都附加版本。所以可以实现隔离级别。是因为有MVCC多版本并发控制
</code></pre><h2 id="事务的并发执行，容易出现的几个现象"><a href="#事务的并发执行，容易出现的几个现象" class="headerlink" title="事务的并发执行，容易出现的几个现象"></a>事务的并发执行，容易出现的几个现象</h2><pre><code>1.脏读
    读未提交,一个事务读取了另外一个事务改写还没有提交的数据，如果另外一个
    事务在稍后的时候回滚。

2.不可重复读
    一个事务进行相同条件查询连续的两次或者两次以上，每次结果都不同。这个是对数据进行了更改
    有其他事务做了update操作。

3.幻读
    和(2)很像，其他事务做了insert操作.1时间查询到了10个，2时刻查询到了20个，是因为中间有做了插入操作
</code></pre><h2 id="隔离级别（由于有MVCC）"><a href="#隔离级别（由于有MVCC）" class="headerlink" title="隔离级别（由于有MVCC）"></a>隔离级别（由于有MVCC）</h2><pre><code>为了避免出现哪种并发现象的。
1        //read uncommitted    ,读未提交        导致脏读导致不可重复读
2        //read committed    ,读已提交        这里避免了脏读
4        //repeatable read    ,可以重复读        这里避免了不可重复读。还是可以让他插入更新，但是读取的是更新之前的值
8        //serializable        ,串行化(悲观锁)        这里避免了换读，串行化不支持并发了就已经。
</code></pre><h2 id="演示mysql事务隔离级别"><a href="#演示mysql事务隔离级别" class="headerlink" title="演示mysql事务隔离级别"></a>演示mysql事务隔离级别</h2><pre><code>1.开启mysql客户端
    mysql&gt;
2.关闭自动提交
    mysql&gt;set autocommit 0 ;
3.每次操作数据,都要开启事务，提交事务。
</code></pre><h2 id="脏读现象"><a href="#脏读现象" class="headerlink" title="脏读现象"></a>脏读现象</h2><pre><code>[A]
    1)mysql&gt;start transaction ;                                -- 开始事务
    2)msyql&gt;update users set age = age + 1 where id = 1 ;    -- 更新数据,没有提交
    6)mysql&gt;rollback ;                                        -- 回滚
    7)mysql&gt;select * from users ;

[B]
    3)mysql&gt;set session transaction isolation level read uncommitted ;    -- 读未提交
    4)msyql&gt;start transaction ;        -- 开始事务
    5)mysql&gt;select * from users ;    -- 13
</code></pre><h2 id="避免脏读"><a href="#避免脏读" class="headerlink" title="避免脏读"></a>避免脏读</h2><pre><code>[A]
    1)mysql&gt;start transaction ;                                -- 开始事务
    2)msyql&gt;update users set age = age + 1 where id = 1 ;    -- 更新数据,没有提交
    6)mysql&gt;rollback ;                                        -- 回滚
    7)mysql&gt;select * from users ;

[B]
    3)mysql&gt;set session transaction isolation level read committed ;    -- 读已提交
    4)msyql&gt;start transaction ;        -- 开始事务
    5)mysql&gt;select * from users ;    -- 13
</code></pre><h2 id="测试不可重复读-隔离级别设置为读已提交不能避免不可重复读。"><a href="#测试不可重复读-隔离级别设置为读已提交不能避免不可重复读。" class="headerlink" title="测试不可重复读(隔离级别设置为读已提交不能避免不可重复读。)"></a>测试不可重复读(隔离级别设置为读已提交不能避免不可重复读。)</h2><pre><code>[A]
    1)mysql&gt;commit ;
    2)mysql&gt;set session transaction isolation level read committed ;    -- 读已提交
    3)mysql&gt;start transaction ;                                            -- 开始事务
    4)mysql&gt;select * from users    ;                                        -- 查询
    9)mysql&gt;select * from users    ;

[B]
    5)mysql&gt;commit;
    6)mysql&gt;start transaction ;    
    7)mysql&gt;update users set age = 15 where id = 1 ;                    -- 更新
    8)mysql&gt;commit;
</code></pre><h2 id="测试避免不可重复读-隔离级别设置为读已提交不能避免不可重复读。"><a href="#测试避免不可重复读-隔离级别设置为读已提交不能避免不可重复读。" class="headerlink" title="测试避免不可重复读(隔离级别设置为读已提交不能避免不可重复读。)"></a>测试避免不可重复读(隔离级别设置为读已提交不能避免不可重复读。)</h2><pre><code>[A]
    1)mysql&gt;commit ;
    2)mysql&gt;set session transaction isolation level repeatable read ;    -- 可以重复读
    3)mysql&gt;start transaction ;                                            -- 开始事务
    4)mysql&gt;select * from users    ;                                        -- 查询
    9)mysql&gt;select * from users    ;

[B]
    5)mysql&gt;commit;
    6)mysql&gt;start transaction ;    
    7)mysql&gt;update users set age = 15 where id = 1 ;                    -- 更新
    8)mysql&gt;commit;
</code></pre><h2 id="测试幻读-隔离级别设置为repeatable"><a href="#测试幻读-隔离级别设置为repeatable" class="headerlink" title="测试幻读(隔离级别设置为repeatable)"></a>测试幻读(隔离级别设置为repeatable)</h2><pre><code>[A]
    1)mysql&gt;commit ;
    2)mysql&gt;set session transaction isolation level serializable;        -- 串行化
    3)mysql&gt;start transaction ;                                            -- 开始事务
    4)mysql&gt;select * from users    ;                                        -- 查询
    9)mysql&gt;select * from users    ;

[B]
    5)mysql&gt;commit;
    6)mysql&gt;start transaction ;    
    7)mysql&gt;insert into users(name,age) values(&apos;tomas&apos;,13);                -- 更新
    8)mysql&gt;commit;
</code></pre><h2 id="ANSI-SQL"><a href="#ANSI-SQL" class="headerlink" title="ANSI SQL"></a>ANSI SQL</h2><pre><code>美国国家标准结构SQL组
select * from users for update ;
</code></pre><h2 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h2><pre><code>1.支持四种隔离级别。
2.默认隔离级别是可以重复读。
3.隔离级别是seriable,不支持并发写。
</code></pre><h2 id="表级锁"><a href="#表级锁" class="headerlink" title="表级锁"></a>表级锁</h2><pre><code>LOCK TABLE t WRITE;    -- 加锁(表级锁,read)

UNLOCK TABLES ;        -- 解除自己所有的所有表级锁

表级锁只能通过命令来解锁。
</code></pre><p><img src="https://i.imgur.com/cLufeUV.png" alt=""></p>
<h2 id="编程实现脏读现象"><a href="#编程实现脏读现象" class="headerlink" title="编程实现脏读现象"></a>编程实现脏读现象</h2><pre><code>package com.it18zhang.jdbcdemo.test;
import org.junit.Test;
import java.sql.*;
/**
 * 测试隔离级别
 */
public class TestIsolationLevel {

    /**
     * 执行写，不提交
     */
    @Test
    public void testA() throws  Exception{
        //创建连接
        String driverClass = &quot;com.mysql.jdbc.Driver&quot;;
        String url = &quot;jdbc:mysql://localhost:3306/big4&quot;;
        String username = &quot;root&quot;;
        String password = &quot;root&quot;;
        Class.forName(driverClass);
        Connection conn = DriverManager.getConnection(url, username, password);
        conn.setAutoCommit(false);
        Statement st = conn.createStatement();
        st.execute(&quot;update users set age = 80 where id = 1&quot;);

        System.out.println(&quot;===============&quot;);
        conn.commit();
        conn.close();
    }

    /**
     * 查询，查到别人没有提交的数据
     */
    @Test
    public void testB() throws  Exception{
        //创建连接
        String driverClass = &quot;com.mysql.jdbc.Driver&quot;;
        String url = &quot;jdbc:mysql://localhost:3306/big4&quot;;
        String username = &quot;root&quot;;
        String password = &quot;root&quot;;
        Class.forName(driverClass);
        Connection conn = DriverManager.getConnection(url, username, password);

        //设置隔离级别读未提交==&gt;导致脏读
        /************************** 设置隔离级别 ***************************************/
        conn.setTransactionIsolation(Connection.TRANSACTION_READ_UNCOMMITTED);
        conn.setAutoCommit(false);
        Statement st = conn.createStatement();


        ResultSet rs = st.executeQuery(&quot;select age from users where id = 1&quot;);
        rs.next();
        int age = rs.getInt(1) ;
        System.out.println(age);
        System.out.println(&quot;===============&quot;);
        conn.commit();
        conn.close();
    }
</code></pre><h2 id="共享读锁"><a href="#共享读锁" class="headerlink" title="共享读锁"></a>共享读锁</h2><h1 id="独占写锁"><a href="#独占写锁" class="headerlink" title="独占写锁"></a>独占写锁</h1><pre><code>一个事务写操作，另一个塞住。行级别的锁
在独占锁这块，开启事务之后，资源存在征用的情况，如果A修改&gt;=2，b修改&lt;=3那么就出现先来先用的情况，没有提交事务之前，B一直要等待状态。然后事务是you原子性的，肯定就是非交集和交集的部分都不能更改。因为原子性。查询是由隔离级别控制。在查询的时候为了不让别人更新设置隔离级别为最高级别，就是8串行化，但是很难受，连接不关掉放到连接池里面，被下一个调用，还是串行化状态，不能写入，性能差。除了设置隔离级别，我们还可以加上一个select * from users for update。这样的话就代表仅仅是对这一个查询语句的时候执行独占写锁。这个时候不需要再设置隔离级别为8了。
</code></pre><h2 id="SQL"><a href="#SQL" class="headerlink" title="SQL"></a>SQL</h2><pre><code>//
insert into users(name,age,...) values(&apos;&apos;,12,..) ;                -- insert
update users set name = &apos;xxx&apos;,age = xxx ,... where id = xxx ;    -- update
delete from users where id = xxx                                -- delete

-- 投影查询 projection.
select id,name from users where ... order by limit xxx            --select

-- 查询时直接上独占写锁
select * from users for update ;    
</code></pre><h2 id="连接查询"><a href="#连接查询" class="headerlink" title="连接查询"></a>连接查询</h2><pre><code>1.准备表[mysql.sql]
drop table if exists customers; -- 删除表
drop table if exists orders ;    -- 删除表
create table customers(id int primary key auto_increment , name varchar(20) , age int);                    -- 创建customers表
create table orders(id int primary key auto_increment , orderno varchar(20) , price float , cid int);    -- 创建orders表

-- 插入数据
insert into customers(name,age) values(&apos;tom&apos;,12);
insert into customers(name,age) values(&apos;tomas&apos;,13);
insert into customers(name,age) values(&apos;tomasLee&apos;,14);
insert into customers(name,age) values(&apos;tomason&apos;,15);

-- 插入订单数据
insert into orders(orderno,price,cid) values(&apos;No001&apos;,12.25,1);
insert into orders(orderno,price,cid) values(&apos;No002&apos;,12.30,1);
insert into orders(orderno,price,cid) values(&apos;No003&apos;,12.25,2);
insert into orders(orderno,price,cid) values(&apos;No004&apos;,12.25,2);
insert into orders(orderno,price,cid) values(&apos;No005&apos;,12.25,2);
insert into orders(orderno,price,cid) values(&apos;No006&apos;,12.25,3);
insert into orders(orderno,price,cid) values(&apos;No007&apos;,12.25,3);
insert into orders(orderno,price,cid) values(&apos;No008&apos;,12.25,3);
insert into orders(orderno,price,cid) values(&apos;No009&apos;,12.25,3);
insert into orders(orderno,price,cid) values(&apos;No0010&apos;,12.25,NULL);

---执行SQL文件
    source d:/SQL/mysql.sql

2.查询--连接查询
mysql&gt;-- 笛卡尔积查询,无连接条件查询
mysql&gt;select a.*,b.* from customers a , orders b ; 
</code></pre><p><img src="https://i.imgur.com/4hC9GS4.png" alt=""></p>
<pre><code>mysql&gt;-- 内连接,查询符合条件的记录.
mysql&gt;select a.*,b.* from customers a , orders b where a.id = b.cid ; 
</code></pre><p><img src="https://i.imgur.com/2bP8sx4.png" alt=""></p>
<pre><code>mysql&gt;-- 左外连接,查询符合条件的记录.
mysql&gt;select a.*,b.* from customers a left outer join orders b on a.id = b.cid ; 
</code></pre><p><img src="https://i.imgur.com/XKWihaX.png" alt=""></p>
<pre><code>mysql&gt;-- 右外连接,查询符合条件的记录.
mysql&gt;select a.*,b.* from customers a right outer join orders b on a.id = b.cid ; 
</code></pre><p><img src="https://i.imgur.com/LiR1yV0.png" alt=""></p>
<p><img src="https://i.imgur.com/6GdI0kM.png" alt=""></p>
<pre><code>mysql&gt;-- 全外连接,查询符合条件的记录(mysql不支持全外链接)
mysql&gt;select a.*,b.* from customers a full outer join orders b on a.id = b.cid ; 
</code></pre><p>2.查询–分组</p>
<pre><code>             字段列表  表       条件        分组        组内条件       排序     分页
mysql&gt;select ... from ... where ... group by ... having ...  order by ... limit ..


mysql&gt;-- 去重查询
mysql&gt;select distinct price,cid from orders ;

mysql&gt;-- 条件查询
mysql&gt;select price,cid from orders where price &gt; 12.27 ;

mysql&gt;-- 聚集查询
mysql&gt;select max(price) from orders ;
mysql&gt;select min(price) from orders ;
mysql&gt;select avg(price) from orders ;
mysql&gt;select sum(price) from orders ;
mysql&gt;select count(id) from orders ;

mysql&gt;-- 分组查询
mysql&gt;select max(price) from orders where cid is not null group by cid ;


mysql&gt;-- 分组查询(组内过滤)
mysql&gt;select cid ,orderno,max(price) as max_price,min(price) from orders where cid is not null group by cid having max_price &gt; 20 ;

mysql&gt;-- 降序查询
mysql&gt;select cid ,orderno,max(price) as max_price,min(price) from orders where cid is not null group by cid having max_price &gt; 20 order by max_price desc;

mysql&gt;-- 模糊查询
mysql&gt;select  * from customers where name like &apos;toma%&apos;        
mysql&gt;select  * from customers where name not like &apos;toma%&apos;        

mysql&gt;-- 范围查询
mysql&gt;select  * from customers where id in (1,2,3)        
mysql&gt;select  * from customers where id not in (1,2,3)        

mysql&gt;-- between 1 and 10,闭区间
mysql&gt;select  * from customers where id between 1 and 3 ;
mysql&gt;select  * from customers where id &gt;= 1 and id &lt;= 3 ;

mysql&gt;-- 嵌套子查询(查询没有订单的客户)
mysql&gt;select  * from customers where id not in (select distinct cid from orders where cid is not null);

mysql&gt;-- 嵌套子查询(查询订单数量&gt;2的客户)
mysql&gt;select * from customers where id in (select cid from orders group by cid having count(cid) &gt; 2);
mysql&gt;select * from customers where id in ( select t.cid from (select cid,count(*) as c from orders group by cid having c &gt; 2) as t);

mysql&gt;--向已有表中添加列
mysql&gt;--alter table orders add column area int;
mysql&gt;--设置area字段的值
mysql&gt;--update orders set area = 2 where id in(1,3,,6,7);
mysql&gt;--设置area字段的值
mysql&gt;--update orders set area = 1 where id not in(1,3,,6,7);
mysql&gt;--设置area字段的值
mysql&gt;--update orders set area = 1 where id not in(1,3,,6,7);
稍微看一下下面这个SQL语句：
mysq&gt;--select cid,area,count(price) from orders group by cid,area;//这句话的意思是对cid和area联合分组，什么意思：就是说要cid和area要都相同才能进入到同一个组里面去。
mysq&gt;--select cid,area,count(price),max(price),min(price) from orders group by cid,area order by cid asc, area desc  ;//这句话的意思是对cid和area联合分组，什么意思：就是说要cid和area要都相同才能进入到同一个组里面去。然后这个cid升序area降序是在1里面对area降序，也就是说先对cid整体升序，然后对升序完之后相同的cid里面进行降序。
</code></pre><p><img src="https://i.imgur.com/QbThVj2.png" alt=""></p>
<pre><code>mysql&gt;-- 嵌套子查询(查询客户id,客户name,订单数量,最贵订单，最便宜订单，平均订单价格 where 订单数量&gt;2的客户)
mysql&gt;select a.id,a.name,b.c,b.max,b.min,b.avg 
      from customers a,((select cid,count(cid) c , max(price) max ,min(price) min,avg(price) avg from orders group by cid having c &gt; 2) as b) 
      where a.id = b.cid ;
</code></pre><h2 id="hadoop"><a href="#hadoop" class="headerlink" title="hadoop"></a>hadoop</h2><pre><code>MR

左外连接.
</code></pre>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/10/25/Hadoop第九天/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/25/Hadoop第九天/" itemprop="url">Hadoop第九天</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-10-25T09:52:10+08:00">
                2018-10-25
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hadoop/" itemprop="url" rel="index">
                    <span itemprop="name">Hadoop</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="复习："><a href="#复习：" class="headerlink" title="复习："></a>复习：</h1><p>1.链式job编程</p>
<pre><code>MR        //Mapper+ / Reduce Mapper*
</code></pre><p>2.DBWritable</p>
<pre><code>和数据库交互。
</code></pre><p>3.Sqoop</p>
<p>4.全排序</p>
<pre><code>对reduce输出的所有结果进行排序。
</code></pre><p>5.二次排序</p>
<pre><code>对value进行排序。
</code></pre><p>6.数据倾斜</p>
<pre><code>1.reduce
2.自定义分区函数
    数据结果错 + 二次job
3.重新设计key
    数据结果错 + 二次job
</code></pre><h1 id="机架感知"><a href="#机架感知" class="headerlink" title="机架感知"></a>机架感知</h1><p>机架有一块网卡。可以连接很多主机，机架上每个刀片服务器主机都连接到这个机架网卡上的。</p>
<p>比如一台刀片服务器上有2个进程，2个进程之间通信需要走机架上的交换机吗。不需要直接走自回环网络即可。走Localhost即可。所以2个进程之间不需要网卡交互数据。2个进程之间的距离就是0。每个进程所在的主机节点，连接他们之间的网络通路的跃点数就是他们之间的距离</p>
<p>如果b中绿色进程和a中红色进程需要交互就需要通过交换机，距离就是2.怎么算的，就是从每个服务器到交换机之间的距离相加，就是1+1；</p>
<p>如果是2个机架也要通过一个交换机连接起来。左边机架a中绿色和右边机架红色通信。距离就是4.</p>
<p>同一个机房通信最多就是4。也就是在通过一个交换机。</p>
<p>Hadoop都有机架感知能力：是他里面的一种策略，都需人为维护。在默认情况下所有节点都在同一个机架上。/default-rack。<br><img src="https://i.imgur.com/jdRJXuc.png" alt=""><br>但是在临近节点数据量非常大的时候，也是不会使用的，就近原则只是一种策略，还有其他策略。</p>
<h2 id="fault-tolerance"><a href="#fault-tolerance" class="headerlink" title="fault tolerance"></a>fault tolerance</h2><pre><code>容错.
针对业务。
map或reduce任务失败，的这种错误。
</code></pre><h2 id="fail-over"><a href="#fail-over" class="headerlink" title="fail over"></a>fail over</h2><pre><code>容灾.
针对硬件故障。
</code></pre><h2 id="master-slave"><a href="#master-slave" class="headerlink" title="master / slave"></a>master / slave</h2><pre><code>主(master,namenode)从(slave,datanode)结构.
</code></pre><p> topology.node.switch.mapping.impl</p>
<h1 id="客户端请求Namenode来读取datanodes的过程"><a href="#客户端请求Namenode来读取datanodes的过程" class="headerlink" title="客户端请求Namenode来读取datanodes的过程"></a>客户端请求Namenode来读取datanodes的过程</h1><p><img src="https://i.imgur.com/XFcQNFV.png" alt=""><br>Namenode只存放文件系统的数据信息，而不存放各个节点的IP地址。存放块ID，块列表。</p>
<h2 id="可靠性"><a href="#可靠性" class="headerlink" title="可靠性"></a>可靠性</h2><pre><code>提供数据安全的能力。
</code></pre><h2 id="可用性"><a href="#可用性" class="headerlink" title="可用性"></a>可用性</h2><pre><code>提供持续服务的能力。
</code></pre><h2 id="默认的副本放置策略"><a href="#默认的副本放置策略" class="headerlink" title="默认的副本放置策略"></a>默认的副本放置策略</h2><pre><code>首选在本地机架的一个node存放副本,另一个副本在本地机架的另一个不同节点。
最后一个副本在不同机架的不同节点上。
</code></pre><p>hads oiv        //image data metadata.    离线镜像查看器<br>hads oev        //edit        编辑日志</p>
<p>镜像文件存放的是元信息，元信息包括块，块ID，块列表，是不是上下级关系，副本数，权限，块大小。除了数据内容本身内容之外的。</p>
<p>通过实现接口改变配置实现一个机架感知。</p>
<h2 id="自定义机架感知-优化hadoop集群一种方式"><a href="#自定义机架感知-优化hadoop集群一种方式" class="headerlink" title="自定义机架感知(优化hadoop集群一种方式)"></a>自定义机架感知(优化hadoop集群一种方式)</h2><pre><code>1.自定义实现类
package com.it18zhang.hdfs.rackaware;

import org.apache.hadoop.net.DNSToSwitchMapping;

import java.io.FileWriter;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

/*机架感知实现类
吧203以下的机器设置为机架1，吧203以上的机架设置为机架2
 */

public class MyRackAware implements DNSToSwitchMapping {

    public List&lt;String&gt; resolve(List&lt;String&gt; names) {
        ArrayList&lt;String&gt; list = new ArrayList&lt;String&gt;();
        //true表示是不是追加模式
        FileWriter fw = null;
        try {
            fw = new FileWriter(&quot;/home/centos/rackaware.txt&quot;, true);
            for (String str : names) {
                fw.write(str + &quot;\r\n&quot;);
                if (str.startsWith(&quot;192&quot;)) {
                    //192.168.192.202
                    String ip = str.substring(str.lastIndexOf(&quot;.&quot;) + 1);
                    if (Integer.parseInt(ip) &lt;= 203) {
                        list.add(&quot;/rack1/&quot; + ip);
                    } else {
                        list.add(&quot;/rack2&quot; + ip);
                    }
                } else if (str.startsWith(&quot;s&quot;)) {
                    String ip = str.substring(1);
                    if (Integer.parseInt(ip) &lt;= 203) {
                        list.add(&quot;/rack1/&quot; + ip);
                    } else {
                        list.add(&quot;/rack2&quot; + ip);
                    }
                }
            }
        } catch (IOException e) {
            e.printStackTrace();
        }
        return list;
    }
    public void reloadCachedMappings() {
    }
    public void reloadCachedMappings(List&lt;String&gt; names) {
    }
}


2.配置core-site.xml
    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
    &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;
    &lt;configuration&gt;
            &lt;property&gt;
                    &lt;name&gt;fs.defaultFS&lt;/name&gt;
                    &lt;value&gt;hdfs://192.168.231.201/&lt;/value&gt;
            &lt;/property&gt;
            &lt;property&gt;
                    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
                    &lt;value&gt;/home/centos/hadoop&lt;/value&gt;
            &lt;/property&gt;
            &lt;property&gt;
                    &lt;name&gt;topology.node.switch.mapping.impl&lt;/name&gt;
                    &lt;value&gt;com.it18zhang.hdfs.rackaware.MyRackAware&lt;/value&gt;
            &lt;/property&gt;
    &lt;/configuration&gt;

3.导出jar包

4.复制jar到/soft/hadoop/shared/hadoop/common/lib（这个是hadoop的类路径）

5.分发jar.(可以不做)
    实际上不需要分发，只在名称节点上运行。

6.重启名称节点
    $&gt;hadoop-daemon.sh stop namenode
    $&gt;hadoop-daemon.sh start namenode
</code></pre><hr>
<p>在s202上传一个文件，最后得出来确实和副本存放策略一致：</p>
<pre><code>首选在本地机架的一个node存放副本,另一个副本在本地机架的另一个不同节点。
最后一个副本在不同机架的不同节点上。
</code></pre><p><img src="https://i.imgur.com/agPBlr6.png" alt=""></p>
<hr>
<h1 id="关于HDFS"><a href="#关于HDFS" class="headerlink" title="关于HDFS"></a>关于HDFS</h1><p>下面这一段是关于HDFS的讲解，HDFS包括namenode和datanode，名称节点和数据节点。名称节点只是保存名称信息。那么如果在HDFS中文件移动的话怎么办，就是名称节点改变目录来移动即可。1kb和1G的速度是一样的。在HDFS中移动只是改变路径而已，块根本就没变，只能重新删掉在put才是真正移动了块。这一节就是通过实验验证了这个问题。</p>
<p><img src="https://i.imgur.com/JozXTOc.png" alt=""></p>
<hr>
<h2 id="去IOE"><a href="#去IOE" class="headerlink" title="去IOE"></a>去IOE</h2><pre><code>IBM            //
Oracle        //
EMC            //
</code></pre><h2 id="HA"><a href="#HA" class="headerlink" title="HA"></a>HA</h2><pre><code>1.NFS
    网络共享存储设备。

2.QJM
    Quorum Journal Manager

3.两个名称节点
    active            //激活
    standby            //待命
</code></pre><p>active            //激活<br>deactive        //钝化</p>
<h2 id="SPOF"><a href="#SPOF" class="headerlink" title="SPOF"></a>SPOF</h2><pre><code>single point of failure,单点故障。
</code></pre><h2 id="事务是个特性"><a href="#事务是个特性" class="headerlink" title="事务是个特性"></a>事务是个特性</h2><pre><code>a        //atomic    原子性
c        //consistent一致性
i        //isolate    隔离型
d        //durable    永久性·
</code></pre><h2 id="majority"><a href="#majority" class="headerlink" title="majority "></a>majority </h2><pre><code>大部分.
</code></pre><h2 id="HA高可用配置"><a href="#HA高可用配置" class="headerlink" title="HA高可用配置"></a>HA高可用配置</h2><pre><code>high availability,高可用.
/home/centos/hadoop/dfs/data/current/BP-2100834435-192.168.231.201-1489328949370/current/finalized/subdir0/subdir0

两个名称节点，一个active(激活态)，一个是standby(slave待命),slave节点维护足够多状态以便于容灾。
和客户端交互的active节点,standby不交互.
两个节点都和JN守护进程构成组的进行通信。

数据节点配置两个名称节点，分别报告各自的信息。

同一时刻只能有一个激活态名称节点。

脑裂:两个节点都是激活态。
为防止脑裂，JNs只允许同一时刻只有一个节点向其写数据。容灾发生时，成为active节点的namenode接管
向jn的写入工作。
</code></pre><h2 id="硬件资源"><a href="#硬件资源" class="headerlink" title="硬件资源"></a>硬件资源</h2><pre><code>名称节点:    硬件配置相同。
JN节点    :    轻量级进程，至少3个节点,允许挂掉的节点数 (n - 1) / 2.
            不需要再运行辅助名称节点。
</code></pre><h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><h2 id="配置细节"><a href="#配置细节" class="headerlink" title="配置细节"></a>配置细节</h2><pre><code>0.s201和s206具有完全一致的配置，尤其是ssh.

1.配置nameservice
    [hdfs-site.xml]
    &lt;property&gt;
        &lt;name&gt;dfs.nameservices&lt;/name&gt;
        &lt;value&gt;mycluster&lt;/value&gt;
    &lt;/property&gt;

2.dfs.ha.namenodes.[nameservice ID]
    [hdfs-site.xml]
    &lt;!-- myucluster下的名称节点两个id --&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt;
        &lt;value&gt;nn1,nn2&lt;/value&gt;
    &lt;/property&gt;

3.dfs.namenode.rpc-address.[nameservice ID].[name node ID] 
    [hdfs-site.xml]
    配置每个nn的rpc地址。
    &lt;property&gt;
        &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt;
        &lt;value&gt;s201:8020&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt;
        &lt;value&gt;s206:8020&lt;/value&gt;
    &lt;/property&gt;

4.dfs.namenode.http-address.[nameservice ID].[name node ID]
    配置webui端口
    [hdfs-site.xml]
    &lt;property&gt;
        &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt;
        &lt;value&gt;s201:50070&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt;
        &lt;value&gt;s206:50070&lt;/value&gt;
    &lt;/property&gt;

5.dfs.namenode.shared.edits.dir
    名称节点共享编辑目录.
    [hdfs-site.xml]
    &lt;property&gt;
        &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;
        &lt;value&gt;qjournal://s202:8485;s203:8485;s204:8485/mycluster&lt;/value&gt;
    &lt;/property&gt;

6.dfs.client.failover.proxy.provider.[nameservice ID]
    java类，client使用它判断哪个节点是激活态。
    [hdfs-site.xml]
    &lt;property&gt;
        &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt;
        &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;
    &lt;/property&gt;

7.dfs.ha.fencing.methods
    脚本列表或者java类，在容灾保护激活态的nn.
    [hdfs-site.xml]
    &lt;property&gt;
        &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;
        &lt;value&gt;
                sshfence
                shell(/bin/true)
        &lt;/value&gt;
    &lt;/property&gt;

    &lt;property&gt;
        &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;
        &lt;value&gt;/home/centos/.ssh/id_rsa&lt;/value&gt;
    &lt;/property&gt;

8.fs.defaultFS 
    配置hdfs文件系统名称服务。
    [core-site.xml]
    &lt;property&gt;
        &lt;name&gt;fs.defaultFS&lt;/name&gt;
        &lt;value&gt;hdfs://mycluster&lt;/value&gt;
    &lt;/property&gt; 

9.dfs.journalnode.edits.dir
    配置JN存放edit的本地路径。
    [hdfs-site.xml]
    &lt;property&gt;
        &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;
        &lt;value&gt;/home/centos/hadoop/journal&lt;/value&gt;
    &lt;/property&gt;
</code></pre><h2 id="部署细节"><a href="#部署细节" class="headerlink" title="部署细节"></a>部署细节</h2><pre><code>1.在jn节点分别启动jn进程
    $&gt;hadoop-daemon.sh start journalnode

2.启动jn之后，在两个NN之间进行disk元数据同步
    a)如果是全新集群，先format文件系统,只需要在一个nn上执行。
        [s201]
        $&gt;hadoop namenode -format
</code></pre><p>格式化文件系统主要就是生成一个新的VERSION，里面内容就是生成一个名称空间ID集群ID时间块池ID</p>
<p><img src="https://i.imgur.com/9loeVXF.png" alt=""></p>
<pre><code>b)如果将非HA集群转换成HA集群，复制原NN的metadata到另一个nn.
    1.步骤一
        [s201]
        $&gt;scp -r /home/centos/hadoop/dfs centos@s206:/home/centos/hadoop/

    2.步骤二



        在新的nn(未格式化的nn)上运行一下命令，实现待命状态引导。
        [s206]
        $&gt;hdfs namenode -bootstrapStandby        //需要s201为启动状态,提示是否格式化,选择N.

    3)在一个NN上执行以下命令，完成edit日志到jn节点的传输。
        $&gt;hdfs namenode -initializeSharedEdits
        #查看s202,s203是否有edit数据.

    4)启动所有节点.
        [s201]
        $&gt;hadoop-daemon.sh start namenode        //启动名称节点
        $&gt;hadoop-daemons.sh start datanode        //启动所有数据节点

        [s206]
        $&gt;hadoop-daemon.sh start namenode        //启动名称节点
</code></pre><h2 id="HA管理"><a href="#HA管理" class="headerlink" title="HA管理"></a>HA管理</h2><pre><code>$&gt;hdfs haadmin -transitionToActive nn1                //切成激活态
$&gt;hdfs haadmin -transitionToStandby nn1                //切成待命态
$&gt;hdfs haadmin -transitionToActive --forceactive nn2//强行激活
$&gt;hdfs haadmin -failover nn1 nn2                    //模拟容灾演示,从nn1切换到nn2
</code></pre>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/10/22/Hadoop第八天/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/22/Hadoop第八天/" itemprop="url">Hadoop第八天</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-10-22T19:40:51+08:00">
                2018-10-22
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hadoop/" itemprop="url" rel="index">
                    <span itemprop="name">Hadoop</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="二次排序链条化"><a href="#二次排序链条化" class="headerlink" title="二次排序链条化"></a>二次排序链条化</h1><p>分组不能解决数据倾斜问题，已经进入到了同一个reduce对象里面了，分一个组出来就进入同一个reduce方法里面。每次分组只是决定调用不调用同一个reduce方法。已经晚了，数据已经进入到了同一个reduce了，分区可以决定数据倾斜问题。</p>
<p>reduce分为方法和对象，有的时候说reduce是指的是方法，有的时候指的是对象，要分开对待。</p>
<p>单词统计做统计肯定不能哈希分区，因为哈希同一个值进入到同一个reduce对象利民区去了</p>
<h1 id="数据倾斜问题-随机分区-二次MR"><a href="#数据倾斜问题-随机分区-二次MR" class="headerlink" title="数据倾斜问题 随机分区 二次MR"></a>数据倾斜问题 随机分区 二次MR</h1><pre><code>1.
2.
3.
4.
</code></pre><p>如果正常按照wordcount来处理会分为</p>
<p>reduce不设置就是1，map没机会设置数量。但是也可以设置map，但是意义，以为mr提交的时候map的个数取决于切片的个数。</p>
<p>切片的计算公式：min block、maxsplit、 blocksize取中间值<br>等于blocksize。</p>
<p>现在文件如下，现在3个文件每个文件有一百万数据且有一百万个hello就对应了3个map。（这里没明白为什么3个切片也就是为啥3个map）</p>
<p>每个切片里，分区个数等于reduce个数，分区的个数取决于reduce个数，之前自己已经在WCAPP里面设置好了reduce个数。</p>
<p>我有3个map，4个reduce。那么每个map里面就有4个分区了。</p>
<p><strong>要解决哈希分区带来的数据倾斜问题，可以通过随机分区，然后在进行二次mr来解决这个问题</strong></p>
<p><img src="https://i.imgur.com/VoHpfBH.png" alt=""></p>
<p>组合combineor是对分区里面的预先聚合，下面的hello就不会发送一百万次，而是提前聚合了发送了hello 一百万次。这个样子。能解决目前的倾斜问题，但是不可能这么多数据，可能量很大</p>
<p>下面这种情况下，所有的hello根据哈希分区就进入到同一个分区，然后一百万个tom分4个分区</p>
<p>通过随机分区可以解决数据倾斜问题，但是会产生新的问题，就是说hello被分到4个不同的reduce里面，导致reduce1产生hello10reduce2产生hello13这种问题，所以通过二次mr来解决这个问题，让hello这个单词统计，进入到同一个reduce结果当中。所以也就是通过随即分区和二次mr解决</p>
<pre><code>[1.txt]1000000
hello tom1
hello tom2
hello tom3
hello tom4
hello tom5
hello tom6
hello tom7
hello tom8
hello tom9
hello tom10

[2.txt]1000000
hello tom11
hello tom12
hello tom13
hello tom14
hello tom15
hello tom16
hello tom17
hello tom18
hello tom19
hello tom20

[3.txt]1000000
hello tom21
hello tom22
hello tom23
hello tom24
hello tom25
hello tom26
hello tom27
hello tom28
hello tom29
hello tom30
</code></pre><p>代码如下：<br>    //自定义分区函数<br>    public class RandomPartitioner extends Partitioner&lt;Text,IntWritable&gt; {<br>        @Override<br>        public int getPartition(Text text, IntWritable intWritable, int numPartitions) {<br>            return new Random().nextInt(numPartitions);<br>        }<br>    }</p>
<hr>
<pre><code>//解决数据倾斜问题：
public class WCSkueApp {

    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;);
        Job job = Job.getInstance(conf);


//        设置作业的各种属性
        job.setJobName(&quot;MaxTempApp&quot;);    //作业名称
        job.setJarByClass(WCSkueApp.class); //搜索类路径
        job.setInputFormatClass(TextInputFormat.class);//设置输入格式类

        //添加输入路径
        FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew&quot;));
        //设置输出路径
        org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(&quot;e:/mr/skew/out&quot;));

        //设置随机分区：
        job.setPartitionerClass(RandomPartitioner.class);

        job.setMapperClass(WCSkueMapper.class);         //mapper类
        job.setReducerClass(WCSkueReducer.class);       //reduce类

        job.setNumReduceTasks(4);


        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(IntWritable.class);

        job.setOutputKeyClass(Text.class);          //设置输出类型
        job.setOutputValueClass(IntWritable.class);

        job.waitForCompletion(true);

    }
}
</code></pre><hr>
<pre><code>public class WCSkueMapper extends Mapper &lt;LongWritable, Text, Text, IntWritable&gt;{
    public WCSkueMapper(){
        System.out.println(&quot;new WCMapper&quot;);
    }

    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        String[] arr = value.toString().split(&quot; &quot;);
        Text keyout= new Text();
        IntWritable valueout = new IntWritable();

        for(String s:arr){
            keyout.set(s);
            valueout.set(1);
            context.write(keyout,valueout);
        }
    }
}
</code></pre><hr>
<pre><code>public class WCSkueReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{

    /**
     * reduce
     */
    protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {
        int count = 0 ;
        for(IntWritable iw : values){
            count = count + iw.get() ;
        }
        context.write(key,new IntWritable(count));


    }
}
</code></pre><hr>
<p>//解决数据倾斜问题：<br>    public class WCSkueApp2 {</p>
<pre><code>    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;);
        Job job = Job.getInstance(conf);


//        设置作业的各种属性
        job.setJobName(&quot;MaxTempApp&quot;);    //作业名称
        job.setJarByClass(WCSkueApp2.class); //搜索类路径
        job.setInputFormatClass(TextInputFormat.class);//设置输入格式类

        //添加输入路径
        FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00000&quot;));
        FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00001&quot;));
        FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00002&quot;));
        FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00003&quot;));
        //设置输出路径
        org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(&quot;e:/mr/skew/out2&quot;));


        job.setMapperClass(WCSkueMapper2.class);         //mapper类
        job.setReducerClass(WCSkueReducer2.class);       //reduce类

        job.setNumReduceTasks(4);


        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(IntWritable.class);

        job.setOutputKeyClass(Text.class);          //设置输出类型
        job.setOutputValueClass(IntWritable.class);

        job.waitForCompletion(true);

    }
}
</code></pre><hr>
<pre><code>public class WCSkueMapper2 extends Mapper &lt;LongWritable, Text, Text, IntWritable&gt;{
    public WCSkueMapper2(){
        System.out.println(&quot;new WCMapper&quot;);
    }

    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        String[] arr = value.toString().split(&quot;\t&quot;);
        Text keyout= new Text();
        IntWritable valueout = new IntWritable();
        context.write(new Text(arr[0]),new IntWritable(Integer.parseInt(arr[1])));
    }
}
</code></pre><hr>
<pre><code>public class WCSkueReducer2 extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{

    /**
     * reduce
     */
    protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {
        int count = 0 ;
        for(IntWritable iw : values){
            count = count + iw.get() ;
        }
        context.write(key,new IntWritable(count));


    }
}
</code></pre><hr>
<p>上面这个就是通过2次mr解决数据倾斜问题    但是不用那么麻烦的去切分了直接使用Keyvalueinputformat即可实现：如下代码：<br>    //解决数据倾斜问题：<br>    public class WCSkueApp2 {</p>
<pre><code>    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;);
        Job job = Job.getInstance(conf);


//        设置作业的各种属性
        job.setJobName(&quot;MaxTempApp&quot;);    //作业名称
        job.setJarByClass(WCSkueApp2.class); //搜索类路径
        job.setInputFormatClass(KeyValueTextInputFormat.class);//设置输入格式类

        //添加输入路径
        FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00000&quot;));
        FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00001&quot;));
        FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00002&quot;));
        FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00003&quot;));
        //设置输出路径
        org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(&quot;e:/mr/skew/out2&quot;));


        job.setMapperClass(WCSkueMapper2.class);         //mapper类
        job.setReducerClass(WCSkueReducer2.class);       //reduce类

        job.setNumReduceTasks(4);


        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(IntWritable.class);

        job.setOutputKeyClass(Text.class);          //设置输出类型
        job.setOutputValueClass(IntWritable.class);

        job.waitForCompletion(true);

    }
}
</code></pre><hr>
<pre><code>public class WCSkueMapper2 extends Mapper &lt;Text, Text, Text, IntWritable&gt;{
    public WCSkueMapper2(){
        System.out.println(&quot;new WCMapper&quot;);
    }

    protected void map(Text key, Text value, Context context) throws IOException, InterruptedException {
        context.write(key,new IntWritable(Integer.parseInt(value.toString())));
    }
}
</code></pre><hr>
<pre><code>public class WCSkueReducer2 extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{

    /**
     * reduce
     */
    protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {
        int count = 0 ;
        for(IntWritable iw : values){
            count = count + iw.get() ;
        }
        context.write(key,new IntWritable(count));
    }
}
</code></pre><hr>
<hr>
<hr>
<h1 id="链条式编程"><a href="#链条式编程" class="headerlink" title="链条式编程"></a>链条式编程</h1><p><img src="https://i.imgur.com/VwHIXax.png" alt=""></p>
<p>讲解上图：<br>首先通过MapMapper1来切割，然后通过MapMapper1过滤掉法轮功，真个是一个链条，在map端，执行完之后开始聚合，在reduce端，在聚合的时候，每个单词都有各自的个数，对产生的结果再次过滤，过滤掉少于5的单词，当然可以放到同一个map里面，但是如果变成模块方法，易于维护。更加方便使用。<br>m阶段可以有多个m但是R阶段只能有一个r。但是r阶段可以有多个m。</p>
<p>代码如下图：<br>//链条式job任务<br>    public class WCChainApp {</p>
<pre><code>    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;);

        Job job = Job.getInstance(conf);

//        设置作业的各种属性
        job.setJobName(&quot;MaxTempApp&quot;);    //作业名称
        job.setJarByClass(WCChainApp.class); //搜索类路径
        job.setInputFormatClass(TextInputFormat.class);//设置输入格式类

        //添加输入路径
        FileInputFormat.addInputPath(job, new Path(&quot;e:/mr/skew&quot;));
        //设置输出路径
        org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(&quot;e:mr/skew/out&quot;));

        //在map链条上添加一个mapper的环节
        ChainMapper.addMapper(job,WCMapMapper1.class,LongWritable.class,Text.class,Text.class,IntWritable.class,conf);
        ChainMapper.addMapper(job,WCMapMapper2.class,Text.class,IntWritable.class,Text.class,IntWritable.class,conf);

        //在reduce链条上设置reduce
        ChainReducer.setReducer(job,WCReducer.class,Text.class,IntWritable.class,Text.class,IntWritable.class,conf);
        ChainReducer.addMapper(job,WCReduceMapper1.class,Text.class,IntWritable.class,Text.class,IntWritable.class,conf);

        job.setNumReduceTasks(3);


        job.waitForCompletion(true);

    }
}
</code></pre><hr>
<pre><code>public class WCMapMapper1 extends Mapper &lt;LongWritable, Text, Text, IntWritable&gt;{

    protected   void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        Text keyout= new Text();
        IntWritable valueout = new IntWritable();
        String[] arr = value.toString().split(&quot; &quot;);
        for(String s:arr){
            keyout.set(s);
            valueout.set(1);
            context.write(keyout,valueout);
        }
    }
}
</code></pre><hr>
<pre><code>public class WCMapMapper2 extends Mapper &lt;Text, IntWritable, Text, IntWritable&gt;{

    protected void map(Text key, IntWritable value, Context context) throws IOException, InterruptedException {
        if(!key.toString().equals(&quot;falungong&quot;)){
            context.write(key , value);
        }

    }
}
</code></pre><hr>
<pre><code>//过滤单词个数
public class WCReduceMapper1 extends Mapper&lt;Text,IntWritable,Text,IntWritable&gt; {
    @Override
    protected void map(Text key, IntWritable value, Context context) throws IOException, InterruptedException {
        if(value.get() &gt; 5){
            context.write(key,value);
        }
    }
}
</code></pre><hr>
<pre><code>/**
 * Reducer
 */
public class WCReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{

    /**
     * reduce
     */
    protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {
        int count = 0 ;
        for(IntWritable iw : values){
            count = count + iw.get() ;
        }
        context.write(key,new IntWritable(count));


    }
}
</code></pre><hr>
<hr>
<hr>
<h2 id="FileInputFormat（读源码）"><a href="#FileInputFormat（读源码）" class="headerlink" title="FileInputFormat（读源码）"></a>FileInputFormat（读源码）</h2><pre><code>获取切片集合。
子类都要重写方法isSplittable();
负责创建RecordReader对象。
设置IO路径。
</code></pre><h2 id="RecordReader（读源码）"><a href="#RecordReader（读源码）" class="headerlink" title="RecordReader（读源码）"></a>RecordReader（读源码）</h2><pre><code>负责从InputSplit中读取KV对。
</code></pre><h2 id="jdbc笔记模板："><a href="#jdbc笔记模板：" class="headerlink" title="jdbc笔记模板："></a>jdbc笔记模板：</h2><pre><code>[写操作]
Class.forName(&quot;com.mysql.jdbc.Driver&quot;);
Connection conn = DriverMananger.getConnection(&quot;jdbc:mysql://localhost:3306/big4&quot;,&quot;root&quot;,&quot;root&quot;);
//预处理语句
PreparedStatement ppst = conn.preparedStatement(&quot;insert into test(id,name,age) values(?,?,?)&quot;);
//绑定参数
ppst.setInteger(1,1);
ppst.setInteger(2,&quot;tom&quot;);
ppst.setInteger(3,12);

ppst.executeUpdate();
ppst.close();
conn.close();


[读操作]
Class.forName(&quot;com.mysql.jdbc.Driver&quot;);
Connection conn = DriverMananger.getConnection(&quot;jdbc:mysql://localhost:3306/big4&quot;,&quot;root&quot;,&quot;root&quot;);

ppst = conn.preparedStatement(&quot;select id,name from test &quot;);
//结果集
ResultSet rs = ppst.executeQuery();
while(rs.next()){
    int id = rs.getInt(&quot;id&quot;);
    String name = rs.getInt(&quot;name&quot;);
}
rs.close();
conn.close();
</code></pre><p>看一下下源码：<br><img src="https://i.imgur.com/5ZrVZsW.png" alt=""><br>看一下写和读入都把结果集和预处理语句都已经封装进去了，剩下的只要填空就好了。</p>
<p>以下是从数据库中读入的代码模板：</p>
<h1 id="使用DBWritable向数据库从数据库中读取"><a href="#使用DBWritable向数据库从数据库中读取" class="headerlink" title="使用DBWritable向数据库从数据库中读取"></a>使用DBWritable向数据库从数据库中读取</h1><h2 id="1-准备数据库"><a href="#1-准备数据库" class="headerlink" title="1.准备数据库"></a>1.准备数据库</h2><pre><code>create database big4 ;
use big4 ;
create table words(id int primary key auto_increment , name varchar(20) , txt varchar(255));

insert into words(name,txt) values(&apos;tomas&apos;,&apos;hello world tom&apos;);
insert into words(txt) values(&apos;hello tom world&apos;);
insert into words(txt) values(&apos;world hello tom&apos;);
insert into words(txt) values(&apos;world tom hello&apos;);
</code></pre><h2 id="2-编写hadoop-MyDBWritable"><a href="#2-编写hadoop-MyDBWritable" class="headerlink" title="2.编写hadoop MyDBWritable."></a>2.编写hadoop MyDBWritable.</h2><pre><code>import org.apache.hadoop.mapreduce.lib.db.DBWritable;

import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;
import java.sql.PreparedStatement;
import java.sql.ResultSet;
import java.sql.SQLException;

/**
 * MyDBWritable
 */
public class MyDBWritable implements DBWritable,Writable {
    private int id ;
    private String name ;
    private String txt ;

    public int getId() {
        return id;
    }

    public void setId(int id) {
        this.id = id;
    }

    public String getName() {
        return name;
    }

    public void setName(String name) {
        this.name = name;
    }

    public String getTxt() {
        return txt;
    }

    public void setTxt(String txt) {
        this.txt = txt;
    }

    public void write(DataOutput out) throws IOException {
        out.writeInt(id);
        out.writeUTF(name);
        out.writeUTF(txt);
    }

    public void readFields(DataInput in) throws IOException {
        id = in.readInt();
        name = in.readUTF();
        txt = in.readUTF();
    }

    /**
     * 写入db
     */
    public void write(PreparedStatement ppst) throws SQLException {
        ppst.setInt(1,id);
        ppst.setString(2,name);
        ppst.setString(3,txt);
    }

    /**
     * 从db读取
     */
    public void readFields(ResultSet rs) throws SQLException {
        id = rs.getInt(1);
        name = rs.getString(2);
        txt = rs.getString(3);
    }
}
</code></pre><h2 id="3-WcMapper"><a href="#3-WcMapper" class="headerlink" title="3.WcMapper"></a>3.WcMapper</h2><pre><code>public class WCMapper extends Mapper&lt;LongWritable,MyDBWritable,Text,IntWritable&gt; {

    protected void map(LongWritable key, MyDBWritable value, Context context) throws IOException, InterruptedException {
        System.out.println(key);
        String line = value.getTxt();
        System.out.println(value.getId() + &quot;,&quot; + value.getName());
        String[] arr = line.split(&quot; &quot;);
        for(String s : arr){
            context.write(new Text(s),new IntWritable(1));
        }
    }
}
</code></pre><h2 id="4-WCReducer"><a href="#4-WCReducer" class="headerlink" title="4.WCReducer"></a>4.WCReducer</h2><pre><code>protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {
    int count = 0 ;
    for(IntWritable w : values){
        count = count + w.get() ;
    }
    context.write(key,new IntWritable(count));
}
</code></pre><h2 id="5-WCApp"><a href="#5-WCApp" class="headerlink" title="5.WCApp"></a>5.WCApp</h2><pre><code>public static void main(String[] args) throws Exception {
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf);

    //设置job的各种属性
    job.setJobName(&quot;MySQLApp&quot;);                        //作业名称
    job.setJarByClass(WCApp.class);                 //搜索类

    //配置数据库信息
    String driverclass = &quot;com.mysql.jdbc.Driver&quot; ;
    String url = &quot;jdbc:mysql://localhost:3306/big4&quot; ;
    String username= &quot;root&quot; ;
    String password = &quot;root&quot; ;
    //设置数据库配置
    DBConfiguration.configureDB(job.getConfiguration(),driverclass,url,username,password);
    //设置数据输入内容
    DBInputFormat.setInput(job,MyDBWritable.class,&quot;select id,name,txt from words&quot;,&quot;select count(*) from words&quot;);

    //设置输出路径
    FileOutputFormat.setOutputPath(job,new Path(&quot;e:/mr/sql/out&quot;));

    //设置分区类
    job.setMapperClass(WCMapper.class);             //mapper类
    job.setReducerClass(WCReducer.class);           //reducer类

    job.setNumReduceTasks(3);                       //reduce个数

    job.setMapOutputKeyClass(Text.class);           //
    job.setMapOutputValueClass(IntWritable.class);  //

    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);     //

    job.waitForCompletion(true);
}
</code></pre><h2 id="6-pom-xml增加mysql驱动"><a href="#6-pom-xml增加mysql驱动" class="headerlink" title="6.pom.xml增加mysql驱动"></a>6.pom.xml增加mysql驱动</h2><pre><code>&lt;dependency&gt;
    &lt;groupId&gt;mysql&lt;/groupId&gt;
    &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;
    &lt;version&gt;5.1.17&lt;/version&gt;
&lt;/dependency&gt;
</code></pre><h2 id="7-将mr的统计结果写入mysql数据库"><a href="#7-将mr的统计结果写入mysql数据库" class="headerlink" title="7.将mr的统计结果写入mysql数据库"></a>7.将mr的统计结果写入mysql数据库</h2><pre><code>a)准备表
    create table stats(word varchar(50),c int);
b)设置App的DBOutputFormat类
    com.it18zhang.hdfs.mr.mysql.WCApp
d)
e)
f)
</code></pre><hr>
<h1 id="mysql分页查询"><a href="#mysql分页查询" class="headerlink" title="mysql分页查询"></a>mysql分页查询</h1><p>如图所示吧：</p>
<p><img src="https://i.imgur.com/0hHJN3B.png" alt=""></p>
<p><img src="https://i.imgur.com/FAbkIff.png" alt=""></p>
<p><img src="https://i.imgur.com/9wCCeH2.png" alt=""></p>
<h1 id="使用DBWritable向数据库从数据库中写入"><a href="#使用DBWritable向数据库从数据库中写入" class="headerlink" title="使用DBWritable向数据库从数据库中写入"></a>使用DBWritable向数据库从数据库中写入</h1><pre><code>public class MyDBWritalbe implements DBWritable,Writable {
    private int id=0;
    private String name=&quot;&quot;;
    private String txt=&quot;&quot;;
    private  String word=&quot;&quot;;
    private int wordcount=0;

    public String getWord() {
        return word;
    }

    public void setWord(String word) {
        this.word = word;
    }

    public int getWordcount() {
        return wordcount;
    }

    public void setWordcount(int wordcount) {
        this.wordcount = wordcount;
    }

    public int getId() {
        return id;
    }

    public void setId(int id) {
        this.id = id;
    }

    public String getName() {
        return name;
    }

    public void setName(String name) {
        this.name = name;
    }

    public String getTxt() {
        return txt;
    }

    public void setTxt(String txt) {
        this.txt = txt;
    }

    public void write(DataOutput out) throws IOException {

        out.writeInt(id);
        out.writeUTF(name);
        out.writeUTF(txt);
        out.writeUTF(word);
        out.writeInt(wordcount);
    }
    public void readFields(DataInput in) throws IOException {
        id=in.readInt();
        name=in.readUTF();
        txt=in.readUTF();
        word=in.readUTF();
        wordcount=in.readInt();

    }
    //向数据库中写入DB
    public void write(PreparedStatement ppst) throws SQLException {
        //要求定制字段列表的时候先单词后个数。

        ppst.setString(1,word);
        ppst.setInt(2,wordcount);

    }
    //从DB中读出
    public void readFields(ResultSet rs) throws SQLException {
        id=rs.getInt(1);
        name=rs.getString(2);
        txt=rs.getString(3);

    }
}
</code></pre><hr>
<pre><code>public class WCApp {
    //
//    public static void main(String[] args) throws Exception {
//        Configuration conf = new Configuration();
//
//        Job job = Job.getInstance(conf);
//
////        设置作业的各种属性
//        job.setJobName(&quot;MySQLApp&quot;);    //作业名称
//        job.setJarByClass(WCApp.class); //搜索类路径
//
//        //配置数据库信息
//        String driverclass = &quot;com.mysql.jdbc.Driver&quot;;
//        String url = &quot;jdbc:mysql://localhost:3306/big4&quot;;
//        String usrname = &quot;root&quot;;
//        String password = &quot;root&quot;;
//        DBConfiguration.configureDB(conf,driverclass,url,usrname,password);
//        //设置数据输入内容
//        DBInputFormat.setInput(job,DBWritable.class,&quot;select id,name,txt from words&quot;,&quot;select count(*) from words&quot;);
//        //设置输出路径
//        FileOutputFormat.setOutputPath(job, new Path(&quot;d:/mr/sql/out&quot;));
//
//        job.setMapperClass(WCMapper.class);         //mapper类
//        job.setReducerClass(WCReducer.class);       //reduce类
//
//        job.setNumReduceTasks(3);
//
//
//        job.setMapOutputKeyClass(Text.class);
//        job.setMapOutputValueClass(IntWritable.class);
//
//        job.setOutputKeyClass(Text.class);          //设置输出类型
//        job.setOutputValueClass(IntWritable.class);
//
//        job.waitForCompletion(true);
//
//    }
//}
    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf);

        //设置job的各种属性
        job.setJobName(&quot;MySQLApp&quot;);                        //作业名称
        job.setJarByClass(WCApp.class);                 //搜索类

        //配置数据库信息
        String driverclass = &quot;com.mysql.jdbc.Driver&quot;;
        String url = &quot;jdbc:mysql://localhost:3306/big4&quot;;
        String username = &quot;root&quot;;
        String password = &quot;root&quot;;
        //设置数据库配置
        DBConfiguration.configureDB(job.getConfiguration(), driverclass, url, username, password);
        //设置数据输入内容
        DBInputFormat.setInput(job, MyDBWritalbe.class, &quot;select id,name,txt from words&quot;, &quot;select count(*) from words&quot;);
        //这里定制字段列表，先单词后个数
        DBOutputFormat.setOutput(job,&quot;stats&quot;,&quot;word&quot;,&quot;c&quot;);

        //设置分区类
        job.setMapperClass(WCMapper.class);             //mapper类
        job.setReducerClass(WCReducer.class);           //reducer类

        job.setNumReduceTasks(3);                       //reduce个数

        job.setMapOutputKeyClass(Text.class);           //
        job.setMapOutputValueClass(IntWritable.class);  //

        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);     //

        job.waitForCompletion(true);
    }
}
</code></pre><hr>
<pre><code>public class WCMapper extends Mapper&lt;LongWritable, MyDBWritalbe, Text, IntWritable&gt; {
    @Override
    protected void map(LongWritable key, MyDBWritalbe value, Context context) throws IOException, InterruptedException {
        System.out.println(key);
        String line = value.getTxt();
        System.out.println(value.getId() + &quot;,&quot; + value.getName());
        String[] arr = line.split(&quot; &quot;);
        for (String s : arr) {
            context.write(new Text(s), new IntWritable(1));
        }
    }
}
</code></pre><hr>
<pre><code>public class WCReducer extends Reducer&lt;Text, IntWritable, MyDBWritalbe, NullWritable&gt; {

    protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {
        int count = 0;
        for (IntWritable w : values) {
            count = count + w.get();
        }
        MyDBWritalbe keyout = new MyDBWritalbe();
        keyout.setWord(key.toString());
        keyout.setWordcount(count);
        context.write(keyout, NullWritable.get());
    }
}
</code></pre><hr>
<h1 id="在虚拟机中跑wordcount写入数据库问题："><a href="#在虚拟机中跑wordcount写入数据库问题：" class="headerlink" title="在虚拟机中跑wordcount写入数据库问题："></a>在虚拟机中跑wordcount写入数据库问题：</h1><h2 id="需要修改的地方："><a href="#需要修改的地方：" class="headerlink" title="需要修改的地方："></a>需要修改的地方：</h2><p>1.修改url</p>
<p><img src="https://i.imgur.com/8lkYD5v.png" alt=""></p>
<p>2.修改core-site.xml或者删除掉</p>
<p><img src="https://i.imgur.com/weFYMBJ.png" alt=""></p>
<p>3.在虚拟机中分发mysql-connector-java-5.1.7-bin.jar将之分发到每个节点的/soft/hadoop/share/hadoop/common/lib目录下。</p>
<p><img src="https://i.imgur.com/ou52Qhr.png" alt=""></p>
<p>4.在运行</p>
<pre><code>hadoop jar HdfsDemo.jar com.it18zhang.hdfs.mr.mysql.WCApp
</code></pre><h2 id="出现的问题："><a href="#出现的问题：" class="headerlink" title="出现的问题："></a>出现的问题：</h2><p>1.出现连接不上的报错：</p>
<p><img src="https://i.imgur.com/JgE2Q3e.png" alt=""></p>
<p>百度了一下完美解决，是由于mysql没有对所有用户开启权限导致：</p>
<p><a href="https://blog.csdn.net/OldTogether/article/details/79612627?utm_source=blogxgwz1" target="_blank" rel="noopener">https://blog.csdn.net/OldTogether/article/details/79612627?utm_source=blogxgwz1</a></p>
<p>2.出现一下问题：</p>
<p><img src="https://i.imgur.com/vgUGlEc.png" alt=""></p>
<p>原因是有一个s205的防火墙没有关掉导致的：</p>
<p><a href="https://blog.csdn.net/shirdrn/article/details/7280040" title="防火墙没关掉导致的" target="_blank" rel="noopener">https://blog.csdn.net/shirdrn/article/details/7280040</a></p>
<hr>
<p>完</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/10/17/Hadoop第七天/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/17/Hadoop第七天/" itemprop="url">Hadoop第七天</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-10-17T16:17:15+08:00">
                2018-10-17
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hadoop/" itemprop="url" rel="index">
                    <span itemprop="name">Hadoop</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="多输入问题"><a href="#多输入问题" class="headerlink" title="多输入问题"></a>多输入问题</h1><p>在IDEA里面代码：</p>
<p>首先明确一点InputInputformat也就是文本输入格式的输入是Longwritblele和Text，然后SequenceFileputFormat的输入是intwritble和text。</p>
<pre><code>public class WCApp {

    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;);

        Job job = Job.getInstance(conf);

//       设置作业的各种属性
        job.setJobName(&quot;WCAppMulti&quot;);    //作业名称
        job.setJarByClass(WCApp.class); //搜索类路径

        //多个输入
        MultipleInputs.addInputPath(job,new Path(&quot;file:///e:/mr/txt&quot;),TextInputFormat.class,WCTextMapper.class);
        MultipleInputs.addInputPath(job,new Path(&quot;file:///e:/mr/seq&quot;),SequenceFileInputFormat.class,WCSeqMapper.class);

        //设置输出
        FileOutputFormat.setOutputPath(job,new Path(args[0]));

        job.setReducerClass(WCReducer.class);//reducer类
        job.setNumReduceTasks(3);//reducer个数


        job.setOutputKeyClass(Text.class);          //设置输出类型
        job.setOutputValueClass(IntWritable.class);

        job.waitForCompletion(true);

    }
}
</code></pre><hr>
<pre><code>ublic class WCReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{

/**
 * reduce
 */
protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {
    int count = 0 ;
    for(IntWritable iw : values){
        count = count + iw.get() ;
    }
    String tno = Thread.currentThread().getName();
    System.out.println(tno + &quot;WCReducer:&quot; + key.toString() + &quot;=&quot; + count);
    context.write(key,new IntWritable(count));
}
</code></pre><p>}</p>
<hr>
<pre><code>public class WCTextMapper extends Mapper &lt;LongWritable, Text, Text, IntWritable&gt;{
    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        String[] arr = value.toString().split(&quot; &quot;);
        Text keyout= new Text();
        IntWritable valueout = new IntWritable();
        for(String s:arr){
            keyout.set(s);
            valueout.set(1);
            context.write(keyout,valueout);
        }
    }
}
</code></pre><hr>
<pre><code>public class WCTextMapper extends Mapper &lt;LongWritable, Text, Text, IntWritable&gt;{
    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        String[] arr = value.toString().split(&quot; &quot;);
        Text keyout= new Text();
        IntWritable valueout = new IntWritable();
        for(String s:arr){
            keyout.set(s);
            valueout.set(1);
            context.write(keyout,valueout);
        }
    }
}
</code></pre><hr>
<p>下图是配置本地的路径。要明确其中的arg[]也就是这个里配置的那个Program arguments的路径。因为只需要一个参数就可以了，因为在代码中已经写好了输入路径了。</p>
<p><img src="https://i.imgur.com/vO9wETa.png" alt=""></p>
<hr>
<hr>
<hr>
<h1 id="计数器"><a href="#计数器" class="headerlink" title="计数器"></a>计数器</h1><p> 日志目录：<br>/soft/hadoop/logs/userlogs</p>
<p>用户打印的日志。Hadoop输出的日志都是在外层的文件之中。userlogs是自己的日志打印。</p>
<h2 id="计数器-1"><a href="#计数器-1" class="headerlink" title="计数器"></a>计数器</h2><p>是其他的内容都不做更改，在Mapper和Reducer里面添加这个语句即可<br>    context.getCounter(“r”, “WCReducer.reduce”).increment(1);</p>
<p>然后扔到虚拟机里面去运行：<br>    hadoop jar HdfsDemo.jar com.it18zhang.hdfs.mr.WCApp hdfs://user/centos/data/ hdfs://user/centos/data/out</p>
<h1 id="单独配置2nn到独立节点"><a href="#单独配置2nn到独立节点" class="headerlink" title="单独配置2nn到独立节点"></a>单独配置2nn到独立节点</h1><p>配置core-site文件</p>
<pre><code>[hdfs-site.xml]
&lt;property&gt;
        &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;
        &lt;value&gt;s206:50090&lt;/value&gt;
&lt;/property&gt;
</code></pre><h1 id="跟踪运行器信息"><a href="#跟踪运行器信息" class="headerlink" title="跟踪运行器信息"></a>跟踪运行器信息</h1><p><img src="https://i.imgur.com/UVIcFDb.png" alt=""></p>
<p>添加一个工具类：</p>
<pre><code>public class Util {

public static String getInfo(Object o,String msg ){
    return getHostname() + &quot;:&quot; + getPID() + &quot;:&quot; + getTID()+&quot;:&quot;+getObjInfo(o) +msg;
}

//得到主机名
public static String getHostname() {
    try {
        return InetAddress.getLocalHost().getHostName();
    } catch (UnknownHostException e) {
        e.printStackTrace();
    }
    return  null;
}
    //获得当前程序的所在的进程ID。
    public static int getPID()  {
        String info = ManagementFactory.getRuntimeMXBean().getName();
        return Integer.parseInt(info.substring(0,info.indexOf(&quot;@&quot;)));
    }
    //返回当前线程ID，
    public static String getTID(){
    return Thread.currentThread().getName();
    }

    public static  String getObjInfo(Object o){
        String sname = o.getClass().getSimpleName();
        return sname + &quot;@&quot;+o.hashCode();

    }
}
</code></pre><p>然后在map和reduce阶段添加：</p>
<pre><code>//每执行一次，计数器对这个组+1
context.getCounter(&quot;m&quot;,Util.getInfo(this,&quot;map&quot;)).increment(1);
</code></pre><p>效果如下图<br><img src="https://i.imgur.com/XeGcZvS.png" alt=""></p>
<h1 id="全排序"><a href="#全排序" class="headerlink" title="全排序"></a>全排序</h1><pre><code>## 普通排序求最高年份温度 ##
</code></pre><p>代码如下：</p>
<pre><code>public class MaxTempApp {

    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;);

        Job job = Job.getInstance(conf);

//        设置作业的各种属性
        job.setJobName(&quot;MaxTempApp&quot;);    //作业名称
        job.setJarByClass(MaxTempApp.class); //搜索类路径
        job.setInputFormatClass(TextInputFormat.class);//设置输入格式类

        //添加输入路径
        FileInputFormat.addInputPath(job, new Path(args[0]));
        //设置输出路径
        org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(args[1]));


        job.setMapperClass(MaxTempMapper.class);         //mapper类
        job.setReducerClass(MaxTempReducer.class);       //reduce类

        job.setNumReduceTasks(3);           //reduce个数


        job.setMapOutputKeyClass(IntWritable.class);
        job.setMapOutputValueClass(IntWritable.class);

        job.setOutputKeyClass(IntWritable.class);          //设置输出类型
        job.setOutputValueClass(IntWritable.class);

        job.waitForCompletion(true);

    }
}
</code></pre><hr>
<pre><code>public class MaxTempMapper extends Mapper&lt;LongWritable, Text, IntWritable, IntWritable&gt; {
    public MaxTempMapper() {
        System.out.println(&quot;new MaxTempMapper&quot;);
    }

    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        String line = value.toString();
        String arr[] = line.split(&quot; &quot;);
        context.write(new IntWritable(Integer.parseInt(arr[0])), new IntWritable((Integer.parseInt(arr[1]))));
    }
}
</code></pre><hr>
<pre><code>public class MaxTempReducer extends Reducer&lt;IntWritable, IntWritable, IntWritable, IntWritable&gt;{

    /**
     * reduce
     */
    protected void reduce(IntWritable key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {
            int max =Integer.MIN_VALUE;
            for(IntWritable iw : values){
                max = max &gt; iw.get() ? max : iw.get() ;
            }
                context.write(key,new IntWritable(max));

    }
}
</code></pre><hr>
<h2 id="全排序代码"><a href="#全排序代码" class="headerlink" title="全排序代码"></a>全排序代码</h2><p>上面代码产生的会是3个文件，文件内部各自排序，但是不是全排序的文件。全排序2种办法：</p>
<p>1、设置分区数是1，但是数据倾斜</p>
<ol start="2">
<li><p>在每个分区里设置0-30，30-60，60-100即可,然后每个分区返回0,1,2进入分成不同的区<br>代码如下图：<br> public class YearPartitioner extends Partitioner&lt;IntWritable, IntWritable&gt; {</p>
<pre><code>public int getPartition(IntWritable year, IntWritable temp, int parts) {
    int y = year.get() - 1970;
    if (y &lt; 33) {
        return 0;
    }
    if (y &gt; 33 &amp;&amp; y &lt; 66) {
        return 1;
    }
    else {
        return 2;
    }
}
</code></pre><p> }</p>
</li>
</ol>
<hr>
<pre><code>public class MaxTempReducer extends Reducer&lt;IntWritable, IntWritable, IntWritable, IntWritable&gt;{

    /**
     * reduce
     */
    protected void reduce(IntWritable key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {
            int max =Integer.MIN_VALUE;
            for(IntWritable iw : values){
                max = max &gt; iw.get() ? max : iw.get() ;
            }
                context.write(key,new IntWritable(max));

    }
}
</code></pre><hr>
<pre><code>public class MaxTempMapper extends Mapper&lt;LongWritable, Text, IntWritable, IntWritable&gt; {
    public MaxTempMapper() {
        System.out.println(&quot;new MaxTempMapper&quot;);
    }

    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        String line = value.toString();
        String arr[] = line.split(&quot; &quot;);
        context.write(new IntWritable(Integer.parseInt(arr[0])), new IntWritable((Integer.parseInt(arr[1]))));
    }
}
</code></pre><hr>
<pre><code>public class MaxTempApp {

    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;);

        Job job = Job.getInstance(conf);

//        设置作业的各种属性
        job.setJobName(&quot;MaxTempApp&quot;);    //作业名称
        job.setJarByClass(MaxTempApp.class); //搜索类路径
        job.setInputFormatClass(TextInputFormat.class);//设置输入格式类

        job.setPartitionerClass(YearPartitioner.class);

        //添加输入路径
        FileInputFormat.addInputPath(job, new Path(args[0]));
        //设置输出路径
        org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(args[1]));


        job.setMapperClass(MaxTempMapper.class);         //mapper类
        job.setReducerClass(MaxTempReducer.class);       //reduce类

        job.setNumReduceTasks(3);           //reduce个数


        job.setMapOutputKeyClass(IntWritable.class);
        job.setMapOutputValueClass(IntWritable.class);

        job.setOutputKeyClass(IntWritable.class);          //设置输出类型
        job.setOutputValueClass(IntWritable.class);

        job.waitForCompletion(true);

    }
}
</code></pre><hr>
<p>其实无非就是加一个partitioner的这个类而已。</p>
<hr>
<hr>
<hr>
<h1 id="全排序采样器"><a href="#全排序采样器" class="headerlink" title="全排序采样器"></a>全排序采样器</h1><p>1.定义1个reduce</p>
<p>2.自定义分区函数。<br>：        自行设置分解区间。</p>
<p>3.使用hadoop采样机制。</p>
<p>通过采样器生成分区文件，结合hadoop的TotalOrderPartitioner进行分区划分</p>
<p>TotalOrderPartitioner        //全排序分区类,读取外部生成的分区文件确定区间。</p>
<p>使用时采样代码在最后端,否则会出现错误。</p>
<p>//分区文件设置，设置的job的配置对象，不要是之前的conf.这里面的getconfiguration()是对最开始new的conf的拷贝，<br>TotalOrderPartitioner.setPartitionFile(job.getConfiguration(),new Path(“d:/mr/par.lst”));</p>
<h2 id="首先一段产生随机年份，温度的代码"><a href="#首先一段产生随机年份，温度的代码" class="headerlink" title="首先一段产生随机年份，温度的代码"></a>首先一段产生随机年份，温度的代码</h2><pre><code>public class PrepareTempData {
    @Test
    public void makeData() throws IOException {
        FileWriter fw = new FileWriter(&quot;e:/mr/temp.txt&quot;);
        for(int i=0;i&lt;6000;i++){
            int year=1970+ new Random().nextInt(100);
            int temp=-30 + new Random().nextInt(600);
            fw.write(&quot; &quot;+ year + &quot; &quot;+ temp + &quot;\r\n&quot; );
        }
            fw.close();
    }
}
</code></pre><hr>
<h2 id="全排序采样器代码"><a href="#全排序采样器代码" class="headerlink" title="全排序采样器代码"></a>全排序采样器代码</h2><pre><code>        public static void main(String[] args) throws Exception {
    Configuration conf = new Configuration();
    conf.set(&quot;fs.defaultFS&quot;, &quot;file:///&quot;);

    Job job = Job.getInstance(conf);

        设置作业的各种属性
    job.setJobName(&quot;MaxTempApp&quot;);    //作业名称
    job.setJarByClass(MaxTempApp.class); //搜索类路径
    job.setInputFormatClass(SequenceFileInputFormat.class);//设置输入格式类

    //添加输入路径
    FileInputFormat.addInputPath(job, new Path(args[0]));
    //设置输出路径
    org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(args[1]));

    job.setMapperClass(MaxTempMapper.class);         //mapper类
    job.setReducerClass(MaxTempReducer.class);       //reduce类


    job.setMapOutputKeyClass(IntWritable.class);
    job.setMapOutputValueClass(IntWritable.class);

    job.setOutputKeyClass(IntWritable.class);          //设置输出类型
    job.setOutputValueClass(IntWritable.class);

    //设置 全排序分区类
    job.setPartitionerClass(TotalOrderPartitioner.class);
    //将sample数据 写入分区文件
    TotalOrderPartitioner.setPartitionFile(conf, new Path(&quot;e:/mr/par.lst&quot;));

    //创建随机采样器对象
    InputSampler.Sampler&lt;IntWritable, IntWritable&gt; sampler = new InputSampler.RandomSampler&lt;IntWritable, IntWritable&gt;(0.1, 10000, 10);
    job.setNumReduceTasks(3);           //reduce个数
    InputSampler.writePartitionFile(job, sampler);


    job.waitForCompletion(true);

    }
}
</code></pre><hr>
<pre><code>public class MaxTempMapper extends Mapper&lt;IntWritable, IntWritable, IntWritable, IntWritable&gt; {
    public MaxTempMapper() {
        System.out.println(&quot;new MaxTempMapper&quot;);
    }

    @Override
    protected void map(IntWritable key, IntWritable value, Context context) throws IOException, InterruptedException {
        context.write(key,value);
    }
}
</code></pre><hr>
<pre><code>public class MaxTempReducer extends Reducer&lt;IntWritable, IntWritable, IntWritable, IntWritable&gt;{

    /**
     * reduce
     */
    protected void reduce(IntWritable key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {
            int max =Integer.MIN_VALUE;
            for(IntWritable iw : values){
                max = max &gt; iw.get() ? max : iw.get() ;
            }
                context.write(key,new IntWritable(max));

    }
}
</code></pre><hr>
<pre><code>public class YearPartitioner extends Partitioner&lt;IntWritable, IntWritable&gt; {

    public int getPartition(IntWritable year, IntWritable temp, int parts) {
        int y = year.get() - 1970;
        if (y &lt; 33) {
            return 0;
        }
        if (y &gt; 33 &amp;&amp; y &lt; 66) {
            return 1;
        }
        else {
            return 2;
        }
    }
}
</code></pre><hr>
<pre><code>全排序官方笔记:
1.定义1个reduce

2.自定义分区函数.
    自行设置分解区间。

3.使用hadoop采样机制。
    通过采样器生成分区文件，结合hadoop的TotalOrderPartitioner进行分区划分。
    TotalOrderPartitioner        //全排序分区类,读取外部生成的分区文件确定区间。

    使用时采样代码在最后端,否则会出现错误。

    //分区文件设置，设置的job的配置对象，不要是之前的conf.
    TotalOrderPartitioner.setPartitionFile(job.getConfiguration(),new Path(&quot;d:/mr/par.lst&quot;));
</code></pre><hr>
<hr>
<h2 id="全排序和部分排序二次排序在面试比例很重的"><a href="#全排序和部分排序二次排序在面试比例很重的" class="headerlink" title="全排序和部分排序二次排序在面试比例很重的"></a>全排序和部分排序二次排序在面试比例很重的</h2><p>分区在map端，分组在reduce端。<br>二次排序就是把value和key组合到一起，然后value就是nullwritable这样子。合在一起成为一个combokey。<br>Combokey不但要可比较还要串行化，hadoop的串行化机制是writable ，串行就是写入到流离去，反串行就是读出来。<br>IntWritable LongWritable这些都是经过串行化实现的类，Writable有很多子类。 </p>
<p>value本身不能排序，为了能让他排序，吧value做到key里面去。吧年份和气温都做到key里面去，这个key是自定义的key,然后在combokey中定义里面的排序规则。自定义key,自定义对比器。</p>
<p>wirtable是串行化机制，java本身也有串行化机制，本质是对象输出流和输入流objectInputStream,objectOutputStream。需要写入和读取就可以了，但是java的串行化效率比较低而且复杂，不能够夸语言，所以hadoop有一个自己的串行化Wriable过程。</p>
<p>这端是reduce端，经过map端的combokey输出的combokey类是这个样子的，如果不经过处理直接聚合，就会发先，每一个combo都是一个新的都不相同，但是我们想让相同年份的进入到同一个组里面去。所以要重写分组。根据年份写分组，只要是同一个年就是同一组</p>
<p>reduce端里面有reduce对象和reduce方法。我们说的分几个区进入到几个reduce的意思是进入到几个reduce对象。</p>
<p><img src="https://i.imgur.com/SSNM7tc.png" alt=""></p>
<p>现在讲一下下图：下图说在没有分组的情况下，一个key对应好多value,也就是一个1978对应好多个12。好多个12进入到迭代器里面去，不断it.next是下一个v。但是key也改变，其实每次都是变化的，<br><img src="https://i.imgur.com/R4Pj8tj.png" alt=""></p>
<p><strong>讲一下二次排序和全排序：</strong>全排序就是整个年份-温度数据，分成几个区，让第一个区的最大值小于第二个区的最小值，然后这样子排下去，既解决了分布式的问题，又解决了数据倾斜的问题，但是value是不能排序的，因为mapreduce天生value就不能够排序。那么如何解决让温度也排序呢，就是要把年份和温度做成一个key，传入，然后再排序。就是二次排序，既实现了年份的排序，又实现了温度的排序，这里说的温度的排序是指同一个年份温度升序降序的问题。</p>
<h1 id="二次排序"><a href="#二次排序" class="headerlink" title="二次排序"></a>二次排序</h1><p>代码如下：</p>
<pre><code>/**
 * 自定义组合key
 */
public class ComboKey implements WritableComparable&lt;ComboKey&gt; {
    private int year;
    private int temp;

    public int getYear() {
        return year;
    }

    public void setYear(int year) {
        this.year = year;
    }

    public int getTemp() {
        return temp;
    }

    public void setTemp(int temp) {
        this.temp = temp;
    }

    /**
     * 对key进行比较实现
     */
    public int compareTo(ComboKey o) {
        int y0 = o.getYear();
        int t0 = o.getTemp();
        //年份相同(升序)
        if (year == y0) {
            //气温降序
            return -(temp - t0);            // //这个地方为什么说temp-t0是升序排列，因为这个temp-t0是默认的。默认就是升序排列加一个负号就是降序排列。本身升序排列的就是temp-t0是大于0的
        } else {
            return year - y0;
        }
    }

    /**
     * 串行化过程
     */
    public void write(DataOutput out) throws IOException {
        //年份
        out.writeInt(year);
        //气温
        out.writeInt(temp);
    }

    public void readFields(DataInput in) throws IOException {
        year = in.readInt();
        temp = in.readInt();
    }
}
</code></pre><hr>
<pre><code>/**
 *ComboKeyComparator
 */
public class ComboKeyComparator extends WritableComparator {

    protected ComboKeyComparator() {
        super(ComboKey.class, true);
    }

    public int compare(WritableComparable a, WritableComparable b) {
        ComboKey k1 = (ComboKey) a;
        ComboKey k2 = (ComboKey) b;
        return k1.compareTo(k2);
    }
}
</code></pre><hr>
<pre><code>    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        conf.set(&quot;fs.defaultFS&quot;, &quot;file:///&quot;);

        Job job = Job.getInstance(conf);

//        设置作业的各种属性
        job.setJobName(&quot;SecondarySortApp&quot;);    //作业名称
        job.setJarByClass(MaxTempApp.class); //搜索类路径
        job.setInputFormatClass(TextInputFormat.class);//设置输入格式类

        //添加输入路径
        FileInputFormat.addInputPath(job, new Path(args[0]));
        //设置输出路径
        org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(args[1]));

        job.setMapperClass(MaxTempMapper.class);         //mapper类
        job.setReducerClass(MaxTempReducer.class);       //reduce类

        //设置map输出类型
        job.setMapOutputKeyClass(ComboKey.class);
        job.setMapOutputValueClass(NullWritable.class);

        //设置Reduceoutput类型
        job.setOutputKeyClass(IntWritable.class);          //设置输出类型
        job.setOutputValueClass(IntWritable.class);

        //设置分区类
        job.setPartitionerClass(YearPartitioner.class);
        //设置分组对比器。
        job.setGroupingComparatorClass(YearGroupComparator.class);
        //设置排序对比器，听说不写那个ComboKeyComparator不设置也可以
        job.setSortComparatorClass(ComboKeyComparator.class);
        //reduce个数
        job.setNumReduceTasks(3);
        job.waitForCompletion(true);

    }
}
</code></pre><hr>
<pre><code>public class MaxTempMapper extends Mapper&lt;LongWritable, Text, ComboKey, NullWritable&gt; {
    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        String line = value.toString();
        String[] arr = line.split(&quot; &quot;);

        ComboKey keyout = new ComboKey();
        keyout.setYear(Integer.parseInt(arr[0]));
        keyout.setTemp(Integer.parseInt(arr[1]));
        context.write(keyout, NullWritable.get());
    }
  }
</code></pre><hr>
<pre><code>/**
 * Reducer
 */
public class MaxTempReducer extends Reducer&lt;ComboKey, NullWritable, IntWritable, IntWritable&gt;{
    protected void reduce(ComboKey key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException {
            int year = key.getYear();
            int temp = key.getTemp();
            context.write(new IntWritable(year),new IntWritable(temp));
    }
}
</code></pre><hr>
<pre><code>public class YearGroupComparator extends WritableComparator {

    protected YearGroupComparator() {
        super(ComboKey.class, true);
    }

    public int compare(WritableComparable a, WritableComparable b) {
        ComboKey k1 = (ComboKey) a ;
        ComboKey k2 = (ComboKey) b ;
        return k1.getYear() - k2.getYear() ;
    }
}
</code></pre><hr>
<pre><code>//自定义分区，在map端执行，是map中的一个阶段，mapkv进kv出，kv出去之后要有一个分区的过程。
//默认是哈希分区，这里边是修改了分区规则。
public class YearPartitioner extends Partitioner&lt;ComboKey,NullWritable&gt; {

public int getPartition(ComboKey key, NullWritable nullWritable, int numPartitions) {
    int year = key.getYear();
    return year % numPartitions;
}
</code></pre><p>}</p>
<hr>
<hr>
<hr>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/10/11/Hadoop第六天之Yarn作业提交/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/11/Hadoop第六天之Yarn作业提交/" itemprop="url">Hadoop第六天之Yarn作业提交</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-10-11T15:48:32+08:00">
                2018-10-11
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hadoop/" itemprop="url" rel="index">
                    <span itemprop="name">Hadoop</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="本地模式job提交流程"><a href="#本地模式job提交流程" class="headerlink" title="本地模式job提交流程"></a>本地模式job提交流程</h2><pre><code>mr.Job = new Job();
job.setxxx();
JobSubmitter.提交
LocalJobRunner.Job();
start();
</code></pre><h2 id="hdfs-write"><a href="#hdfs-write" class="headerlink" title="hdfs write"></a>hdfs write</h2><pre><code>packet,
</code></pre><h2 id="hdfs-切片计算方式"><a href="#hdfs-切片计算方式" class="headerlink" title="hdfs 切片计算方式"></a>hdfs 切片计算方式</h2><pre><code>getFormatMinSplitSize() = 1

//最小值(&gt;=1)                            1                        0
long minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));

//最大值(&lt;= Long.Max , mapreduce.input.fileinputformat.split.maxsize=)
long maxSize = getMaxSplitSize(job);

//得到block大小
long blockSize = file.getBlockSize();

//minSplit maxSplit blockSize
//Math.max(minSize, Math.min(maxSize, blockSize));
</code></pre><p>LF : Line feed,换行符<br>private static final byte CR = ‘\r’;<br>private static final byte LF = ‘\n’;</p>
<h2 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h2><pre><code>1.Windows
    源文件大小:82.8k
    源文件类型:txt
    压缩性能比较
                |    DeflateCodec    GzipCodec    BZip2Codec    Lz4Codec    SnappyCodec    |结论
    ------------|-------------------------------------------------------------------|----------------------
    压缩时间(ms)|    450                7            196            44            不支持        |Gzip &gt; Lz4 &gt; BZip2 &gt; Deflate
    ------------|-------------------------------------------------------------------|----------------------
    解压时间(ms)|    444                66            85            33                        |lz4  &gt; gzip &gt; bzip2 &gt; Deflate
    ------------|-------------------------------------------------------------------|----------------------
    占用空间(k)    |    19k                19k            17k            31k            不支持        |Bzip &gt; Deflate = Gzip &gt; Lz4
                |                                                                    |

2.CentOS
    源文件大小:82.8k
    源文件类型:txt
                |    DeflateCodec    GzipCodec    BZip2Codec    Lz4Codec    LZO        SnappyCodec    |结论
    ------------|---------------------------------------------------------------------------|----------------------
    压缩时间(ms)|    944                77            261            53            77        不支持        |Gzip &gt; Lz4 &gt; BZip2 &gt; Deflate
    ------------|---------------------------------------------------------------------------|----------------------
    解压时间(ms)|    67                66            106            52            73                    |lz4  &gt; gzip &gt; Deflate&gt; lzo &gt; bzip2 
    ------------|---------------------------------------------------------------------------|----------------------
    占用空间(k)    |    19k                19k            17k            31k            34k                    |Bzip &gt; Deflate = Gzip &gt; Lz4 &gt; lzo
</code></pre><h2 id="远程调试"><a href="#远程调试" class="headerlink" title="远程调试"></a>远程调试</h2><pre><code>1.设置服务器java vm的-agentlib:jdwp选项.
[server]
//windwos
//set JAVA_OPTS=%JAVA_OPTS% -agentlib:jdwp=transport=dt_socket,address=8888,server=y,suspend=n

//linux
export HADOOP_CLIENT_OPTS=-agentlib:jdwp=transport=dt_socket,address=8888,server=y,suspend=y

2.在server启动java程序
    hadoop jar HdfsDemo.jar com.it18zhang.hdfs.mr.compress.TestCompress

3.server会暂挂在8888.
    Listening ...

4.客户端通过远程调试连接到远程主机的8888.

5.客户端就可以调试了。
</code></pre><p>hadoop jar<br>java </p>
<p>hadoop jar com.it18zhang.hdfs.mr.compress.TestCompress</p>
<p>export HADOOP_CLIENT_OPTS=-agentlib:jdwp=transport=dt_socket,address=8888,server=y,suspend=y</p>
<h2 id="在pom-xml中引入新的插件-maven-antrun-plugin-实现文件的复制"><a href="#在pom-xml中引入新的插件-maven-antrun-plugin-实现文件的复制" class="headerlink" title="在pom.xml中引入新的插件(maven-antrun-plugin),实现文件的复制."></a>在pom.xml中引入新的插件(maven-antrun-plugin),实现文件的复制.</h2><pre><code>[pom.xml]
&lt;project&gt;
    ...
    &lt;build&gt;
        &lt;plugins&gt;
            ...
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-antrun-plugin&lt;/artifactId&gt;
                &lt;version&gt;1.8&lt;/version&gt;
                &lt;executions&gt;
                    &lt;execution&gt;
                        &lt;phase&gt;package&lt;/phase&gt;
                        &lt;goals&gt;
                            &lt;goal&gt;run&lt;/goal&gt;
                        &lt;/goals&gt;
                        &lt;configuration&gt;
                            &lt;tasks&gt;
                                &lt;echo&gt;---------开始复制jar包到共享目录下----------&lt;/echo&gt;
                                &lt;delete file=&quot;D:\downloads\bigdata\data\HdfsDemo-1.0-SNAPSHOT.jar&quot;&gt;&lt;/delete&gt;
                                &lt;copy file=&quot;target/HdfsDemo-1.0-SNAPSHOT.jar&quot; toFile=&quot;D:\downloads\bigdata\data\HdfsDemo.jar&quot;&gt;
                                &lt;/copy&gt;
                            &lt;/tasks&gt;
                        &lt;/configuration&gt;
                    &lt;/execution&gt;
                &lt;/executions&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;
    ...
&lt;/project&gt;
</code></pre><h2 id="在centos上使用yum安装snappy压缩库文件"><a href="#在centos上使用yum安装snappy压缩库文件" class="headerlink" title="在centos上使用yum安装snappy压缩库文件"></a>在centos上使用yum安装snappy压缩库文件</h2><pre><code>[google snappy]
$&gt;sudo yum search snappy                #查看是否有snappy库
$&gt;sudo yum install -y snappy.x86_64        #安装snappy压缩解压缩库
</code></pre><h2 id="库文件"><a href="#库文件" class="headerlink" title="库文件"></a>库文件</h2><pre><code>windows    :dll(dynamic linked library)
linux    :so(shared object)
</code></pre><h2 id="LZO"><a href="#LZO" class="headerlink" title="LZO"></a>LZO</h2><pre><code>1.在pom.xml引入lzo依赖
    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
    &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;
             xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
             xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
        &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
        &lt;groupId&gt;com.it18zhang&lt;/groupId&gt;
        &lt;artifactId&gt;HdfsDemo&lt;/artifactId&gt;
        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
        &lt;packaging&gt;jar&lt;/packaging&gt;
        &lt;build&gt;
            &lt;plugins&gt;
                &lt;plugin&gt;
                    &lt;groupId&gt;org.apache.maven.plugins
                    &lt;/groupId&gt;
                    &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
                    &lt;configuration&gt;
                        &lt;source&gt;1.8&lt;/source&gt;
                        &lt;target&gt;1.8&lt;/target&gt;
                    &lt;/configuration&gt;
                &lt;/plugin&gt;
                &lt;plugin&gt;
                    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                    &lt;artifactId&gt;maven-antrun-plugin&lt;/artifactId&gt;
                    &lt;version&gt;1.8&lt;/version&gt;
                    &lt;executions&gt;
                        &lt;execution&gt;
                            &lt;phase&gt;package&lt;/phase&gt;
                            &lt;goals&gt;
                                &lt;goal&gt;run&lt;/goal&gt;
                            &lt;/goals&gt;
                            &lt;configuration&gt;
                                &lt;tasks&gt;
                                    &lt;echo&gt;---------开始复制jar包到共享目录下----------&lt;/echo&gt;
                                    &lt;delete file=&quot;D:\downloads\bigdata\data\HdfsDemo-1.0-SNAPSHOT.jar&quot;&gt;&lt;/delete&gt;
                                    &lt;copy file=&quot;target/HdfsDemo-1.0-SNAPSHOT.jar&quot; toFile=&quot;D:\downloads\bigdata\data\HdfsDemo.jar&quot;&gt;
                                    &lt;/copy&gt;
                                &lt;/tasks&gt;
                            &lt;/configuration&gt;
                        &lt;/execution&gt;
                    &lt;/executions&gt;
                &lt;/plugin&gt;
            &lt;/plugins&gt;
        &lt;/build&gt;
        &lt;dependencies&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
                &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;
                &lt;version&gt;2.7.3&lt;/version&gt;
            &lt;/dependency&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
                &lt;artifactId&gt;hadoop-yarn-common&lt;/artifactId&gt;
                &lt;version&gt;2.7.3&lt;/version&gt;
            &lt;/dependency&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
                &lt;artifactId&gt;hadoop-yarn-client&lt;/artifactId&gt;
                &lt;version&gt;2.7.3&lt;/version&gt;
            &lt;/dependency&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
                &lt;artifactId&gt;hadoop-yarn-server-resourcemanager&lt;/artifactId&gt;
                &lt;version&gt;2.7.3&lt;/version&gt;
            &lt;/dependency&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;org.anarres.lzo&lt;/groupId&gt;
                &lt;artifactId&gt;lzo-hadoop&lt;/artifactId&gt;
                &lt;version&gt;1.0.0&lt;/version&gt;
                &lt;scope&gt;compile&lt;/scope&gt;
            &lt;/dependency&gt;

            &lt;dependency&gt;
                &lt;groupId&gt;junit&lt;/groupId&gt;
                &lt;artifactId&gt;junit&lt;/artifactId&gt;
                &lt;version&gt;4.11&lt;/version&gt;
            &lt;/dependency&gt;

        &lt;/dependencies&gt;
    &lt;/project&gt;

2.在centos上安装lzo库
    $&gt;sudo yum -y install lzo

3.使用mvn命令下载工件中的所有依赖
    进入pom.xml所在目录，运行cmd：
    mvn -DoutputDirectory=./lib -DgroupId=com.it18zhang -DartifactId=HdfsDemo -Dversion=1.0-SNAPSHOT dependency:copy-dependencies

4.在lib下存放依赖所有的第三方jar

5.找出lzo-hadoop.jar + lzo-core.jar复制到hadoop的响应目录下。
    $&gt;cp lzo-hadoop.jar lzo-core.jar /soft/hadoop/shared/hadoop/common/lib

6.执行远程程序即可。
</code></pre><h2 id="修改maven使用aliyun镜像。"><a href="#修改maven使用aliyun镜像。" class="headerlink" title="修改maven使用aliyun镜像。"></a>修改maven使用aliyun镜像。</h2><pre><code>[maven/conf/settings.xml]
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;settings xmlns=&quot;http://maven.apache.org/SETTINGS/1.0.0&quot;
          xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
          xsi:schemaLocation=&quot;http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd&quot;&gt;
  &lt;pluginGroups&gt;
  &lt;/pluginGroups&gt;

  &lt;proxies&gt;
  &lt;/proxies&gt;

&lt;servers&gt;
    &lt;server&gt;
        &lt;id&gt;releases&lt;/id&gt;
        &lt;username&gt;admin&lt;/username&gt;
        &lt;password&gt;admin123&lt;/password&gt;
    &lt;/server&gt;
    &lt;server&gt;
        &lt;id&gt;snapshots&lt;/id&gt;
        &lt;username&gt;admin&lt;/username&gt;
        &lt;password&gt;admin123&lt;/password&gt;
    &lt;/server&gt;
    &lt;server&gt;
        &lt;id&gt;Tomcat7&lt;/id&gt;
        &lt;username&gt;tomcat&lt;/username&gt;
        &lt;password&gt;tomcat&lt;/password&gt;
    &lt;/server&gt;
&lt;/servers&gt;

&lt;mirrors&gt;
     &lt;mirror&gt;
        &lt;id&gt;nexus-aliyun&lt;/id&gt;
        &lt;mirrorOf&gt;*&lt;/mirrorOf&gt;
        &lt;name&gt;Nexus aliyun&lt;/name&gt;
        &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt;
    &lt;/mirror&gt; 
&lt;/mirrors&gt;
&lt;/settings&gt;
</code></pre><h2 id="文件格式-SequenceFile"><a href="#文件格式-SequenceFile" class="headerlink" title="文件格式:SequenceFile"></a>文件格式:SequenceFile</h2><pre><code>1.SequenceFile
    Key-Value对方式。

2.不是文本文件，是二进制文件。

3.可切割
    因为有同步点。
    reader.sync(pos);    //定位到pos之后的第一个同步点。
    writer.sync();        //写入同步点

4.压缩方式
    不压缩
    record压缩            //只压缩value
    块压缩                //按照多个record形成一个block.
</code></pre><h2 id="文件格式-MapFile"><a href="#文件格式-MapFile" class="headerlink" title="文件格式:MapFile"></a>文件格式:MapFile</h2><pre><code>1.Key-value
2.key按升序写入(可重复)。
3.mapFile对应一个目录，目录下有index和data文件,都是序列文件。
4.index文件划分key区间,用于快速定位。
</code></pre><h2 id="自定义分区函数"><a href="#自定义分区函数" class="headerlink" title="自定义分区函数"></a>自定义分区函数</h2><pre><code>1.定义分区类
    public class MyPartitioner extends Partitioner&lt;Text, IntWritable&gt;{
        public int getPartition(Text text, IntWritable intWritable, int numPartitions) {
            return 0;
        }
    }
2.程序中配置使用分区类
    job.setPartitionerClass(MyPartitioner.class);
</code></pre><h2 id="combinerd（合成）-继承了Reducer-任何的Reducer类都能被他使用"><a href="#combinerd（合成）-继承了Reducer-任何的Reducer类都能被他使用" class="headerlink" title="combinerd（合成） 继承了Reducer 任何的Reducer类都能被他使用"></a>combinerd（合成） 继承了Reducer 任何的Reducer类都能被他使用</h2><p> Map端的Reducer  预先化简<br>1，为了减少网络带宽 将Map端发出的的数据进行聚合 并不是所有的都可以用combiner<br>2,combiner </p>
<hr>
<p>切片个数是四个，mapper就需要也是4个 </p>
<p>下图是一个客户端的写入过程：首先形成一个管线，因为有很多节点，节点形成管线，现在本地构建2个队列，一个发送队列sendQueue，一个是确认队列ackQueue，客户端吧消息封装成Packet放到队列里面，而且结束后会放一个空包进去，发送的话，是以管线的方式发给A，A在发给B，B在发给C，在发送的过程中有一个校验的过程chunk(4+512)就是4个校验位+512个数据字节，一个Packet是64K+33个头的字节，在确认队列里面是一个编号seqNo如果在丢列中ABC中都回执，就吧seqNo在ackQueue中移除。发送如果失败，就吧管线重新建立，吧没法送的包返回到队列之前的一个容错机制。<br><img src="https://i.imgur.com/43B2Ebp.png" alt=""></p>
<p>输入格式定位文件，在输入格式中间有一个交互reader，在Map和Reduce都有这么一些方法，在Mapper和Reducer中有回调方法，就是本来使用的是子类的方法，但是子类中调用有父类的的run方法，然后run方法里面有reduce方法也就又调用了子类的方法，这叫做回调 。Mapper是通过reader读取下一条数据传进来的。Map和Reduce中间有一个shuffer的混洗过程，网络间分发数据的过程， Map里面先分区，分区个数等于Reduce的个数，所以有几个reduce就有几个分区，默认情况下，Map分区通过哈希算法来分区的，对K哈希。只要K相同，一定会被哈希到一个分区里面，分区也就是桶的概念，也就是只要K相同进入到同一个桶里面，只要是同一个分区就是进入到同一个reduce，默认情况下inputformat的格式，他的K就是偏移量。如果是图片格式的话就可能是别的。只有在文本输入的时候才是偏移量，当然也可以自己定义Inputformat格式。在mr计算之前切片就已经算好了，产生的切片文件和配置XML文件放到了临时目录下了。<br><img src="https://i.imgur.com/7qjZLGJ.png" alt=""></p>
<p>看下图，首先客户端运行作业，第一步run job，第二部走ResourceManager，是Yarn层面，负责资源调度，所有集群的资源都属于ResourceManager，第二部get new application，首先得到应用的ID，第三部copyt job resources，复制切片信息和xml配置信息和jar包到HDFS，第四部提交给资源管理器，然后资源管理器start container，然后通过节点管理器启动一个MRAPPMaster（应用管理程序），第六步初始化Job，第七部检索切片信息，第八步请求资源管理器去分配一个资源列表，第九步启动节点管理器，通过节点管理器来启动一个JVM虚拟机，在虚拟机里面启动yarn子进程，第十部通过子进程读取切片信息，然后在运行MapTast或者ReduceTask<br><img src="https://i.imgur.com/lqVQLT1.png" alt=""></p>
<p>客户端提取一个作业，然后联系资源管理器，为了得到id,在完全分布式下客户端client叫job，一旦提交就叫做application了，先请求，然后得到了appid返回给了客户端，第三部将作业信息上传到HDFS的临时目录，存档切片信息和jar包，然后找到节点管理器，然后NM孵化一个MRAPPMaster，然后检索HDFS文件系统，获取到有多少个要执行的并发任务，拿到任务之后，要求资源管理器分配一个集合，哪些节点可以用。 然后MRAppmaster在联系其他的NM，让NM去开启Yarn子进程，子进程在去检索HDFS信息，在开启Map和Reduce</p>
<p><img src="https://i.imgur.com/bn7A9H6.png" alt=""></p>
<h2 id="hdfs-切片计算方式-1"><a href="#hdfs-切片计算方式-1" class="headerlink" title="hdfs 切片计算方式"></a>hdfs 切片计算方式</h2><pre><code>getFormatMinSplitSize() = 1

//最小值(&gt;=1)                            1                        0
long minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));

//最大值(&lt;= Long.Max , mapreduce.input.fileinputformat.split.maxsize=)
long maxSize = getMaxSplitSize(job);

//得到block大小
long blockSize = file.getBlockSize();

//minSplit maxSplit blockSize
//Math.max(minSize, Math.min(maxSize, blockSize));
</code></pre><p>在计算切片的时候一般块大小不设置，是最好的，一般就是128M，采用本地优先策略，一个快大小是128m如果一旦把他边变1m那么切片大小就会变成1M，一个，然后128M的块大小会被分发128份切片分别传输给MR去计算，这样的话，就会增加网络间的传输，增加网络负载。所以128m是最合适的</p>
<hr>
<p>CR=‘\r’回车符<br>LF+’\n’换行符</p>
<p>windows系统里面是\r\n。回车带换行。<br>linux系统只有一个\n</p>
<p>切片是定义大方向的，阅读器recorderreader是控制细节的。切片定义任务数量，开几个map但是不一定是每个map都有任务。</p>
<p><strong>切片问题</strong>是物理设置，但是是逻辑读取。</p>
<p><img src="https://i.imgur.com/tgAbsGx.png" alt=""></p>
<p>打个比方说上图这个东西比如说第一个例子，前面的设置物理划分，根据字节数划分成了4个切片，也就是有4个Mapper,然后四个切片开始读取，其中第一个物理切割到t但是实际上，由于切片读取到换行符所以，第一个Mapper也就是读取到了hello world tom，然后第二个，第二个偏移量是13+13.偏移量是13，在物理读取13个直接，但是由于Mapper检测不是行首，所以直接从下一行读取也在读取一整行。这个就是每个切片128MB的缩影，吧128M变成了13而已。然后其实Textinputformat下一个环节是。recoderreader。这个会先检测是否是行首，<br><img src="https://i.imgur.com/7qjZLGJ.png" alt="">      </p>
<h2 id="压缩问题"><a href="#压缩问题" class="headerlink" title="压缩问题"></a>压缩问题</h2><pre><code>package com.it18zhang.hdfs.mr.compress;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.io.IOUtils;
import org.apache.hadoop.io.compress.*;
import org.apache.hadoop.util.ReflectionUtils;
import org.junit.Test;

import java.io.FileInputStream;
import java.io.FileOutputStream;

/**
 * @Title:TestCompress
 * @Description:
 * @Auther: Eric Hunn
 * @Version:1.0
 * @Create 2018/10/14 19:39
 */
public class TestCompress {
    @Test
    public void deflateCompress() throws Exception {
        Class[] zipClasses = {
                DeflateCodec.class,
                GzipCodec.class,
                BZip2Codec.class,
                Lz4Codec.class,
        };

        for(Class c : zipClasses){
            zip(c);
        }
    }

    /*压缩测试
     *
     * */
    public void zip(Class codecClass) throws Exception {
        long start = System.currentTimeMillis();
        //实例化对象
        CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, new Configuration());
        //创建文件输出流，得到默认拓展名
        FileOutputStream fos = new FileOutputStream(&quot;e:/comp/b&quot; + codec.getDefaultExtension());
        //得到压缩流
        CompressionOutputStream zipOut = codec.createOutputStream(fos);
        IOUtils.copyBytes(new FileInputStream(&quot;e:/11111.txt/&quot;), zipOut, 1024);
        zipOut.close();
        System.out.println(codecClass.getSimpleName() + &quot; ：&quot;);
    }
}                                     
</code></pre><p>压缩一般是在文件Map输出的时候压缩，Map输出的时候内存不够一般要往磁盘里面溢出，在溢出的时候可以让他压缩溢出，这样reduce的时候就要解压缩。上面代码测试过lz4不行，但是视频上是可以的，暂时不知道哪里出错了。</p>
<pre><code>package com.it18zhang.hdfs.mr.compress;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.io.IOUtils;
import org.apache.hadoop.io.compress.*;
import org.apache.hadoop.util.ReflectionUtils;
import org.junit.Test;

import java.io.FileInputStream;
import java.io.FileOutputStream;

/**
 * @Title:TestCompress
 * @Description:
 * @Auther: Eric Hunn
 * @Version:1.0
 * @Create 2018/10/14 19:39
 */
public class TestCompress {
    @Test
    public void deflateCompress() throws Exception {
        Class[] zipClasses = {
                DeflateCodec.class,
                GzipCodec.class,
                BZip2Codec.class,
//                Lz4Codec.class,
//                SnappyCodec.class,
        };

        for(Class c : zipClasses){
            unzip(c);
        }
    }

    /*压缩测试
     *
     * */
    public void zip(Class codecClass) throws Exception {
        long start = System.currentTimeMillis();
        //实例化对象
        CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, new Configuration());
        //创建文件输出流，得到默认拓展名
        FileOutputStream fos = new FileOutputStream(&quot;e:/comp/b&quot; + codec.getDefaultExtension());
        //得到压缩流
        CompressionOutputStream zipOut = codec.createOutputStream(fos);
        IOUtils.copyBytes(new FileInputStream(&quot;e:/11111.txt/&quot;), zipOut, 1024);
        zipOut.close();
        System.out.println(codecClass.getSimpleName() + &quot; ：&quot;);
    }


    public void unzip(Class codecClass) throws Exception {
        long start = System.currentTimeMillis();
        //实例化对象
        CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, new Configuration());
        //创建文件输出流，得到默认拓展名
        FileInputStream fis = new FileInputStream(&quot;e:/comp/b&quot; + codec.getDefaultExtension());
        //得到压缩流
        CompressionInputStream zipIn = codec.createInputStream(fis);
        IOUtils.copyBytes(zipIn,new FileOutputStream(&quot;e:/comp/b&quot; + codec.getDefaultExtension() + &quot;.txt&quot;), 1024);
        zipIn.close();
        System.out.println(codecClass.getSimpleName() + &quot; ：&quot; + (System.currentTimeMillis() - start));
    }
}
</code></pre><h2 id="在集群上运"><a href="#在集群上运" class="headerlink" title="在集群上运"></a>在集群上运</h2><pre><code>package com.it18zhang.hdfs.mr.compress;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.io.IOUtils;
import org.apache.hadoop.io.compress.*;
import org.apache.hadoop.util.ReflectionUtils;

import java.io.FileInputStream;
import java.io.FileOutputStream;

/**
 * @Title:TestCompress
 * @Description:
 * @Auther: Eric Hunn
 * @Version:1.0
 * @Create 2018/10/14 19:39
 */
public class TestCompress {
    public static void main(String[] args) throws Exception {
        Class[] zipClasses = {
                DeflateCodec.class,
                GzipCodec.class,
                BZip2Codec.class,
//                Lz4Codec.class,
//                SnappyCodec.class,
        };

        for(Class c : zipClasses){
            zip(c);
        }
        System.out.println(&quot;==================================&quot;);
        for(Class c : zipClasses){
            unzip(c);
        }
    }

    /*压缩测试
     *
     * */
    public static void zip(Class codecClass) throws Exception {
        long start = System.currentTimeMillis();
        //实例化对象
        CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, new Configuration());
        //创建文件输出流，得到默认拓展名
        FileOutputStream fos = new FileOutputStream(&quot;/home/centos/zip/b&quot; + codec.getDefaultExtension());
        //得到压缩流
        CompressionOutputStream zipOut = codec.createOutputStream(fos);
        IOUtils.copyBytes(new FileInputStream(&quot;/home/centos/zip/a.txt&quot;), zipOut, 1024);
        zipOut.close();
        System.out.println(codecClass.getSimpleName() + &quot; ：&quot;);
    }


    public static void unzip(Class codecClass) throws Exception {
        long start = System.currentTimeMillis();
        //实例化对象
        CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, new Configuration());
        //创建文件输出流，得到默认拓展名
        FileInputStream fis = new FileInputStream(&quot;/home/centos/zip/b&quot; + codec.getDefaultExtension());
        //得到压缩流
        CompressionInputStream zipIn = codec.createInputStream(fis);
        IOUtils.copyBytes(zipIn,new FileOutputStream(&quot;/home/centos/zip/b&quot; + codec.getDefaultExtension() + &quot;.txt&quot;), 1024);
        zipIn.close();
        System.out.println(codecClass.getSimpleName() + &quot; ：&quot; + (System.currentTimeMillis() - start));
    }
}
</code></pre><h2 id="远程调试-1"><a href="#远程调试-1" class="headerlink" title="远程调试"></a>远程调试</h2><pre><code>1.设置服务器java vm的-agentlib:jdwp选项.
[server]
//windwos
//set JAVA_OPTS=%JAVA_OPTS% -agentlib:jdwp=transport=dt_socket,address=8888,server=y,suspend=n

//linux


2.在server启动java程序
    hadoop jar HdfsDemo.jar com.it18zhang.hdfs.mr.compress.TestCompress

3.server会暂挂在8888.
    Listening ...

4.客户端通过远程调试连接到远程主机的8888.

5.客户端就可以调试了。
</code></pre><hr>
<hr>
<h3 id="通过MapFile来写入"><a href="#通过MapFile来写入" class="headerlink" title="通过MapFile来写入"></a>通过MapFile来写入</h3><pre><code>    /*写操作
 * */
@Test
public void save() throws Exception {
    Configuration conf = new Configuration();
    conf.set(&quot;fs.defaultFS &quot;, &quot;file:///&quot;);
    FileSystem fs = FileSystem.get(conf);
    Path p = new Path(&quot;e:/seq/1.seq&quot;);
    MapFile.Writer writer = new MapFile.Writer(conf, fs, &quot;e:/map&quot;, IntWritable.class, Text.class);


    for (int i = 0; i &lt; 100000; i++) {
        writer.append(new IntWritable(i), new Text(&quot;tom&quot; + i));
    }
</code></pre><p>//        for(int i =0 ;i &lt; 10 ; i++){<br>//            writer.append(new IntWritable(i),new Text(“tom” + i));<br>//        }<br>        writer.close();<br>    }</p>
<hr>
<h3 id="通过MapFile来读取"><a href="#通过MapFile来读取" class="headerlink" title="通过MapFile来读取"></a>通过MapFile来读取</h3><pre><code>/*读取Mapfile文件
 * */
@Test
public void readMapfile() throws Exception {
    Configuration conf = new Configuration();
    conf.set(&quot;fs.defaultFS &quot;, &quot;file:///&quot;);
    FileSystem fs = FileSystem.get(conf);
    Path p = new Path(&quot;e:/seq/1.seq&quot;);
    MapFile.Reader reader = new MapFile.Reader(fs, &quot;e:/map&quot;, conf);
    IntWritable key = new IntWritable();
    Text value = new Text();
    while (reader.next(key, value)) {
        System.out.println(key.get() + &quot;:&quot; + value.toString());
    }
    reader.close();


} 
</code></pre><hr>
<p>Map的分区是哈希分区</p>
<p>combiner的好处是：map端的reduce在map端做集合，减少了shuffle量。统计每个单词hello 1发了十万次，不如发一个hello1在map端做聚合发过去hello十万，即刻。就是在map端口预先reduce化简的过程。不是所有的作业都可以把reduce做成combiner。最大值最小值都可以combiner但是平均值就不行了。</p>
<p>第六天的最后两个视频没有往下写代码，因为太多太杂了。主要讲解MR计数器，数据倾斜，Mapfile 序列化读取，压缩这些。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/10/05/Hadoop第五天01之hdfs写入剖析/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/05/Hadoop第五天01之hdfs写入剖析/" itemprop="url">Hadoop第五天01之hdfs写入剖析</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-10-05T16:31:19+08:00">
                2018-10-05
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hadoop/" itemprop="url" rel="index">
                    <span itemprop="name">Hadoop</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>一段HDFS写入流源码分析。</p>
<p><img src="https://i.imgur.com/17cBkFO.png" alt=""></p>
<p>首先这个得到了一个抽象类，这个FileSystemm是一个抽象类。</p>
<p><img src="https://i.imgur.com/Rlbd5VC.png" alt=""></p>
<p>这个抽象类有子类，左边有一个箭头，点击得到了一个这个抽象类的所有的子类，简单看下，因为Hadoop包类的hadfs这个，得到的子类是DistributedFileSystem这个分布式文件系统。</p>
<p><img src="https://i.imgur.com/bzQ91XF.png" alt=""></p>
<p>也可以吧鼠标放到fs上会显示返回的类</p>
<p><img src="https://i.imgur.com/JOfPZvk.png" alt=""></p>
<p>也可以在IDEA的右下角的类标签里面找到：</p>
<p><img src="https://i.imgur.com/WSz2duV.png" alt=""></p>
<p>也就是说返回了一个DistributedFIleSystem,</p>
<p>然后文件系统fs.create调用了create方法返回了一个FSDataOutputStream流，这个是一个HDFS数据输出流</p>
<p><img src="https://i.imgur.com/SKFBqkH.png" alt=""></p>
<p><img src="https://i.imgur.com/0aIVTzf.png" alt=""></p>
<p>单机F5单部进入第一个：</p>
<p><img src="https://i.imgur.com/nnxWOri.png" alt=""></p>
<p><img src="https://i.imgur.com/f434RJR.png" alt=""></p>
<p><img src="https://i.imgur.com/4Cf1ljy.png" alt=""></p>
<p>看这个，进到下一步，调用checkClosed()，方法，这个方法其实是继承FSOutput流里面的东西。  </p>
<p><img src="https://i.imgur.com/lHPzWs4.png" alt=""><br>看到这个里面out=DFSOutputStream。这个是被装饰的流。调用这个流的write方法</p>
<p>HDFS流是对DFS输出流的包装</p>
<p>进去这个是装饰模式。</p>
<p>在这个构造模式中也声明了字段。</p>
<p><img src="https://i.imgur.com/8CsurDw.png" alt=""></p>
<p>下一步：</p>
<p><img src="https://i.imgur.com/0f1RcbV.png" alt=""></p>
<p>调用了Close方法因为是继承    都是FSoutput流的子类。一个检查的方法，判断是否数组越界。</p>
<p>下面这个for是个循环，循环写入，。 </p>
<p><img src="https://i.imgur.com/MTsauqO.png" alt=""></p>
<p>然后下一步，进入到write1方法。</p>
<p><img src="https://i.imgur.com/nAIKdn5.png" alt=""></p>
<p>里面的buf是一个缓冲区，count是一个成员常量</p>
<p><img src="https://i.imgur.com/aLCcnkl.png" alt=""></p>
<p>上图的buf看注释，是一个缓冲区用哦过来存储数据，在他被checksum校验和之前</p>
<p>校验和就是检查是否是比之前设置的最小块大小大，而且还必须是512字节的倍数</p>
<p><img src="https://i.imgur.com/pAFducf.png" alt=""></p>
<p>上图点击进入到一个校验和类的里面看注释，校验和用于数据传输之前。</p>
<p><img src="https://i.imgur.com/E8qEJy0.png" alt=""></p>
<p>上图这两行，通过一个算法来实现校验和。通过CRC32算法类似于哈希算法。</p>
<p><img src="https://i.imgur.com/RJ1vXHv.png" alt=""></p>
<p>不管他，回到buf缓冲这个地方，单部进入</p>
<p><img src="https://i.imgur.com/RKkQGry.png" alt=""></p>
<p>首先在缓冲区进行一个判定</p>
<p><img src="https://i.imgur.com/qfwP1Yn.png" alt=""></p>
<p>拷贝本地数据到本地的一个buf，显示Localbuffer=4608,count=0.要拷贝的字节是4608。<br>如果拷贝的内容没有缓冲区大就考数据内容，要是比缓冲区大的话，就拷贝缓冲区大小的内容。</p>
<p><img src="https://i.imgur.com/rk8wX6W.png" alt=""></p>
<p>在单部进入到上图。</p>
<p><img src="https://i.imgur.com/PjqlOXa.png" alt=""></p>
<p>返回到代码。进入到源代码中，如上图</p>
<p>在单部进入到这个Close里面：如下图：<br><img src="https://i.imgur.com/97VYh5X.png" alt=""></p>
<p>这个close是out.close。out是调用父类的close方法，而且父类还是一个包装类，所以进入了包装的列的close方法。<br>再进入到close()方法。如下图：</p>
<p><img src="https://i.imgur.com/2ExMNXU.png" alt=""></p>
<p>out的本类HDFSOutputStream，然后这个又是一个装饰类，本质上是DFSOutputStream这个类的方法。</p>
<p>单部进入到Out里面，看一下调用的到底是谁的out。再单部进入到Out里面</p>
<p><img src="https://i.imgur.com/RG98Z9F.png" alt=""></p>
<p>看注释，关闭输出流并且    释放与之相关联的系统资源。<br>上图最终进入到了DFSOutputstream的close()方法里面了。</p>
<p><img src="https://i.imgur.com/dDhSzjg.png" alt=""></p>
<p>接上图，看到在这个close(）方法里面有一个closeImp（)方法。调用自己的关闭实现方法<br>还在这个类里面执行呢：继续在这个类里面往下走:如下图：</p>
<p><img src="https://i.imgur.com/7Vkorih.png" alt=""></p>
<p>这个里面有一个flushbuffer()方法， 在单部进入到这个方法里面。如下图：</p>
<p><img src="https://i.imgur.com/YSCeFmf.png" alt=""></p>
<p>看到这个图里面this和当前类不太一样说明这两个东西也是继承关系。</p>
<p>清理缓冲区两个参数，看注释：强制任何输出流字节被校验和并且被写入底层的输出流。</p>
<p>再单步进入：</p>
<p><img src="https://i.imgur.com/jfWYEI1.png" alt=""></p>
<p><img src="https://i.imgur.com/4iqESya.png" alt=""></p>
<p>看到如上图的内容。然后到了writeChecksumChunks这个方法里面，单部进入到这个里面：</p>
<p><img src="https://i.imgur.com/FM0MpyT.png" alt=""></p>
<p>对给定的数据小块来生成校验和，并且根据小块和校验和给底层的数据输出流。 </p>
<p>单部进入到这个sum.calculateChunckedSum方法里面。</p>
<p><img src="https://i.imgur.com/EVQOpOL.png" alt=""></p>
<p><img src="https://i.imgur.com/UKsSHTO.png" alt=""></p>
<p>下一步</p>
<p><img src="https://i.imgur.com/ifq7oZR.png" alt=""></p>
<p>上图吧数据写入了底层里面去了。</p>
<p>下一步</p>
<p><img src="https://i.imgur.com/WLEldHU.png" alt=""></p>
<p>下一步</p>
<p><img src="https://i.imgur.com/TN0fU7T.png" alt=""></p>
<p>下一步</p>
<p><img src="https://i.imgur.com/X3dHeJt.png" alt=""></p>
<p><img src="https://i.imgur.com/W5NCoWA.png" alt=""></p>
<p>上图就是将底层的内容打成包，再去通过网络传输。在包里面加编号和序号，也就是加顺序。</p>
<p><img src="https://i.imgur.com/INeugMK.png" alt=""></p>
<p>往下不写了太多了。现在的视频位置是hdfs写入剖析2的开头。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/4/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><span class="space">&hellip;</span><a class="page-number" href="/page/8/">8</a><a class="extend next" rel="next" href="/page/6/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/touxiang.jpg"
                alt="Eric Hunn" />
            
              <p class="site-author-name" itemprop="name">Eric Hunn</p>
              <p class="site-description motion-element" itemprop="description">大数据|动漫迷|科技控</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">71</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">16</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">107</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/erichunn" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:erichunn.me@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://weibo.com/u/2944687303/home?wvr=5" target="_blank" title="Webibo">
                      
                        <i class="fa fa-fw fa-weibo"></i>Webibo</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://github.com/drakeet/" title="Drakeet" target="_blank">Drakeet</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      

      
	  
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=1311545319&auto=1&height=66"></iframe>

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Eric Hunn</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  

  

  

</body>
</html>
