<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />





  <link rel="alternate" href="/atom.xml" title="无心是一首歌" type="application/atom+xml" />






<meta name="description" content="大数据|动漫迷|科技控">
<meta property="og:type" content="website">
<meta property="og:title" content="无心是一首歌">
<meta property="og:url" content="http://erichunn.github.io/page/3/index.html">
<meta property="og:site_name" content="无心是一首歌">
<meta property="og:description" content="大数据|动漫迷|科技控">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="无心是一首歌">
<meta name="twitter:description" content="大数据|动漫迷|科技控">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: 'undefined',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://erichunn.github.io/page/3/"/>





  <title>无心是一首歌</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?02fe19ac065353167cab1b453f2909ec";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">无心是一首歌</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-首页">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-关于我">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于我
          </a>
        </li>
      
        
        <li class="menu-item menu-item-标签">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-分类">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-归档">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-站点地图">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br />
            
            站点地图
          </a>
        </li>
      
        
        <li class="menu-item menu-item-腾讯公益">
          <a href="/404/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br />
            
            腾讯公益
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/11/16/Hbase第二天/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/16/Hbase第二天/" itemprop="url">Hbase第二天</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-11-16T09:33:51+08:00">
                2018-11-16
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>start-hbase.sh<br>    hbase-daemon.sh start master<br>    habse-daemon.sh    start regionserver</p>
<p>hbase的ha设置：<br>    直接打开S202或者s203的master进程即可，启动命令如上图。</p>
<p>hbase shell操作：<br>    $&gt;hbaes shell<br>    $hbase&gt;help</p>
<p>namespace 类似于Mysql库的概念</p>
<p>insert into<br>nosql: not only SQL<br>key-value<br>put用来放kv对。<br>在建表的时候没有指定列明，指定的是列族名，然后这个列族下的列是可以增减的。<br>help ‘put’</p>
<p><img src="https://i.imgur.com/5JUqC0b.png" alt=""></p>
<p>habase shell 操作：</p>
<pre><code>$&gt;hbase shell                                    //登陆shell终端
$hbase&gt;help                                        //    
$hbase&gt;help &apos;list_namespace&apos;                    //查看特定 的命令帮助
$hbase&gt;list_namespace                            //列出名字空间（数据库）
$hbase&gt;list_namespace_tables &apos;default&apos;            //列出名字空间
$hbase&gt;create_namespace &apos;ns1&apos;                    //创建名字空间
$hbase&gt;help &apos;create&apos;                            //
$hbase&gt;create &apos;ns1:t1&apos;,&apos;f1&apos;                        //创建表，指定空间下
$hbase&gt;put &apos;ns1:t1&apos;,&apos;row1&apos;,&apos;f1:id&apos;,100
$hbase&gt;put &apos;ns1:t1&apos;,&apos;row1&apos;,&apos;f1:name&apos;,tom
$hbase&gt;get &apos;ns1:t1&apos;,&apos;row1&apos;                        //指定查询row
$hbase&gt;scan &apos;ns1:t1&apos;                            //权标扫描扫描ns1列族的t1列
</code></pre><p>三级坐标定位，一个是列族，一个是row一个是时间戳如下图;</p>
<p><img src="https://i.imgur.com/ipxtMbm.png" alt=""></p>
<p><img src="https://i.imgur.com/uz0TL9q.png" alt=""></p>
<p><img src="https://i.imgur.com/3HmcLCa.png" alt=""></p>
<p>通过java api操作hbase:<br>    package com.it18zhang.hbasedemo;</p>
<pre><code>import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.TableName;
import org.apache.hadoop.hbase.client.Connection;
import org.apache.hadoop.hbase.client.ConnectionFactory;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.client.Table;
import org.apache.hadoop.hbase.util.Bytes;
import org.junit.Test;

eate 2018/11/17 11:56
 */
public class TestCRUD {
    @Test
    public void put() throws Exception {
        Configuration conf = HBaseConfiguration.create();
        Connection conn = ConnectionFactory.createConnection(conf);

        TableName tname = TableName.valueOf(&quot;ns1:t1&quot;);
        Table table = conn.getTable(tname);
        byte[] rowid = Bytes.toBytes(&quot;row2&quot;);
        byte[] f1 = Bytes.toBytes(&quot;f1&quot;);
        byte[] id = Bytes.toBytes(&quot;id&quot;);
        byte[] value = Bytes.toBytes(102);
        //创建put对象

        Put put = new Put(rowid);
        put.addColumn(f1, id, value);
        table.put(put);

    }
}
</code></pre><p>pom文件：</p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;
         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;groupId&gt;com.it18zhang&lt;/groupId&gt;
    &lt;artifactId&gt;HbaseDemo&lt;/artifactId&gt;
    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;junit&lt;/groupId&gt;
            &lt;artifactId&gt;junit&lt;/artifactId&gt;
            &lt;version&gt;4.11&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;
            &lt;artifactId&gt;hbase-client&lt;/artifactId&gt;
            &lt;version&gt;1.2.3&lt;/version&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;


&lt;/project&gt;
</code></pre><hr>
<p>hbase架构介绍：</p>
<p><img src="https://i.imgur.com/BaJwZ3z.png" alt=""></p>
<h1 id="关于区域服务器"><a href="#关于区域服务器" class="headerlink" title="关于区域服务器"></a>关于区域服务器</h1><p>  每个大表按照rowkey来切分，rowkey是可排序的，每个切分的被分派到每个区域分五期里面。key是有序的。而且每个取余服务器的表都是有范围的，所以在get的时候要指定rowkey，然后查询的时候定位到这个rowkey在哪个区域服务器上，因为所有的区域服务器信息都在meta表这个自带的表里面，叫元数据表。里面存储着rowkey。也就是每个区域的rowkey的范围。</p>
<p><img src="https://i.imgur.com/4nzxjz8.png" alt=""></p>
<p>看里面的内容hbase；namespace,,14….,74….<br>这个就是名字空间表，起始的位置，结束的rowkey位置。<br>前4大段，是指的是一大行的前4列。两个逗号意味着没有切割。就只是在一个区域服务器里面</p>
<p>再看下面的ns1:t1表的都是一行，然后3列不同的列，如果查询这部分直接联系s204,如果204挂了， 这个表就会更新，变成承接的新表的数据。这个ns1:t1的这个表就没有分割。column是列，info是列族，server是列</p>
<p>在hbase查询的时候是3级定位。时间戳，列族（列），rowkey（行）</p>
<p>看一下下面的这个目录：</p>
<p>hbase,数据，ns1名字空间，t1表，区域服务器,f1列族，然后列族下面就是文件了二进制存储，内容都在这个二进制文件里面。</p>
<p><img src="https://i.imgur.com/18bQKMj.png" alt=""></p>
<p>一个表可以有多个区域，一个区域服务器肯定有多个区域，由于有副本的存在所以一个区域可以在多个服务器上，区域服务器是来提供服务的，读写过程是由区域服务器完成，相当于数据节点，这2个都是完成数据的读写过程的。</p>
<p>在区域里面有个store在里面有MemStore。所以这些内容都是在内存当中，如果内存满了，就会被溢出到磁盘当中，Storefile就是在磁盘当中了，Hfile通过和底层交互</p>
<h1 id="hbase的写入过程："><a href="#hbase的写入过程：" class="headerlink" title="hbase的写入过程："></a>hbase的写入过程：</h1><p><img src="https://i.imgur.com/XfOpVug.png" alt=""></p>
<p>root这个地方写错了是老版本的，应该是meta表<br><img src="https://i.imgur.com/Imht8Oq.png" alt=""><br><img src="https://i.imgur.com/peKNm1w.png" alt=""></p>
<p>现在妖王一个表里写数据，需要先查询这个表在那个区域服务器上，要先查询hbase空间下的meta表，因为这个表里面标识了类似于一个目录的内容，标识那个区域子在那个服务器上，也就是rowkey的范围。这个meta表在哪里呢怎么找到呢？ meta表要通过zk来找到。因为zk集群在，所以没有zk的情况下hbase都起不来。</p>
<p>进入到hbase shell里面</p>
<p><img src="https://i.imgur.com/Rx4QHIF.png" alt=""></p>
<p>通过进入到shell里面发现这个meta表大概可能在这个里面，进去之后发现这个meta表在s203里面。如下图</p>
<p><img src="https://i.imgur.com/SHSVJwz.png" alt=""></p>
<p>所以实际上就是先联系zk然后通过zk找到他的位置在s203里面，这个meta表在s203里面</p>
<p>ta/hbase/meta/1588230740/info/da7a63e29d1c4fb588068ea9c187ae27</p>
<hr>
<h1 id="hbase基于hdfs"><a href="#hbase基于hdfs" class="headerlink" title="hbase基于hdfs"></a>hbase基于hdfs</h1><p>【表数据的存储结构目录构成】</p>
<pre><code>hdfs://s201:8020/hbase/data/${名字空间}/${表名}/区域名称/列族名称
</code></pre><p>相同列族的数据存放在一个文件中，</p>
<p>【WAL写前日志目录结构构成】</p>
<pre><code>hdfs://s201:8020/hbase/wals/s202,16020,1542534586029/s202%2C16020%2C1542534586029.default.1542541792199

hdfs://s201:8020/hbase/wals/${区域服务器名称}/主机名，端口号，时间戳/
</code></pre><h1 id="client端交互过程"><a href="#client端交互过程" class="headerlink" title="client端交互过程"></a>client端交互过程</h1><p>0.集群启动时，master负责分配区域到指定的区域服务器</p>
<p>1.联系zk找出meta表所在的区域服务器rs(regionserver)<br>        /meta/meta-region-server<br>    定位到所在的服务器</p>
<p>2.定位rowkey，找到对应的rs(regionserver)</p>
<p>3.缓存信息到本地，</p>
<p>4.联系regionserver</p>
<p>5.HRegionServe负责open-HRegion对象打开H区域服务器对象，为每个列族创建store实例，说明store是和列族对应的，store包涵多个storefile实例，他们是对hfile的轻量级封装，每个store还对应一个memstroe（用于内存储速度快），</p>
<p><img src="https://i.imgur.com/ks0t9JW.png" alt=""></p>
<hr>
<h1 id="在百万数据存储的时候："><a href="#在百万数据存储的时候：" class="headerlink" title="在百万数据存储的时候："></a>在百万数据存储的时候：</h1><p>关闭WALS</p>
<p><img src="https://i.imgur.com/ITwCty6.png" alt=""></p>
<p>代码如下：</p>
<pre><code>@Test
   public void biginsert() throws Exception {
       long start=System.currentTimeMillis();
       Configuration conf = HBaseConfiguration.create();
       Connection conn = ConnectionFactory.createConnection(conf);
       TableName tname = TableName.valueOf(&quot;ns1:t1&quot;);
       HTable table = (HTable) conn.getTable(tname);
       //不要自动清理缓冲区
       table.setAutoFlushTo(false);

       for (int i = 0; i &lt; 1000000; i++) {
           Put put = new Put(Bytes.toBytes(&quot;row&quot; + i));
           //关闭写前日志
           put.setWriteToWAL(false);
           put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;), Bytes.toBytes(i));
           put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;), Bytes.toBytes(&quot;tom&quot; + i));
           put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;), Bytes.toBytes(i % 100));
           table.put(put);
           if (i % 2000 == 0) {
               table.flushCommits();
           }
       }
       table.flushCommits();
       System.out.println(System.currentTimeMillis()-start);

   }
</code></pre><hr>
<p>hbase shell命令：</p>
<p>要想删除表，先要禁用表。</p>
<pre><code>$hbase&gt;flush &apos;ns1:t1&apos;        //清理内存数据到磁盘
$hbase&gt;count &apos;ns1:t1&apos;        //统计函数
$hbase&gt;disable &apos;ns1:t1&apos;        //删除表之前要禁用表
$hbase&gt;drop &apos;ns1:t1&apos;        //删除表 
$hbase&gt;count &apos;hbase:meta&apos;     //查看元数据表
</code></pre><p><img src="https://i.imgur.com/AAzzOmp.png" alt=""></p>
<hr>
<h1 id="格式化代码，设置固定数字格式"><a href="#格式化代码，设置固定数字格式" class="headerlink" title="格式化代码，设置固定数字格式"></a>格式化代码，设置固定数字格式</h1><pre><code> @Test
    public void formatNum(){
        DecimalFormat format =new DecimalFormat();
        format.applyPattern(&quot;0000000&quot;);
//        format.applyPattern(&quot;###,###,00&quot;);
        System.out.println(format.format(8));
    }
</code></pre><hr>
<p>为什么要格式化rowid，因为rowid1008和8相比较8比1008还要大，所以要格式化一下。</p>
<p>经过格式化rowid的代码：</p>
<pre><code> @Test
public void biginsert() throws Exception {

    DecimalFormat format =new DecimalFormat();
    format.applyPattern(&quot;0000000&quot;);
    System.out.println(format.format(8));

    long start=System.currentTimeMillis();
    Configuration conf = HBaseConfiguration.create();
    Connection conn = ConnectionFactory.createConnection(conf);
    TableName tname = TableName.valueOf(&quot;ns1:t1&quot;);
    HTable table = (HTable) conn.getTable(tname);
    //不要自动清理缓冲区
    table.setAutoFlushTo(false);

    for (int i = 0; i &lt; 10000; i++) {
        Put put = new Put(Bytes.toBytes(&quot;row&quot; + format.format(i)));
        //关闭写前日志
        put.setWriteToWAL(false);
        put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;), Bytes.toBytes(i));
        put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;), Bytes.toBytes(&quot;tom&quot; + i));
        put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;), Bytes.toBytes(i % 100));
        table.put(put);
        if (i % 2000 == 0) {
            table.flushCommits();
        }
    }
    table.flushCommits();
    System.out.println(System.currentTimeMillis()-start);

}
</code></pre><hr>
<h1 id="flush命令"><a href="#flush命令" class="headerlink" title="flush命令"></a>flush命令</h1><pre><code>$hbase:flush：清理内存数据到磁盘
</code></pre><h1 id="region拆分切割"><a href="#region拆分切割" class="headerlink" title="region拆分切割"></a>region拆分切割</h1><p><img src="https://i.imgur.com/v0N169Q.png" alt=""><br><img src="https://i.imgur.com/XQesWs6.png" alt=""></p>
<p>hbase默认切割文件是10G，超过切割。</p>
<pre><code>$hbase&gt;count &apos;ns1:t1&apos;        //统计函数
</code></pre><p><img src="https://i.imgur.com/sjQ5dq7.png" alt=""></p>
<pre><code>切割而一旦完成会保留原来的表结构，但是原来的数据内容已经没有了被删除了 
</code></pre><p><img src="https://i.imgur.com/UhuexPk.png" alt=""></p>
<p><img src="https://i.imgur.com/0NfLQSJ.png" alt=""></p>
<p><img src="https://i.imgur.com/oJTAFPt.png" alt=""></p>
<hr>
<h1 id="hbase和hadoop的ha集成"><a href="#hbase和hadoop的ha集成" class="headerlink" title="hbase和hadoop的ha集成"></a>hbase和hadoop的ha集成</h1><p>1.在hbase-env.sh文件添加hadoop配置文件目录到hbase_classpath环境变量[/soft/hbase/conf/hbase-env]并且分发。</p>
<pre><code>export HBASE_CLASSPATH=$HBASE_CLASSPATH:/soft/hadoop/    etc/hadoop
</code></pre><p>2.在hbase/conf目录下创建到hadoop的hdfs-site.xml的符号链接</p>
<pre><code>    $&gt;ln -s /soft/hadoop/etc/hadoop/hdfs-site.xml
/soft/hbase/conf/hdfs-site.xml
</code></pre><p>3.修改Hbase-site.xml文件中hbase.rootdir的目录值<br>        /soft/hbase/conf/hbase-site.xml<br>4.将之都分发出去。</p>
<p>继承了之后及时关闭了s201的namenode，hbase的Hmaster也不会关闭，也就是形成了高可用状态。 </p>
<hr>
<h1 id="hbase手动移动区域"><a href="#hbase手动移动区域" class="headerlink" title="hbase手动移动区域"></a>hbase手动移动区域</h1><p>手动移动区域<br><img src="https://i.imgur.com/UayeavF.png" alt=""></p>
<p>手动强行合并hbase块<br><img src="https://i.imgur.com/CrvZAFo.png" alt=""><br><img src="https://i.imgur.com/JCFdxeV.png" alt=""></p>
<p>手动切割：</p>
<h1 id="拆分风暴："><a href="#拆分风暴：" class="headerlink" title="拆分风暴："></a>拆分风暴：</h1><p><img src="https://i.imgur.com/wP6mfUJ.png" alt=""></p>
<p>在达到10G以后，几个区域同时增长，同时到达10G临界点，同时切割额，服务器工作瞬间增大。每个都在切割，就叫切割风暴，我们要避免这种情况，通过使用手动切割，避免拆分风暴。</p>
<hr>
<p>代码操作增删改查<br>    package com.it18zhang.hbasedemo;</p>
<pre><code>import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.*;
import org.apache.hadoop.hbase.client.*;
import org.apache.hadoop.hbase.util.Bytes;
import org.junit.Test;

import java.io.IOException;
import java.text.DecimalFormat;
import java.util.Iterator;
import java.util.Map;
import java.util.NavigableMap;

/**
 * @Title:TestCRUD
 * @Description:
 * @Auther: Eric Hunn
 * @Version:1.0
 * @Create 2018/11/17 11:56
 */
public class TestCRUD {
    @Test
    public void put() throws Exception {
        Configuration conf = HBaseConfiguration.create();
        Connection conn = ConnectionFactory.createConnection(conf);

        TableName tname = TableName.valueOf(&quot;ns1:t1&quot;);
        Table table = conn.getTable(tname);
        byte[] rowid = Bytes.toBytes(&quot;row2&quot;);
        byte[] f1 = Bytes.toBytes(&quot;f1&quot;);
        byte[] id = Bytes.toBytes(&quot;id&quot;);
        byte[] value = Bytes.toBytes(102);
        //创建put对象

        Put put = new Put(rowid);
        put.addColumn(f1, id, value);
        table.put(put);

    }

    @Test
    public void biginsert() throws Exception {

        DecimalFormat format = new DecimalFormat();
        format.applyPattern(&quot;0000000&quot;);
        System.out.println(format.format(8));

        long start = System.currentTimeMillis();
        Configuration conf = HBaseConfiguration.create();
        Connection conn = ConnectionFactory.createConnection(conf);
        TableName tname = TableName.valueOf(&quot;ns1:t1&quot;);
        HTable table = (HTable) conn.getTable(tname);
        //不要自动清理缓冲区
        table.setAutoFlushTo(false);

        for (int i = 0; i &lt; 10000; i++) {
            Put put = new Put(Bytes.toBytes(&quot;row&quot; + format.format(i)));
            //关闭写前日志
            put.setWriteToWAL(false);
            put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;), Bytes.toBytes(i));
            put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;), Bytes.toBytes(&quot;tom&quot; + i));
            put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;), Bytes.toBytes(i % 100));
            table.put(put);
            if (i % 2000 == 0) {
                table.flushCommits();
            }
        }
        table.flushCommits();
        System.out.println(System.currentTimeMillis() - start);

    }

    @Test
    public void formatNum() {
        DecimalFormat format = new DecimalFormat();
        format.applyPattern(&quot;0000000&quot;);
//        format.applyPattern(&quot;###,###,00&quot;);
        System.out.println(format.format(8));
    }

    @Test
    public void createNamespace() throws Exception {
        //创建conf对象
        Configuration conf = HBaseConfiguration.create();
        //通过工厂创建连接对象
        Connection conn = ConnectionFactory.createConnection(conf);
        Admin admin = conn.getAdmin();
        NamespaceDescriptor nsd = NamespaceDescriptor.create(&quot;ns2&quot;).build();
        admin.createNamespace(nsd);

        NamespaceDescriptor[] ns = admin.listNamespaceDescriptors();
        for (NamespaceDescriptor n : ns) {
            System.out.println(n.getName());
        }

    }

    @Test
    public void listNamespaces() throws Exception {
        //创建conf对象
        Configuration conf = HBaseConfiguration.create();
        //通过工厂创建连接对象
        Connection conn = ConnectionFactory.createConnection(conf);
        Admin admin = conn.getAdmin();
        NamespaceDescriptor[] ns = admin.listNamespaceDescriptors();
        for (NamespaceDescriptor n : ns) {
            System.out.println(n.getName());
        }
    }

    @Test
    public void createTables() throws Exception {
        //创建conf对象
        Configuration conf = HBaseConfiguration.create();
        //通过工厂创建连接对象
        Connection conn = ConnectionFactory.createConnection(conf);
        Admin admin = conn.getAdmin();
        //创建表名对象
        TableName tbn = TableName.valueOf(&quot;ns2:t2&quot;);
        //创建表描述符对象
        HTableDescriptor tbl = new HTableDescriptor(tbn);
        //在表描述符中添加列族创建列族描述符
        HColumnDescriptor col = new HColumnDescriptor(&quot;f1&quot;);
        tbl.addFamily(col);

        admin.createTable(tbl);
        System.out.println(&quot;over&quot;);
    }

    @Test
    public void disableTable() throws Exception {
        Configuration conf = HBaseConfiguration.create();
        Connection conn = ConnectionFactory.createConnection(conf);
        Admin admin = conn.getAdmin();
        admin.disableTable(TableName.valueOf(&quot;ns2:t2&quot;));
    }

    @Test
    public void dropTable() throws Exception {
        Configuration conf = HBaseConfiguration.create();
        Connection conn = ConnectionFactory.createConnection(conf);
        Admin admin = conn.getAdmin();
        admin.disableTable(TableName.valueOf(&quot;ns2:t2&quot;));
        admin.deleteTable(TableName.valueOf(&quot;ns2:t2&quot;));
    }

    @Test
    public void deleteData() throws IOException {

        Configuration conf = HBaseConfiguration.create();
        Connection conn = ConnectionFactory.createConnection(conf);
        TableName tname = TableName.valueOf(&quot;ns1:t1&quot;);
        Table table = conn.getTable(tname);
        Delete del = new Delete(Bytes.toBytes(&quot;row0000001&quot;));
        del.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;));
        del.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;));

        table.delete(del);
        System.out.println(&quot;over&quot;);

    }

    @Test
    public void scanall() throws IOException {
        Configuration conf = HBaseConfiguration.create();
        Connection conn = ConnectionFactory.createConnection(conf);
        TableName tname = TableName.valueOf(&quot;ns1:t1&quot;);
        Table table = conn.getTable(tname);
        Scan scan = new Scan();
        ResultScanner rs = table.getScanner(scan);
        Iterator&lt;Result&gt; it = rs.iterator();
        while (it.hasNext()) {
            Result r = it.next();//理解这个r是对一整行的封装
            byte[] value = r.getValue(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;));
            System.out.println(Bytes.toString(value));
        }
    }


    @Test
    public void scan() throws IOException {
        Configuration conf = HBaseConfiguration.create();
        Connection conn = ConnectionFactory.createConnection(conf);
        TableName tname = TableName.valueOf(&quot;ns1:t1&quot;);
        Table table = conn.getTable(tname);
        Scan scan = new Scan();
        scan.setStartRow(Bytes.toBytes(&quot;row0005000&quot;));
        scan.setStopRow(Bytes.toBytes(&quot;row0008000&quot;));
        ResultScanner rs = table.getScanner(scan);
        Iterator&lt;Result&gt; it = rs.iterator();
        while (it.hasNext()) {
            Result r = it.next();//理解这个r是对一整行的封装
            byte[] value = r.getValue(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;));
            System.out.println(Bytes.toString(value));
        }
    }

    //动态获取所有的列和列族，动态遍历，每个列和值的集合，由于强行getvalue转换成string类型，所以难免出现乱码情况
    @Test
    public void scan2() throws IOException {
        Configuration conf = HBaseConfiguration.create();
        Connection conn = ConnectionFactory.createConnection(conf);
        TableName tname = TableName.valueOf(&quot;ns1:t1&quot;);
        Table table = conn.getTable(tname);
        Scan scan = new Scan();
        ResultScanner rs = table.getScanner(scan);
        Iterator&lt;Result&gt; it = rs.iterator();
        while (it.hasNext()) {
            Result r = it.next();//理解这个r是对一整行的封装
            Map&lt;byte[], byte[]&gt; map = r.getFamilyMap(Bytes.toBytes(&quot;f1&quot;));
            for (Map.Entry&lt;byte[], byte[]&gt; entrySet : map.entrySet()) {
                String col = Bytes.toString(entrySet.getKey());
                String val = Bytes.toString(entrySet.getValue());
                System.out.println(col + &quot;:&quot; + val + &quot;,&quot;);
            }
            System.out.println();
        }
    }

    @Test
    public void scan3() throws IOException {
        Configuration conf = HBaseConfiguration.create();
        Connection conn = ConnectionFactory.createConnection(conf);
        TableName tname = TableName.valueOf(&quot;ns1:t1&quot;);
        Table table = conn.getTable(tname);
        Scan scan = new Scan();
        ResultScanner rs = table.getScanner(scan);
        Iterator&lt;Result&gt; it = rs.iterator();
        while (it.hasNext()) {
            Result r = it.next();//理解这个r是对一整行的封装
            //得到一行的所有map，key=f1,value=Map&lt;Col,Map&lt;Timestamp,value&gt;&gt;这个结构

            NavigableMap&lt;byte[], NavigableMap&lt;byte[], NavigableMap&lt;Long, byte[]&gt;&gt;&gt; map = r.getMap();
            for (Map.Entry&lt;byte[], NavigableMap&lt;byte[], NavigableMap&lt;Long, byte[]&gt;&gt;&gt; entry : map.entrySet()) {
                //得到列族
                String f = Bytes.toString(entry.getKey());
                NavigableMap&lt;byte[], NavigableMap&lt;Long, byte[]&gt;&gt; colDataMap = entry.getValue();
                for (Map.Entry&lt;byte[], NavigableMap&lt;Long, byte[]&gt;&gt; ets : colDataMap.entrySet()) {
                    String c = Bytes.toString(ets.getKey());
                    Map&lt;Long, byte[]&gt; tsValueMap = ets.getValue();
                    for (Map.Entry&lt;Long, byte[]&gt; e : tsValueMap.entrySet()) {
                        Long ts = e.getKey();
                        String value = Bytes.toString(e.getValue());
                        System.out.println(f + &quot;:&quot; + c + &quot;:&quot; + ts + &quot;=&quot; + value + &quot;,&quot;);
                    }
                }
            }
        }
    }
}
</code></pre>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/11/16/Hbase第一天/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/16/Hbase第一天/" itemprop="url">Hbase第一天</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-11-16T09:33:51+08:00">
                2018-11-16
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/11/15/Zookeeper第二天/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/15/Zookeeper第二天/" itemprop="url">Zookeeper第二天</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-11-15T16:45:25+08:00">
                2018-11-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="leader推选过程-最小号选举法"><a href="#leader推选过程-最小号选举法" class="headerlink" title="leader推选过程(最小号选举法)"></a>leader推选过程(最小号选举法)</h2><pre><code>1.所有节点在同一目录下创建临时序列节点。
2.节点下会生成/xxx/xx000000001等节点。
3.序号最小的节点就是leader，其余就是follower.
4.每个节点观察小于自己节点的主机。(注册观察者)
5.如果leader挂了，对应znode删除了。
6.观察者收到通知。
</code></pre><p><img src="https://i.imgur.com/O6wXdY8.png" alt=""></p>
<h2 id="配置完全分布式zk集群"><a href="#配置完全分布式zk集群" class="headerlink" title="配置完全分布式zk集群"></a>配置完全分布式zk集群</h2><pre><code>1.挑选3台主机
    s201 ~ s203
2.每台机器都安装zk
    tar
    环境变量

3.配置zk配置文件
    s201 ~ s203
    [/soft/zk/conf/zoo.cfg]
    ...
    dataDir=/home/centos/zookeeper

    server.1=s201:2888:3888
    server.2=s202:2888:3888 
    server.3=s203:2888:3888

4.在每台主机的/home/centos/zookeeper中添加myid,内容分别是1,2,3
    [s201]
    $&gt;echo 1 &gt; /home/centos/zookeeper/myid
    [s202]
    $&gt;echo 2 &gt; /home/centos/zookeeper/myid
    [s203]
    $&gt;echo 3 &gt; /home/centos/zookeeper/myid

5.启动服务器集群 
    $&gt;zkServer.sh start
    ...

6.查看每台服务器的状态
    $&gt;zkServer.sh status

7.修改zk的log目录

    vi /soft/zk/conf/log4j.properties
</code></pre><p>修改如下：</p>
<p><img src="https://i.imgur.com/xaRGDXr.png" alt=""></p>
<pre><code>8.创建log目录：
    xcall.sh &quot;mkdir /home/centos/zookeeper/log&quot;
</code></pre><h2 id="部署细节"><a href="#部署细节" class="headerlink" title="部署细节"></a>部署细节</h2><pre><code>1.在jn节点分别启动jn进程
    $&gt;hadoop-daemon.sh start journalnode

2.启动jn之后，在两个NN之间进行disk元数据同步
    a)如果是全新集群，先format文件系统,只需要在一个nn上执行。
        [s201]
        $&gt;hadoop namenode -format

    b)如果将非HA集群转换成HA集群，复制原NN的metadata到另一个nn.
        1.步骤一
            [s201]
            $&gt;scp -r /home/centos/hadoop/dfs centos@s206:/home/centos/hadoop/

        2.步骤二
            在新的nn(未格式化的nn)上运行一下命令，实现待命状态引导。
            [s206]
            $&gt;hdfs namenode -bootstrapStandby        //需要s201为启动状态,提示是否格式化,选择N.

        3)在一个NN上执行以下命令，完成edit日志到jn节点的传输。
            $&gt;hdfs namenode -initializeSharedEdits
            #查看s202,s203是否有edit数据.

        4)启动所有节点.
            [s201]
            $&gt;hadoop-daemon.sh start namenode        //启动名称节点
            $&gt;hadoop-daemons.sh start datanode        //启动所有数据节点

            [s206]
            $&gt;hadoop-daemon.sh start namenode        //启动名称节点
</code></pre><h2 id="HA管理"><a href="#HA管理" class="headerlink" title="HA管理"></a>HA管理</h2><pre><code>$&gt;hdfs haadmin -transitionToActive nn1                //切成激活态
$&gt;hdfs haadmin -transitionToStandby nn1                //切成待命态
$&gt;hdfs haadmin -transitionToActive --forceactive nn2//强行激活
$&gt;hdfs haadmin -failover nn1 nn2                    //模拟容灾演示,从nn1切换到nn2
</code></pre><h2 id="完全0开始部署hadoop-HDFS的HA集群，使用zk实现自动容灾"><a href="#完全0开始部署hadoop-HDFS的HA集群，使用zk实现自动容灾" class="headerlink" title="完全0开始部署hadoop HDFS的HA集群，使用zk实现自动容灾"></a>完全0开始部署hadoop HDFS的HA集群，使用zk实现自动容灾</h2><pre><code>1.停掉hadoop的所有进程

2.删除所有节点的日志和本地数据.
    删除/home/centos/hadoop下的所有和
        /home/centos/journal下的所有

3.改换hadoop符号连接为ha

4.登录每台JN节点主机，启动JN进程.
    [s202-s204]
    $&gt;hadoop-daemon.sh start journalnode

5.登录其中一个NN,格式化文件系统(s201)
    $&gt;hadoop namenode -format

6.复制201目录的下nn的元数据到s206
    $&gt;scp -r ~/hadoop/* centos@s206:/home/centos/hadoop

7.在未格式化的NN(s206)节点上做standby引导.
    7.1)需要保证201的NN启动
        $&gt;hadoop-daemon.sh start namenode

    7.2)登录到s206节点，做standby引导.
        $&gt;hdfs namenode -bootstrapStandby

    7.3)登录201，将s201的edit日志初始化到JN节点。
        $&gt;hdfs namenode -initializeSharedEdits

8.启动所有数据节点.
    $&gt;hadoop-daemons.sh start datanode

9.登录到206,启动NN
    $&gt;hadoop-daemon.sh start namenode

10.查看webui
    http://s201:50070/
    http://s206:50070/

11.自动容灾
    11.1)介绍
        自动容灾引入两个组件，zk quarum + zk容灾控制器(ZKFC)。



        运行NN的主机还要运行ZKFC进程，主要负责:
        a.健康监控
        b.session管理
        c.选举
    11.2部署容灾
        a.停止所有进程
            $&gt;stop-all.sh

        b.配置hdfs-site.xml，启用自动容灾.
            [hdfs-site.xml]
                &lt;property&gt;
                    &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;
                    &lt;value&gt;true&lt;/value&gt;
                &lt;/property&gt;

        c.配置core-site.xml，指定zk的连接地址.
            &lt;property&gt;
                &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;
                &lt;value&gt;s201:2181,s202:2181,s203:2181&lt;/value&gt;
            &lt;/property&gt;

        d.分发以上两个文件到所有节点。

12.登录其中的一台NN(s201),在ZK中初始化HA状态
    $&gt;hdfs zkfc -formatZK

13.启动hdfs进程.
    $&gt;start-dfs.sh

14.测试自动容在(206是活跃节点)
    $&gt;kill -9
</code></pre><h2 id="配置RM的HA自动容灾"><a href="#配置RM的HA自动容灾" class="headerlink" title="配置RM的HA自动容灾"></a>配置RM的HA自动容灾</h2><pre><code>1.配置yarn-site.xml
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;
        &lt;value&gt;true&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;
        &lt;value&gt;cluster1&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;
        &lt;value&gt;rm1,rm2&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;
        &lt;value&gt;s201&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;
        &lt;value&gt;s206&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt;
        &lt;value&gt;s201:8088&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt;
        &lt;value&gt;s206:8088&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;
        &lt;value&gt;s201:2181,s202:2181,s203:2181&lt;/value&gt;
    &lt;/property&gt;    

2.使用管理命令
    //查看状态
    $&gt;yarn rmadmin -getServiceState rm1
    //切换状态到standby
    $&gt;yarn rmadmin -transitionToStandby rm1

3.启动yarn集群
    $&gt;start-yarn.sh

4.hadoop没有启动两个resourcemanager,需要手动启动另外一个
    $&gt;yarn-daemon.sh start resourcemanager

5.查看webui

6.做容灾模拟.
    kill -9
</code></pre><h2 id="hive的注意事项"><a href="#hive的注意事项" class="headerlink" title="hive的注意事项"></a>hive的注意事项</h2><pre><code>如果配置hadoop HA之前，搭建了Hive的话，在HA之后，需要调整路径信息.
主要是修改mysql中的dbs,tbls等相关表。
</code></pre><h2 id="Hbase"><a href="#Hbase" class="headerlink" title="Hbase"></a>Hbase</h2><pre><code>hadoop数据库，分布式可伸缩大型数据存储。
用户对随机、实时读写数据。
十亿行 x 百万列。
版本化、非关系型数据库。
</code></pre><h2 id="Feature"><a href="#Feature" class="headerlink" title="Feature"></a>Feature</h2><pre><code>Linear and modular scalability.                    //线性模块化扩展方式。
Strictly consistent reads and writes.            //严格一致性读写
Automatic and configurable sharding of tables    //自动可配置表切割
Automatic failover support between RegionServers.    //区域服务器之间自动容在
Convenient base classes for backing Hadoop MapReduce jobs with Apache HBase tables.        //
Easy to use Java API for client access.            //java API
Block cache and Bloom Filters for real-time queries    //块缓存和布隆过滤器用于实时查询 
Query predicate push down via server side Filters    //通过服务器端过滤器实现查询预测
Thrift gateway and a REST-ful Web service that supports XML, Protobuf, and binary data encoding options    //
Extensible jruby-based (JIRB) shell                    //
Support for exporting metrics via the Hadoop metrics subsystem to files or Ganglia; or via JMX            //可视化
面向列数据库。
</code></pre><h2 id="hbase存储机制"><a href="#hbase存储机制" class="headerlink" title="hbase存储机制"></a>hbase存储机制</h2><pre><code>面向列存储，table是按row排序。
</code></pre><h2 id="搭建hbase集群"><a href="#搭建hbase集群" class="headerlink" title="搭建hbase集群"></a>搭建hbase集群</h2><pre><code>0.选择安装的主机
    s201 ~ s204
1.jdk
    略
2.hadoop
    略
3.tar 
    略
4.环境变量
    略

5.验证安装是否成功
    $&gt;hbase version

5.配置hbase模式
    5.1)本地模式
        [hbase/conf/hbase-env.sh]
        EXPORT JAVA_HOME=/soft/jdk

        [hbase/conf/hbase-site.xml]
        ...
        &lt;property&gt;
            &lt;name&gt;hbase.rootdir&lt;/name&gt;
            &lt;value&gt;file:/home/hadoop/HBase/HFiles&lt;/value&gt;
        &lt;/property&gt;

    5.2)伪分布式
        [hbase/conf/hbase-env.sh]
        EXPORT JAVA_HOME=/soft/jdk

        [hbase/conf/hbase-site.xml]
        &lt;property&gt;
            &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;
            &lt;value&gt;true&lt;/value&gt;
        &lt;/property
        &lt;property&gt;
            &lt;name&gt;hbase.rootdir&lt;/name&gt;
            &lt;value&gt;hdfs://localhost:8030/hbase&lt;/value&gt;
        &lt;/property&gt;

    5.3)完全分布式(必做)
        [hbase/conf/hbase-env.sh]
        export JAVA_HOME=/soft/jdk
        export HBASE_MANAGES_ZK=false

        [hbse-site.xml]
        &lt;!-- 使用完全分布式 --&gt;
        &lt;property&gt;
            &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;
            &lt;value&gt;true&lt;/value&gt;
        &lt;/property&gt;

        &lt;!-- 指定hbase数据在hdfs上的存放路径 --&gt;
        &lt;property&gt;
            &lt;name&gt;hbase.rootdir&lt;/name&gt;
            &lt;value&gt;hdfs://s201:8020/hbase&lt;/value&gt;
        &lt;/property&gt;
        &lt;!-- 配置zk地址 --&gt;
        &lt;property&gt;
            &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;
            &lt;value&gt;s201:2181,s202:2181,s203:2181&lt;/value&gt;
        &lt;/property&gt;
        &lt;!-- zk的本地目录 --&gt;
        &lt;property&gt;
            &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;
            &lt;value&gt;/home/centos/zookeeper&lt;/value&gt;
        &lt;/property&gt;

6.配置regionservers
    [hbase/conf/regionservers]
    s202
    s203
    s204

7.启动hbase集群(s201)
    $&gt;start-hbase.sh

8.登录hbase的webui
    http://s201:16010
</code></pre>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/11/11/Avro-protobuf/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/11/Avro-protobuf/" itemprop="url">Avro&protobuf</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-11-11T19:52:10+08:00">
                2018-11-11
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Avro-Protobuf/" itemprop="url" rel="index">
                    <span itemprop="name">Avro&Protobuf</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="hive"><a href="#hive" class="headerlink" title="hive"></a>hive</h2><pre><code>数据倾斜.
$hive&gt;SET hive.optimize.skewjoin=true;
$hive&gt;SET hive.skewjoin.key=100000;
$hive&gt;SET hive.groupby.skewindata=true;
</code></pre><p>CREATE TABLE mydb.doc(line string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’ ;</p>
<p>select t.word,count(*) from (select explode(split(line,’ ‘)) word from doc ) t group by t.word ;</p>
<h2 id="java串行化"><a href="#java串行化" class="headerlink" title="java串行化"></a>java串行化</h2><h2 id="串行化系统"><a href="#串行化系统" class="headerlink" title="串行化系统"></a>串行化系统</h2><p>protobuf，协议缓冲区。</p>
<p>在Hadoop里面的代码很多是通过相关语言自动生成的。</p>
<p>hadoop底层的rpc都是自动生成的。</p>
<p>java也有串行化，但是hadoop不适用，因为相对来讲比protobuf和avro复杂而且性能差一些。</p>
<p>当吧对象写入磁盘文件或者用于网络传输的时候要把对象变成字节数组。</p>
<p>串行化从本质上来讲是数据格式，就是一种格式，吧对象变成字节数组，也就是说所有数据都要落实到字节数组上，都需要这样，不管是影音文件还是别的文件，通过串行化吧他边恒字节数组    </p>
<h1 id="关于Javabean"><a href="#关于Javabean" class="headerlink" title="关于Javabean:"></a>关于Javabean:</h1><p>标准javabean(pojo,plain old java object)</p>
<p>任何一个Java类也可以叫javabean.广义上。</p>
<p>狭义上的javabean：就是普通古老的java对象pojo:plain old java object </p>
<p>也就是私有的属性，getset方法，空的构造函数。比如person,学生，人。都是javabean。现实生活中的名词，在开发中都被定义成一个类，也就是说的javabean。</p>
<p>下面的代码就是一段javabean<br>    class Person{<br>        public Person(){<br>    }<br>    private String name;<br>    public  void setName(String name){<br>        this.name=name;<br>    }<br>    publc String genName(){<br>        return name; }<br>}</p>
<h2 id="google-protobuf"><a href="#google-protobuf" class="headerlink" title="google protobuf"></a>google protobuf</h2><pre><code>1.下载google protobuf.配置环境
    protoc-2.5.0-win32.zip
</code></pre><p><img src="https://i.imgur.com/7QVjPjY.png" alt=""></p>
<pre><code>1&apos;.pom.xml
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.google.protobuf&lt;/groupId&gt;
            &lt;artifactId&gt;protobuf-java&lt;/artifactId&gt;
            &lt;version&gt;2.5.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;junit&lt;/groupId&gt;
            &lt;artifactId&gt;junit&lt;/artifactId&gt;
            &lt;version&gt;4.11&lt;/version&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;

2.设计对象
    ...
3.描述对象
    package tutorial;
    option java_package = &quot;com.example.tutorial&quot;;
    option java_outer_classname = &quot;AddressBookProtos&quot;;
    //这个是一个javabean，在protobuf里面叫message
        message Person {
        required string name = 1;
        required int32 id = 2;
        optional string email = 3;
    //下面这个是一个phonetype的枚举类
        enum PhoneType {
            MOBILE = 0;
            HOME = 1;
            WORK = 2;
        }
    //这个也是一个javabean。
        message PhoneNumber {
            required string number = 1;
            optional PhoneType type = 2 [default = HOME];
        }
        repeated PhoneNumber phone = 4;
    }
    message AddressBook {
        repeated Person person = 1;
    }

4.编译描述
    cmd&gt;protoc --java_out . xxx.proto

5.导入源代码到项目中
    ...

6.使用对象
    public class TestProtoBuf {

        @Test
        public void write() throws Exception{
            AddressBookProtos.Person john = AddressBookProtos.Person.newBuilder()
                    .setId(12345)
                    .setName(&quot;tomas&quot;)
                    .setEmail(&quot;123@123.123&quot;)
                    .addPhone(AddressBookProtos.Person.PhoneNumber.newBuilder()
                            .setNumber(&quot;+351 999 999 999&quot;)
                            .setType(AddressBookProtos.Person.PhoneType.HOME)
                            .build())
                    .build();
            john.writeTo(new FileOutputStream(&quot;d:/prototbuf.data&quot;));
        }

        @Test
        public void read() throws Exception{
            AddressBookProtos.Person john = AddressBookProtos.Person.parseFrom(new FileInputStream(&quot;d:/prototbuf.data&quot;));
            System.out.println(john.getName());
        }
    }
</code></pre><p>上一段是用Protobuf进行编译和反编译的过程。然后下面这段图片是用java进行编译和反编译的过程：</p>
<p><img src="https://i.imgur.com/8rzRjOu.png" alt=""></p>
<h2 id="xml"><a href="#xml" class="headerlink" title="xml"></a>xml</h2><pre><code>&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;persons&gt;
    &lt;person id=&quot;&quot; name=&quot;&quot;&gt;
        &lt;age&gt;12&lt;/age&gt;
    &lt;/person&gt;
&lt;/person&gt;
</code></pre><h2 id="json"><a href="#json" class="headerlink" title="json"></a>json</h2><pre><code>[{
    &quot;id&quot; : 1,
    &quot;nmae&quot; : &quot;tom&quot;,
    &quot;age&quot; : 20
},
{
    &quot;id&quot; : 2,
    &quot;nmae&quot; : &quot;tomas&quot;,
    &quot;age&quot; : 30
}
]
</code></pre><h2 id="avro-doug-cutting"><a href="#avro-doug-cutting" class="headerlink" title="avro (doug cutting)"></a>avro (doug cutting)</h2><pre><code>1.数据串行化系统
2.自描述语言.
    数据结构和数据都存在文件中。跨语言。
    使用json格式存储数据。
3.可压缩 + 可切割。
4.使用avro
    a)定义schema
    b)编译schema，生成java类
        {    //名字空间是这个，然后类型，然后名字，然后是字段数组
            &quot;namespace&quot;: &quot;tutorialspoint.com&quot;,
            &quot;type&quot;: &quot;record&quot;,
            &quot;name&quot;: &quot;emp&quot;,
            &quot;fields&quot;: [
                {&quot;name&quot;: &quot;name&quot;, &quot;type&quot;: &quot;string&quot;},
                {&quot;name&quot;: &quot;id&quot;, &quot;type&quot;: &quot;int&quot;},
                {&quot;name&quot;: &quot;salary&quot;, &quot;type&quot;: &quot;int&quot;},
                {&quot;name&quot;: &quot;age&quot;, &quot;type&quot;: &quot;int&quot;},
                {&quot;name&quot;: &quot;address&quot;, &quot;type&quot;: &quot;string&quot;}
            ]
        } 
    c)使用java类
        cmd&gt;java -jar avro-tools-1.7.7.jar compile schema emp.avsc .

    d)单元测试
        package com.it18zhang.avrodemo.test;

        import org.apache.avro.Schema;
        import org.apache.avro.file.DataFileReader;
        import org.apache.avro.file.DataFileWriter;
        import org.apache.avro.generic.GenericData;
        import org.apache.avro.generic.GenericRecord;
        import org.apache.avro.io.DatumWriter;
        import org.apache.avro.specific.SpecificDatumReader;
        import org.apache.avro.specific.SpecificDatumWriter;
        import org.junit.Test;

        import java.io.File;
        import java.io.IOException;
        import java.util.Iterator;

        /**
         * Created by Administrator on 2017/3/23.
         */
        public class TestAvro {

        //    @Test
        //    public void write() throws Exception {
        //        //创建writer对象
        //        SpecificDatumWriter empDatumWriter = new SpecificDatumWriter&lt;Employee&gt;(Employee.class);
        //        //写入文件
        //        DataFileWriter&lt;Employee&gt; empFileWriter = new DataFileWriter&lt;Employee&gt;(empDatumWriter);
        //
        //        //创建对象
        //        Employee e1 = new Employee();
        //        e1.setName(&quot;tomas&quot;);
        //        e1.setAge(12);
        //
        //        //串行化数据到磁盘
        //        empFileWriter.create(e1.getSchema(), new File(&quot;d:/avro/data/e1.avro&quot;));
        //        empFileWriter.append(e1);
        //        empFileWriter.append(e1);
        //        empFileWriter.append(e1);
        //        empFileWriter.append(e1);
        //        //关闭流
        //        empFileWriter.close();
        //    }
        //
        //    @Test
        //    public void read() throws Exception {
        //        //创建writer对象
        //        SpecificDatumReader empDatumReader = new SpecificDatumReader&lt;Employee&gt;(Employee.class);
        //        //写入文件
        //        DataFileReader&lt;Employee&gt; dataReader = new DataFileReader&lt;Employee&gt;(new File(&quot;d:/avro/data/e1.avro&quot;)  ,empDatumReader);
        //        Iterator&lt;Employee&gt; it = dataReader.iterator();
        //        while(it.hasNext()){
        //            System.out.println(it.next().getName());
        //        }
        //    }

            /**
             * 直接使用schema文件进行读写，不需要编译
             */
            @Test
            public void writeInSchema() throws  Exception {
                //指定定义的avsc文件。
                Schema schema = new Schema.Parser().parse(new File(&quot;d:/avro/emp.avsc&quot;));

                //创建GenericRecord,相当于Employee
                GenericRecord e1 = new GenericData.Record(schema);
                //设置javabean属性
                e1.put(&quot;Name&quot;, &quot;ramu&quot;);
        //        e1.put(&quot;id&quot;, 001);
        //        e1.put(&quot;salary&quot;, 30000);
                e1.put(&quot;age&quot;, 25);
        //        e1.put(&quot;address&quot;, &quot;chennai&quot;);

                //
                DatumWriter&lt;GenericRecord&gt; empDatumWriter = new SpecificDatumWriter&lt;GenericRecord&gt;(GenericRecord.class);
                DataFileWriter&lt;GenericRecord&gt; empFileWriter = new DataFileWriter&lt;GenericRecord&gt;(empDatumWriter);
                empFileWriter.create(schema,new File(&quot;d:/avro/data/e2.avro&quot;)) ;
                empFileWriter.append(e1);
                empFileWriter.append(e1);
                empFileWriter.append(e1);
                empFileWriter.append(e1);
                empFileWriter.close();

            }

        }
</code></pre><p>看一下AVSC这个编译好的avro文件里面的是什么结构：</p>
<p>他其实是一个json格式的结构：</p>
<p><img src="https://i.imgur.com/dKcBw8b.png" alt=""></p>
<pre><code>非编译模式
---------------
    @Test
    public void writeInSchema() throws  Exception {
        //指定定义的avsc文件。
        Schema schema = new Schema.Parser().parse(new File(&quot;d:/avro/emp.avsc&quot;));

        //创建GenericRecord,相当于Employee
        GenericRecord e1 = new GenericData.Record(schema);
        //设置javabean属性
        e1.put(&quot;name&quot;, &quot;ramu&quot;);
//        e1.put(&quot;id&quot;, 001);
//        e1.put(&quot;salary&quot;, 30000);
        e1.put(&quot;age&quot;, 25);
//        e1.put(&quot;address&quot;, &quot;chennai&quot;);

        //
        DatumWriter w1 = new SpecificDatumWriter (schema);
        DataFileWriter w2 = new DataFileWriter(w1);
        w2.create(schema,new File(&quot;d:/avro/data/e2.avro&quot;)) ;
        w2.append(e1);
        w2.append(e1);
        w2.close();
    }

    /**
     * 反串行avro数据
     */
    @Test
    public void readInSchema() throws  Exception {
        //指定定义的avsc文件。
        Schema schema = new Schema.Parser().parse(new File(&quot;d:/avro/emp.avsc&quot;));

        GenericRecord e1 = new GenericData.Record(schema);
        DatumReader r1 = new SpecificDatumReader (schema);
        DataFileReader r2 = new DataFileReader(new File(&quot;d:/avro/data/e2.avro&quot;),r1);
        while(r2.hasNext()){
            GenericRecord rec = (GenericRecord)r2.next();
            System.out.println(rec.get(&quot;name&quot;));
        }
        r2.close();
    }
</code></pre>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/11/11/笔记重点总结/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/11/笔记重点总结/" itemprop="url">笔记重点总结</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-11-11T19:51:41+08:00">
                2018-11-11
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/笔记总结/" itemprop="url" rel="index">
                    <span itemprop="name">笔记总结</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>在Hadoop中重点是，全排序，二次排序。<br>在视频的最后有一天是讲解一个二次排序的实例，我没有细看。</p>
<p>在Hive中我认为重点的是关于查询的内容，因为在hive中一般都是加载数据然后查询，查询之后有一个自定义函数UDF的内容，这部分的内容看的不是很清晰。</p>
<p>在avro和Protoc里面主要讲解2中反编译的方法，主要是一个面向对象开发的一个过程，还是java编程不熟悉，然后代码跟着走了一遍但是肯定还是有问题，视频里面的老师是在一遍看外文的书籍，根据里面的代码模板来写的。在自己敲的过程中protoc的代码引入maven依赖包之后，还是没有能够将编译好的文件识别出来，不知道什么问题，但是avo就引入maven之后解决了这个问题。</p>
<p>在讲解zookeeper的时候我觉得在选举算法的时候他没有细致的讲解，<br>然后这边主要是zookeeper部署在hadoop集群和使用API控制zookeeper。在集群部署高可用的时候是201202203部署zk202203204部署journalnode节点，（journalnode节点已经快忘记了）。</p>
<p>容灾管理器进程ZKFC在配置的时候后来跟着yarn启动了起来。下面图是整个的进程图：</p>
<p><img src="https://i.imgur.com/osWW4ZM.png" alt=""></p>
<p>这个zookeeper的地方看的比较快，对于每个目录做什么的比较忘记，然后在配置的时候没有细究配置的意义。就是跟着走了一遍。但是最后配置了起来了。配置hdfs高可用的时候比较费时费力，配置内容比较多，配置yarn的时候比较简单，配置一下即可。</p>
<hr>
<p>在hbase安装的过程中，由于开始的时候设置了zookeeper的HA自动容灾，所以最开始的时候s206自动设置为active模式，要把s206设置为<br>standby模式才能在s201里面设置s201为HMaster。</p>
<p>也就是说要杀死掉s206里面的namenode或者说通过转换吧他变成standby然后s201设置为active模式。不然会导致出错</p>
<hr>
<p>几个端口2181 8080 50070  8020 16010</p>
<hr>
<p>在讲解hbase 第二天的动态遍历的那个三层For循环并没有看明白，也就是说Java的基础还是很薄弱的。讲解hbase一个三层循环嵌套来scan的没有弄明白。hbase的时候讲解的重点在于协处理器的理解和那个电信的一个calllogs的rowkey的设计。利用了一个二级索引的方式。这个地方的原理很重要。然后最后有一个平时用于生产的工具叫phoenix的工具，可以用类sql的语句写出来，避免了自己设二次索引。里面有大量协处理器的封装，可以直接用sql语句。这边又讲了一个hive和hbase集成的一个问题，这个直接在hive中直接写语句即可。注意看一下phoenix和hive集成的区别。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/11/09/人为什么要努力？——2018-11-09/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/09/人为什么要努力？——2018-11-09/" itemprop="url">心情日记——2018.11.09</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-11-09T22:38:08+08:00">
                2018-11-09
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/心情记/" itemprop="url" rel="index">
                    <span itemprop="name">心情记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>无意之间从知乎里面看见的题目是：人这一辈子为什么要努力？</p>
<p>泪目，与诸君共勉。</p>
<pre><code>奶奶五十岁才来的美国，身无分文，也没有一技之长，随身携带的只有长年辛勤劳作留下的一身病根。

那是九十年代初，她去了唐人街的扎花厂上班，每天工作十个小时以上，一年工作三百六十五天。

扎花按件算钱，她眼神虽然不好，却比谁扎得都快。

有人弹吉他磨出了茧，有人搬砖头磨出了茧，她被针头扎出了茧。

回国看我的时候，她给我带了费列罗巧克力，翘胡子薯片，Jif花生酱。

她给我买好看的小西装，给我买一斤几十元的黄螺。

她带我去动物园，游乐园，森林公园，带我去北京看长城，看天安门，看毛主席。

“奶奶，美国比北京还好吗？”

她用最朴实的语言向我描述纽约的繁华，告诉我美国好极了，一切都在等着我。

知乎上经常讨论富养女孩，我有一个男孩被富养的故事。

有一年，我的巧克力吃完了，薯片吃完了，花生酱吃完了，奶奶还是没有回来。

那是我第一个不愿意知道的生活真相，她中风了，瘫了半边身子。



奶奶移民美国时带去了三个未成年的儿子，二叔，三叔，四叔。

二叔和三叔在登陆美国的第二天就打工去了，年幼的四叔读了几年书后也离开了学校。

他们在亲戚家的餐馆打工，每天工作十二个小时，每周工作六天。

餐馆一年营业三百六十四天，只在感恩节那天歇业。

奶奶瘫痪后，叔叔们轮流回国，给我带好吃的，带我出去玩。

我喜欢打乒乓球，二叔给我买了乒乓球，乒乓球拍，乒乓球桌。

有一年流行四驱车，三叔给我买了一辆遥控越野车，助我碾压所有的小伙伴。

四叔年少风流，出门把妹的时候总不忘带上我，要把我培养成下一代情圣。

他们绝口不提在美国生存的艰辛，那是大人们的秘密，和我无关。



奶奶出国五年后，爸妈也去了美国。

怎一个落魄了得？夫妻俩连属于自己的房间都没有。

扎花已经被时代淘汰，只有重活可以干。我爸当过屠夫，货柜工人，货运司机。

细节不必赘述，无非就是 12小时 x 365天的陈词滥调。

后来，他们四兄弟聚首，开了一家小超市，一家餐馆，后来又开了第二家小超市，第二家餐馆。

钱赚得越来越多，老家伙们拼命工作的老毛病却没有得到丝毫缓解。



爸妈出国五年后，我也来了美国，看清了生活本来的面目。

我在纽约生活了几个月，带着半身不遂的奶奶看遍了世界之都的繁华。

在这之前，她几乎没有走出过唐人街的范围，以前对我描绘纽约时一半是转述，一半靠想象。

后来，我回到了父母的身边，结束了长达五年的骨肉分离。

我爸来车站接我，把我带到了一栋小别墅前，很得意地告诉我：“这是我们家，知道你要来，刚买的。”

我去过奶奶刚来美国时住过的那个阴暗破旧的小公寓楼，也去过爸妈栖身过的那个散发着霉味的地下室，我知道我在新家会有一张床，一个房间，但我没想到，我会有一个别墅，一个前院，一个后院，一整个铺垫好的未来。



人这一生为什么要努力？

奶奶的答案是我，爸妈的答案是我，我的答案是他们，以及把身家性命托付于我的媳妇和孩子。

对于我来说，长大了，责任多了，自然而然地想要努力，不是为了赚很多很多钱，而是为了过上自己想要的生活，过上奶奶和父母不曾拥有过的生活。他们替我吃完了所有的苦，帮我走了九十九步，我自己只需再走一步，哪敢迟疑？

从外曾祖父母算起，我们家族四代八十多口人已经在美国奋斗了半个多世纪。我们人人都在努力，我们一代好过一代。



如果你的人生起点不高，不曾有人为你走过人生百步中的任何一步，不打紧，你尽管努力，多出来的步数不会被浪费掉，总有你在乎的人用得着，而你迟早会遇到你在乎的人。

与诸君共勉。

顾宇的知乎回答索引
</code></pre>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/11/08/Hive第二天/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/08/Hive第二天/" itemprop="url">Hive第二天</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-11-08T16:17:04+08:00">
                2018-11-08
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hive/" itemprop="url" rel="index">
                    <span itemprop="name">Hive</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="hive"><a href="#hive" class="headerlink" title="hive"></a>hive</h2><pre><code>数据仓库,在线分析处理。
HiveQL,类似sql语言。
表,metadata-&gt;rdbms.
hive处理的数据是hdfs.
MR,聚合操作。
</code></pre><h1 id="hive的2个重点一个是分区一个是桶表"><a href="#hive的2个重点一个是分区一个是桶表" class="headerlink" title="hive的2个重点一个是分区一个是桶表"></a>hive的2个重点一个是分区一个是桶表</h1><h1 id="内部表-管理表-托管表"><a href="#内部表-管理表-托管表" class="headerlink" title="内部表,管理表,托管表"></a>内部表,管理表,托管表</h1><pre><code>hive,drop ,数据也删除
</code></pre><h1 id="外部表"><a href="#外部表" class="headerlink" title="外部表"></a>外部表</h1><pre><code>hive表结构。
</code></pre><h1 id="分区表"><a href="#分区表" class="headerlink" title="分区表"></a>分区表</h1><pre><code>目录.
where 缩小查询范围。
</code></pre><h1 id="bucket表"><a href="#bucket表" class="headerlink" title="bucket表"></a>bucket表</h1><pre><code>文件。
hash
clustered by &apos;&apos;
</code></pre><h1 id="join"><a href="#join" class="headerlink" title="join"></a>join</h1><pre><code>横向连接，也就是作外连接和右外连接
</code></pre><h1 id="union"><a href="#union" class="headerlink" title="union"></a>union</h1><pre><code>竖向连接。只能指定相匹配的字段

select id ,name from a1 union select id ,cid from a2;
</code></pre><h1 id="hive-union操作"><a href="#hive-union操作" class="headerlink" title="hive union操作"></a>hive union操作</h1><pre><code>select id,name from customers union select id,orderno from orders ;
$&gt;hive                            //hive --service cli 
$&gt;hive --servic hiveserver2        //启动hiveserver2，10000 [thriftServer]
$&gt;hive --service beeline        //beeline    
</code></pre><h1 id="hive使用jdbc协议实现远程访问"><a href="#hive使用jdbc协议实现远程访问" class="headerlink" title="hive使用jdbc协议实现远程访问"></a>hive使用jdbc协议实现远程访问</h1><h1 id="hive-1"><a href="#hive-1" class="headerlink" title="hive"></a>hive</h1><pre><code>$hive&gt;CREATE TABLE t3(id int,name string,age int) PARTITIONED BY (Year INT, Month INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;
</code></pre><h1 id="export"><a href="#export" class="headerlink" title="export"></a>export</h1><pre><code>$hive&gt;EXPORT TABLE customers TO &apos;/user/centos/tmp.txt&apos;;        //导出表结构+数据到hdfs目录。
看一下导出来的东西，是一个目录，包括表结构和表内容。
</code></pre><p><img src="https://i.imgur.com/pFsQ5fZ.png" alt=""></p>
<h1 id="order全排序"><a href="#order全排序" class="headerlink" title="/order全排序"></a>/order全排序</h1><pre><code>$hive&gt;select * from orders order by id asc ;
</code></pre><h1 id="sort-map端排序-本地有序。"><a href="#sort-map端排序-本地有序。" class="headerlink" title="sort,map端排序,本地有序。"></a>sort,map端排序,本地有序。</h1><pre><code>$hive&gt;select * from orders sort by id asc ;
</code></pre><h1 id="distribute-by-amp-cluster-by-amp-sort-by"><a href="#distribute-by-amp-cluster-by-amp-sort-by" class="headerlink" title="distribute by &amp; cluster by  &amp; sort by"></a>distribute by &amp; cluster by  &amp; sort by</h1><p>类似于mysql的group by,进行分区操作。</p>
<pre><code>//select cid , ... from orders distribute by cid sort by name ;            //注意顺序.
$hive&gt;select id,orderno,cid from orders distribute by cid sort by cid desc ;

//cluster by ===&gt;  distribute by cid sort by cid
</code></pre><p><strong>destribut by 和cluster by图解对比：</strong></p>
<p><img src="https://i.imgur.com/BQ7NHQU.png" alt=""></p>
<p>这个地方没有细讲，说主要应用还是group by 只要记住这个cluster是dirstribute by 和sort by 的组合极客</p>
<h2 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h2><pre><code>mysql&gt;select concat(&apos;tom&apos;,1000) ;
$hive&gt;select current_database(),current_user() ;
$hive&gt;tab                                //查看帮助
</code></pre><h2 id="设置作业参数"><a href="#设置作业参数" class="headerlink" title="设置作业参数"></a>设置作业参数</h2><pre><code>$hive&gt;set hive.exec.reducers.bytes.per.reducer=xxx            //设置reducetask的字节数。
$hive&gt;set hive.exec.reducers.max=0                            //设置reduce task的最大任务数
$hive&gt;set mapreduce.job.reduces=0                            //设置reducetask个数。
</code></pre><h2 id="动态分区-严格模式-非严格模式"><a href="#动态分区-严格模式-非严格模式" class="headerlink" title="动态分区-严格模式-非严格模式"></a>动态分区-严格模式-非严格模式</h2><pre><code>动态分区模式:strict-严格模式，插入时至少指定一个静态分区，nonstrict-非严格模式-可以不指定静态分区。
set hive.exec.dynamic.partition.mode=nonstrict            //设置非严格模式
$hive&gt;INSERT [OVERWRITE] into TABLE employees PARTITION (country, state) SELECT ..., se.cnty, se.st FROM staged_employees se WHERE se.cnty = &apos;US&apos;;
</code></pre><p><img src="https://i.imgur.com/8zgzuuO.png" alt=""></p>
<p>本来有一个已经分区好了的表，然后再建立一个表，比如也是相同的字段，然后新表没有分区要插进去老表，让他自动分区，就要用到动态分区，动态分区分为严格模式和非严格模式。如下图先使用了严格模式的情况，出错，说要至少指定一个分区也就是</p>
<pre><code>$hive&gt;INSERT [OVERWRITE] into TABLE employees PARTITION (country=&apos;US&apos;, state) SELECT ..., se.cnty, se.st FROM staged_employees;
</code></pre><p>但是严格模式还是要制定这个分区，所以用一下非严格模式先设置一下：</p>
<pre><code>set hive.exec.dynamic.partition.mode=nonstrict            //设置非严格模式
</code></pre><p>然后我们开始插入：</p>
<pre><code>$hive&gt;INSERT [OVERWRITE] into TABLE employees PARTITION (country, state) SELECT ..., se.cnty, se.st FROM staged_employees;
</code></pre><p>这样就实现了非严格模式下的动态分区</p>
<h1 id="hive事务处理在-gt-0-13-0之后支持行级事务。-（这个地方老师没讲清楚自己没搞出来）"><a href="#hive事务处理在-gt-0-13-0之后支持行级事务。-（这个地方老师没讲清楚自己没搞出来）" class="headerlink" title="hive事务处理在&gt;0.13.0之后支持行级事务。 #（这个地方老师没讲清楚自己没搞出来）"></a>hive事务处理在&gt;0.13.0之后支持行级事务。 #（这个地方老师没讲清楚自己没搞出来）</h1><pre><code>1.所有事务自动提交。
2.只支持orc格式。
3.使用bucket表。
4.配置hive参数，使其支持事务。
</code></pre><p>$hive&gt;SET hive.support.concurrency = true;<br>$hive&gt;SET hive.enforce.bucketing = true;<br>$hive&gt;SET hive.exec.dynamic.partition.mode = nonstrict;<br>$hive&gt;SET hive.txn.manager = org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;<br>$hive&gt;SET hive.compactor.initiator.on = true;<br>$hive&gt;SET hive.compactor.worker.threads = 1;</p>
<pre><code>5.使用事务性操作
    $&gt;CREATE TABLE tx(id int,name string,age int) CLUSTERED BY (id) INTO 3 BUCKETS ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; stored as orc TBLPROPERTIES (&apos;transactional&apos;=&apos;true&apos;);
</code></pre><p><img src="https://i.imgur.com/UWJd7M5.png" alt=""></p>
<hr>
<p>mysql支持这样的操作，左边选择的查询的列，并不出现在右边的分组当中这样可以查询，但是hive中不允许这样。</p>
<p><img src="https://i.imgur.com/vpfbmjD.png" alt=""></p>
<p>但是在hive当中只可以如下图查询：</p>
<p><img src="https://i.imgur.com/oOqn35d.png" alt=""></p>
<h2 id="聚合处理"><a href="#聚合处理" class="headerlink" title="聚合处理"></a>聚合处理</h2><pre><code>每个用户购买商品订单大于1的：这个按照cid分组的，查询每个分组里面的count(*)，也就是每个不同的cid 的

$hive&gt;select cid,count(*) c ,max(price) from orders group by cid having c &gt; 1 ; 
</code></pre><h1 id="wordcount"><a href="#wordcount" class="headerlink" title="wordcount"></a>wordcount</h1><p>创建新表:stats(word string,c int) ;<br>    将查询结果插入到指定表中。</p>
<p><img src="https://i.imgur.com/wxA99my.png" alt=""></p>
<p>按照下图这个切割方法，用一个split的函数切割，切割之后形成一个数组类型的东西</p>
<p><img src="https://i.imgur.com/NVQuyss.png" alt=""></p>
<p>explode这个函数传入的需要是一个经过split切割之后的数组，传入之后再去展开得到如上图所示的东西。但是切出来还是需要聚合，查询每个单词的个数</p>
<p>对下面这个SQL语句进行讲解。先看括号里面的最里面的括号，select explode(split(line,’ ‘)) as word from doc这句话都能理解，就是先切割成数组，然后对这个数据进行explode炸裂成单词，然后as word就是别名叫word，然后再把整个查询的as t别名t，然后select t.word,count(*) c from …group by t.word order by c desc limit2;就是对t的word字段进行分组，对分组的t的字段word和计数总和进行查询，然后按照降序排序，然后limit 2取最高的2个数。</p>
<pre><code>$hive&gt;select t.word,count(*) c from ((select explode(split(line, &apos; &apos;)) as word from doc) as t) group by t.word order by c desc limit 2 ;
</code></pre><h1 id="view-视图-虚表"><a href="#view-视图-虚表" class="headerlink" title="view:视图,虚表"></a>view:视图,虚表</h1><p>是一个虚拟的表，类似于对这个语句的封装，或者说起了一个别名，并不是真的表。</p>
<p>这个地方创建视图的时候不能</p>
<pre><code>$hive&gt;create view v1 as select a.*,b.*from customers a left outer join default.tt b on a.id = b.cid ;
</code></pre><p>上图这样是错误的，因为在a和b里面有相同的字段，要把相同的字段修改为不同的字段，如下图：</p>
<pre><code>//创建视图
$hive&gt;create view v1 as select a.id aid,a.name ,b.id bid , b.order from customers a left outer join default.tt b on a.id = b.cid ;
</code></pre><p>可以在续表的基础上在查询，如下所示：</p>
<pre><code>//查看视图
$hive&gt;show tables ;
$hive&gt;select * from v1 ;
</code></pre><p>view这个在hdfs上没有哦，而是存放在了mysql里面</p>
<h2 id="一个mysql里面的操作：-G-行转列"><a href="#一个mysql里面的操作：-G-行转列" class="headerlink" title="一个mysql里面的操作：\G 行转列"></a>一个mysql里面的操作：\G 行转列</h2><p>select * from tbls \G：结果如下：</p>
<p><img src="https://i.imgur.com/AbE1mvx.png" alt=""></p>
<h2 id="Map端连接"><a href="#Map端连接" class="headerlink" title="Map端连接"></a>Map端连接</h2><pre><code>$hive&gt;set hive.auto.convert.join=true            //设置自动转换连接,默认开启了。

//使用mapjoin连接暗示实现mapjoin
$hive&gt;select /*+ mapjoin(customers) */ a.*,b.* from customers a left outer join orders b on a.id = b.cid ;
</code></pre><h2 id="调优"><a href="#调优" class="headerlink" title="调优"></a>调优</h2><pre><code>1.explain
    使用explain查看查询计划
    hive&gt;explain [extended] select count(*) from customers ;
    hive&gt;explain select t.name , count(*) from (select a.name ,b.id,b.orderno from customers a ,orders b where a.id = b.cid) t group by t.name ;

    //设置limit优化测，避免全部查询.但是树上说因为是随机取样，可能会出问题。
    hive&gt;set hive.limit.optimize.enable=true

    //本地模式
    $hive&gt;set mapred.job.tracker=local;            //
    $hive&gt;set hive.exec.mode.local.auto=true    //自动本地模式,主要用于测试不用于实战


    //并行执行,同时执行不存在依赖关系的阶段。??
    $hive&gt;set hive.exec.parallel=true            //是自动设置好的

    //严格模式,
    $hive&gt;set hive.mapred.mode=strict            //手动设置之后。1.分区表必须指定分区进行查询，否则不让查询。
                                                //2.order by时必须使用limit子句。
                                                //3.不允许笛卡尔积.


    //设置MR的数量
    hive&gt; set hive.exec.reducers.bytes.per.reducer=750000000;    //设置reduce处理的字节数。

    //JVM重用
    $hive&gt;set mapreduce.job.jvm.numtasks=1        //-1没有限制，使用大量小文件。


    //UDF
    //User define function,用户自定义函数
    //current_database(),current_user();

    //显式所有函数
    $hive&gt;show functions;
    $hive&gt;select array(1,2,3) ;

    //显式指定函数帮助
    $hive&gt;desc function current_database();

    //表生成函数,多行函数。
    $hive&gt;explode(str,exp);            //按照exp切割str.
</code></pre><h2 id="自定义函数"><a href="#自定义函数" class="headerlink" title="自定义函数"></a>自定义函数</h2><p><img src="https://i.imgur.com/lvT1nkj.png" alt=""></p>
<p>上图带括号的都是函数，不带括号的是命令</p>
<pre><code>1.创建类，继承UDF
    package com.it18zhang.hivedemo.udf;

    import org.apache.hadoop.hive.ql.exec.Description;
    import org.apache.hadoop.hive.ql.exec.UDF;

    /**
     * 自定义hive函数
     */
    @Description(name = &quot;myadd&quot;,
            value = &quot;myadd(int a , int b) ==&gt; return a + b &quot;,
            extended = &quot;Example:\n&quot;
                    + &quot; myadd(1,1) ==&gt; 2 \n&quot;
                    + &quot; myadd(1,2,3) ==&gt; 6;&quot;)
    public class AddUDF extends UDF {

        public int evaluate(int a ,int b) {
            return a + b ;
        }

        public int evaluate(int a ,int b , int c) {
            return a + b + c;
        }
    }
2.打成jar包。
    cmd&gt;cd {classes所在目录}
    cmd&gt;jar cvf HiveDemo.jar -C x/x/x/x/classes/ .
3.添加jar包到hive的类路径
    //添加jar到类路径
    $&gt;cp /mnt/hgfs/downloads/bigdata/data/HiveDemo.jar /soft/hive/lib

3.重进入hive
    $&gt;....

4.创建临时函数
    //
    CREATE TEMPORARY FUNCTION myadd AS &apos;com.it18zhang.hivedemo.udf.AddUDF&apos;;

5.在查询中使用自定义函数
    $hive&gt;select myadd(1,2)  ;

6.定义日期函数
    1)定义类
    public class ToCharUDF extends UDF {
        /**
         * 取出服务器的当前系统时间 2017/3/21 16:53:55
         */
        public String evaluate() {
            Date date = new Date();
            SimpleDateFormat sdf = new SimpleDateFormat();
            sdf.applyPattern(&quot;yyyy/MM/dd hh:mm:ss&quot;);
            return sdf.format(date) ;
        }
        public String evaluate(Date date) {
            SimpleDateFormat sdf = new SimpleDateFormat();
            sdf.applyPattern(&quot;yyyy/MM/dd hh:mm:ss&quot;);
            return sdf.format(date) ;
        }

        public String evaluate(Date date,String frt) {
            SimpleDateFormat sdf = new SimpleDateFormat();
            sdf.applyPattern(frt);
            return sdf.format(date) ;
        }
    }

    2)导出jar包，通过命令添加到hive的类路径(不需要重进hive)。
        $hive&gt;add jar /mnt/hgfs/downloads/bigdata/data/HiveDemo-1.0-SNAPSHOT.jar

    3)注册函数
        $hive&gt;CREATE TEMPORARY FUNCTION to_char AS &apos;com.it18zhang.hivedemo.udf.ToCharUDF&apos;;
        $hive&gt;CREATE TEMPORARY FUNCTION to_date AS &apos;com.it18zhang.hivedemo.udf.ToCharUDF&apos;;
</code></pre><h2 id="定义Nvl函数-这个是在英文版的Hadoop权威指南上给的"><a href="#定义Nvl函数-这个是在英文版的Hadoop权威指南上给的" class="headerlink" title="定义Nvl函数(这个是在英文版的Hadoop权威指南上给的)"></a>定义Nvl函数(这个是在英文版的Hadoop权威指南上给的)</h2><pre><code>package com.it18zhang.hivedemo.udf;

import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;
import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;
import org.apache.hadoop.hive.ql.metadata.HiveException;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;

/**
 * 自定义null值处理函数
 */
public class Nvl extends GenericUDF {
    private GenericUDFUtils.ReturnObjectInspectorResolver returnOIResolver;
    private ObjectInspector[] argumentOIs;

    public ObjectInspector initialize(ObjectInspector[] arguments)
            throws UDFArgumentException {
        argumentOIs = arguments;
        //检查参数个数
        if (arguments.length != 2) {
            throw new UDFArgumentLengthException(
                    &quot;The operator &apos;NVL&apos; accepts 2 arguments.&quot;);
        }
        returnOIResolver = new GenericUDFUtils.ReturnObjectInspectorResolver(true);
        //检查参数类型
        if (!(returnOIResolver.update(arguments[0]) &amp;&amp; returnOIResolver
                .update(arguments[1]))) {
            throw new UDFArgumentTypeException(2,
                    &quot;The 1st and 2nd args of function NLV should have the same type, &quot;
                            + &quot;but they are different: \&quot;&quot; + arguments[0].getTypeName()
                            + &quot;\&quot; and \&quot;&quot; + arguments[1].getTypeName() + &quot;\&quot;&quot;);
        }
        return returnOIResolver.get();
    }

    public Object evaluate(DeferredObject[] arguments) throws HiveException {
        Object retVal = returnOIResolver.convertIfNecessary(arguments[0].get(), argumentOIs[0]);
        if (retVal == null) {
            retVal = returnOIResolver.convertIfNecessary(arguments[1].get(),
                    argumentOIs[1]);
        }
        return retVal;
    }

    public String getDisplayString(String[] children) {
        StringBuilder sb = new StringBuilder();
        sb.append(&quot;if &quot;);
        sb.append(children[0]);
        sb.append(&quot; is null &quot;);
        sb.append(&quot;returns&quot;);
        sb.append(children[1]);
        return sb.toString();
    }
}

2)添加jar到类路径
    ...
3)注册函数
    $hive&gt;CREATE TEMPORARY FUNCTION nvl AS &apos;org.apache.hadoop.hive.ql.udf.generic.GenericUDFNvl&apos;;
</code></pre><h1 id="一些课上用的PPT"><a href="#一些课上用的PPT" class="headerlink" title="一些课上用的PPT"></a>一些课上用的PPT</h1><p><img src="https://i.imgur.com/6OyT9i3.png" alt=""><br><img src="https://i.imgur.com/CLZNHSV.png" alt=""><br><img src="https://i.imgur.com/Rx5Hslt.png" alt=""><br><img src="https://i.imgur.com/fb0PfG0.png" alt=""><br><img src="https://i.imgur.com/P8kwdax.png" alt=""><br><img src="https://i.imgur.com/avoDPKV.png" alt=""><br><img src="https://i.imgur.com/OwHiTIn.png" alt=""><br><img src="https://i.imgur.com/x4qXG7v.png" alt=""><br><img src="https://i.imgur.com/mIQbSR1.png" alt=""><br><img src="https://i.imgur.com/2bMpEMd.png" alt=""><br><img src="https://i.imgur.com/bSF4lMN.png" alt=""><br><img src="https://i.imgur.com/C3IMpOQ.png" alt=""><br><img src="https://i.imgur.com/6xpn8Ul.png" alt=""></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/11/06/Hive第一天/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/06/Hive第一天/" itemprop="url">Hive第一天</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-11-06T16:35:55+08:00">
                2018-11-06
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hive/" itemprop="url" rel="index">
                    <span itemprop="name">Hive</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="hive"><a href="#hive" class="headerlink" title="hive"></a>hive</h2><pre><code>在hadoop处理结构化数据的数据仓库。
不是:    关系数据库
        不是OLTP
        实时查询和行级更新。
</code></pre><h2 id="hive特点"><a href="#hive特点" class="headerlink" title="hive特点"></a>hive特点</h2><pre><code>hive存储数据结构(schema)在数据库中,处理的数据进入hdfs.
OLAP
HQL / HiveQL
</code></pre><h2 id="hive安装"><a href="#hive安装" class="headerlink" title="hive安装"></a>hive安装</h2><pre><code>1.下载hive2.1-tar.gz
2.tar开
    $&gt;tar -xzvf hive-2.1.0.tar.gz -C /soft    //tar开
    $&gt;cd /soft/hive-2.1.0                    //
    $&gt;ln -s hive-2.1.0 hive                    //符号连接

3.配置环境变量
    [/etc/profile]
    HIVE_HOME=/soft/hive
    PATH=...:$HIVE_HOME/bin

4.验证hive安装成功
    $&gt;hive --v

5.配置hive,使用win7的mysql存放hive的元数据.
    a)复制mysql驱动程序到hive的lib目录下。
        ...
    b)配置hive-site.xml
        复制hive-default.xml.template为hive-site.xml
        修改连接信息为mysql链接地址，将${system:...字样替换成具体路径。
        [hive/conf/hive-site.xml]
        &lt;property&gt;
            &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;
            &lt;value&gt;root&lt;/value&gt;
            &lt;description&gt;password to use against metastore database&lt;/description&gt;
        &lt;/property&gt;
        &lt;property&gt;
            &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;
            &lt;value&gt;root&lt;/value&gt;
            &lt;description&gt;Username to use against metastore database&lt;/description&gt;
        &lt;/property&gt;
        &lt;property&gt;
            &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;
            &lt;value&gt;jdbc:mysql://192.168.231.1:3306/hive2&lt;/value&gt;
        &lt;/property&gt;
        &lt;property&gt;
            &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;
            &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;
            &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;
        &lt;/property&gt;

    6)在msyql中创建存放hive信息的数据库
        mysql&gt;create database hive2 ;

    6)初始化hive的元数据(表结构)到mysql中。
        $&gt;cd /soft/hive/bin
        $&gt;schematool -dbType mysql -initSchema
</code></pre><h1 id="hive命令行操作"><a href="#hive命令行操作" class="headerlink" title="hive命令行操作"></a>hive命令行操作</h1><h2 id="1-创建hive的数据库"><a href="#1-创建hive的数据库" class="headerlink" title="1.创建hive的数据库"></a>1.创建hive的数据库</h2><pre><code>$hive&gt;hive --version                //
$hive&gt;hive --help                    //

$hive&gt;create database mydb2 ;                //
$hive&gt;show databases ;
$hive&gt;use mydb2 ;
$hive&gt;create table mydb2.t(id int,name string,age int);
$hive&gt;drop table t ;
$hive&gt;drop table mydb2.t ;
$hive&gt;select * from mydb2.t ;        //查看指定库的表
$hive&gt;exit ;                        //退出

$&gt;hive                                //hive --service cli
$&gt;hive                                //hive --service cli
</code></pre><h2 id="2-通过远程jdbc方式连接到hive数据仓库"><a href="#2-通过远程jdbc方式连接到hive数据仓库" class="headerlink" title="2.通过远程jdbc方式连接到hive数据仓库"></a>2.通过远程jdbc方式连接到hive数据仓库</h2><pre><code>1.启动hiveserver2服务器，监听端口10000
    $&gt;hive --service hiveserver2 &amp;

2.通过beeline命令行连接到hiveserver2
    $&gt;beeline                                            //进入beeline命令行(于hive --service beeline)
    $beeline&gt;!help                                        //查看帮助
    $beeline&gt;!quit                                        //退出
    $beeline&gt;!connect jdbc:hive2://localhost:10000/mydb2//连接到hibve数据
    $beeline&gt;show databases ;
    $beeline&gt;use mydb2 ;
    $beeline&gt;show tables;                                //显式表
</code></pre><h2 id="使用Hive-jdbc驱动程序采用jdbc方式访问远程数据仓库"><a href="#使用Hive-jdbc驱动程序采用jdbc方式访问远程数据仓库" class="headerlink" title="使用Hive-jdbc驱动程序采用jdbc方式访问远程数据仓库"></a>使用Hive-jdbc驱动程序采用jdbc方式访问远程数据仓库</h2><pre><code>1.创建java模块
2.引入maven
3.添加hive-jdbc依赖
    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
    &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;
             xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
             xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
        &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

        &lt;groupId&gt;com.it18zhang&lt;/groupId&gt;
        &lt;artifactId&gt;HiveDemo&lt;/artifactId&gt;
        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;

        &lt;dependencies&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;
                &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt;
                &lt;version&gt;2.1.0&lt;/version&gt;
            &lt;/dependency&gt;
        &lt;/dependencies&gt;
    &lt;/project&gt;

4.App
    package com.it18zhang.hivedemo;

    import java.sql.Connection;
    import java.sql.DriverManager;
    import java.sql.ResultSet;
    import java.sql.Statement;

    /**
     * 使用jdbc方式连接到hive数据仓库，数据仓库需要开启hiveserver2服务。
     */
    public class App {
        public static void main(String[] args) throws  Exception {
            Class.forName(&quot;org.apache.hive.jdbc.HiveDriver&quot;);
            Connection conn = DriverManager.getConnection(&quot;jdbc:hive2://192.168.231.201:10000/mydb2&quot;);
            Statement st = conn.createStatement();
            ResultSet rs = st.executeQuery(&quot;select id , name ,age from t&quot;);
            while(rs.next()){
                System.out.println(rs.getInt(1) + &quot;,&quot; + rs.getString(2)) ;
            }
            rs.close();
            st.close();
            conn.close();
        }
    }
</code></pre><h1 id="hive中表"><a href="#hive中表" class="headerlink" title="hive中表"></a>hive中表</h1><h2 id="1-managed-table"><a href="#1-managed-table" class="headerlink" title="1.managed table"></a>1.managed table</h2><pre><code>托管表。
删除表时，数据也删除了。
</code></pre><h2 id="2-external-table"><a href="#2-external-table" class="headerlink" title="2.external table"></a>2.external table</h2><pre><code>外部表。
删除表时，数据不删。
</code></pre><h1 id="hive命令"><a href="#hive命令" class="headerlink" title="hive命令"></a>hive命令</h1><pre><code>//创建表,external 外部表
$hive&gt;CREATE external TABLE IF NOT EXISTS t2(id int,name string,age int)
COMMENT &apos;xx&apos; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; STORED AS TEXTFILE ; 

//查看表数据
$hive&gt;desc t2 ;
$hive&gt;desc formatted t2 ;

//加载数据到hive表
$hive&gt;load data local inpath &apos;/home/centos/customers.txt&apos; into table t2 ;    //local上传文件
$hive&gt;load data inpath &apos;/user/centos/customers.txt&apos; [overwrite] into table t2 ;    //移动文件

//复制表
mysql&gt;create table tt as select * from users ;        //携带数据和表结构
mysql&gt;create table tt like users ;            //不带数据，只有表结构

hive&gt;create table tt as select * from users ;    
hive&gt;create table tt like users ;    


//count()查询要转成mr
$hive&gt;select count(*) from t2 ;
$hive&gt;select id,name from t2 ;


//
$hive&gt;select * from t2 order by id desc ;                //MR

//启用/禁用表
$hive&gt;ALTER TABLE t2 ENABLE NO_DROP;    //不允许删除
$hive&gt;ALTER TABLE t2 DISABLE NO_DROP;    //允许删除
</code></pre><h1 id="分区表"><a href="#分区表" class="headerlink" title="分区表"></a>分区表</h1><pre><code>优化手段之一，从目录的层面控制搜索数据的范围。
//创建分区表.
$hive&gt;CREATE TABLE t3(id int,name string,age int) PARTITIONED BY (Year INT, Month INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;

//显式表的分区信息
$hive&gt;SHOW PARTITIONS t3;

//添加分区,创建目录
$hive&gt; alter table t3 add partition (year=2014, month=12) partition (year=2014,month=11);

//删除分区
hive&gt;ALTER TABLE employee_partitioned DROP IF EXISTS PARTITION (year=2014, month=11);

//分区结构
hive&gt;/user/hive/warehouse/mydb2.db/t3/year=2014/month=11
hive&gt;/user/hive/warehouse/mydb2.db/t3/year=2014/month=12


//加载数据到分区表
hive&gt;load data local inpath &apos;/home/centos/customers.txt&apos; into table t3 partition(year=2014,month=11);

//查询分区表
hive&gt;select * from t3 where year = 2014 and month =11;
</code></pre><p><img src="https://i.imgur.com/Ad6JAI7.png" alt=""></p>
<p>分区对应的是文件夹是目录，分桶对应的是文件，按照id分桶的意思就是按照id哈希不同的哈希进入不同的桶</p>
<pre><code>//创建桶表
$hive&gt;CREATE TABLE t4(id int,name string,age int) CLUSTERED BY (id) INTO 3 BUCKETS ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;

//加载数据不会进行分桶操作
$hive&gt;load data local inpath &apos;/home/centos/customers.txt&apos; into table t4 ;

//查询t3表数据插入到t4中。t3是分区表还多了年份和月份的2个字段所以要投影查询不能直接insert into t4 select * from t3;这是错的
$hive&gt;insert into t4 select id,name,age from t3 ;
</code></pre><p><img src="https://i.imgur.com/5Rpz8su.png" alt=""></p>
<pre><code>//桶表的数量如何设置?
//评估数据量，保证每个桶的数据量block的2倍大小。


//连接查询
$hive&gt;CREATE TABLE customers(id int,name string,age int) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;
$hive&gt;CREATE TABLE orders(id int,orderno string,price float,cid int) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;

//加载数据到表
//内连接查询
hive&gt;select a.*,b.* from customers a , orders b where a.id = b.cid ;
//左外
hive&gt;select a.*,b.* from customers a left outer join orders b on a.id = b.cid ;
hive&gt;select a.*,b.* from customers a right outer join orders b on a.id = b.cid ;
hive&gt;select a.*,b.* from customers a full outer join orders b on a.id = b.cid ;

//explode,炸裂,表生成函数。
//使用hive实现单词统计
//1.建表
$hive&gt;CREATE TABLE doc(line string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;
</code></pre>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/11/01/Hadoop第十一天/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/01/Hadoop第十一天/" itemprop="url">Hadoop第十一天</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-11-01T18:11:38+08:00">
                2018-11-01
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hadoop/" itemprop="url" rel="index">
                    <span itemprop="name">Hadoop</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h2><pre><code>[customers.txt]
1,tom,12
2,tom,13
3,tom,14
4,tom,15

[orders.txt]
1,no001,12.23,1
2,no001,12.23,1
3,no001,12.23,2
4,no001,12.23,2
5,no001,12.23,2
6,no001,12.23,3
7,no001,12.23,3
8,no001,12.23,3
9,no001,12.23,3
</code></pre><h2 id="map端join"><a href="#map端join" class="headerlink" title="map端join"></a>map端join</h2><pre><code>1.创建Mapper
    package com.it18zhang.hdfs.mr.mapjoin;

    import org.apache.hadoop.conf.Configuration;
    import org.apache.hadoop.fs.FSDataInputStream;
    import org.apache.hadoop.fs.FileSystem;
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.io.LongWritable;
    import org.apache.hadoop.io.NullWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Mapper;

    import java.io.BufferedReader;
    import java.io.IOException;
    import java.io.InputStreamReader;
    import java.util.HashMap;
    import java.util.Map;

    /**
     * join操作，map端连接。
     */
    public class MapJoinMapper extends Mapper&lt;LongWritable,Text,Text,NullWritable&gt; {

        private Map&lt;String,String&gt; allCustomers = new HashMap&lt;String,String&gt;();

        //启动,初始化客户信息
        protected void setup(Context context) throws IOException, InterruptedException {
            try {
                Configuration conf = context.getConfiguration();
                FileSystem fs = FileSystem.get(conf);
                FSDataInputStream fis = fs.open(new Path(&quot;file:///d:/mr/mapjoin/customers.txt&quot;));
                //得到缓冲区阅读器
                BufferedReader br = new BufferedReader(new InputStreamReader(fis));
                String line = null ;
                while((line = br.readLine()) != null){
                    //得到cid
                    String cid = line.substring(0,line.indexOf(&quot;,&quot;));
                    allCustomers.put(cid,line);
                }
            } catch (Exception e) {
                e.printStackTrace();
            }
        }

        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            //订单信息
            String line = value.toString();
            //提取customer id
            String cid = line.substring(line.lastIndexOf(&quot;,&quot;) + 1);
            //订单信息
            String orderInfo = line.substring(0,line.lastIndexOf(&quot;,&quot;));

            //连接customer + &quot;,&quot; + order
            String customerInfo = allCustomers.get(cid);
            context.write(new Text(customerInfo + &quot;,&quot; + orderInfo),NullWritable.get());
        }

    }

2.创建App
    package com.it18zhang.hdfs.mr.mapjoin;

    import org.apache.hadoop.conf.Configuration;
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.io.NullWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Job;
    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

    /**
     *
     */
    public class MapJoinApp {
        public static void main(String[] args) throws Exception {

            Configuration conf = new Configuration();
            conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;);
            Job job = Job.getInstance(conf);

            //设置job的各种属性
            job.setJobName(&quot;MapJoinApp&quot;);                        //作业名称
            job.setJarByClass(MapJoinApp.class);                 //搜索类

            //添加输入路径
            FileInputFormat.addInputPath(job,new Path(args[0]));
            //设置输出路径
            FileOutputFormat.setOutputPath(job,new Path(args[1]));

            //没有reduce

            job.setNumReduceTasks(0);

            job.setMapperClass(MapJoinMapper.class);             //mapper类

            job.setMapOutputKeyClass(Text.class);           //
            job.setMapOutputValueClass(NullWritable.class);  //

            job.waitForCompletion(true);
        }
    }
</code></pre><h2 id="join端连接"><a href="#join端连接" class="headerlink" title="join端连接"></a>join端连接</h2><pre><code>1.自定义key
    package com.it18zhang.hdfs.mr.mapjoin.reducejoin;

    import org.apache.hadoop.io.WritableComparable;

    import java.io.DataInput;
    import java.io.DataOutput;
    import java.io.IOException;

    /**
     */
    public class ComboKey2 implements WritableComparable&lt;ComboKey2&gt; {
        //0-customer 1-order
        private int type ;
        private int cid ;
        private int oid ;
        private String customerInfo = &quot;&quot; ;
        private String orderInfo = &quot;&quot; ;


        public int compareTo(ComboKey2 o) {
            int type0 = o.type ;
            int cid0= o.cid;
            int oid0 = o.oid;
            String customerInfo0 = o.customerInfo;
            String orderInfo0 = o.orderInfo ;
            //是否同一个customer的数据
            if(cid == cid0){
                //同一个客户的两个订单
                if(type == type0){
                    return oid - oid0 ;
                }
                //一个Customer + 他的order
                else{
                    if(type ==0)
                        return -1 ;
                    else
                        return 1 ;
                }
            }
            //cid不同
            else{
                return cid - cid0 ;
            }
        }

        public void write(DataOutput out) throws IOException {
            out.writeInt(type);
            out.writeInt(cid);
            out.writeInt(oid);
            out.writeUTF(customerInfo);
            out.writeUTF(orderInfo);
        }

        public void readFields(DataInput in) throws IOException {
            this.type = in.readInt();
            this.cid = in.readInt();
            this.oid = in.readInt();
            this.customerInfo = in.readUTF();
            this.orderInfo = in.readUTF();
        }
    }

2.自定义分区类
    public class CIDPartitioner extends Partitioner&lt;ComboKey2,NullWritable&gt;{

        public int getPartition(ComboKey2 key, NullWritable nullWritable, int numPartitions) {
            return key.getCid() % numPartitions;
        }
    }
3.创建Mapper
    package com.it18zhang.hdfs.mr.mapjoin.reducejoin;

    import org.apache.hadoop.io.LongWritable;
    import org.apache.hadoop.io.NullWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.InputSplit;
    import org.apache.hadoop.mapreduce.Mapper;
    import org.apache.hadoop.mapreduce.lib.input.FileSplit;

    import java.io.IOException;

    /**
     * mapper
     */
    public class ReduceJoinMapper extends Mapper&lt;LongWritable,Text,ComboKey2,NullWritable&gt; {

        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            //
            String line = value.toString() ;

            //判断是customer还是order
            FileSplit split = (FileSplit)context.getInputSplit();
            String path = split.getPath().toString();
            //客户信息
            ComboKey2 key2 = new ComboKey2();
            if(path.contains(&quot;customers&quot;)){
                String cid = line.substring(0,line.indexOf(&quot;,&quot;));
                String custInfo = line ;
                key2.setType(0);
                key2.setCid(Integer.parseInt(cid));
                key2.setCustomerInfo(custInfo);
            }
            //order info
            else{
                String cid = line.substring(line.lastIndexOf(&quot;,&quot;) + 1);
                String oid = line.substring(0, line.indexOf(&quot;,&quot;));
                String oinfo = line.substring(0, line.lastIndexOf(&quot;,&quot;));
                key2.setType(1);
                key2.setCid(Integer.parseInt(cid));
                key2.setOid(Integer.parseInt(oid));
                key2.setOrderInfo(oinfo);
            }
            context.write(key2,NullWritable.get());
        }
    }

4.创建Reducer
    package com.it18zhang.hdfs.mr.mapjoin.reducejoin;

    import org.apache.hadoop.io.NullWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Reducer;

    import java.io.IOException;
    import java.util.Iterator;

    /**
     * ReduceJoinReducer,reducer端连接实现。
     */
    public class ReduceJoinReducer extends Reducer&lt;ComboKey2,NullWritable,Text,NullWritable&gt; {

        protected void reduce(ComboKey2 key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException {
            Iterator&lt;NullWritable&gt; it = values.iterator();
            it.next();
            int type = key.getType();
            int cid = key.getCid() ;
            String cinfo = key.getCustomerInfo() ;
            while(it.hasNext()){
                it.next();
                String oinfo = key.getOrderInfo();
                context.write(new Text(cinfo + &quot;,&quot; + oinfo),NullWritable.get());
            }
        }
    }

5.创建排序对比器
    package com.it18zhang.hdfs.mr.mapjoin.reducejoin;

    import com.it18zhang.hdfs.maxtemp.allsort.secondarysort.ComboKey;
    import org.apache.hadoop.io.WritableComparable;
    import org.apache.hadoop.io.WritableComparator;

    /**
     * 组合Key排序对比器
     */
    public class ComboKey2Comparator extends WritableComparator {
        protected ComboKey2Comparator() {
            super(ComboKey2.class, true);
        }

        public int compare(WritableComparable a, WritableComparable b) {
            ComboKey2 k1 = (ComboKey2) a;
            ComboKey2 k2 = (ComboKey2) b;
            return k1.compareTo(k2);
        }
    }

6.分组对比器
    package com.it18zhang.hdfs.mr.mapjoin.reducejoin;

    import com.it18zhang.hdfs.maxtemp.allsort.secondarysort.ComboKey;
    import org.apache.hadoop.io.WritableComparable;
    import org.apache.hadoop.io.WritableComparator;

    /**
     * CID分组对比器
     */
    public class CIDGroupComparator extends WritableComparator{

        protected CIDGroupComparator() {
            super(ComboKey2.class, true);
        }

        public int compare(WritableComparable a, WritableComparable b) {
            ComboKey2 k1 = (ComboKey2) a;
            ComboKey2 k2 = (ComboKey2) b;
            return k1.getCid() - k2.getCid();
        }
    }

7.App
    package com.it18zhang.hdfs.mr.mapjoin.reducejoin;

    import com.it18zhang.hdfs.maxtemp.allsort.secondarysort.*;
    import org.apache.hadoop.conf.Configuration;
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.io.IntWritable;
    import org.apache.hadoop.io.NullWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Job;
    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
    import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

    /**
     *
     */
    public class ReduceJoinApp {
        public static void main(String[] args) throws Exception {

            Configuration conf = new Configuration();
            conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;);

            Job job = Job.getInstance(conf);

            //设置job的各种属性
            job.setJobName(&quot;ReduceJoinApp&quot;);                        //作业名称
            job.setJarByClass(ReduceJoinApp.class);                 //搜索类

            //添加输入路径
            FileInputFormat.addInputPath(job,new Path(&quot;D:\\mr\\reducejoin&quot;));
            //设置输出路径
            FileOutputFormat.setOutputPath(job,new Path(&quot;D:\\mr\\reducejoin\\out&quot;));

            job.setMapperClass(ReduceJoinMapper.class);             //mapper类
            job.setReducerClass(ReduceJoinReducer.class);           //reducer类

            //设置Map输出类型
            job.setMapOutputKeyClass(ComboKey2.class);            //
            job.setMapOutputValueClass(NullWritable.class);      //

            //设置ReduceOutput类型
            job.setOutputKeyClass(Text.class);
            job.setOutputValueClass(NullWritable.class);         //

            //设置分区类
            job.setPartitionerClass(CIDPartitioner.class);
            //设置分组对比器
            job.setGroupingComparatorClass(CIDGroupComparator.class);
            //设置排序对比器
            job.setSortComparatorClass(ComboKey2Comparator.class);
            job.setNumReduceTasks(2);                           //reduce个数
            job.waitForCompletion(true);
        }
    }
</code></pre><h2 id="hive"><a href="#hive" class="headerlink" title="hive"></a>hive</h2><pre><code>在hadoop处理结构化数据的数据仓库。
不是:    关系数据库
        不是OLTP
        实时查询和行级更新。
</code></pre><h2 id="hive特点"><a href="#hive特点" class="headerlink" title="hive特点"></a>hive特点</h2><pre><code>hive存储数据结构(schema)在数据库中,处理的数据进入hdfs.
OLAP
HQL / HiveQL
</code></pre><h2 id="hive安装"><a href="#hive安装" class="headerlink" title="hive安装"></a>hive安装</h2><pre><code>1.下载hive2.1-tar.gz
2.tar开
    $&gt;tar -xzvf hive-2.1.0.tar.gz -C /soft    //tar开
    $&gt;cd /soft/hive-2.1.0                    //
    $&gt;ln -s hive-2.1.0 hive                    //符号连接

3.配置环境变量
    [/etc/profile]
    HIVE_HOME=/soft/hive
    PATH=...:$HIVE_HOME/bin

4.验证hive安装成功
    $&gt;hive --v

5.配置hive,使用win7的mysql存放hive的元数据.
    a)复制mysql驱动程序到hive的lib目录下。
        ...
    b)配置hive-site.xml
        复制hive-default.xml.template为hive-site.xml
        修改连接信息为mysql链接地址，将${system:...字样替换成具体路径。
        [hive/conf/hive-site.xml]
        &lt;property&gt;
            &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;
            &lt;value&gt;root&lt;/value&gt;
            &lt;description&gt;password to use against metastore database&lt;/description&gt;
        &lt;/property&gt;
        &lt;property&gt;
            &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;
            &lt;value&gt;root&lt;/value&gt;
            &lt;description&gt;Username to use against metastore database&lt;/description&gt;
        &lt;/property&gt;
        &lt;property&gt;
            &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;
            &lt;value&gt;jdbc:mysql://192.168.231.1:3306/hive2&lt;/value&gt;
        &lt;/property&gt;
        &lt;property&gt;
            &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;
            &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;
            &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;
        &lt;/property&gt;

    6)在msyql中创建存放hive信息的数据库
        mysql&gt;create database hive2 ;

    6)初始化hive的元数据(表结构)到mysql中。
        $&gt;cd /soft/hive/bin
        $&gt;schematool -dbType mysql -initSchema
</code></pre><h2 id="hive命令行操作"><a href="#hive命令行操作" class="headerlink" title="hive命令行操作"></a>hive命令行操作</h2><pre><code>1.创建hive的数据库

    $hive&gt;hive --version                //
    $hive&gt;hive --help                    //

    $hive&gt;create database mydb2 ;                //
    $hive&gt;show databases ;
    $hive&gt;use mydb2 ;
    $hive&gt;create table mydb2.t(id int,name string,age int);
    $hive&gt;drop table t ;
    $hive&gt;drop table mydb2.t ;
    $hive&gt;select * from mydb2.t ;        //查看指定库的表
    $hive&gt;exit ;                        //退出

    $&gt;hive                                //hive --service cli
    $&gt;hive                                //hive --service cli
</code></pre><h2 id="通过远程jdbc方式连接到hive数据仓库"><a href="#通过远程jdbc方式连接到hive数据仓库" class="headerlink" title="通过远程jdbc方式连接到hive数据仓库"></a>通过远程jdbc方式连接到hive数据仓库</h2><pre><code>1.启动hiveserver2服务器，监听端口10000
    $&gt;hive --service hiveserver2 &amp;

2.通过beeline命令行连接到hiveserver2
    $&gt;beeline                                            //进入beeline命令行(于hive --service beeline)
    $beeline&gt;!help                                        //查看帮助
    $beeline&gt;!quit                                        //退出
    $beeline&gt;!connect jdbc:hive2://localhost:10000/mydb2//连接到hibve数据
    $beeline&gt;show databases ;
    $beeline&gt;use mydb2 ;
    $beeline&gt;show tables;                                //显式表
</code></pre><h2 id="使用Hive-jdbc驱动程序采用jdbc方式访问远程数据仓库"><a href="#使用Hive-jdbc驱动程序采用jdbc方式访问远程数据仓库" class="headerlink" title="使用Hive-jdbc驱动程序采用jdbc方式访问远程数据仓库"></a>使用Hive-jdbc驱动程序采用jdbc方式访问远程数据仓库</h2><pre><code>1.创建java模块
2.引入maven
3.添加hive-jdbc依赖
    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
    &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;
             xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
             xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
        &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

        &lt;groupId&gt;com.it18zhang&lt;/groupId&gt;
        &lt;artifactId&gt;HiveDemo&lt;/artifactId&gt;
        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;

        &lt;dependencies&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;
                &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt;
                &lt;version&gt;2.1.0&lt;/version&gt;
            &lt;/dependency&gt;
        &lt;/dependencies&gt;
    &lt;/project&gt;

4.App
    package com.it18zhang.hivedemo;

    import java.sql.Connection;
    import java.sql.DriverManager;
    import java.sql.ResultSet;
    import java.sql.Statement;

    /**
     * 使用jdbc方式连接到hive数据仓库，数据仓库需要开启hiveserver2服务。
     */
    public class App {
        public static void main(String[] args) throws  Exception {
            Class.forName(&quot;org.apache.hive.jdbc.HiveDriver&quot;);
            Connection conn = DriverManager.getConnection(&quot;jdbc:hive2://192.168.231.201:10000/mydb2&quot;);
            Statement st = conn.createStatement();
            ResultSet rs = st.executeQuery(&quot;select id , name ,age from t&quot;);
            while(rs.next()){
                System.out.println(rs.getInt(1) + &quot;,&quot; + rs.getString(2)) ;
            }
            rs.close();
            st.close();
            conn.close();
        }
    }
</code></pre><h2 id="hive中表"><a href="#hive中表" class="headerlink" title="hive中表"></a>hive中表</h2><pre><code>1.managed table
    托管表。
    删除表时，数据也删除了。

2.external table
    外部表。
    删除表时，数据不删。
</code></pre><h2 id="hive命令"><a href="#hive命令" class="headerlink" title="hive命令"></a>hive命令</h2><pre><code>//创建表,external 外部表
$hive&gt;CREATE external TABLE IF NOT EXISTS t2(id int,name string,age int)
COMMENT &apos;xx&apos; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; STORED AS TEXTFILE ; 

//查看表数据
$hive&gt;desc t2 ;
$hive&gt;desc formatted t2 ;

//加载数据到hive表
$hive&gt;load data local inpath &apos;/home/centos/customers.txt&apos; into table t2 ;    //local上传文件
$hive&gt;load data inpath &apos;/user/centos/customers.txt&apos; [overwrite] into table t2 ;    //移动文件

//复制表
mysql&gt;create table tt as select * from users ;        //携带数据和表结构
mysql&gt;create table tt like users ;            //不带数据，只有表结构

hive&gt;create table tt as select * from users ;    
hive&gt;create table tt like users ;    


//count()查询要转成mr
$hive&gt;select count(*) from t2 ;
$hive&gt;select id,name from t2 ;


//
$hive&gt;select * from t2 order by id desc ;                //MR

//启用/禁用表
$hive&gt;ALTER TABLE t2 ENABLE NO_DROP;    //不允许删除
$hive&gt;ALTER TABLE t2 DISABLE NO_DROP;    //允许删除


//分区表,优化手段之一，从目录的层面控制搜索数据的范围。
//创建分区表.
$hive&gt;CREATE TABLE t3(id int,name string,age int) PARTITIONED BY (Year INT, Month INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;

//显式表的分区信息
$hive&gt;SHOW PARTITIONS t3;

//添加分区,创建目录
$hive&gt;alter table t3 add partition (year=2014, month=12);

//删除分区
hive&gt;ALTER TABLE employee_partitioned DROP IF EXISTS PARTITION (year=2014, month=11);

//分区结构
hive&gt;/user/hive/warehouse/mydb2.db/t3/year=2014/month=11
hive&gt;/user/hive/warehouse/mydb2.db/t3/year=2014/month=12


//加载数据到分区表
hive&gt;load data local inpath &apos;/home/centos/customers.txt&apos; into table t3 partition(year=2014,month=11);

//创建桶表
$hive&gt;CREATE TABLE t4(id int,name string,age int) CLUSTERED BY (id) INTO 3 BUCKETS ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;

//加载数据不会进行分桶操作
$hive&gt;load data local inpath &apos;/home/centos/customers.txt&apos; into table t4 ;

//查询t3表数据插入到t4中。
$hive&gt;insert into t4 select id,name,age from t3 ;

//桶表的数量如何设置?
//评估数据量，保证每个桶的数据量block的2倍大小。


//连接查询
$hive&gt;CREATE TABLE customers(id int,name string,age int) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;
$hive&gt;CREATE TABLE orders(id int,orderno string,price float,cid int) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;

//加载数据到表
//内连接查询
hive&gt;select a.*,b.* from customers a , orders b where a.id = b.cid ;
//左外
hive&gt;select a.*,b.* from customers a left outer join orders b on a.id = b.cid ;
hive&gt;select a.*,b.* from customers a right outer join orders b on a.id = b.cid ;
hive&gt;select a.*,b.* from customers a full outer join orders b on a.id = b.cid ;

//explode,炸裂,表生成函数。
//使用hive实现单词统计
//1.建表
$hive&gt;CREATE TABLE doc(line string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;
</code></pre>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/10/28/Hadoop第十天/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/28/Hadoop第十天/" itemprop="url">Hadoop第十天</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-10-28T10:31:16+08:00">
                2018-10-28
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hadoop/" itemprop="url" rel="index">
                    <span itemprop="name">Hadoop</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hadoop/Mysql/" itemprop="url" rel="index">
                    <span itemprop="name">Mysql</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="HA"><a href="#HA" class="headerlink" title="HA"></a>HA</h2><pre><code>3个JournalNode,edit log
两个NN，active | standby
2NN
</code></pre><h2 id="ha的管理命令"><a href="#ha的管理命令" class="headerlink" title="ha的管理命令"></a>ha的管理命令</h2><pre><code>hdfs haadmin -getServiceState nn1        //查看服务
hdfs haadmin -transitionToActive nn1    //激活
hdfs haadmin -transitionToStandby nn2    //待命
hdfs haadmin -failover nn1 nn2            //对调
</code></pre><h2 id="数据仓库"><a href="#数据仓库" class="headerlink" title="数据仓库"></a>数据仓库</h2><pre><code>OLAP        //online analyze process,在线分析处理
            //延迟性高.
在于后期海量数据的查询统计hive是延迟性比较高的操作实时性不好都是批量计算
</code></pre><h2 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h2><pre><code>OLTP        //online transaction process在线事务处理.
            //实时性好。延迟性很低
        mysql针对事务性处理保证ACI特性。        
</code></pre><h2 id="HA-1"><a href="#HA-1" class="headerlink" title="HA"></a>HA</h2><pre><code>3个JournalNode,edit log
两个NN，active | standby
2NN
</code></pre><h2 id="ha的管理命令-1"><a href="#ha的管理命令-1" class="headerlink" title="ha的管理命令"></a>ha的管理命令</h2><pre><code>hdfs haadmin -getServiceState nn1        //查看服务
hdfs haadmin -transitionToActive nn1    //激活
hdfs haadmin -transitionToStandby nn2    //待命
hdfs haadmin -failover nn1 nn2            //对调
</code></pre><h2 id="数据仓库-1"><a href="#数据仓库-1" class="headerlink" title="数据仓库"></a>数据仓库</h2><pre><code>OLAP        //online analyze process,在线分析处理
            //延迟性高.
</code></pre><h2 id="数据库-1"><a href="#数据库-1" class="headerlink" title="数据库"></a>数据库</h2><pre><code>OLTP        //online transaction process在线事务处理.
            //实时性好。
</code></pre><h2 id="jdbc"><a href="#jdbc" class="headerlink" title="jdbc"></a>jdbc</h2><pre><code>java database connection,java数据库连接。
</code></pre><p>java数据库连接设计套接字编程吗：面向接口编程，依托mysql驱动程序和mysql操作， jdbc就是socket，musql就是serversocket。</p>
<p><img src="https://i.imgur.com/5s8oEhk.png" alt=""><br>0</p>
<pre><code>1.创建mysql数据库和表
     create table users(id int primary key auto_increment , name varchar(20) , age int);
2.idea中创建jdbcDemo模块
</code></pre><h2 id="事务-transaction"><a href="#事务-transaction" class="headerlink" title="事务:transaction,"></a>事务:transaction,</h2><pre><code>和数据库之间的一组操作。
特点.
a        //atomic,原子性,不可分割.
c        //consistent,不能破坏掉
i        //isolate,隔离型.
d        //durable.永久性
</code></pre><h2 id="truncate"><a href="#truncate" class="headerlink" title="truncate"></a>truncate</h2><pre><code>截断表，类似于delete操作，速度快，数据无法回滚。在Mysql里面操作
truncate table users ;
</code></pre><h2 id="sql语句"><a href="#sql语句" class="headerlink" title="sql语句"></a>sql语句</h2><pre><code>1.
2.
3.
</code></pre><h2 id="Transaction"><a href="#Transaction" class="headerlink" title="Transaction"></a>Transaction</h2><pre><code>commit            //提交
rollback        //回滚
savePoint        //保存点
</code></pre><h1 id="插入10万条数据不用预处理语句"><a href="#插入10万条数据不用预处理语句" class="headerlink" title="插入10万条数据不用预处理语句"></a>插入10万条数据不用预处理语句</h1><pre><code>import org.junit.Test;
import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.Statement;

//测试增删改查基本功能
public class TestCRUD {
    @Test
    public void testStatement() throws Exception{
        long start=System.currentTimeMillis();
        //创建连接
        String diverClass = &quot;com.mysql.jdbc.Driver&quot;;
        String url = &quot;jdbc:mysql://localhost:3306/big4&quot;;
        String username = &quot;root&quot;;
        String password = &quot;root&quot;;
        Class.forName(diverClass);
        Connection conn = DriverManager.getConnection(url, username, password);
        //关闭自动提交
        conn.setAutoCommit(false);

        //创建语句对象
        Statement st = conn.createStatement();
        for(int i=0;i&lt;100000;i++){
            String sql = &quot;insert into users(name,age) values(&apos;tomas&quot; + i + &quot;&apos;,&quot; + (i % 100) + &quot;)&quot;;
            st.execute(sql);
        }
        conn.commit();
        st.close();
        conn.close();
        System.out.println(System.currentTimeMillis() - start);
    }
}
</code></pre><h1 id="使用预处理语句："><a href="#使用预处理语句：" class="headerlink" title="使用预处理语句："></a>使用预处理语句：</h1><pre><code>import org.junit.Test;
import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.PreparedStatement;

//测试增删改查基本功能
public class TestCRUD {
    @Test
    public void testPreparedStatement() throws Exception{
        long start=System.currentTimeMillis();
        //创建连接
        String diverClass = &quot;com.mysql.jdbc.Driver&quot;;
        String url = &quot;jdbc:mysql://localhost:3306/big4&quot;;
        String username = &quot;root&quot;;
        String password = &quot;root&quot;;
        Class.forName(diverClass);
        Connection conn = DriverManager.getConnection(url, username, password);
        //关闭自动提交
        conn.setAutoCommit(false);

        //创建语句对象
        String sql = &quot;insert into users(name,age) value(?,?)&quot;;
        PreparedStatement ppst = conn.prepareStatement(sql);
        for(int i =0;i &lt; 10000;i++){
                ppst.setString(1,&quot;tom&quot;+i);
                ppst.setInt(2,i%100);
                ppst.executeUpdate();//执行更新
             }
        conn.commit();
        ppst.close();
        conn.close();
        System.out.println(System.currentTimeMillis() - start);
    }
}
</code></pre><h2 id="上述代码加一个批处理："><a href="#上述代码加一个批处理：" class="headerlink" title="上述代码加一个批处理："></a>上述代码加一个批处理：</h2><pre><code>import org.junit.Test;
import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.PreparedStatement;

//测试增删改查基本功能
public class TestCRUD {
    @Test
    public void testPreparedStatement() throws Exception {
        long start = System.currentTimeMillis();
        //创建连接
        String diverClass = &quot;com.mysql.jdbc.Driver&quot;;
        String url = &quot;jdbc:mysql://localhost:3306/big4&quot;;
        String username = &quot;root&quot;;
        String password = &quot;root&quot;;
        Class.forName(diverClass);
        Connection conn = DriverManager.getConnection(url, username, password);
        //关闭自动提交
        conn.setAutoCommit(false);

        //创建语句对象
        String sql = &quot;insert into users(name,age) value(?,?)&quot;;
        PreparedStatement ppst = conn.prepareStatement(sql);
        for (int i = 0; i &lt; 10000; i++) {
            ppst.setString(1, &quot;tom&quot; + i);
            ppst.setInt(2, i % 100);
            ppst.addBatch();//这个地方相当于一个缓冲区，相当于缓冲区里面放了2000个在传出去，减少了网络带宽速度会变快
            if (i % 200 == 0) {
                ppst.executeUpdate();//执行更新
            }
        }
        ppst.addBatch();//最后不够2000在进行一个批处理
        conn.commit();
        ppst.close();
        conn.close();
        System.out.println(System.currentTimeMillis() - start);
    }
}
</code></pre><p>这边的一个通过自回环传输一百万次插入，传输量巨大。<br>但是要是直接在Mysql端直接给个命令做一百万次插入，就很快。不用和服务器端交互很多的数据量。通过存储过程来写，存储过程就是一组sql语句。这些语句已经预先在数据库中存放了。无非就是循环了。要执行这些语句，就要发一条执行存储过程的指令，服务器已收到命令，就直接执行。 速度很快的。</p>
<p><img src="https://i.imgur.com/7TubkBX.png" alt=""></p>
<h1 id="100000条数据通过普通事务，预处理，批处理时间"><a href="#100000条数据通过普通事务，预处理，批处理时间" class="headerlink" title="100000条数据通过普通事务，预处理，批处理时间"></a>100000条数据通过普通事务，预处理，批处理时间</h1><pre><code>Statement                //46698
PreparedStatent            //43338
CallableStatement        //14385
</code></pre><p><img src="https://i.imgur.com/YJmcnEF.png" alt=""></p>
<h1 id="mysql存储过程"><a href="#mysql存储过程" class="headerlink" title="mysql存储过程"></a>mysql存储过程</h1><p>msyql&gt;– 定义新的终止符,<strong>*</strong>不要带空格这个是注释<strong>*</strong><br>mysql&gt;delimiter //</p>
<p>mysql&gt;– 创建存储过程<br>mysql&gt;CREATE PROCEDURE simpleproc (OUT param1 INT)<br>        BEGIN<br>        SELECT COUNT(*) INTO param1 FROM users;        – into 是赋值方式之一<br>        END<br>        //</p>
<p>mysql&gt;– 查看存储过程的状态<br>mysql&gt;show procedure status //</p>
<p>mysql&gt;– 查看指定存储过程创建语句<br>mysql&gt;show create procedure simpleproc ;</p>
<p>mysql&gt;– 调用存储过程,@a在命令中定义变量<br>mysql&gt;call simpleproc(@a)</p>
<p>mysql&gt;– 删除存储过程<br>mysql&gt;show drop procedure simpleproc ;</p>
<p>mysql&gt;– 定义加法存储过程,set赋值语句 :=<br>mysql&gt;create procedure sp_add(in a int,in b int, out c int)<br>            begin<br>            set c := a + b ;<br>        end<br>        //</p>
<h2 id="java访问存储过程（调用的是上一步c-a-b这个过程）"><a href="#java访问存储过程（调用的是上一步c-a-b这个过程）" class="headerlink" title="java访问存储过程（调用的是上一步c=a+b这个过程）"></a>java访问存储过程（调用的是上一步c=a+b这个过程）</h2><pre><code>import org.junit.Test;
import java.sql.*;

/**
 * 测试基本操作
 */
public class TestCRUD {
    /**
     * 存储过程
     */
    @Test
    public void testCallableStatement() throws Exception {
        long start = System.currentTimeMillis();
        //创建连接
        String driverClass = &quot;com.mysql.jdbc.Driver&quot;;
        String url = &quot;jdbc:mysql://localhost:3306/big4&quot;;
        String username = &quot;root&quot;;
        String password = &quot;root&quot;;
        Class.forName(driverClass);
        Connection conn = DriverManager.getConnection(url, username, password);
        //关闭自动提交
        conn.setAutoCommit(false);

        //创建可调用语句，调用存储过程
        CallableStatement cst = conn.prepareCall(&quot;{call sp_add(?,?,?)}&quot;);
        cst.setInt(1,2);        //绑定参数
        cst.setInt(2,3);
        //注册输出参数类型
        cst.registerOutParameter(3,Types.INTEGER);
        cst.execute();
        int sum = cst.getInt(3);
        System.out.println(sum);
        conn.commit();
        conn.close();
        System.out.println(System.currentTimeMillis() - start);
    }
}
</code></pre><h2 id="百万数据插入，存储过程的性能"><a href="#百万数据插入，存储过程的性能" class="headerlink" title="百万数据插入，存储过程的性能"></a>百万数据插入，存储过程的性能</h2><pre><code>1.创建存储过程
    mysql&gt;create procedure sp_batchinsert(in n int)
        begin
        DECLARE name0 varchar(20);    -- 定义在begin内部
        DECLARE age0 int;
        DECLARE i int default 0 ;
            while i &lt; n do
                set name0 := concat(&apos;tom&apos;,i) ;
                set age0 := i % 100 ;
                insert into users(name,age) values(name0,age0);
                set i := i + 1 ;
            end while ;
        end 

        //
2.java代码
    @Test
    public void testCallableStatement() throws Exception {
        long start = System.currentTimeMillis();
        //创建连接
        String driverClass = &quot;com.mysql.jdbc.Driver&quot;;
        String url = &quot;jdbc:mysql://localhost:3306/big4&quot;;
        String username = &quot;root&quot;;
        String password = &quot;root&quot;;
        Class.forName(driverClass);
        Connection conn = DriverManager.getConnection(url, username, password);
        //关闭自动提交
        conn.setAutoCommit(false);

        //创建可调用语句，调用存储过程
        CallableStatement cst = conn.prepareCall(&quot;{call sp_batchinsert(?)}&quot;);
        cst.setInt(1,1000000);        //绑定参数
        //注册输出参数类型
        cst.execute();
        conn.commit();
        conn.close();
        System.out.println(System.currentTimeMillis() - start);
    }
</code></pre><h1 id="mysql函数"><a href="#mysql函数" class="headerlink" title="mysql函数"></a>mysql函数</h1><p>这个是创建函数的SQL参考手册：</p>
<p><img src="https://i.imgur.com/1gvxZzF.png" alt=""></p>
<pre><code>1.函数和存储过程相似，只是多了返回值声明.
2.创建函数
    mysql&gt;create function sf_add(a int ,b int) returns int
        begin
            return a + b ;
        end
        //

3.显式创建的函数
    mysql&gt;show function status                -- 
    mysql&gt;show function status like &apos;%add%&apos;    -- 
    mysql&gt;select sf_add(1,2)                --

4.java调用函数
    @Test
    public void testFunction() throws Exception {
        long start = System.currentTimeMillis();
        //创建连接
        String driverClass = &quot;com.mysql.jdbc.Driver&quot;;
        String url = &quot;jdbc:mysql://localhost:3306/big4&quot;;
        String username = &quot;root&quot;;
        String password = &quot;root&quot;;
        Class.forName(driverClass);
        Connection conn = DriverManager.getConnection(url, username, password);
        //关闭自动提交
        conn.setAutoCommit(false);

        //创建可调用语句，调用存储过程
        CallableStatement cst = conn.prepareCall(&quot;{? = call sf_add(?,?)}&quot;);
        cst.setInt(2,100);//设置第二个参数是100第三个参数是200，第一个参数是输出。
        cst.setInt(3,200);
        cst.registerOutParameter(1,Types.INTEGER);
        //注册输出参数类型
        cst.execute();
        System.out.println(cst.getInt(1));
        conn.commit();
        conn.close();
        System.out.println(System.currentTimeMillis() - start);
    }
</code></pre><h2 id="MVCC"><a href="#MVCC" class="headerlink" title="MVCC"></a>MVCC</h2><pre><code>multiple version concurrent control,多版本并发控制。
数据库的记录存储有多条，每一条都附加版本。所以可以实现隔离级别。是因为有MVCC多版本并发控制
</code></pre><h2 id="事务的并发执行，容易出现的几个现象"><a href="#事务的并发执行，容易出现的几个现象" class="headerlink" title="事务的并发执行，容易出现的几个现象"></a>事务的并发执行，容易出现的几个现象</h2><pre><code>1.脏读
    读未提交,一个事务读取了另外一个事务改写还没有提交的数据，如果另外一个
    事务在稍后的时候回滚。

2.不可重复读
    一个事务进行相同条件查询连续的两次或者两次以上，每次结果都不同。这个是对数据进行了更改
    有其他事务做了update操作。

3.幻读
    和(2)很像，其他事务做了insert操作.1时间查询到了10个，2时刻查询到了20个，是因为中间有做了插入操作
</code></pre><h2 id="隔离级别（由于有MVCC）"><a href="#隔离级别（由于有MVCC）" class="headerlink" title="隔离级别（由于有MVCC）"></a>隔离级别（由于有MVCC）</h2><pre><code>为了避免出现哪种并发现象的。
1        //read uncommitted    ,读未提交        导致脏读导致不可重复读
2        //read committed    ,读已提交        这里避免了脏读
4        //repeatable read    ,可以重复读        这里避免了不可重复读。还是可以让他插入更新，但是读取的是更新之前的值
8        //serializable        ,串行化(悲观锁)        这里避免了换读，串行化不支持并发了就已经。
</code></pre><h2 id="演示mysql事务隔离级别"><a href="#演示mysql事务隔离级别" class="headerlink" title="演示mysql事务隔离级别"></a>演示mysql事务隔离级别</h2><pre><code>1.开启mysql客户端
    mysql&gt;
2.关闭自动提交
    mysql&gt;set autocommit 0 ;
3.每次操作数据,都要开启事务，提交事务。
</code></pre><h2 id="脏读现象"><a href="#脏读现象" class="headerlink" title="脏读现象"></a>脏读现象</h2><pre><code>[A]
    1)mysql&gt;start transaction ;                                -- 开始事务
    2)msyql&gt;update users set age = age + 1 where id = 1 ;    -- 更新数据,没有提交
    6)mysql&gt;rollback ;                                        -- 回滚
    7)mysql&gt;select * from users ;

[B]
    3)mysql&gt;set session transaction isolation level read uncommitted ;    -- 读未提交
    4)msyql&gt;start transaction ;        -- 开始事务
    5)mysql&gt;select * from users ;    -- 13
</code></pre><h2 id="避免脏读"><a href="#避免脏读" class="headerlink" title="避免脏读"></a>避免脏读</h2><pre><code>[A]
    1)mysql&gt;start transaction ;                                -- 开始事务
    2)msyql&gt;update users set age = age + 1 where id = 1 ;    -- 更新数据,没有提交
    6)mysql&gt;rollback ;                                        -- 回滚
    7)mysql&gt;select * from users ;

[B]
    3)mysql&gt;set session transaction isolation level read committed ;    -- 读已提交
    4)msyql&gt;start transaction ;        -- 开始事务
    5)mysql&gt;select * from users ;    -- 13
</code></pre><h2 id="测试不可重复读-隔离级别设置为读已提交不能避免不可重复读。"><a href="#测试不可重复读-隔离级别设置为读已提交不能避免不可重复读。" class="headerlink" title="测试不可重复读(隔离级别设置为读已提交不能避免不可重复读。)"></a>测试不可重复读(隔离级别设置为读已提交不能避免不可重复读。)</h2><pre><code>[A]
    1)mysql&gt;commit ;
    2)mysql&gt;set session transaction isolation level read committed ;    -- 读已提交
    3)mysql&gt;start transaction ;                                            -- 开始事务
    4)mysql&gt;select * from users    ;                                        -- 查询
    9)mysql&gt;select * from users    ;

[B]
    5)mysql&gt;commit;
    6)mysql&gt;start transaction ;    
    7)mysql&gt;update users set age = 15 where id = 1 ;                    -- 更新
    8)mysql&gt;commit;
</code></pre><h2 id="测试避免不可重复读-隔离级别设置为读已提交不能避免不可重复读。"><a href="#测试避免不可重复读-隔离级别设置为读已提交不能避免不可重复读。" class="headerlink" title="测试避免不可重复读(隔离级别设置为读已提交不能避免不可重复读。)"></a>测试避免不可重复读(隔离级别设置为读已提交不能避免不可重复读。)</h2><pre><code>[A]
    1)mysql&gt;commit ;
    2)mysql&gt;set session transaction isolation level repeatable read ;    -- 可以重复读
    3)mysql&gt;start transaction ;                                            -- 开始事务
    4)mysql&gt;select * from users    ;                                        -- 查询
    9)mysql&gt;select * from users    ;

[B]
    5)mysql&gt;commit;
    6)mysql&gt;start transaction ;    
    7)mysql&gt;update users set age = 15 where id = 1 ;                    -- 更新
    8)mysql&gt;commit;
</code></pre><h2 id="测试幻读-隔离级别设置为repeatable"><a href="#测试幻读-隔离级别设置为repeatable" class="headerlink" title="测试幻读(隔离级别设置为repeatable)"></a>测试幻读(隔离级别设置为repeatable)</h2><pre><code>[A]
    1)mysql&gt;commit ;
    2)mysql&gt;set session transaction isolation level serializable;        -- 串行化
    3)mysql&gt;start transaction ;                                            -- 开始事务
    4)mysql&gt;select * from users    ;                                        -- 查询
    9)mysql&gt;select * from users    ;

[B]
    5)mysql&gt;commit;
    6)mysql&gt;start transaction ;    
    7)mysql&gt;insert into users(name,age) values(&apos;tomas&apos;,13);                -- 更新
    8)mysql&gt;commit;
</code></pre><h2 id="ANSI-SQL"><a href="#ANSI-SQL" class="headerlink" title="ANSI SQL"></a>ANSI SQL</h2><pre><code>美国国家标准结构SQL组
select * from users for update ;
</code></pre><h2 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h2><pre><code>1.支持四种隔离级别。
2.默认隔离级别是可以重复读。
3.隔离级别是seriable,不支持并发写。
</code></pre><h2 id="表级锁"><a href="#表级锁" class="headerlink" title="表级锁"></a>表级锁</h2><pre><code>LOCK TABLE t WRITE;    -- 加锁(表级锁,read)

UNLOCK TABLES ;        -- 解除自己所有的所有表级锁

表级锁只能通过命令来解锁。
</code></pre><p><img src="https://i.imgur.com/cLufeUV.png" alt=""></p>
<h2 id="编程实现脏读现象"><a href="#编程实现脏读现象" class="headerlink" title="编程实现脏读现象"></a>编程实现脏读现象</h2><pre><code>package com.it18zhang.jdbcdemo.test;
import org.junit.Test;
import java.sql.*;
/**
 * 测试隔离级别
 */
public class TestIsolationLevel {

    /**
     * 执行写，不提交
     */
    @Test
    public void testA() throws  Exception{
        //创建连接
        String driverClass = &quot;com.mysql.jdbc.Driver&quot;;
        String url = &quot;jdbc:mysql://localhost:3306/big4&quot;;
        String username = &quot;root&quot;;
        String password = &quot;root&quot;;
        Class.forName(driverClass);
        Connection conn = DriverManager.getConnection(url, username, password);
        conn.setAutoCommit(false);
        Statement st = conn.createStatement();
        st.execute(&quot;update users set age = 80 where id = 1&quot;);

        System.out.println(&quot;===============&quot;);
        conn.commit();
        conn.close();
    }

    /**
     * 查询，查到别人没有提交的数据
     */
    @Test
    public void testB() throws  Exception{
        //创建连接
        String driverClass = &quot;com.mysql.jdbc.Driver&quot;;
        String url = &quot;jdbc:mysql://localhost:3306/big4&quot;;
        String username = &quot;root&quot;;
        String password = &quot;root&quot;;
        Class.forName(driverClass);
        Connection conn = DriverManager.getConnection(url, username, password);

        //设置隔离级别读未提交==&gt;导致脏读
        /************************** 设置隔离级别 ***************************************/
        conn.setTransactionIsolation(Connection.TRANSACTION_READ_UNCOMMITTED);
        conn.setAutoCommit(false);
        Statement st = conn.createStatement();


        ResultSet rs = st.executeQuery(&quot;select age from users where id = 1&quot;);
        rs.next();
        int age = rs.getInt(1) ;
        System.out.println(age);
        System.out.println(&quot;===============&quot;);
        conn.commit();
        conn.close();
    }
</code></pre><h2 id="共享读锁"><a href="#共享读锁" class="headerlink" title="共享读锁"></a>共享读锁</h2><h1 id="独占写锁"><a href="#独占写锁" class="headerlink" title="独占写锁"></a>独占写锁</h1><pre><code>一个事务写操作，另一个塞住。行级别的锁
在独占锁这块，开启事务之后，资源存在征用的情况，如果A修改&gt;=2，b修改&lt;=3那么就出现先来先用的情况，没有提交事务之前，B一直要等待状态。然后事务是you原子性的，肯定就是非交集和交集的部分都不能更改。因为原子性。查询是由隔离级别控制。在查询的时候为了不让别人更新设置隔离级别为最高级别，就是8串行化，但是很难受，连接不关掉放到连接池里面，被下一个调用，还是串行化状态，不能写入，性能差。除了设置隔离级别，我们还可以加上一个select * from users for update。这样的话就代表仅仅是对这一个查询语句的时候执行独占写锁。这个时候不需要再设置隔离级别为8了。
</code></pre><h2 id="SQL"><a href="#SQL" class="headerlink" title="SQL"></a>SQL</h2><pre><code>//
insert into users(name,age,...) values(&apos;&apos;,12,..) ;                -- insert
update users set name = &apos;xxx&apos;,age = xxx ,... where id = xxx ;    -- update
delete from users where id = xxx                                -- delete

-- 投影查询 projection.
select id,name from users where ... order by limit xxx            --select

-- 查询时直接上独占写锁
select * from users for update ;    
</code></pre><h2 id="连接查询"><a href="#连接查询" class="headerlink" title="连接查询"></a>连接查询</h2><pre><code>1.准备表[mysql.sql]
drop table if exists customers; -- 删除表
drop table if exists orders ;    -- 删除表
create table customers(id int primary key auto_increment , name varchar(20) , age int);                    -- 创建customers表
create table orders(id int primary key auto_increment , orderno varchar(20) , price float , cid int);    -- 创建orders表

-- 插入数据
insert into customers(name,age) values(&apos;tom&apos;,12);
insert into customers(name,age) values(&apos;tomas&apos;,13);
insert into customers(name,age) values(&apos;tomasLee&apos;,14);
insert into customers(name,age) values(&apos;tomason&apos;,15);

-- 插入订单数据
insert into orders(orderno,price,cid) values(&apos;No001&apos;,12.25,1);
insert into orders(orderno,price,cid) values(&apos;No002&apos;,12.30,1);
insert into orders(orderno,price,cid) values(&apos;No003&apos;,12.25,2);
insert into orders(orderno,price,cid) values(&apos;No004&apos;,12.25,2);
insert into orders(orderno,price,cid) values(&apos;No005&apos;,12.25,2);
insert into orders(orderno,price,cid) values(&apos;No006&apos;,12.25,3);
insert into orders(orderno,price,cid) values(&apos;No007&apos;,12.25,3);
insert into orders(orderno,price,cid) values(&apos;No008&apos;,12.25,3);
insert into orders(orderno,price,cid) values(&apos;No009&apos;,12.25,3);
insert into orders(orderno,price,cid) values(&apos;No0010&apos;,12.25,NULL);

---执行SQL文件
    source d:/SQL/mysql.sql

2.查询--连接查询
mysql&gt;-- 笛卡尔积查询,无连接条件查询
mysql&gt;select a.*,b.* from customers a , orders b ; 
</code></pre><p><img src="https://i.imgur.com/4hC9GS4.png" alt=""></p>
<pre><code>mysql&gt;-- 内连接,查询符合条件的记录.
mysql&gt;select a.*,b.* from customers a , orders b where a.id = b.cid ; 
</code></pre><p><img src="https://i.imgur.com/2bP8sx4.png" alt=""></p>
<pre><code>mysql&gt;-- 左外连接,查询符合条件的记录.
mysql&gt;select a.*,b.* from customers a left outer join orders b on a.id = b.cid ; 
</code></pre><p><img src="https://i.imgur.com/XKWihaX.png" alt=""></p>
<pre><code>mysql&gt;-- 右外连接,查询符合条件的记录.
mysql&gt;select a.*,b.* from customers a right outer join orders b on a.id = b.cid ; 
</code></pre><p><img src="https://i.imgur.com/LiR1yV0.png" alt=""></p>
<p><img src="https://i.imgur.com/6GdI0kM.png" alt=""></p>
<pre><code>mysql&gt;-- 全外连接,查询符合条件的记录(mysql不支持全外链接)
mysql&gt;select a.*,b.* from customers a full outer join orders b on a.id = b.cid ; 
</code></pre><p>2.查询–分组</p>
<pre><code>             字段列表  表       条件        分组        组内条件       排序     分页
mysql&gt;select ... from ... where ... group by ... having ...  order by ... limit ..


mysql&gt;-- 去重查询
mysql&gt;select distinct price,cid from orders ;

mysql&gt;-- 条件查询
mysql&gt;select price,cid from orders where price &gt; 12.27 ;

mysql&gt;-- 聚集查询
mysql&gt;select max(price) from orders ;
mysql&gt;select min(price) from orders ;
mysql&gt;select avg(price) from orders ;
mysql&gt;select sum(price) from orders ;
mysql&gt;select count(id) from orders ;

mysql&gt;-- 分组查询
mysql&gt;select max(price) from orders where cid is not null group by cid ;


mysql&gt;-- 分组查询(组内过滤)
mysql&gt;select cid ,orderno,max(price) as max_price,min(price) from orders where cid is not null group by cid having max_price &gt; 20 ;

mysql&gt;-- 降序查询
mysql&gt;select cid ,orderno,max(price) as max_price,min(price) from orders where cid is not null group by cid having max_price &gt; 20 order by max_price desc;

mysql&gt;-- 模糊查询
mysql&gt;select  * from customers where name like &apos;toma%&apos;        
mysql&gt;select  * from customers where name not like &apos;toma%&apos;        

mysql&gt;-- 范围查询
mysql&gt;select  * from customers where id in (1,2,3)        
mysql&gt;select  * from customers where id not in (1,2,3)        

mysql&gt;-- between 1 and 10,闭区间
mysql&gt;select  * from customers where id between 1 and 3 ;
mysql&gt;select  * from customers where id &gt;= 1 and id &lt;= 3 ;

mysql&gt;-- 嵌套子查询(查询没有订单的客户)
mysql&gt;select  * from customers where id not in (select distinct cid from orders where cid is not null);

mysql&gt;-- 嵌套子查询(查询订单数量&gt;2的客户)
mysql&gt;select * from customers where id in (select cid from orders group by cid having count(cid) &gt; 2);
mysql&gt;select * from customers where id in ( select t.cid from (select cid,count(*) as c from orders group by cid having c &gt; 2) as t);

mysql&gt;--向已有表中添加列
mysql&gt;--alter table orders add column area int;
mysql&gt;--设置area字段的值
mysql&gt;--update orders set area = 2 where id in(1,3,,6,7);
mysql&gt;--设置area字段的值
mysql&gt;--update orders set area = 1 where id not in(1,3,,6,7);
mysql&gt;--设置area字段的值
mysql&gt;--update orders set area = 1 where id not in(1,3,,6,7);
稍微看一下下面这个SQL语句：
mysq&gt;--select cid,area,count(price) from orders group by cid,area;//这句话的意思是对cid和area联合分组，什么意思：就是说要cid和area要都相同才能进入到同一个组里面去。
mysq&gt;--select cid,area,count(price),max(price),min(price) from orders group by cid,area order by cid asc, area desc  ;//这句话的意思是对cid和area联合分组，什么意思：就是说要cid和area要都相同才能进入到同一个组里面去。然后这个cid升序area降序是在1里面对area降序，也就是说先对cid整体升序，然后对升序完之后相同的cid里面进行降序。
</code></pre><p><img src="https://i.imgur.com/QbThVj2.png" alt=""></p>
<pre><code>mysql&gt;-- 嵌套子查询(查询客户id,客户name,订单数量,最贵订单，最便宜订单，平均订单价格 where 订单数量&gt;2的客户)
mysql&gt;select a.id,a.name,b.c,b.max,b.min,b.avg 
      from customers a,((select cid,count(cid) c , max(price) max ,min(price) min,avg(price) avg from orders group by cid having c &gt; 2) as b) 
      where a.id = b.cid ;
</code></pre><h2 id="hadoop"><a href="#hadoop" class="headerlink" title="hadoop"></a>hadoop</h2><pre><code>MR

左外连接.
</code></pre>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/touxiang.jpg"
                alt="Eric Hunn" />
            
              <p class="site-author-name" itemprop="name">Eric Hunn</p>
              <p class="site-description motion-element" itemprop="description">大数据|动漫迷|科技控</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">56</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">15</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">104</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/erichunn" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:erichunn.me@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://weibo.com/u/2944687303/home?wvr=5" target="_blank" title="Webibo">
                      
                        <i class="fa fa-fw fa-weibo"></i>Webibo</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://github.com/drakeet/" title="Drakeet" target="_blank">Drakeet</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      

      
	  
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=1311545319&auto=1&height=66"></iframe>

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Eric Hunn</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  

  

  

</body>
</html>
