<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />





  <link rel="alternate" href="/atom.xml" title="无心是一首歌" type="application/atom+xml" />






<meta name="description" content="大数据|动漫迷|科技控">
<meta property="og:type" content="website">
<meta property="og:title" content="无心是一首歌">
<meta property="og:url" content="http://erichunn.github.io/page/3/index.html">
<meta property="og:site_name" content="无心是一首歌">
<meta property="og:description" content="大数据|动漫迷|科技控">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="无心是一首歌">
<meta name="twitter:description" content="大数据|动漫迷|科技控">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: 'undefined',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://erichunn.github.io/page/3/"/>





  <title>无心是一首歌</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?02fe19ac065353167cab1b453f2909ec";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">无心是一首歌</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-首页">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-关于我">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于我
          </a>
        </li>
      
        
        <li class="menu-item menu-item-标签">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-分类">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-归档">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-站点地图">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br />
            
            站点地图
          </a>
        </li>
      
        
        <li class="menu-item menu-item-腾讯公益">
          <a href="/404/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br />
            
            腾讯公益
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2019/09/12/Spark第二天/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/12/Spark第二天/" itemprop="url">Untitled</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-09-12T20:37:10+08:00">
                2019-09-12
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h2><pre><code>通用性。
</code></pre><h2 id="Spark模块"><a href="#Spark模块" class="headerlink" title="Spark模块"></a>Spark模块</h2><pre><code>Spark Core            //核心库
Spark SQL            //SQL
Spark Streaming        //准实时计算。
Spark MLlib            //机器学习库
Spark graph            //图计算
</code></pre><h2 id="Spark集群运行"><a href="#Spark集群运行" class="headerlink" title="Spark集群运行"></a>Spark集群运行</h2><pre><code>1.local            //本地模式
2.standalone    //独立模式
3.yarn            //yarn模式
4.mesos            //mesos
</code></pre><h2 id="start-all-sh"><a href="#start-all-sh" class="headerlink" title="start-all.sh"></a>start-all.sh</h2><pre><code>start-master.sh    //RPC端口 7077
start-slave.sh    spark://s201:7077
</code></pre><h2 id="webui"><a href="#webui" class="headerlink" title="webui"></a>webui</h2><pre><code>http://s201:8080
</code></pre><h2 id="添加针对scala文件的编译插件"><a href="#添加针对scala文件的编译插件" class="headerlink" title="添加针对scala文件的编译插件"></a>添加针对scala文件的编译插件</h2><p>在IDEA的settings里面没有设置自动编译的情况下，需要记入scala编译插件，所以打包不含scala的类。</p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;
         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;groupId&gt;com.it18zhang&lt;/groupId&gt;
    &lt;artifactId&gt;SparkDemo1&lt;/artifactId&gt;
    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;

    &lt;build&gt;
        &lt;sourceDirectory&gt;src/main/java&lt;/sourceDirectory&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
                &lt;configuration&gt;
                    &lt;source&gt;1.8&lt;/source&gt;
                    &lt;target&gt;1.8&lt;/target&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt;
                &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt;
                &lt;version&gt;3.2.2&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;recompileMode&gt;incremental&lt;/recompileMode&gt;
                &lt;/configuration&gt;
                &lt;executions&gt;
                    &lt;execution&gt;
                        &lt;goals&gt;
                            &lt;goal&gt;compile&lt;/goal&gt;
                            &lt;goal&gt;testCompile&lt;/goal&gt;
                        &lt;/goals&gt;
                    &lt;/execution&gt;
                &lt;/executions&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;

    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
            &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;
            &lt;version&gt;2.1.0&lt;/version&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
&lt;/project&gt;
</code></pre><p>C:\Users\Administrator.m2\repository\net<br>C:\Users\Administrator.m2\repository\net\alchim31\maven...</p>
<h2 id="SparkContext"><a href="#SparkContext" class="headerlink" title="SparkContext:"></a>SparkContext:</h2><pre><code>Spark集群的连接。主要入口点。
SparkConf = new ();
conf.setApp(&quot;&quot;)
conf.setMaster(&quot;local&quot;) ;
sc = new SparkContext(conf);
//RDD : Resilient distributed dataset,弹性分布式数据集。
val rdd1 = sc.textFile(&quot;d:/scala/test.txt&quot;);
val rdd2 = rdd1.flatMap(line=&gt;line.split(&quot; &quot;));
val rdd3 = rdd2.map(word=&gt;(word,1));
val rdd4 = rdd3.reduceByKey(_ + _) ;
val list = rdd4.collect()
list.foreach(e=&gt;println(e));


//
sc.textFile(&quot;d:/scala&quot;).flatMap(_.split(&quot; &quot;)).map((_1)).reduceByKey(_ + _).collect().foreach(println)
</code></pre><h2 id="spark"><a href="#spark" class="headerlink" title="spark"></a>spark</h2><pre><code>基于hadoop的mr，扩展MR模型高效使用MR模型，内存型集群计算，提高app处理速度。
</code></pre><h2 id="spark特点"><a href="#spark特点" class="headerlink" title="spark特点"></a>spark特点</h2><pre><code>速度:在内存中存储中间结果。
支持多种语言.
内置了80+的算子.
高级分析:MR，SQL/ Streamming /mllib / graph
</code></pre><p>spark模块<br>    core        //通用执行引擎，提供内存计算和对外部数据集的引用。<br>    SQL            //构建在core之上，引入新的抽象SchemaRDD，提供了结构化和半结构化支持。</p>
<pre><code>Streaming    //小批量计算，用的是RDD.

MLlib        //机器学习库。core在。
Graphx        //图计算。
</code></pre><h2 id="RDD"><a href="#RDD" class="headerlink" title="RDD:"></a>RDD:</h2><pre><code>是spark的基本数据结构，是不可变数据集。RDD中的数据集进行逻辑分区，每个分区可以单独在集群节点
进行计算。可以包含任何java,scala，python和自定义类型。

RDD是只读的记录分区集合。RDD具有容错机制。

创建RDD方式，一、并行化一个现有集合。

hadoop 花费90%时间用户rw。、但是也不一定，如果是一次ｍｒ可能就不进入到磁盘里面，如果多次ｍｒ肯定进入到磁盘里面了。要等到数据都写入才可以，收到磁盘Ｉｏ影响严重

内存处理计算。在job间进行数据共享。内存的IO速率高于网络和disk的10 ~ 100之间。

内部包含5个主要属性
-----------------------
1.分区列表
2.针对每个split的计算函数。
3.对其他rdd的依赖列表
4.可选，如果是KeyValueRDD的话，可以带分区类。
5.可选，首选块位置列表(hdfs block location);

//默认并发度下面从下往上面看是sc.textFile的源码过程
这边确定一下，这个textFile里面的defaultMinPartitions指的是分区个数。
然后在代码里面conf.setMaster(&quot;local[2]&quot;)这个里面的2指的才是线程个数才是并发度。这个线程就类似于我们完全分布下的节点

local.backend.defaultParallelism() = scheduler.conf.getInt(&quot;spark.default.parallelism&quot;, totalCores)
taskScheduler.defaultParallelism = backend.defaultParallelism()
sc.defaultParallelism =...; taskScheduler.defaultParallelism
defaultMinPartitions = math.min(defaultParallelism, 2)
sc.textFile(path,defaultMinPartitions)            //1,2
</code></pre><p>先说一下mapreduce里面的Map和reduce可以和spark里面的Map和reduce相结合看一下：</p>
<p>Map过程：并行读取文本，对读取的单词进行map操作，每个词都以&lt;key,value&gt;形式生成。</p>
<pre><code>　　一个有三行文本的文件进行MapReduce操作。
　　读取第一行Hello World Bye World ，分割单词形成Map。
　　&lt;Hello,1&gt; &lt;World,1&gt; &lt;Bye,1&gt; &lt;World,1&gt;
　　读取第二行Hello Hadoop Bye Hadoop ，分割单词形成Map。
　　&lt;Hello,1&gt; &lt;Hadoop,1&gt; &lt;Bye,1&gt; &lt;Hadoop,1&gt;
　　读取第三行Bye Hadoop Hello Hadoop，分割单词形成Map。
　　&lt;Bye,1&gt; &lt;Hadoop,1&gt; &lt;Hello,1&gt; &lt;Hadoop,1&gt;
</code></pre><p>Reduce操作是对map的结果进行排序，合并，最后得出词频。<br>我的理解：</p>
<pre><code>　　经过进一步处理(combiner),将形成的Map根据相同的key组合成value数组。

　　&lt;Bye,1,1,1&gt; &lt;Hadoop,1,1,1,1&gt; &lt;Hello,1,1,1&gt; &lt;World,1,1&gt;

　　循环执行Reduce(K,V[])，分别统计每个单词出现的次数。

　　&lt;Bye,3&gt; &lt;Hadoop,4&gt; &lt;Hello,3&gt; &lt;World,2&gt;
</code></pre><h2 id="RDD变换"><a href="#RDD变换" class="headerlink" title="RDD变换"></a>RDD变换</h2><pre><code>返回指向新rdd的指针，在rdd之间创建依赖关系。每个rdd都有计算函数和指向父RDD的指针。

map()                                    //对每个元素进行变换，应用变换函数
                                        //(T)=&gt;V


filter()                                //过滤器,(T)=&gt;Boolean
flatMap()                                //压扁,T =&gt; TraversableOnce[U]

mapPartitions()                            //对每个分区进行应用变换，输入的Iterator,返回新的迭代器，可以对分区进行函数处理。
                                        //Iterator&lt;T&gt; =&gt; Iterator&lt;U&gt;

mapPartitionsWithIndex(func)            //同上，(Int, Iterator&lt;T&gt;) =&gt; Iterator&lt;U&gt;

sample(withReplacement, fraction, seed)    //采样返回采样的RDD子集。
                                        //withReplacement 元素是否可以多次采样.
                                        //fraction : 期望采样数量.[0,1]

union()                                    //类似于mysql union操作。
                                        //select * from persons where id &lt; 10 
                                        //union select * from id persons where id &gt; 29 ;

intersection                            //交集,提取两个rdd中都含有的元素。
distinct([numTasks]))                    //去重,去除重复的元素。

groupByKey()                            //(K,V) =&gt; (K,Iterable&lt;V&gt;)

reduceByKey(*)                            //按key聚合。 

aggregateByKey(zeroValue)(seqOp, combOp, [numTasks])
                                        //按照key进行聚合
key:String U:Int = 0

sortByKey                                //排序

join(otherDataset, [numTasks])            //连接,(K,V).join(K,W) =&gt;(K,(V,W)) 

cogroup                                    //协分组
                                        //(K,V).cogroup(K,W) =&gt;(K,(Iterable&lt;V&gt;,Iterable&lt;!-- &lt;W&gt; --&gt;)) 
cartesian(otherDataset)                    //笛卡尔积,RR[T] RDD[U] =&gt; RDD[(T,U)]

pipe                                    //将rdd的元素传递给脚本或者命令，执行结果返回形成新的RDD
coalesce(numPartitions)                    //减少分区
repartition                                //可增可减
repartitionAndSortWithinPartitions(partitioner)
                                        //再分区并在分区内进行排序
</code></pre><h2 id="RDD-Action"><a href="#RDD-Action" class="headerlink" title="RDD Action"></a>RDD Action</h2><pre><code>collect()                                //收集rdd元素形成数组.
count()                                    //统计rdd元素的个数
reduce()                                //聚合,返回一个值。
first                                    //取出第一个元素take(1)
take                                    //
takeSample (withReplacement,num, [seed])
takeOrdered(n, [ordering])

saveAsTextFile(path)                    //保存到文件
saveAsSequenceFile(path)                //保存成序列文件

saveAsObjectFile(path) (Java and Scala)

countByKey()                            //按照key,统计每个key下value的个数.
</code></pre><h2 id="spark集成hadoop-ha"><a href="#spark集成hadoop-ha" class="headerlink" title="spark集成hadoop ha"></a>spark集成hadoop ha</h2><pre><code>1.复制core-site.xml + hdfs-site.xml到spark/conf目录下
2.分发文件到spark所有work节点
3.启动spark集群
4.启动spark-shell,连接spark集群上
    $&gt;spark-shell --master spark://s201:7077
    $scala&gt;sc.textFile(&quot;hdfs://mycluster/user/centos/test.txt&quot;).collect();    
</code></pre>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2019/09/11/Mybatis第二天/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/11/Mybatis第二天/" itemprop="url">Untitled</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-09-11T21:32:10+08:00">
                2019-09-11
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><img src="https://i.imgur.com/CduGMIo.png" alt=""></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2019/09/10/Mybatis第一天/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/10/Mybatis第一天/" itemprop="url">Untitled</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-09-10T15:05:48+08:00">
                2019-09-10
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>mybatis框架<br>共四天<br>第一天：mybatis入门<br>    mybatis的概述<br>    mybatis的环境搭建<br>    mybatis入门案例<br>    自定义mybatis框架（主要的目的是为了让大家了解mybatis中执行细节）<br>第二天：mybatis基本使用<br>    mybatis的单表crud操作<br>    mybatis的参数和返回值<br>    mybatis的dao编写<br>    mybatis配置的细节<br>        几个标签的使用<br>第三天：mybatis的深入和多表<br>    mybatis的连接池<br>    mybatis的事务控制及设计的方法<br>    mybatis的多表查询<br>        一对多（多对一）<br>        多对多<br>第四天：mybatis的缓存和注解开发<br>    mybatis中的加载时机（查询的时机）<br>    mybatis中的一级缓存和二级缓存<br>    mybatis的注解开发<br>        单表CRUD</p>
<pre><code>多表查询
</code></pre><hr>
<p>1、什么是框架？<br>    它是我们软件开发中的一套解决方案，不同的框架解决的是不同的问题。<br>    使用框架的好处：<br>        框架封装了很多的细节，使开发者可以使用极简的方式实现功能。大大提高开发效率。<br>2、三层架构<br>    表现层：<br>        是用于展示数据的<br>    业务层：<br>        是处理业务需求<br>    持久层：<br>        是和数据库交互的<br>3、持久层技术解决方案<br>    JDBC技术：<br>        Connection<br>        PreparedStatement<br>        ResultSet<br>    Spring的JdbcTemplate：<br>        Spring中对jdbc的简单封装<br>    Apache的DBUtils：<br>        它和Spring的JdbcTemplate很像，也是对Jdbc的简单封装</p>
<pre><code>以上这些都不是框架
    JDBC是规范
    Spring的JdbcTemplate和Apache的DBUtils都只是工具类
</code></pre><p>4、mybatis的概述<br>    mybatis是一个持久层框架，用java编写的。<br>    它封装了jdbc操作的很多细节，使开发者只需要关注sql语句本身，而无需关注注册驱动，创建连接等繁杂过程<br>    它使用了ORM思想实现了结果集的封装。</p>
<pre><code>ORM：
    Object Relational Mappging 对象关系映射
    简单的说：
        就是把数据库表和实体类及实体类的属性对应起来
        让我们可以操作实体类就实现操作数据库表。

        user            User
        id            userId
        user_name        userName
今天我们需要做到
    实体类中的属性和数据库表的字段名称保持一致。
        user            User
        id            id
        user_name        user_name
</code></pre><p>5、mybatis的入门<br>    mybatis的环境搭建<br>        第一步：创建maven工程并导入坐标<br>        第二步：创建实体类和dao的接口<br>        第三步：创建Mybatis的主配置文件<br>                SqlMapConifg.xml<br>        第四步：创建映射配置文件<br>                IUserDao.xml<br>    环境搭建的注意事项：<br>        第一个：创建IUserDao.xml 和 IUserDao.java时名称是为了和我们之前的知识保持一致。<br>            在Mybatis中它把持久层的操作接口名称和映射文件也叫做：Mapper<br>            所以：IUserDao 和 IUserMapper是一样的<br>        第二个：在idea中创建目录的时候，它和包是不一样的<br>            包在创建时：com.itheima.dao它是三级结构<br>            目录在创建时：com.itheima.dao是一级目录<br>        第三个：mybatis的映射配置文件位置必须和dao接口的包结构相同<br>        第四个：映射配置文件的mapper标签namespace属性的取值必须是dao接口的全限定类名<br>        第五个：映射配置文件的操作配置（select），id属性的取值必须是dao接口的方法名</p>
<pre><code>    当我们遵从了第三，四，五点之后，我们在开发中就无须再写dao的实现类。
mybatis的入门案例
    第一步：读取配置文件
    第二步：创建SqlSessionFactory工厂
    第三步：创建SqlSession
    第四步：创建Dao接口的代理对象
    第五步：执行dao中的方法
    第六步：释放资源

    注意事项：
        不要忘记在映射配置中告知mybatis要封装到哪个实体类中
        配置的方式：指定实体类的全限定类名

    mybatis基于注解的入门案例：
        把IUserDao.xml移除，在dao接口的方法上使用@Select注解，并且指定SQL语句
        同时需要在SqlMapConfig.xml中的mapper配置时，使用class属性指定dao接口的全限定类名。
明确：
    我们在实际开发中，都是越简便越好，所以都是采用不写dao实现类的方式。
    不管使用XML还是注解配置。
    但是Mybatis它是支持写dao实现类的。
</code></pre><p>6、自定义Mybatis的分析：<br>    mybatis在使用代理dao的方式实现增删改查时做什么事呢？<br>        只有两件事：<br>            第一：创建代理对象<br>            第二：在代理对象中调用selectList</p>
<pre><code>自定义mybatis能通过入门案例看到类
    class Resources
    class SqlSessionFactoryBuilder
    interface SqlSessionFactory
    interface SqlSession
</code></pre>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2019/09/07/Spark第一天/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/07/Spark第一天/" itemprop="url">Untitled</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-09-07T16:22:05+08:00">
                2019-09-07
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="并行"><a href="#并行" class="headerlink" title="并行"></a>并行</h2><pre><code>集群计算。
并行计算。
</code></pre><h2 id="并发"><a href="#并发" class="headerlink" title="并发"></a>并发</h2><pre><code>并发执行。
</code></pre><h2 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h2><pre><code>Lightning-fast cluster computing。
快如闪电的集群计算。
大规模快速通用的计算引擎。
速度:    比hadoop 100x,磁盘计算快10x
使用:    java / Scala /R /python
        提供80+算子(操作符)，容易构建并行应用。
通用:    组合SQL ，流计算 + 复杂分析。

运行：    Hadoop, Mesos, standalone, or in the cloud,local.
</code></pre><h2 id="Spark模块"><a href="#Spark模块" class="headerlink" title="Spark模块"></a>Spark模块</h2><pre><code>Spark core        //核心模块
Spark SQL        //SQL
Spark Streaming    //流计算
Spark MLlib        //机器学习
Spark graph        //图计算



DAG        //direct acycle graph,有向无环图。
</code></pre><h2 id="安装Spark"><a href="#安装Spark" class="headerlink" title="安装Spark"></a>安装Spark</h2><pre><code>1.下载spark-2.1.0-bin-hadoop2.7.tgz
    ..
2.解压
    ..
3.环境变量
    [/etc/profile]
    SPARK_HOME=/soft/spark
    PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin

    [source]
    $&gt;source /etc/profile

4.验证spark

    $&gt;cd /soft/spark
    $&gt;./spark-shell

5.webui
    http://s201:4040/
</code></pre><h2 id="体验spark"><a href="#体验spark" class="headerlink" title="体验spark"></a>体验spark</h2><pre><code>0.sc
    SparkContext，Spark程序的入口点，封装了整个spark运行环境的信息。

1.进入spark-shell
    $&gt;spark-shell
    $scala&gt;sc
</code></pre><h2 id="API"><a href="#API" class="headerlink" title="API"></a>API</h2><pre><code>[SparkContext]
    Spark程序的入口点，封装了整个spark运行环境的信息。

[RDD]
    resilient distributed dataset,弹性分布式数据集。等价于集合。
</code></pre><h2 id="spark实现word-count"><a href="#spark实现word-count" class="headerlink" title="spark实现word count"></a>spark实现word count</h2><pre><code>//加载文本文件,以换行符方式切割文本.Array(hello  world2,hello world2 ,...)
val rdd1 = sc.textFile(&quot;/home/centos/test.txt&quot;);

//单词统计1
$scala&gt;val rdd1 = sc.textFile(&quot;/home/centos/test.txt&quot;)
$scala&gt;val rdd2 = rdd1.flatMap(line=&gt;line.split(&quot; &quot;))
$scala&gt;val rdd3 = rdd2.map(word = &gt; (word,1))
$scala&gt;val rdd4 = rdd3.reduceByKey(_ + _)
$scala&gt;rdd4.collect

//单词统计2
sc.textFile(&quot;/home/centos/test.txt&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_ + _).collect

//统计所有含有wor字样到单词个数。filter

//过滤单词
sc.textFile(&quot;/home/centos/test.txt&quot;).flatMap(_.split(&quot; &quot;)).filter(_.contains(&quot;wor&quot;)).map((_,1)).reduceByKey(_ + _).collect
</code></pre><p>[API]<br>    SparkContext:<br>        Spark功能的主要入口点。代表到Spark集群的连接，可以创建RDD、累加器和广播变量.<br>        每个JVM只能激活一个SparkContext对象，在创建sc之前需要stop掉active的sc。</p>
<pre><code>SparkConf:
    spark配置对象，设置Spark应用各种参数，kv形式。
</code></pre><h2 id="编写scala程序，引入spark类库，完成wordcount"><a href="#编写scala程序，引入spark类库，完成wordcount" class="headerlink" title="编写scala程序，引入spark类库，完成wordcount"></a>编写scala程序，引入spark类库，完成wordcount</h2><pre><code>1.创建Scala模块,并添加pom.xml
    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
    &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;
             xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
             xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
        &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

        &lt;groupId&gt;com.it18zhang&lt;/groupId&gt;
        &lt;artifactId&gt;SparkDemo1&lt;/artifactId&gt;
        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
        &lt;dependencies&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
                &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;
                &lt;version&gt;2.1.0&lt;/version&gt;
            &lt;/dependency&gt;
        &lt;/dependencies&gt;
    &lt;/project&gt;

2.编写scala文件
    import org.apache.spark.{SparkConf, SparkContext}

    /**
      * Created by Administrator on 2017/4/20.
      */
    object WordCountDemo {
        def main(args: Array[String]): Unit = {
            //创建Spark配置对象
            val conf = new SparkConf();
            conf.setAppName(&quot;WordCountSpark&quot;)
            //设置master属性
            conf.setMaster(&quot;local&quot;) ;

            //通过conf创建sc
            val sc = new SparkContext(conf);

            //加载文本文件
            val rdd1 = sc.textFile(&quot;d:/scala/test.txt&quot;);
            //压扁
            val rdd2 = rdd1.flatMap(line =&gt; line.split(&quot; &quot;)) ;
            //映射w =&gt; (w,1)
            val rdd3 = rdd2.map((_,1))
            val rdd4 = rdd3.reduceByKey(_ + _)
            val r = rdd4.collect()
            r.foreach(println)
        }
    }
</code></pre><h2 id="java版单词统计"><a href="#java版单词统计" class="headerlink" title="java版单词统计"></a>java版单词统计</h2><pre><code>import org.apache.spark.SparkConf;
import org.apache.spark.SparkContext;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.FlatMapFunction;
import org.apache.spark.api.java.function.Function2;
import org.apache.spark.api.java.function.PairFunction;
import scala.Tuple2;

import java.util.ArrayList;
import java.util.Iterator;
import java.util.List;

/**
 * java版
 */
public class WordCountJava2 {
    public static void main(String[] args) {
        //创建SparkConf对象
        SparkConf conf = new SparkConf();
        conf.setAppName(&quot;WordCountJava2&quot;);
        conf.setMaster(&quot;local&quot;);

        //创建java sc
        JavaSparkContext sc = new JavaSparkContext(conf);
        //加载文本文件
        JavaRDD&lt;String&gt; rdd1 = sc.textFile(&quot;d:/scala//test.txt&quot;);

        //压扁
        JavaRDD&lt;String&gt; rdd2 = rdd1.flatMap(new FlatMapFunction&lt;String, String&gt;() {
            public Iterator&lt;String&gt; call(String s) throws Exception {
                List&lt;String&gt; list = new ArrayList&lt;String&gt;();
                String[] arr = s.split(&quot; &quot;);
                for(String ss :arr){
                    list.add(ss);
                }
                return list.iterator();
            }
        });

        //映射,word -&gt; (word,1)
        JavaPairRDD&lt;String,Integer&gt; rdd3 = rdd2.mapToPair(new PairFunction&lt;String, String, Integer&gt;() {
            public Tuple2&lt;String, Integer&gt; call(String s) throws Exception {
                return new Tuple2&lt;String, Integer&gt;(s,1);
            }
        });

        //reduce化简
        JavaPairRDD&lt;String,Integer&gt; rdd4 = rdd3.reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() {
            public Integer call(Integer v1, Integer v2) throws Exception {
                return v1 + v2;
            }
        });

        //
        List&lt;Tuple2&lt;String,Integer&gt;&gt; list = rdd4.collect();
        for(Tuple2&lt;String, Integer&gt; t : list){
            System.out.println(t._1() + &quot; : &quot; + t._2());
        }
    }
}
</code></pre><p>Spark2.1.0最新版是基于Scala2.11.8版本，因此安装scala2.11.8版本，</p>
<h2 id="否则如果基于2-12-0版本编译会出现找不到包的问题。"><a href="#否则如果基于2-12-0版本编译会出现找不到包的问题。" class="headerlink" title="否则如果基于2.12.0版本编译会出现找不到包的问题。"></a>否则如果基于2.12.0版本编译会出现找不到包的问题。</h2><pre><code>1.卸载原来的scala.
2.重新安装scala2.11.8版本
3.配置idea的全局库
    project settings -&gt; global library -&gt; 删除原来的scala sdk
    project settings -&gt; global library -&gt; 添加sdk -&gt; browser -&gt; 定位scala安装目录 -&gt;选中scala-compiler.jar + 
                                                                                        scala-library.jar + 
                                                                                        scala-reflect.jar

4.在模块中添加scala sdk 2.11.8版本

5.重新编译项目 -&gt; 导入jar -&gt;丢到集群运行。
</code></pre><h2 id="提交作业到spark集群运行"><a href="#提交作业到spark集群运行" class="headerlink" title="提交作业到spark集群运行"></a>提交作业到spark集群运行</h2><pre><code>1.导出jar包
2.spark-submit提交命令运行job
    //Scala版本
    $&gt;spark-submit --master local --name MyWordCount --class com.it18zhang.spark.scala.WordCountScala SparkDemo1-1.0-SNAPSHOT.jar /home/centos/test.txt
    //java版
    $&gt;spark-submit --master local --name MyWordCount --class com.it18zhang.spark.java.WordCountJava SparkDemo1-1.0-SNAPSHOT.jar /home/centos/test.txt
</code></pre><h2 id="Spark集群模式"><a href="#Spark集群模式" class="headerlink" title="Spark集群模式"></a>Spark集群模式</h2><pre><code>1.local
    nothing!
    spark-shell --master local;        //默认

2.standalone
    独立模式。
    a)复制spark安装目录到其他几个主机
    b)配置其他主机的所有环境变量
        [/etc/profile]
        SPARK_HOME
        PATH

    c)配置master节点的slaves
        [/soft/spark/conf/slaves]文件中添加下面的
        s202
        s203
        s204

    d)启动spark集群，和hadoop一样，但是要指定目录下即可。这个是独立模式和没有关系
        /soft/spark/sbin/start-all.sh

    e)查看进程
        $&gt;xcall.jps jps
            master        //s201
            worker        //s202
            worker        //s203
            worker        //s204
    e)webui，然后验证webui，8080端口。本地模式是4040端口
        http://s201:8080/
</code></pre><h2 id="提交作业jar到完全分布式spark集群"><a href="#提交作业jar到完全分布式spark集群" class="headerlink" title="提交作业jar到完全分布式spark集群"></a>提交作业jar到完全分布式spark集群</h2><pre><code>1.需要启动hadoop集群(只需要hdfs，是standalone模式，不需要yarn调度)
    $&gt;start-dfs.sh
2.put文件到hdfs.

3.运行spark-submit
    $&gt;spark-submit 
                --master spark://s201:7077 
                --name MyWordCount 
                --class com.it18zhang.spark.scala.WordCountScala 
                SparkDemo1-1.0-SNAPSHOT.jar 
                hdfs://s201:8020/user/centos/test.txt
</code></pre><h2 id="脚本分析"><a href="#脚本分析" class="headerlink" title="脚本分析"></a>脚本分析</h2><pre><code>[start-all.sh]
    sbin/spark-config.sh
    sbin/spark-master.sh        //启动master进程
    sbin/spark-slaves.sh        //启动worker进程

[start-master.sh]
    sbin/spark-config.sh
    org.apache.spark.deploy.master.Master
    spark-daemon.sh start org.apache.spark.deploy.master.Master --host --port --webui-port ...

[spark-slaves.sh]
    sbin/spark-config.sh
    slaves.sh                //主要循环这个文件conf/slaves

[slaves.sh]
    for conf/slaves{
        ssh host start-slave.sh ...
    }

[start-slave.sh]
</code></pre><p>先找到这个work类<br>        CLASS=”org.apache.spark.deploy.worker.Worker”<br>        sbin/spark-config.sh 走这个配置<br>        for ((  .. )) ; do<br>            start_instance $(( 1 + $i )) “$@”<br>        done </p>
<pre><code>$&gt;cd /soft/spark/sbin
$&gt;./stop-all.sh                //停掉整个spark集群.
$&gt;./start-master.sh            //停掉整个spark集群.
$&gt;./start-master.sh            //启动master节点
$&gt;./start-slaves.sh            //启动所有worker节点
从s204里面启动某一个slave ./start-slave.sh spark://s201:7077


对上面几个命令了解可以通过 start-master.sh --help来查询，其他类似
</code></pre>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2019/03/04/编程题/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/04/编程题/" itemprop="url">编程题</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-04T14:40:20+08:00">
                2019-03-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>public class Problem3 {<br>    public static void main1(String[] args) {<br>        int[] arr = new int[]{2, 3, 1, 0, 2, 5, 3};<br>        int[] ints = new int[10];<br>        boolean duplicate = duplicate(arr, 6, ints);<br>        System.out.println(ints[0]);</p>
<pre><code>    System.out.println();
}


public static boolean duplicate(int numbers[], int length, int[] duplication) {
    if (numbers == null || length &lt;= 0)
        return false;
    for (int a : numbers) {
        if (a &lt; 0 || a &gt;= length)
            return false;
    }

    int temp;
    for (int i = 0; i &lt; length; i++) {
        while (numbers[i] != i) {
            if (numbers[numbers[i]] == numbers[i]) {
                duplication[0] = numbers[i];
                return true;
            }
            temp = numbers[i];
            numbers[i] = numbers[temp];
            numbers[temp] = temp;
        }
    }
    return false;
}
</code></pre><p>//另一种方法<br>    /**</p>
<pre><code> * 避免使用辅助空间
 */
public int getDuplication(int[] arr)
{
    for(int i = 0;i &lt; arr.length;i++)
    {
        if(arr[i] &lt; 0 || arr[i] &gt;= arr.length)
            throw new IllegalArgumentException(&quot;输入参数不合法&quot;);
    }
    int start = 0;
    int end = arr.length-1;
    int flag = 0;
    int middle = 0;
    while(end &gt;= start)
    {
        if(flag == 0)
            middle = (end + start)/2;
        int count = countRange(arr,start,middle);
        if(end == start)
        {
            if(count &gt; 1)
                return start;
            else
                break;
        }
        if(count &gt; (middle-start+1))//说明(start,middle)这个区间有重复的数
        {
            end = middle;
            flag = 0;
        }else if(count == (middle-start+1))//不能判断(start,middle)这个区间有重复的数
        {
            middle = middle - 1;
            if(middle &lt; start)//说明(start,middle)这个区间没有重复的数
            {
                start = (start+end)/2 + 1;
                flag = 0;
            }else
                flag = 1;
        }else //说明(middle+1,end)这个区间有重复的数
        {
            start = middle + 1;
            flag = 0;
        }
    }
    return -1;
}

private int countRange(int[] arr, int start, int end)
{
    int count = 0;
    for(int i = 0;i &lt; arr.length;i++)
    {
        if(arr[i] &gt;= start &amp;&amp; arr[i] &lt;= end)
            ++count;
    }
    return count;
}

public  static void main(String[] args) {

    Problem3 test = new Problem3();
    int[] arr = {0,3,5,4,1,2,6,7};
    int value = test.getDuplication(arr);
}
</code></pre><p>}</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2019/01/28/剑指offer第45题/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/28/剑指offer第45题/" itemprop="url">剑指offer第45题</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-01-28T21:12:17+08:00">
                2019-01-28
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>关于compare排序的讲解：<br><a href="https://blog.csdn.net/lx_nhs/article/details/78871295" target="_blank" rel="noopener">https://blog.csdn.net/lx_nhs/article/details/78871295</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2019/01/15/实战大数据电信项目面试总结/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/15/实战大数据电信项目面试总结/" itemprop="url">实战大数据电信项目面试总结</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-01-15T22:27:10+08:00">
                2019-01-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><img src="https://i.imgur.com/GR90bE7.png" alt=""></p>
<p>4.Hadoop以及HBase的HA集群配置与实战。<br>  hadoop的使用QJM的高可用架构配置讲解，ResourceManager的高可用架构配置讲解。<br>  zookeeper的工作原理以及配置、实操演练，hbase与Hadoop HA集成注意事项以及客户端<br>  API编程细节处理。</p>
<h2 id="hadoop-HDFS-HA高可用配置也就是实现2个namenode一个active一个standby，利用zk实现自动容灾"><a href="#hadoop-HDFS-HA高可用配置也就是实现2个namenode一个active一个standby，利用zk实现自动容灾" class="headerlink" title="hadoop HDFS HA高可用配置也就是实现2个namenode一个active一个standby，利用zk实现自动容灾"></a>hadoop HDFS HA高可用配置也就是实现2个namenode一个active一个standby，利用zk实现自动容灾</h2><p>配置hdfs-site和core-site.xml指定zk地址。</p>
<pre><code>自动容灾引入两个组件，zk quarum + zk容灾控制器(ZKFC)。



运行NN的主机还要运行ZKFC进程，主要负责:
a.健康监控
b.session管理
c.选举
</code></pre><h2 id="resourcemanager自动容灾配置"><a href="#resourcemanager自动容灾配置" class="headerlink" title="resourcemanager自动容灾配置"></a>resourcemanager自动容灾配置</h2><p>对yarn-site.xml进行配置配置自动容灾并且将zk地址配制进去。</p>
<h2 id="部署zk集群"><a href="#部署zk集群" class="headerlink" title="部署zk集群"></a>部署zk集群</h2><p>1、配置zk配置文件</p>
<p> [/soft/zk/conf/zoo.cfg]<br>    …<br>    dataDir=/home/centos/zookeeper</p>
<pre><code>server.1=s201:2888:3888
server.2=s202:2888:3888 
server.3=s203:2888:3888
</code></pre><p>2、在每台主机的/home/centos/zookeeper中添加myid,内容分别是1,2,3</p>
<h2 id="配置HBase和Hbase高可用"><a href="#配置HBase和Hbase高可用" class="headerlink" title="配置HBase和Hbase高可用"></a>配置HBase和Hbase高可用</h2><h3 id="安装配置"><a href="#安装配置" class="headerlink" title="安装配置"></a>安装配置</h3><p>1、安装后配置环境后在修改配置文件[hbase/conf/hbase-env.sh]，添加hbase在hdfs存放路径、zk地址、zk本地目录、使用完全分布式true。</p>
<p>2、配置regionservers</p>
<pre><code>[hbase/conf/regionservers]
s202
s203
s204
</code></pre><h3 id="高可用"><a href="#高可用" class="headerlink" title="高可用"></a>高可用</h3><p>直接在2台机器上执行命令：</p>
<p>hbase-daemon.sh start master</p>
<h2 id="配置Kafka集群"><a href="#配置Kafka集群" class="headerlink" title="配置Kafka集群"></a>配置Kafka集群</h2><p>1、配置kafka<br>    [kafka/config/server.properties]<br>    …<br>    broker.id=202<br>    …<br>    listeners=PLAINTEXT://:9092<br>    …<br>    log.dirs=/home/centos/kafka/logs<br>    …<br>    zookeeper.connect=s201:2181,s202:2181,s203:2181</p>
<p>2、分发server.properties，同时修改每个文件的broker.id</p>
<h2 id="配置Flume"><a href="#配置Flume" class="headerlink" title="配置Flume"></a>配置Flume</h2><p>配置flume将其配置文件修改为source为某个文件夹，然后sink为kafka集群</p>
<hr>
<h2 id="创建hbase名字空间-表"><a href="#创建hbase名字空间-表" class="headerlink" title="创建hbase名字空间+表"></a>创建hbase名字空间+表</h2><pre><code>1.创建名字空间和表。表是：名字空间+列族。
    $&gt;hbase shell                        //进入hbase shell
    $hbase&gt;create_namespace &apos;ns1&apos;        //创建空间
    $hbase&gt;create &apos;ns1:calllogs&apos; , &apos;f1&apos;    //创建表

    $hbase&gt;truncate &apos;ns1:calllogs&apos;        //重建表
</code></pre><h2 id="创建kafka消费者，订阅calllog主题"><a href="#创建kafka消费者，订阅calllog主题" class="headerlink" title="创建kafka消费者，订阅calllog主题"></a>创建kafka消费者，订阅calllog主题</h2><pre><code>1.设计rowkey
    业务数据: caller , callee , date , duration

    分区号    号码     时间,标记  (对方号码),时长
    regionNo, caller,date , flag,callee,duration

    caller(11) + 通话时间(201701) = 
    (后四位) + 201701 = % 100
</code></pre><p>对日志信息进行整理，删除多余的部分如/:之类的<br>解析日志数据将日志数据截取串，实例化put对象，插入Hbase中去。</p>
<p>打成jar包放到classpath之中，将kafka消费者打成Jar包，并且，用maven在windows将其的依赖都下载下来放到classpath当中。</p>
<p>将这个文件夹都放置到Linux下，然后写一个执行这个jar包的脚本：</p>
<pre><code>java -cp CallLogConsumerModule.jar;./lib/activation-1.1.jar;./lib/apacheds-i18n-2.0.0-M15.jar;./lib/apacheds-kerberos-codec-2.0.0-M15.jar;./lib/api-asn1-api-1.0.0-M20.jar;./lib/api-util-1.0.0-M20.jar;./lib/avro-1.7.4.jar;./lib/commons-beanutils-1.7.0.jar;./lib/commons-beanutils-core-1.8.0.jar;./lib/commons-cli-1.2.jar;./lib/commons-codec-1.9.jar;./lib/commons-collections-3.2.2.jar;./lib/commons-compress-1.4.1.jar;./lib/commons-configuration-1.6.jar;./lib/commons-digester-1.8.jar;./lib/commons-el-1.0.jar;./lib/commons-httpclient-3.1.jar;./lib/commons-io-2.4.jar;./lib/commons-lang-2.6.jar;./lib/commons-logging-1.2.jar;./lib/commons-math3-3.1.1.jar;./lib/commons-net-3.1.jar;./lib/findbugs-annotations-1.3.9-1.jar;./lib/guava-12.0.1.jar;./lib/hadoop-annotations-2.5.1.jar;./lib/hadoop-auth-2.5.1.jar;./lib/hadoop-common-2.5.1.jar;./lib/hadoop-mapreduce-client-core-2.5.1.jar;./lib/hadoop-yarn-api-2.5.1.jar;./lib/hadoop-yarn-common-2.5.1.jar;./lib/hamcrest-core-1.3.jar;./lib/hbase-annotations-1.2.4.jar;./lib/hbase-client-1.2.4.jar;./lib/hbase-common-1.2.4.jar;./lib/hbase-protocol-1.2.4.jar;./lib/htrace-core-3.1.0-incubating.jar;./lib/httpclient-4.2.5.jar;./lib/httpcore-4.2.4.jar;./lib/jackson-core-asl-1.9.13.jar;./lib/jackson-mapper-asl-1.9.13.jar;./lib/jaxb-api-2.2.2.jar;./lib/jcodings-1.0.8.jar;./lib/jdk.tools-1.6.jar;./lib/jetty-util-6.1.26.jar;./lib/jline-0.9.94.jar;./lib/joni-2.1.2.jar;./lib/jopt-simple-4.9.jar;./lib/jsch-0.1.42.jar;./lib/jsr305-1.3.9.jar;./lib/junit-4.12.jar;./lib/kafka-clients-0.10.0.1.jar;./lib/kafka_2.11-0.10.0.1.jar;./lib/log4j-1.2.15.jar;./lib/lz4-1.3.0.jar;./lib/mail-1.4.jar;./lib/metrics-core-2.2.0.jar;./lib/netty-3.7.0.Final.jar;./lib/netty-all-4.0.23.Final.jar;./lib/paranamer-2.3.jar;./lib/protobuf-java-2.5.0.jar;./lib/scala-library-2.11.8.jar;./lib/scala-parser-combinators_2.11-1.0.4.jar;./lib/slf4j-api-1.6.1.jar;./lib/slf4j-log4j12-1.7.21.jar;./lib/snappy-java-1.1.2.6.jar;./lib/stax-api-1.0-2.jar;./lib/xmlenc-0.52.jar;./lib/xz-1.0.jar;./lib/zkclient-0.8.jar;./lib/zookeeper-3.4.6.jar com.it18zhang.calllog.consumer.HbaseConsumer
</code></pre><p>执行kafak消费者</p>
<h2 id="业务场景一：输入电话号码查询所有该用户所有通话记录（利用Hbase查询）"><a href="#业务场景一：输入电话号码查询所有该用户所有通话记录（利用Hbase查询）" class="headerlink" title="业务场景一：输入电话号码查询所有该用户所有通话记录（利用Hbase查询）"></a>业务场景一：输入电话号码查询所有该用户所有通话记录（利用Hbase查询）</h2><p>通过在service中建立和hbase的连接，通过使用scan对象，将查询的内容填充到实体类中，将实体类添加到list当中去，然后返回list，在controller层调用service层的findall方法。返回给前段，前段通过jsp界面，将返回的数据填充到表格中去。</p>
<h2 id="业务场景二：根据主叫和开始时间年月，结束时间年月来查询该用户的通话记录包括主叫和被叫-利用hbase查询"><a href="#业务场景二：根据主叫和开始时间年月，结束时间年月来查询该用户的通话记录包括主叫和被叫-利用hbase查询" class="headerlink" title="业务场景二：根据主叫和开始时间年月，结束时间年月来查询该用户的通话记录包括主叫和被叫(利用hbase查询)"></a>业务场景二：根据主叫和开始时间年月，结束时间年月来查询该用户的通话记录包括主叫和被叫(利用hbase查询)</h2><p>这块就有点内容了，首先hbase的rowkey的实际根据实际的业务需要把rowkey设置成reginonumber,callerid calltime(精确到分) flag calleeid duration.为什么设置成这样根据rowkey的设计原则，吧尽可能多的内容设计到rowkey里面，可以直接查询的到。最常用的放到前面，可以通过callerid和time直接查询的到。设计成不同的regionnumber将不同的号码进行类似于分桶可以切割放到不同的服务器里面。</p>
<p>在查询rowkey的时候考虑到根据时间段查询的时候，由于rowkey的设计是哈希号，主叫，时间，被叫，所以哈希号是个根据主叫+时间（年月）来设计的，这块根据主叫+年月的设计是因为要是淡出根据手机号来哈希的话大多数都是138值类的会造成热点问题。所以根据时间+手机号来设计，然后搜易在根据时间来查找的时候，要考虑起止时间是否是同年月，不同年月，如果是同年月的话就是说在同一个哈希号之间，开始时间就是开始时间，结束时间是day+1，前包后不包，如果是不同年月的话就在不同的区号之间，比如是2017.3.11-2017.5.9就要搜索2017.3.11-2017.4,2017.4-2017.5,2017.5-2017,5,9</p>
<p>要取到callerid+年月作为hash的值而不能取到时，分作为哈希的值，因为取到时，分的话就每个都是不一样的哈希的值，查询的时候就要根据时分来查询而不能根据年月来查询。因为每次查询的时候要指定callerid和年月。如果是根据callerid+时分来计算的话，每次存储的时候即便是同一天同一个月也不会分散的同一个哈希区域，而是分散到不同的区域，难以查询到，如果是这样只能通过每一分钟每一分钟的查询，没法查询了。</p>
<p>由于在输入数据的时候只是插入主叫的数据，在查询通话详单的时候要查询该号码即是主叫，又是被叫的情况下，在rowkey是rno,callerid,calltime,calleeid,calleeid,duration的情况下要查询位置在后面的calleeid的情况下，几乎要全表扫描。所以这块我们在每次put进来数据的时候我们使用协处理器，每次添加主叫的时候，在添加一个被叫。这个被叫的Rowkey是哈希+被叫+时间+1+主叫+时长。然后在put这个被叫的时候区域号也就是rno还是根据主叫+时间片来哈希的。就可以让被叫信息和主叫信息在同一个哈希区域内，然后插入flag=1的rowkey，然后他的value值就是原来主叫表的rowkey的各个值。这样的话避免了将之前主叫表的冗余的value在重复插入一遍。也就是一个二级索引的思想，在协处理器里面重写Postput方法和postScannerNext()方法，postput方法的作用就是在插入一条主叫记录的同时，在插入一条被叫记录，而postgetOp()方法的作用是查询被叫返回主叫信息。在查询的时候用的是scan的API然后查询value值，所以在查询被叫的时候让他返回主叫信息，（这边视频中这边考虑的是如果得到rowkey还需要解析，就算了这样说的），</p>
<h2 id="业务场景三：输入手机号和开始年月，结束年月，查询用户通话记录的主叫被叫实名（利用hiveSQL）"><a href="#业务场景三：输入手机号和开始年月，结束年月，查询用户通话记录的主叫被叫实名（利用hiveSQL）" class="headerlink" title="业务场景三：输入手机号和开始年月，结束年月，查询用户通话记录的主叫被叫实名（利用hiveSQL）"></a>业务场景三：输入手机号和开始年月，结束年月，查询用户通话记录的主叫被叫实名（利用hiveSQL）</h2><p>业务场景是吧人员信息放到一个简单的关系型数据库中，也就是人员信息在公安部的信息中，然后实现一个和关系型数据库的交互查询    。</p>
<h2 id="查询电话号码最近通话记录"><a href="#查询电话号码最近通话记录" class="headerlink" title="查询电话号码最近通话记录"></a>查询电话号码最近通话记录</h2><p>通过完成hive到hbase表的映射，实现对最近通话信息的查询，<br>从现有手段hbase查询，首先rowkey没法确定。rowkey是手机号+年份+月份。因为每个月份的哈希code都不一样，所以只能一个月一个月查。很麻烦。但是我们可以通过hive里面的max()聚集函数查询，借助于mr，也就是用Hive查询。通过hive操纵hbase里面的表，创建一个外部表映射到hbase上去通过Hive的聚集函数通过max min count等聚集函数来查询。</p>
<p><img src="https://i.imgur.com/FFumqvN.png" alt=""></p>
<p>由于hive操作是通过hive和beeline来操作。hive客户端只能本地使用并且不能并发，所以使用hiveserver2服务通过beeline.sh这个脚本，㑨10000，走的是thrift服务器。而ssm是通过ssm里面的service通过jdbc和hiveserver2交互，hiveserver2找到hive外部表并且操作这个是走的jdbc可以实现远程访问。</p>
<p>那么怎样在hive里面查询最近通话详单呢：（最近一条记录）<br>$hive&gt;select * from ext_calllogs_in_hbase where id like ‘%xxxx%’ order by callTime desc limit 1 ;</p>
<p><img src="https://i.imgur.com/7N6lXVy.png" alt=""></p>
<h2 id="业务场景四：查询用户各个月份的通话次数并且以echart柱状图展示（利用HiveSQL）"><a href="#业务场景四：查询用户各个月份的通话次数并且以echart柱状图展示（利用HiveSQL）" class="headerlink" title="业务场景四：查询用户各个月份的通话次数并且以echart柱状图展示（利用HiveSQL）"></a>业务场景四：查询用户各个月份的通话次数并且以echart柱状图展示（利用HiveSQL）</h2><p>hive中查询某个人一年的通话记录按月份进行分组：</p>
<p>select count(*) , substr(calltime,1,6) from ext_calllogs_in_hbase where caller = ‘15032293356’ and substr(calltime,1,4) == ‘2017’ group by substr(calltime,1,6) ;</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2019/01/11/实战大数据电信项目（四）/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/11/实战大数据电信项目（四）/" itemprop="url">实战大数据电信项目（四）</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-01-11T15:56:51+08:00">
                2019-01-11
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>hive统计某个人的通话次数：<br>数据在kafka最多是7天时间。<br>hive中查询某个人一年的通话记录按月份进行分组：<br>select count(*) , substr(calltime,1,6) from ext_calllogs_in_hbase where caller = ‘15032293356’ and substr(calltime,1,4) == ‘2017’ group by substr(calltime,1,6) ;</p>
<p>要写hiveservice，在里面添加</p>
<pre><code>1.HiveCallLogService.java

        /**
         * 查询指定人员指定年份中各个月份的通话次数
         */
        public List&lt;CallLogStat&gt; statCallLogsCount(String caller, String year){
            List&lt;CallLogStat&gt; list = new ArrayList&lt;CallLogStat&gt;() ;
            try {
                Connection conn = DriverManager.getConnection(url);
                Statement st = conn.createStatement();
                String sql = &quot;select count(*) ,substr(calltime,1,6) from ext_calllogs_in_hbase &quot; +
                        &quot;where caller = &apos;&quot; + caller+&quot;&apos; and substr(calltime,1,4) == &apos;&quot; + year
                        + &quot;&apos; group by substr(calltime,1,6);&quot;;
                ResultSet rs = st.executeQuery(sql);
                CallLog log = null;
                while (rs.next()) {
                    CallLogStat logSt = new CallLogStat();
                    logSt.setCount(rs.getInt(1));
                    logSt.setYearMonth(rs.getString(2));
                    list.add(logSt);
                }
                rs.close();
                return list;
            } catch (Exception e) {
                e.printStackTrace();
            }
            return null;
        }
</code></pre><p>2.CallLogController.java</p>
<pre><code>    /**
     * 统计指定人员，指定月份的通话次数
     */
    @RequestMapping(&quot;/callLog/toStatCallLog&quot;)
    public String toStatCallLog(){
        return &quot;callLog/statCallLog&quot; ;
    }

    /**
     * 统计指定人员，指定月份的通话次数
     */
    @RequestMapping(&quot;/callLog/statCallLog&quot;)
    public String statCallLog(Model m ,@RequestParam(&quot;caller&quot;) String caller ,@RequestParam(&quot;year&quot;) String year){
        List&lt;CallLogStat&gt; list = hcs.statCallLogsCount(caller, year);
        m.addAttribute(&quot;stat&quot; , list) ;
        return &quot;callLog/statCallLog&quot; ;
    }

3.statCallLog.jsp

    &lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt;
    &lt;%@ taglib uri=&quot;http://java.sun.com/jsp/jstl/core&quot; prefix=&quot;c&quot; %&gt;
    &lt;html&gt;
    &lt;head&gt;
        &lt;title&gt;通话记录统计结果&lt;/title&gt;
        &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;../css/my.css&quot;&gt;
        &lt;script type=&quot;text/javascript&quot; src=&quot;../js/jquery-3.2.0.min.js&quot;&gt;&lt;/script&gt;
        &lt;script type=&quot;text/javascript&quot;&gt;

            //定义函数
            function refreshTable(){
                $(&quot;#t1 tbody&quot;).empty();
                $.getJSON(&quot;/callLog/json/findAll&quot;, function (data) {
                    $.each(data, function (i, obj) {
                        var str = &quot;&lt;tr&gt;&lt;td&gt;&quot; + obj.caller + &quot;&lt;/td&gt;&quot;;
                        str = str + &quot;&lt;td&gt; &quot; + obj.callerName + &quot;&lt;/td&gt;&quot;;
                        str = str + &quot;&lt;td&gt; &quot; + obj.callee + &quot;&lt;/td&gt;&quot;;
                        str = str + &quot;&lt;td&gt; &quot; + obj.calleeName + &quot;&lt;/td&gt;&quot;;
                        str = str + &quot;&lt;td&gt;&lt;/td&gt;&quot;;
                        str = str + &quot;&lt;td&gt; &quot; + obj.callTime + &quot;&lt;/td&gt;&quot;;
                        str = str + &quot;&lt;td&gt; &quot; + obj.callDuration + &quot;&lt;/td&gt;&quot;;
                        str = str + &quot;&lt;/tr&gt;&quot;;
                        $(&quot;#t1 tbody&quot;).append(str);
                    });
                });
            }

            $(function(){
                setInterval(refreshTable, 2000);
            })
        &lt;/script&gt;
    &lt;/head&gt;
    &lt;body&gt;
        &lt;form action=&apos;&lt;c:url value=&quot;/callLog/statCallLog&quot; /&gt;&apos; method=&quot;post&quot;&gt;
            电话号码 : &lt;input type=&quot;text&quot; name=&quot;caller&quot;&gt;&lt;br&gt;
            年 份:  &lt;input type=&quot;text&quot; name=&quot;year&quot;&gt;&lt;br&gt;
            &lt;input type=&quot;submit&quot; name=&quot;查询&quot;&gt;
        &lt;/form&gt;
        &lt;br&gt;
        &lt;table id=&quot;t1&quot; border=&quot;1px&quot; class=&quot;t-1&quot; style=&quot;width: 800px&quot;&gt;
            &lt;thead&gt;
                &lt;tr&gt;
                    &lt;td&gt;月份&lt;/td&gt;
                    &lt;td&gt;次数&lt;/td&gt;
                &lt;/tr&gt;
            &lt;/thead&gt;
            &lt;tbody&gt;
                &lt;c:forEach items=&quot;${stat}&quot; var=&quot;s&quot;&gt;
                    &lt;tr&gt;
                        &lt;td&gt;&lt;c:out value=&quot;${s.yearMonth}&quot;/&gt;&lt;/td&gt;
                        &lt;td&gt;&lt;c:out value=&quot;${s.count}&quot;/&gt;&lt;/td&gt;
                    &lt;/tr&gt;
                &lt;/c:forEach&gt;
            &lt;/tbody&gt;

        &lt;/table&gt;
    &lt;/body&gt;
    &lt;/html&gt;
</code></pre><h2 id="做了一个xcall-sh和xkill脚本"><a href="#做了一个xcall-sh和xkill脚本" class="headerlink" title="做了一个xcall.sh和xkill脚本"></a>做了一个xcall.sh和xkill脚本</h2><p>[xkill.sh]</p>
<pre><code>#!/bin/bash

pids=`jps | grep $1 | awk &apos;{print $1}&apos;`

for pid in $pids ; do
    kill -9 $pid
done
</code></pre><p>[xcall.sh]</p>
<pre><code>#!/bin/bash
params=$@
i=201
for (( i=201 ; i &lt;= 206 ; i = $i + 1 )) ; do
    tput setaf 2
    echo ============= s$i =============
    tput setaf 7
    ssh -4 s$i &quot;source /etc/profile ; $params&quot;
done
</code></pre><p>//开启kafka集群<br>[/usr/local/bin/xkafka-cluster-start.sh]</p>
<pre><code>#!/bin/bash
servsers=&quot;s202 s203 s204&quot;
for s in $servers ; do
    ssh $s &quot;source /etc/profile ; kafka-server-start.sh -daemon /soft/kakfa/config/server.properties&quot;
done
</code></pre><p>//启动zk集群<br>[/usr/local/bin/xzk-cluster-start.sh]</p>
<pre><code>#!/bin/bash
servers=&quot;s201 s202 s203&quot;
for s in $servers ; do
    ssh $s &quot;source /etc/profile ; zkServer.sh start&quot;
done
</code></pre><p>//xconsumer-start.sh<br>[/usr/local/bin/xconsumer-start.sh]</p>
<pre><code>#!/bin/bash
cd /home/centos/KafkaHbaseConsumer
run.sh &amp;
</code></pre><p>//s201:xflume-calllog-start.sh<br>[/usr/local/bin/xconsumer-start.sh]</p>
<pre><code>#!/bin/bash
cd /soft/flume/conf
flume-ng agent -f calllog.conf -n a1 &amp;
</code></pre><hr>
<p>查询所有用户的各个月份的通话次数     </p>
<p>使用echart实现数据可视化</p>
<p>业务场景：根据电话号码，年份实现每个月的电话次数以echar可视化的图标展示<br>通过在service连接hive服务器，执行Hivesql查询查询到的内容，通过controller层调用返回的内容传到前台。前台jsp界面继承echart和c标签库展示后台查询的内容</p>
<p>在集群中安装ganglia监控集群CPU内存进程监控fulme kafka gendata数据生成进程    </p>
<h2 id="ganglia"><a href="#ganglia" class="headerlink" title="ganglia"></a>ganglia</h2><pre><code>集群监控.
不仅能够监控单个主机的资源情况，还可以对集群整个资源进行统计。
gmond            //在每个节点收集资源数据的。
gmetad            //接受每个节点发送资源数据
gweb            //webui,展示数据web程序，和gmetad通信。
</code></pre><h2 id="安装ganglia"><a href="#安装ganglia" class="headerlink" title="安装ganglia"></a>安装ganglia</h2><pre><code>1.ganglia-gmond
    所有节点。
    $&gt;sudo yum install -y ganglia-gmond

2.ganglia-gmetad
    s201
    $&gt;sudo yum install -y ganglia-gmetad

3.ganglia-gweb
    [s201]
    a)安装依赖
        $&gt;sudo yum install -y httpd php

    b)下载ganglia-web-3.5.12.tar.gz程序
        wget http://ncu.dl.sourceforge.net/project/ganglia/ganglia-web/3.5.12/ganglia-web-3.5.12.tar.gz

    c)tar开文件

    d)修改Makefile文件，执行编译命令sudo make install
        ...

    e)启动服务
        [s201]
        $&gt;sudo service httpd start 
        $&gt;sudo service gmetad start 
        $&gt;sudo service gmond start 

        [s202]
        $&gt;sudo service gmond start 
</code></pre>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2019/01/11/实战大数据电信项目（三）/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/11/实战大数据电信项目（三）/" itemprop="url">实战大数据电信项目（三）</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-01-11T15:56:40+08:00">
                2019-01-11
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>前段请求和hive交互，在hive之中创建外部表，映射到Hbase当中<br>一般hive在远端通过jdbc方式来交互，要想走jdbc协议hive还需要打开hiveserver2，其实就是启动thrift服务器，端口是10000.而ssm是在controller里面，交互service在SSM里面得有一个service。service通过jdbc协议和hive server2交互，转换hive语句，在通过hbase交互。</p>
<p>为什么要用到Hive映射到hbase表：</p>
<h2 id="要想查询用户最近通话信息-，"><a href="#要想查询用户最近通话信息-，" class="headerlink" title="要想查询用户最近通话信息 ##，"></a>要想查询用户最近通话信息 ##，</h2><p>从现有手段hbase查询，首先rowkey没法确定。rowkey是手机号+年份+月份。因为每个月份的哈希code都不一样，所以只能一个月一个月查。很麻烦。但是我们可以通过hive里面的max()聚集函数查询，借助于mr，也就是用Hive查询。通过hive操纵hbase里面的表，创建一个外部表映射到hbase上去通过Hive的聚集函数通过max min count等聚集函数来查询。</p>
<p><img src="https://i.imgur.com/FFumqvN.png" alt=""></p>
<p>由于hive操作是通过hive和beeline来操作。hive客户端只能本地使用并且不能并发，所以使用hiveserver2服务通过beeline.sh这个脚本，㑨10000，走的是thrift服务器。而ssm是通过ssm里面的service通过jdbc和hiveserver2交互，hiveserver2找到hive外部表并且操作这个是走的jdbc可以实现远程访问。</p>
<p>那么怎样在hive里面查询最近通话详单呢：（最近一条记录）<br>$hive&gt;select * from ext_calllogs_in_hbase where id like ‘%xxxx%’ order by callTime desc limit 1 ;</p>
<p><img src="https://i.imgur.com/7N6lXVy.png" alt=""></p>
<p>1.SSm中创建service做一个hive的聚集表的查询，查询最近的通话记录进行Mr查询。加一个pom的hive驱动<br>    @Service(“hiveCallLogService”)<br>    public class HiveCallLogService {</p>
<pre><code>    //hiveserver2连接串
    private static String url = &quot;jdbc:hive2://s201:10000/&quot; ;
    //驱动程序类
    private static String driverClass = &quot;org.apache.hive.jdbc.HiveDriver&quot; ;

    static{
        try {
            Class.forName(driverClass);
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
    /**
     * 查询最近的通话记录,使用hive进行mr查询.
     */
    public CallLog findLatestCallLog(String phoneNum){
        try {
            Connection conn = DriverManager.getConnection(url);
            Statement st = conn.createStatement();
            String sql = &quot;select * from ext_calllogs_in_hbase where id like &apos;%&quot;+ phoneNum+&quot;%&apos; order by callTime desc limit 1&quot; ;
            ResultSet rs = st.executeQuery(sql);
            CallLog log = null ;
            if(rs.next()){
                log = new CallLog();
                log.setCaller(rs.getString(&quot;caller&quot;));
                log.setCallee(rs.getString(&quot;caller&quot;));
                log.setCallTime(rs.getString(&quot;callTime&quot;));
                log.setCallDuration(rs.getString(&quot;callDuration&quot;));
            }
            rs.close();
            return log ;
        } catch (Exception e) {
             e.printStackTrace();
        }
        return null ;
    }
}
</code></pre><p>在controller层添加内容：</p>
<pre><code>/**
     * 查询最近通话记录
     */
    @RequestMapping(value = &quot;/callLog/findLatestCallLog&quot;,method = RequestMethod.POST)
    public String findLatestCallLog(Model m , @RequestParam(&quot;caller&quot;) String caller){
        CallLog log = hcs.findLatestCallLog(caller);
        if(log != null){
            m.addAttribute(&quot;log&quot;, log);
        }
        return &quot;callLog/latestCallLog&quot; ;
    }
    /**
     * 查询最近通话记录
     */
    @RequestMapping(value = &quot;/callLog/toFindLatestCallLog&quot;)
    public String toFindLatestCallLog(){
        return &quot;callLog/findLatestCallLog&quot; ;
    }
}
</code></pre><p>jsp界面编写：</p>
<pre><code>&lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt;
&lt;%@ taglib uri=&quot;http://java.sun.com/jsp/jstl/core&quot; prefix=&quot;c&quot; %&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;title&gt;通话记录&lt;/title&gt;
    &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;../css/my.css&quot;&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;c:if test=&quot;${log == null}&quot;&gt;
        无记录！
    &lt;/c:if&gt;
    &lt;c:if test=&quot;${log != null}&quot;&gt;
        &lt;table id=&quot;t1&quot; border=&quot;1px&quot; class=&quot;t-1&quot; style=&quot;width: 800px&quot;&gt;
            &lt;tr&gt;
                &lt;td&gt;电话1&lt;/td&gt;
                &lt;td&gt;&lt;c:out value=&quot;${log.caller}&quot; /&gt;&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr&gt;
                &lt;td&gt;电话2&lt;/td&gt;
                &lt;td&gt;&lt;c:out value=&quot;${log.callee}&quot;/&gt;&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr&gt;
                &lt;td&gt;时间&lt;/td&gt;
                &lt;td&gt;&lt;c:out value=&quot;${log.callTime}&quot;/&gt;&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr&gt;
                &lt;td&gt;时长&lt;/td&gt;
                &lt;td&gt;&lt;c:out value=&quot;${log.callDuration}&quot;/&gt;&lt;/td&gt;
            &lt;/tr&gt;

        &lt;/table&gt;
    &lt;/c:if&gt;

&lt;/body&gt;
&lt;/html&gt;    
</code></pre><p>在中间有一个报错问题：<br><img src="https://i.imgur.com/qyHtkg0.png" alt=""><br><img src="https://i.imgur.com/V9gqUmv.png" alt=""><br><img src="https://i.imgur.com/QCKt99s.png" alt=""><br>解决办法是更新继承的tomcat倒9。0版本。然后hive的依赖百年城1.2.1</p>
<h2 id="然后做一个增加人员信息的内容：然后完成一个人员信息查询在界面显示的内容"><a href="#然后做一个增加人员信息的内容：然后完成一个人员信息查询在界面显示的内容" class="headerlink" title="然后做一个增加人员信息的内容：然后完成一个人员信息查询在界面显示的内容"></a>然后做一个增加人员信息的内容：然后完成一个人员信息查询在界面显示的内容</h2><p>业务场景是吧人员信息放到一个简单的关系型数据库中，也就是人员信息在公安部的信息中，然后实现一个和关系型数据库的交互查询    </p>
<pre><code>1.建表
create table persons(id ...) ;

2.domain
public class Person {
    private Integer id ;
    private String name ;
    private String phone  ;
    ...
}

3.dao

4.service

5.
</code></pre><p>//在查询之中集成MYSQL的查询到的内容，整理一下下就是写mapper的SQL语句，在DAO层getSqlSession().selectOne(“persons.selectNameByPhone”,phone)通过这句话来返回，然后service层调用这个方法，然后在到personserviceimpl中调用这个方法，然后在calllogseriviceimpl中调用这个方法返回内容填充到实体类里面，controller层调用这些方法然后在jsp文件里面进行一个内容的填充。<br>——————</p>
<h2 id="MR运行参数配置，关闭物理内存和虚拟内存对容器的限制"><a href="#MR运行参数配置，关闭物理内存和虚拟内存对容器的限制" class="headerlink" title="MR运行参数配置，关闭物理内存和虚拟内存对容器的限制"></a>MR运行参数配置，关闭物理内存和虚拟内存对容器的限制</h2><pre><code>默认限制是开启的，最多分配给容器8G的物理内存，虚拟内存是物理内存的2.1倍。
[yarn-site.xml]
&lt;property&gt;
    &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;
    &lt;value&gt;8192&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
    &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
    &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
    &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt;
    &lt;value&gt;2.1&lt;/value&gt;
&lt;/property&gt;
</code></pre><hr>
<p>0310然后实现一个生成带名字的数据源的代码。然后要做一个局部刷新的fastjson。<br>0311实现一个通过服务器返回给客户端json格式的内容，通过集成jQuery实现ajax访问，动态刷新通话记录。</p>
<p>启动顺序，先启动kafka，启动kafka消费者，启动flume，启动ssm，启动日志生成程序。.<br>flume-ng agent -f calllog.conf -n a1在s201中<br>kafka-server-start.sh -daemon /soft/kafka/config/server.properties启动3个<br>启动kafka消费者：kafka-console-consumer.sh –zookeeper s202:2181 –topic calllog<br>开启hadoop集群和hbase集群</p>
<p>真机使用：<br>10个kafka3台zk我们是1，2，3是zk。3台zk里面分配好myid。配置好zoo.cfg里面配置好3台主机的Ip地址<br>kafka配置配置kafka/config/server.properties 。kafka依赖于zk在kafka的配置文件里面配置zk的ip<br>单独安装flume配置配置文件。配置source channel sink<br>一边生成日志一边搜集。</p>
<hr>
<h2 id="协处理器-批处理。"><a href="#协处理器-批处理。" class="headerlink" title="协处理器:批处理。"></a>协处理器:批处理。</h2><pre><code>1.类似于触发器。
    完成被叫日志的写入过程。
2.重写postPut()/postGetOp()/postScannNext();
    put / get / scann
    直接返回主叫。
</code></pre><h2 id="按时间段查询通话记录"><a href="#按时间段查询通话记录" class="headerlink" title="按时间段查询通话记录"></a>按时间段查询通话记录</h2><pre><code>hashcode            //确定分区。100
</code></pre><h2 id="用户最近的通话信息"><a href="#用户最近的通话信息" class="headerlink" title="用户最近的通话信息"></a>用户最近的通话信息</h2><pre><code>hbase:rowkey

max()聚集函数。
</code></pre><h2 id="mr-hive"><a href="#mr-hive" class="headerlink" title="mr:hive"></a>mr:hive</h2><pre><code>MapReduce.
</code></pre><h2 id="用户最近的通话信息-1"><a href="#用户最近的通话信息-1" class="headerlink" title="用户最近的通话信息"></a>用户最近的通话信息</h2><pre><code>1.启动hadoop的yarn集群
    [s201]
    $&gt;start-yarn.sh

    [s206]
    $&gt;yarn-daemon.sh start resourcemanager

    [验证]
    http://s201:8088/

2.初始化hive

    $&gt;cd /soft/hive/bin
    $&gt;./schemaTool -dbType mysql -initSchema

    $&gt;hive            //进入hive的shell
    $hive&gt;create database mydb ;
    $hive&gt;use mydb ;
    $hive&gt;create external table ext_calllogs_in_hbase(id string, caller string,callTime string,callee string,callDuration string) STORED BY &apos;org.apache.hadoop.hive.hbase.HBaseStorageHandler&apos; WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,f1:caller,f1:callTime,f1:callee,f1:callDuration&quot;) TBLPROPERTIES (&quot;hbase.table.name&quot; = &quot;ns1:calllogs&quot;);

    $hive&gt;select * from ext_calllogs_in_hbase where id like &apos;%xxxx%&apos; order by callTime desc limit 1 ;
    $hive&gt;select * from ext_calllogs_in_hbase where callTime = (select max(tt.callTime) from ext_calllogs_in_hbase tt where tt.id like &apos;%xxx%&apos;);

3.ssm中创建service，查询hive表中数据。
    a.增加依赖
        [pom.xml]
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;
            &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt;
            &lt;version&gt;1.2.1&lt;/version&gt;
        &lt;/dependency&gt;

    b.编写类
        package com.it18zhang.ssm.hive;

        import com.it18zhang.ssm.domain.CallLog;

        import java.sql.Connection;
        import java.sql.DriverManager;

        /**
         * Created by Administrator on 2017/4/14.
         */
        public class HiveCallLogService {

            //hiveserver2连接串
            private static String url = &quot;jdbc:hive2://s201:10000/mydb&quot; ;
            //驱动程序类
            private static String driverClass = &quot;org.apache.hive.jdbc.HiveDriver&quot; ;

            static{
                try {
                    Class.forName(driverClass);
                } catch (Exception e) {
                    e.printStackTrace();
                }
            }
            /**
             * 查询最近的通话记录,使用hive进行mr查询.
             */
            public CallLog findLatestCallLog(){
                try {
                    Connection conn = DriverManager.getConnection(url);
                    System.out.println(conn);
                } catch (Exception e) {
                    e.printStackTrace();
                }
                return null ;
            }
        }

    c.启动hiveserver2服务器
        $&gt;hive/bin/hiveserver2 &amp;

    d.验证hiveserver2端口
        $&gt;netstat -anop | grep 10000

4.测试类
    package com.it18zhang.ssm.hive;

    import com.it18zhang.ssm.domain.CallLog;
    import org.apache.hadoop.hbase.client.Result;

    import java.sql.Connection;
    import java.sql.DriverManager;
    import java.sql.ResultSet;
    import java.sql.Statement;

    /**
     *
     */
    public class HiveCallLogService {

        //hiveserver2连接串
        private static String url = &quot;jdbc:hive2://s201:10000/&quot; ;
        //驱动程序类
        private static String driverClass = &quot;org.apache.hive.jdbc.HiveDriver&quot; ;

        static{
            try {
                Class.forName(driverClass);
            } catch (Exception e) {
                e.printStackTrace();
            }
        }
        /**
         * 查询最近的通话记录,使用hive进行mr查询.
         */
        public CallLog findLatestCallLog(){
            try {
                Connection conn = DriverManager.getConnection(url);
                Statement st = conn.createStatement();
                ResultSet rs = st.executeQuery(&quot;select * from ext_calllogs_in_hbase&quot;);
                while(rs.next()){
                    String id = rs.getString(&quot;id&quot;);
                    String caller = rs.getString(&quot;caller&quot;);
                    String callee = rs.getString(&quot;callee&quot;);
                    String callTime = rs.getString(&quot;callTime&quot;);
                    String callDuration = rs.getString(&quot;callDuration&quot;);

                    System.out.println(id + &quot; :  &quot; + caller);
                }
                rs.close();
                System.out.println(conn);
            } catch (Exception e) {
                e.printStackTrace();
            }
            return null ;
        }
    }

5.注意事项
    SSM集成hive-jdbc访问hive的hiveserver2时，需要如下处理:

    5.1)使用hive-jdbc-1.2.1的依赖版本
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;
            &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt;
            &lt;version&gt;1.2.1&lt;/version&gt;
        &lt;/dependency&gt;

    5.5)需要集成apache-tomcat-9.0.0.M19版本，否则报编译器错误。
</code></pre><h2 id="添加人员信息"><a href="#添加人员信息" class="headerlink" title="添加人员信息"></a>添加人员信息</h2><pre><code>1.建表
create table persons(id ...) ;

2.domain
public class Person {
    private Integer id ;
    private String name ;
    private String phone  ;
    ...
}

3.dao

4.service

5.
</code></pre><h2 id="MR运行参数配置，关闭物理内存和虚拟内存对容器的限制-1"><a href="#MR运行参数配置，关闭物理内存和虚拟内存对容器的限制-1" class="headerlink" title="MR运行参数配置，关闭物理内存和虚拟内存对容器的限制"></a>MR运行参数配置，关闭物理内存和虚拟内存对容器的限制</h2><pre><code>默认限制是开启的，最多分配给容器8G的物理内存，虚拟内存是物理内存的2.1倍。
[yarn-site.xml]
&lt;property&gt;
    &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;
    &lt;value&gt;8192&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
    &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
    &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
    &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt;
    &lt;value&gt;2.1&lt;/value&gt;
&lt;/property&gt;
</code></pre><h2 id="实现局部实时刷新通话记录的功能"><a href="#实现局部实时刷新通话记录的功能" class="headerlink" title="实现局部实时刷新通话记录的功能"></a>实现局部实时刷新通话记录的功能</h2><pre><code>1.引入pom.xml
    &lt;dependency&gt;
        &lt;groupId&gt;com.alibaba&lt;/groupId&gt;
        &lt;artifactId&gt;fastjson&lt;/artifactId&gt;
        &lt;version&gt;1.2.24&lt;/version&gt;
    &lt;/dependency&gt;

2.编写Controller，增加方法
    @RequestMapping(&quot;/callLog/json/findAll&quot;)
    public String findAllJson(HttpServletResponse response) {
        List&lt;CallLog&gt; list = cs.findAll();
        String json = JSON.toJSONString(list);
        //内容类型
        response.setContentType(&quot;application/json&quot;);
        try {
            OutputStream out = response.getOutputStream();
            out.write(json.getBytes());
            out.flush();
            out.close();
        } catch (IOException e) {
            e.printStackTrace();
        }
        return  null;
    }
</code></pre><p>1.启动顺序<br>    a)1.zookeeper<br>    b)2.hadoop<br>    c)3.hbase<br>    d)4.kakfa<br>    e)5.HbaseConsumer<br>    f)6.flume<br>    g)7.web程序<br>    h)8.数据生成程序.<br>2.<br>3.<br>4.<br>5.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2019/01/11/实战大数据电信项目（二）/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/11/实战大数据电信项目（二）/" itemprop="url">实战大数据电信项目（二）</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-01-11T15:56:27+08:00">
                2019-01-11
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>按照时间段查询通话记录：</p>
<p>1.按照时间段查询通话记录，设置startRow+endRow。<br>2.web部分设置一个电话号码输入起始时间输入结束时间输入，然后在SSMweb层controller设置一个查询使用hbaseapi。重点在月份值取出来哈希构造rowkey。</p>
<p>设置一个表单的查询的jsp界面，action是动态嵌套打印的用url标识库，提交给/callLog/finCallLog通过Post方式，返回输入值起始时间和结束时间到controller层接受参数，</p>
<p>传值到controller层之后，controller层拿到了参数，拿到starttime callerid endtie。</p>
<p>对起始日期和结束日期进行一个calendar.getinstance()，日历这个可以计算天数月份数，有多少东西都可以算出来。</p>
<p>编程：在得到的起始时间和结束时间对2个时间点做操作：得到一个结束的endpoint=开始的前6个数字是年和月+最后2个数字是天+1。</p>
<p>如果是同年月，起始点就是起始串，结束点是天+1的endpoint。<br>    public static List<calllogrange> getCallLogRanges(String startStr ,String endStr){<br>            try{<br>                SimpleDateFormat sdfYMD = new SimpleDateFormat(“yyyyMMdd”);<br>                SimpleDateFormat sdfYM = new SimpleDateFormat(“yyyyMM”);<br>                DecimalFormat df00 = new DecimalFormat(“00”);</calllogrange></p>
<pre><code>            //
            List&lt;CallLogRange&gt; list = new ArrayList&lt;CallLogRange&gt;();
            //字符串时间
            String startPrefix = startStr.substring(0, 6);

            String endPrefix = endStr.substring(0, 6);
            int endDay = Integer.parseInt(endStr.substring(6, 8));
            //结束点
            String endPoint = endPrefix + df00.format(endDay + 1);

            //日历对象
            Calendar c = Calendar.getInstance();

            //同年月
            if (startPrefix.equals(endPrefix)) {
                CallLogRange range = new CallLogRange();
                range.setStartPoint(startStr);          //设置起始点

                range.setEndPoint(endPoint);            //设置结束点
                list.add(range);
            } else {
                //1.起始月
                CallLogRange range = new CallLogRange();
                range.setStartPoint(startStr);

                //设置日历的时间对象
                c.setTime(sdfYMD.parse(startStr));
                c.add(Calendar.MONTH, 1);
                range.setEndPoint(sdfYM.format(c.getTime()));
                list.add(range);

                //是否是最后一月
                while (true) {
                    //到了结束月份
                    if (endStr.startsWith(sdfYM.format(c.getTime()))) {
                        range = new CallLogRange();
                        range.setStartPoint(sdfYM.format(c.getTime()));
                        range.setEndPoint(endPoint);
                        list.add(range);
                        break;
                    } else {
                        range = new CallLogRange();
                        //起始时间
                        range.setStartPoint(sdfYM.format(c.getTime()));

                        //增加月份
                        c.add(Calendar.MONTH, 1);
                        range.setEndPoint(sdfYM.format(c.getTime()));
                        list.add(range);
                    }
                }
            }
            return list ;
        }
        catch(Exception e){
            e.printStackTrace();
        }
        return null ;
    }

    /**
     * 对时间进行格式化
     */
    public static String formatDate(String timeStr){
        try {
            return sdfFriend.format(sdf.parse(timeStr));
        } catch (Exception e) {
            e.printStackTrace();
        }
        return null ;
    }
}
</code></pre><p>上图是工具类时间的代码。</p>
<p>下面代码是service层的具体实现内容：<br>    /**</p>
<pre><code>     * 按照范围查询通话记录
     */
    public List&lt;CallLog&gt; findCallogs(String call , List&lt;CallLogRange&gt; ranges){
        List&lt;CallLog&gt; logs = new ArrayList&lt;CallLog&gt;();
        try {
            for(CallLogRange range : ranges){
                Scan scan = new Scan();
                //设置扫描起始行
                scan.setStartRow(Bytes.toBytes(CallLogUtil.getStartRowkey(call, range.getStartPoint(),100)));
                //设置扫描结束行
                scan.setStopRow(Bytes.toBytes(CallLogUtil.getStopRowkey(call, range.getStartPoint(), range.getEndPoint(),100)));

                ResultScanner rs = table.getScanner(scan);
                Iterator&lt;Result&gt; it = rs.iterator();
                byte[] f = Bytes.toBytes(&quot;f1&quot;);

                byte[] caller = Bytes.toBytes(&quot;caller&quot;);
                byte[] callee = Bytes.toBytes(&quot;callee&quot;);
                byte[] callTime = Bytes.toBytes(&quot;callTime&quot;);
                byte[] callDuration = Bytes.toBytes(&quot;callDuration&quot;);
                CallLog log = null;
                while (it.hasNext()) {
                    log = new CallLog();
                    Result r = it.next();
                    //rowkey
                    String rowkey = Bytes.toString(r.getRow());
                    String flag = rowkey.split(&quot;,&quot;)[3] ;
                    log.setFlag(flag.equals(&quot;0&quot;)?true:false);
                    //caller
                    log.setCaller(Bytes.toString(r.getValue(f, caller)));
                    //callee
                    log.setCallee(Bytes.toString(r.getValue(f, callee)));
                    //callTime
                    log.setCallTime(Bytes.toString(r.getValue(f, callTime)));
                    //callDuration
                    log.setCallDuration(Bytes.toString(r.getValue(f, callDuration)));
                    logs.add(log);
                }
            }
            return logs;
        } catch (Exception e) {
            e.printStackTrace();
        }
        return null;
    }
}
</code></pre><p>在查询的controller层里面写调用工具类里面对时间进行格式化。格式化之后的List集合作为参数传入到service层的findCallogs（）里面查询到hbase内容集合logs，将logs传入Model.addAttribute里面（向模型中传入数据也就是让前台可以调用）。下面是controller层的内容：</p>
<pre><code>    /**
     * 进入查询通话记录的页面,form
     */
    @RequestMapping(&quot;/callLog/toFindCallLogPage&quot;)
    public String toFindCallLogPage(){
        return &quot;callLog/findCallLog&quot; ;
    }

    @RequestMapping(value = &quot;/callLog/findCallLog&quot;,method = RequestMethod.POST)
    public String findCallLog(Model m , @RequestParam(&quot;caller&quot;) String caller, @RequestParam(&quot;startTime&quot;) String startTime, @RequestParam(&quot;endTime&quot;) String endTime){
        List&lt;CallLogRange&gt; list = CallLogUtil.getCallLogRanges(startTime, endTime);
        List&lt;CallLog&gt; logs = cs.findCallogs(caller,list);
        m.addAttribute(&quot;callLogs&quot;, logs);
        return &quot;callLog/callLogList&quot; ;
    }
}
</code></pre><p>上述就完成了主叫查询功能。但是查询的只有主叫功能，没有被叫功能，我们在编写一个协处理器处理被叫查询功能：<br>      /**</p>
<pre><code> * Put后处理
 */
public void postPut(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Put put, WALEdit edit, Durability durability) throws IOException {
    super.postPut(e, put, edit, durability);
    //
    String tableName0 = TableName.valueOf(CALL_LOG_TABLE_NAME).getNameAsString();

    //得到当前的TableName对象
    String tableName1 = e.getEnvironment().getRegion().getRegionInfo().getTable().getNameAsString();
    //判断是否是ns1:calllogs表
    if (!tableName0.equals(tableName1)) {
        return;
    }

    //得到主叫的rowkey,
    String rowkey = Bytes.toString(put.getRow());

    //如果被叫就放行
    String[] arr = rowkey.split(&quot;,&quot;);
    if (arr[3].equals(&quot;1&quot;)) {
        return;
    }
    //hashcode,caller,time,flag,callee,duration
    String caller = arr[1] ;        //主叫
    String callTime = arr[2] ;      //通话时间
    String callee = arr[4] ;        //被叫
    String callDuration = arr[5] ;  //通话时长

    //被叫hashcode
    String hashcode = CallLogUtil.getHashcode(callee,callTime,100);
    //被叫rowkey
    String calleeRowKey = hashcode + &quot;,&quot; + callee + &quot;,&quot; + callTime + &quot;,1,&quot; + caller + &quot;,&quot; + callDuration;
    Put newPut = new Put(Bytes.toBytes(calleeRowKey));
    newPut.addColumn(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(REF_ROW_ID), Bytes.toBytes(rowkey));
    TableName tn = TableName.valueOf(CALL_LOG_TABLE_NAME);
    Table t = e.getEnvironment().getTable(tn);
    t.put(newPut);
}
</code></pre><p>也就是创建协处理器，在postput的方法下（这个方法本来是用于在添加一条主叫信息后在添加一条被叫信息）重写之后，在添加主叫信息后，在添加一条被叫信息，该被叫信息的value值插入到f2列族，refrowid列。然后value值就是 原来主叫的rowkey,这个也就是二级索引的思想，避免了原来的主叫的value重复存储，减少了冗余。所以是重写postput方法。</p>
<p>在没有重写postgetOp()方法的时候返回的是这种情况：</p>
<p><img src="https://i.imgur.com/gB8tQaU.png" alt=""></p>
<p>发现里面是被叫的时候没有电话1和电话2为什么呢？<br>因为在检索时候的是这样子写的：检索填充的值是通过getvalue方法，而通过协处理器填进去的是一个被叫信息是控制，所以get不到value信息。</p>
<p><img src="https://i.imgur.com/0EPdPzM.png" alt=""></p>
<p>但是查询的时候，还是不能查询到主叫，所以重写检索方法,在协处理器中重写    postscannerNext（）方法。完成被叫查询返回主叫的rowkey值。</p>
<pre><code>/**
     *
     */
    public boolean postScannerNext(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, InternalScanner s, List&lt;Result&gt; results, int limit, boolean hasMore) throws IOException {
        boolean b = super.postScannerNext(e, s, results, limit, hasMore);

        //新集合
        List&lt;Result&gt; newList = new ArrayList&lt;Result&gt;();

        //获得表名
        String tableName = e.getEnvironment().getRegion().getRegionInfo().getTable().getNameAsString();

        //判断表名是否是ns1:calllogs
        if (tableName.equals(CALL_LOG_TABLE_NAME)) {
            Table tt = e.getEnvironment().getTable(TableName.valueOf(CALL_LOG_TABLE_NAME));
            for(Result r : results){
                //rowkey
                String rowkey = Bytes.toString(r.getRow());
                String flag = rowkey.split(&quot;,&quot;)[3] ;
                //主叫
                if(flag.equals(&quot;0&quot;)){
                    newList.add(r) ;
                }
                //被叫
                else{
                    //取出主叫号码
                    byte[] refrowkey = r.getValue(Bytes.toBytes(&quot;f2&quot;),Bytes.toBytes(REF_ROW_ID)) ;
                    Get newGet = new Get(refrowkey);
                    newList.add(tt.get(newGet));
                }
            }
            results.clear();
            results.addAll(newList);
        }
        return b ;
    }
}
</code></pre><p><img src="https://i.imgur.com/cJBPeDB.png" alt=""></p>
<pre><code>1.hbase交互
    Scan : 设置startRow + endRow
    rowkey : hashcode , + callerid , + callTime, 0 , callee , duration.
</code></pre><p>2.<br>3.</p>
<h2 id="编写CallLogController-java"><a href="#编写CallLogController-java" class="headerlink" title="编写CallLogController.java"></a>编写CallLogController.java</h2><pre><code>/**
 * 进入查询通话记录的页面,form
 */
@RequestMapping(&quot;/callLog/toFindCallLogPage&quot;)
public String toFindCallLogPage(){
    return &quot;callLog/findCallLog&quot; ;
}

@RequestMapping(value = &quot;/callLog/findCallLog&quot;,method = RequestMethod.POST)
public String findCallLog(@RequestParam(&quot;caller&quot;) String caller, @RequestParam(&quot;startTime&quot;) String startTime, @RequestParam(&quot;endTime&quot;) String endTime){
    Calendar startCalendar = Calendar.getInstance();
    Calendar endCalendar = Calendar.getInstance();

    return &quot;callLog/callLogList&quot; ;
}
</code></pre><h2 id="编写jsp"><a href="#编写jsp" class="headerlink" title="编写jsp"></a>编写jsp</h2><pre><code>&lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt;
&lt;%@ taglib uri=&quot;http://java.sun.com/jsp/jstl/core&quot; prefix=&quot;c&quot; %&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;title&gt;查询通话记录&lt;/title&gt;
    &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;../css/my.css&quot;&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;form action=&apos;&lt;c:url value=&quot;/callLog/findCallLog&quot; /&gt;&apos; method=&quot;post&quot;&gt;
    &lt;table&gt;
        &lt;tr&gt;
            &lt;td&gt;电话号码 :&lt;/td&gt;
            &lt;td&gt;&lt;input type=&quot;text&quot; name=&quot;caller&quot;&gt;&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;起始时间 :&lt;/td&gt;
            &lt;td&gt;&lt;input type=&quot;text&quot; name=&quot;startTime&quot;&gt;&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;结束时间:&lt;/td&gt;
            &lt;td&gt;&lt;input type=&quot;text&quot; name=&quot;endTime&quot;&gt;&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td colspan=&quot;2&quot;&gt;
                &lt;input type=&quot;submit&quot; value=&quot;查询&quot;/&gt;
            &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/table&gt;
&lt;/form&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre><h2 id="实现时间段查询"><a href="#实现时间段查询" class="headerlink" title="实现时间段查询"></a>实现时间段查询</h2><pre><code>1.提取时间范围
    [CallLogUtil.java]
    /**
     * 起始时间
     */
    public static String getStartRowkey(String caller, String startTime, int partitions){
        String hashcode = getHashcode(caller, startTime,partitions);
        return hashcode + &quot;,&quot; + caller + &quot;,&quot; + startTime ;
    }

    /**
     * 结束时间
     */
    public static String getStopRowkey(String caller, String startTime,String endTime, int partitions){
        String hashcode = getHashcode(caller, startTime,partitions);
        return hashcode + &quot;,&quot; + caller + &quot;,&quot; + endTime ;
    }

    /**
     * 计算查询时间范围
     */
    public static List&lt;CallLogRange&gt; getCallLogRanges(String startStr ,String endStr){
        try{
            SimpleDateFormat sdfYMD = new SimpleDateFormat(&quot;yyyyMMdd&quot;);
            SimpleDateFormat sdfYM = new SimpleDateFormat(&quot;yyyyMM&quot;);
            DecimalFormat df00 = new DecimalFormat(&quot;00&quot;);

            //
            List&lt;CallLogRange&gt; list = new ArrayList&lt;CallLogRange&gt;();
            //字符串时间
            String startPrefix = startStr.substring(0, 6);

            String endPrefix = endStr.substring(0, 6);
            int endDay = Integer.parseInt(endStr.substring(6, 8));
            //结束点
            String endPoint = endPrefix + df00.format(endDay + 1);

            //日历对象
            Calendar c = Calendar.getInstance();

            //同年月
            if (startPrefix.equals(endPrefix)) {
                CallLogRange range = new CallLogRange();
                range.setStartPoint(startStr);          //设置起始点

                range.setEndPoint(endPoint);            //设置结束点
                list.add(range);
            } else {
                //1.起始月
                CallLogRange range = new CallLogRange();
                range.setStartPoint(startStr);

                //设置日历的时间对象
                c.setTime(sdfYMD.parse(startStr));
                c.add(Calendar.MONTH, 1);
                range.setEndPoint(sdfYM.format(c.getTime()));
                list.add(range);

                //是否是最后一月
                while (true) {
                    //到了结束月份
                    if (endStr.startsWith(sdfYM.format(c.getTime()))) {
                        range = new CallLogRange();
                        range.setStartPoint(sdfYM.format(c.getTime()));
                        range.setEndPoint(endPoint);
                        list.add(range);
                        break;
                    } else {
                        range = new CallLogRange();
                        //起始时间
                        range.setStartPoint(sdfYM.format(c.getTime()));

                        //增加月份
                        c.add(Calendar.MONTH, 1);
                        range.setEndPoint(sdfYM.format(c.getTime()));
                        list.add(range);
                    }
                }
            }
            return list ;
        }
        catch(Exception e){
            e.printStackTrace();
        }
        return null ;
    }

2.编写service.

3.
4.
</code></pre><h2 id="实现hbase的协处理器"><a href="#实现hbase的协处理器" class="headerlink" title="实现hbase的协处理器"></a>实现hbase的协处理器</h2><pre><code>0.说明
    HBaseConsumer put的数据都是主叫，被叫数据在Coprossor中完成。

1.创建协处理器
    package com.it18zhang.calllog.coprossor;

    import org.apache.hadoop.hbase.TableName;
    import org.apache.hadoop.hbase.client.Durability;
    import org.apache.hadoop.hbase.client.Put;
    import org.apache.hadoop.hbase.client.Table;
    import org.apache.hadoop.hbase.coprocessor.BaseRegionObserver;
    import org.apache.hadoop.hbase.coprocessor.ObserverContext;
    import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;
    import org.apache.hadoop.hbase.regionserver.wal.WALEdit;
    import org.apache.hadoop.hbase.util.Bytes;

    import java.io.IOException;

    /**
     * 协处理器,
     */
    public class CallLogRegionObserver extends BaseRegionObserver {

        /**
         * Put后处理
         */
        public void postPut(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Put put, WALEdit edit, Durability durability) throws IOException {
            super.postPut(e, put, edit, durability);
            //
            String tableName0 = TableName.valueOf(&quot;ns1:calllogs&quot;).getNameAsString();

            //得到当前的TableName对象
            String tableName1 = e.getEnvironment().getRegion().getRegionInfo().getTable().getNameAsString();
            //判断是否是ns1:calllogs表
            if (!tableName0.equals(tableName1)) {
                return;
            }

            //得到主叫的rowkey,
            String rowkey = Bytes.toString(put.getRow());

            //如果被叫就放行
            String[] arr = rowkey.split(&quot;,&quot;);
            if (arr[3].equals(&quot;1&quot;)) {
                return;
            }
            //hashcode,caller,time,flag,callee,duration
            String caller = arr[1] ;        //主叫
            String callTime = arr[2] ;      //通话时间
            String callee = arr[4] ;        //被叫
            String callDuration = arr[5] ;  //通话时长

            //被叫hashcode
            String hashcode = CallLogUtil.getHashcode(callee,callTime,100);
            //被叫rowkey
            String calleeRowKey = hashcode + &quot;,&quot; + callee + &quot;,&quot; + callTime + &quot;,1,&quot; + caller + &quot;,&quot; + callDuration;
            Put newPut = new Put(Bytes.toBytes(calleeRowKey));
            newPut.addColumn(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;refrowid&quot;), Bytes.toBytes(rowkey));
            TableName tn = TableName.valueOf(&quot;ns1:calllogs&quot;);
            Table t = e.getEnvironment().getTable(tn);
            t.put(newPut);
        }
    }

2.注册协处理器
    a)导出jar包,分到集群.
        ...
    b)修改hbase配置文件并分发.
        [hbase-site.xml]
        &lt;property&gt;
                &lt;name&gt;hbase.coprocessor.region.classes&lt;/name&gt;
                &lt;value&gt;com.it18zhang.calllog.coprossor.CallLogRegionObserver&lt;/value&gt;
        &lt;/property&gt;

    c)停止hbase集群

    d)重新启动
        ...
    e)进入hbase shell,重建ns1:calllogs
        $hbase&gt;create &apos;ns1:calllogs&apos; , &apos;f1&apos;,&apos;f2&apos;
</code></pre><h2 id="主叫"><a href="#主叫" class="headerlink" title="主叫"></a>主叫</h2><pre><code>rowkey:
12,1234,xxx,0,5678,60,.,.,.,.,
</code></pre><h2 id="被叫"><a href="#被叫" class="headerlink" title="被叫"></a>被叫</h2><pre><code>rowkey:                        f2:refrowkey
98,5678,xxxx,1,1234,60   --&gt; 12,1234,xxx,0,5678,60,.
</code></pre><h2 id="重写RegionObserver的postGetOp方法-完成被叫查询时，直接返回主叫的记录"><a href="#重写RegionObserver的postGetOp方法-完成被叫查询时，直接返回主叫的记录" class="headerlink" title="重写RegionObserver的postGetOp方法,完成被叫查询时，直接返回主叫的记录"></a>重写RegionObserver的postGetOp方法,完成被叫查询时，直接返回主叫的记录</h2><pre><code>1.重写
[CallLogRegionObserver.java]
package com.it18zhang.calllog.coprossor;

import org.apache.hadoop.hbase.Cell;
import org.apache.hadoop.hbase.CellUtil;
import org.apache.hadoop.hbase.TableName;
import org.apache.hadoop.hbase.client.*;
import org.apache.hadoop.hbase.coprocessor.BaseRegionObserver;
import org.apache.hadoop.hbase.coprocessor.ObserverContext;
import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;
import org.apache.hadoop.hbase.regionserver.InternalScanner;
import org.apache.hadoop.hbase.regionserver.wal.WALEdit;
import org.apache.hadoop.hbase.util.Bytes;

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

/**
 * 协处理器,
 */
public class CallLogRegionObserver extends BaseRegionObserver {

    //被叫引用id
    private static final String REF_ROW_ID = &quot;refrowid&quot; ;
    //通话记录表名
    private static final String CALL_LOG_TABLE_NAME = &quot;ns1:calllogs&quot; ;

    /**
     * Put后处理
     */
    public void postPut(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Put put, WALEdit edit, Durability durability) throws IOException {
        super.postPut(e, put, edit, durability);
        //
        String tableName0 = TableName.valueOf(CALL_LOG_TABLE_NAME).getNameAsString();

        //得到当前的TableName对象
        String tableName1 = e.getEnvironment().getRegion().getRegionInfo().getTable().getNameAsString();
        //判断是否是ns1:calllogs表
        if (!tableName0.equals(tableName1)) {
            return;
        }

        //得到主叫的rowkey,
        String rowkey = Bytes.toString(put.getRow());

        //如果被叫就放行
        String[] arr = rowkey.split(&quot;,&quot;);
        if (arr[3].equals(&quot;1&quot;)) {
            return;
        }
        //hashcode,caller,time,flag,callee,duration
        String caller = arr[1] ;        //主叫
        String callTime = arr[2] ;      //通话时间
        String callee = arr[4] ;        //被叫
        String callDuration = arr[5] ;  //通话时长

        //被叫hashcode
        String hashcode = CallLogUtil.getHashcode(callee,callTime,100);
        //被叫rowkey
        String calleeRowKey = hashcode + &quot;,&quot; + callee + &quot;,&quot; + callTime + &quot;,1,&quot; + caller + &quot;,&quot; + callDuration;
        Put newPut = new Put(Bytes.toBytes(calleeRowKey));
        newPut.addColumn(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(REF_ROW_ID), Bytes.toBytes(rowkey));
        TableName tn = TableName.valueOf(CALL_LOG_TABLE_NAME);
        Table t = e.getEnvironment().getTable(tn);
        t.put(newPut);
    }

    /**
     * 重写方法，完成被叫查询，返回主叫结果。
     */
    public void postGetOp(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Get get, List&lt;Cell&gt; results) throws IOException {
        //获得表名
        String tableName = e.getEnvironment().getRegion().getRegionInfo().getTable().getNameAsString();

        //判断表名是否是ns1:calllogs
        if(!tableName.equals(CALL_LOG_TABLE_NAME)){
            super.preGetOp(e, get, results);
        }
        else{
            //得到rowkey
            String rowkey = Bytes.toString(get.getRow());
            //
            String[] arr = rowkey.split(&quot;,&quot;);
            //主叫
            if(arr[3].equals(&quot;0&quot;)){
                super.postGetOp(e, get, results);
            }
            //被叫
            else{
                //得到主叫方的rowkey
                String refrowid = Bytes.toString(CellUtil.cloneValue(results.get(0)));
                //
                Table tt = e.getEnvironment().getTable(TableName.valueOf(CALL_LOG_TABLE_NAME));
                Get g = new Get(Bytes.toBytes(refrowid));
                Result r = tt.get(g);
                List&lt;Cell&gt; newList = r.listCells();
                results.clear();
                results.addAll(newList);
            }
        }
    }

    /**
     *
     */
    public boolean postScannerNext(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, InternalScanner s, List&lt;Result&gt; results, int limit, boolean hasMore) throws IOException {
        boolean b = super.postScannerNext(e, s, results, limit, hasMore);

        //新集合
        List&lt;Result&gt; newList = new ArrayList&lt;Result&gt;();

        //获得表名
        String tableName = e.getEnvironment().getRegion().getRegionInfo().getTable().getNameAsString();

        //判断表名是否是ns1:calllogs
        if (tableName.equals(CALL_LOG_TABLE_NAME)) {
            Table tt = e.getEnvironment().getTable(TableName.valueOf(CALL_LOG_TABLE_NAME));
            for(Result r : results){
                //rowkey
                String rowkey = Bytes.toString(r.getRow());
                String flag = rowkey.split(&quot;,&quot;)[3] ;
                //主叫
                if(flag.equals(&quot;0&quot;)){
                    newList.add(r) ;
                }
                //被叫
                else{
                    //取出主叫号码
                    byte[] refrowkey = r.getValue(Bytes.toBytes(&quot;f2&quot;),Bytes.toBytes(REF_ROW_ID)) ;
                    Get newGet = new Get(refrowkey);
                    newList.add(tt.get(newGet));
                }
            }
            results.clear();
            results.addAll(newList);
        }
        return b ;
    }
}

1&apos;.修改jsp页面
    [callLogList.jsp]
    &lt;c:if test=&quot;${log.caller == param.caller}&quot;&gt;主叫&lt;/c:if&gt;
    &lt;c:if test=&quot;${log.caller != param.caller}&quot;&gt;被叫&lt;/c:if&gt;

2.部署jar包

3.测试
    ...

4.
</code></pre>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/9/">9</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/touxiang.jpg"
                alt="Eric Hunn" />
            
              <p class="site-author-name" itemprop="name">Eric Hunn</p>
              <p class="site-description motion-element" itemprop="description">大数据|动漫迷|科技控</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">82</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">17</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">116</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/erichunn" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:erichunn.me@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://weibo.com/u/2944687303/home?wvr=5" target="_blank" title="Webibo">
                      
                        <i class="fa fa-fw fa-weibo"></i>Webibo</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://github.com/drakeet/" title="Drakeet" target="_blank">Drakeet</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      

      
	  
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=1311545319&auto=1&height=66"></iframe>

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Eric Hunn</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  

  

  

</body>
</html>
