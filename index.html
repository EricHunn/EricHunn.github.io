<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />





  <link rel="alternate" href="/atom.xml" title="无心是一首歌" type="application/atom+xml" />






<meta name="description" content="大数据|动漫迷|科技控">
<meta property="og:type" content="website">
<meta property="og:title" content="无心是一首歌">
<meta property="og:url" content="http://erichunn.github.io/index.html">
<meta property="og:site_name" content="无心是一首歌">
<meta property="og:description" content="大数据|动漫迷|科技控">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="无心是一首歌">
<meta name="twitter:description" content="大数据|动漫迷|科技控">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: 'undefined',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://erichunn.github.io/"/>





  <title>无心是一首歌</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?02fe19ac065353167cab1b453f2909ec";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">无心是一首歌</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-首页">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-关于我">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于我
          </a>
        </li>
      
        
        <li class="menu-item menu-item-标签">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-分类">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-归档">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-站点地图">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br />
            
            站点地图
          </a>
        </li>
      
        
        <li class="menu-item menu-item-腾讯公益">
          <a href="/404/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br />
            
            腾讯公益
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/10/25/Hadoop第九天/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/25/Hadoop第九天/" itemprop="url">Hadoop第九天</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-10-25T09:52:10+08:00">
                2018-10-25
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hadoop/" itemprop="url" rel="index">
                    <span itemprop="name">Hadoop</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="复习："><a href="#复习：" class="headerlink" title="复习："></a>复习：</h1><p>1.链式job编程</p>
<pre><code>MR        //Mapper+ / Reduce Mapper*
</code></pre><p>2.DBWritable</p>
<pre><code>和数据库交互。
</code></pre><p>3.Sqoop</p>
<p>4.全排序</p>
<pre><code>对reduce输出的所有结果进行排序。
</code></pre><p>5.二次排序</p>
<pre><code>对value进行排序。
</code></pre><p>6.数据倾斜</p>
<pre><code>1.reduce
2.自定义分区函数
    数据结果错 + 二次job
3.重新设计key
    数据结果错 + 二次job
</code></pre><h1 id="机架感知"><a href="#机架感知" class="headerlink" title="机架感知"></a>机架感知</h1><p>机架有一块网卡。可以连接很多主机，机架上每个刀片服务器主机都连接到这个机架网卡上的。</p>
<p>比如一台刀片服务器上有2个进程，2个进程之间通信需要走机架上的交换机吗。不需要直接走自回环网络即可。走Localhost即可。所以2个进程之间不需要网卡交互数据。2个进程之间的距离就是0。每个进程所在的主机节点，连接他们之间的网络通路的跃点数就是他们之间的距离</p>
<p>如果b中绿色进程和a中红色进程需要交互就需要通过交换机，距离就是2.怎么算的，就是从每个服务器到交换机之间的距离相加，就是1+1；</p>
<p>如果是2个机架也要通过一个交换机连接起来。左边机架a中绿色和右边机架红色通信。距离就是4.</p>
<p>同一个机房通信最多就是4。也就是在通过一个交换机。</p>
<p>Hadoop都有机架感知能力：是他里面的一种策略，都需人为维护。在默认情况下所有节点都在同一个机架上。/default-rack。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/10/22/Hadoop第八天/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/22/Hadoop第八天/" itemprop="url">Hadoop第八天</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-10-22T19:40:51+08:00">
                2018-10-22
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hadoop/" itemprop="url" rel="index">
                    <span itemprop="name">Hadoop</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="二次排序链条化"><a href="#二次排序链条化" class="headerlink" title="二次排序链条化"></a>二次排序链条化</h1><p>分组不能解决数据倾斜问题，已经进入到了同一个reduce对象里面了，分一个组出来就进入同一个reduce方法里面。每次分组只是决定调用不调用同一个reduce方法。已经晚了，数据已经进入到了同一个reduce了，分区可以决定数据倾斜问题。</p>
<p>reduce分为方法和对象，有的时候说reduce是指的是方法，有的时候指的是对象，要分开对待。</p>
<p>单词统计做统计肯定不能哈希分区，因为哈希同一个值进入到同一个reduce对象利民区去了</p>
<h1 id="数据倾斜问题-随机分区-二次MR"><a href="#数据倾斜问题-随机分区-二次MR" class="headerlink" title="数据倾斜问题 随机分区 二次MR"></a>数据倾斜问题 随机分区 二次MR</h1><pre><code>1.
2.
3.
4.
</code></pre><p>如果正常按照wordcount来处理会分为</p>
<p>reduce不设置就是1，map没机会设置数量。但是也可以设置map，但是意义，以为mr提交的时候map的个数取决于切片的个数。</p>
<p>切片的计算公式：min block、maxsplit、 blocksize取中间值<br>等于blocksize。</p>
<p>现在文件如下，现在3个文件每个文件有一百万数据且有一百万个hello就对应了3个map。（这里没明白为什么3个切片也就是为啥3个map）</p>
<p>每个切片里，分区个数等于reduce个数，分区的个数取决于reduce个数，之前自己已经在WCAPP里面设置好了reduce个数。</p>
<p>我有3个map，4个reduce。那么每个map里面就有4个分区了。</p>
<p><strong>要解决哈希分区带来的数据倾斜问题，可以通过随机分区，然后在进行二次mr来解决这个问题</strong></p>
<p><img src="https://i.imgur.com/VoHpfBH.png" alt=""></p>
<p>组合combineor是对分区里面的预先聚合，下面的hello就不会发送一百万次，而是提前聚合了发送了hello 一百万次。这个样子。能解决目前的倾斜问题，但是不可能这么多数据，可能量很大</p>
<p>下面这种情况下，所有的hello根据哈希分区就进入到同一个分区，然后一百万个tom分4个分区</p>
<p>通过随机分区可以解决数据倾斜问题，但是会产生新的问题，就是说hello被分到4个不同的reduce里面，导致reduce1产生hello10reduce2产生hello13这种问题，所以通过二次mr来解决这个问题，让hello这个单词统计，进入到同一个reduce结果当中。所以也就是通过随即分区和二次mr解决</p>
<pre><code>[1.txt]1000000
hello tom1
hello tom2
hello tom3
hello tom4
hello tom5
hello tom6
hello tom7
hello tom8
hello tom9
hello tom10

[2.txt]1000000
hello tom11
hello tom12
hello tom13
hello tom14
hello tom15
hello tom16
hello tom17
hello tom18
hello tom19
hello tom20

[3.txt]1000000
hello tom21
hello tom22
hello tom23
hello tom24
hello tom25
hello tom26
hello tom27
hello tom28
hello tom29
hello tom30
</code></pre><p>代码如下：<br>    //自定义分区函数<br>    public class RandomPartitioner extends Partitioner&lt;Text,IntWritable&gt; {<br>        @Override<br>        public int getPartition(Text text, IntWritable intWritable, int numPartitions) {<br>            return new Random().nextInt(numPartitions);<br>        }<br>    }</p>
<hr>
<pre><code>//解决数据倾斜问题：
public class WCSkueApp {

    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;);
        Job job = Job.getInstance(conf);


//        设置作业的各种属性
        job.setJobName(&quot;MaxTempApp&quot;);    //作业名称
        job.setJarByClass(WCSkueApp.class); //搜索类路径
        job.setInputFormatClass(TextInputFormat.class);//设置输入格式类

        //添加输入路径
        FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew&quot;));
        //设置输出路径
        org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(&quot;e:/mr/skew/out&quot;));

        //设置随机分区：
        job.setPartitionerClass(RandomPartitioner.class);

        job.setMapperClass(WCSkueMapper.class);         //mapper类
        job.setReducerClass(WCSkueReducer.class);       //reduce类

        job.setNumReduceTasks(4);


        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(IntWritable.class);

        job.setOutputKeyClass(Text.class);          //设置输出类型
        job.setOutputValueClass(IntWritable.class);

        job.waitForCompletion(true);

    }
}
</code></pre><hr>
<pre><code>public class WCSkueMapper extends Mapper &lt;LongWritable, Text, Text, IntWritable&gt;{
    public WCSkueMapper(){
        System.out.println(&quot;new WCMapper&quot;);
    }

    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        String[] arr = value.toString().split(&quot; &quot;);
        Text keyout= new Text();
        IntWritable valueout = new IntWritable();

        for(String s:arr){
            keyout.set(s);
            valueout.set(1);
            context.write(keyout,valueout);
        }
    }
}
</code></pre><hr>
<pre><code>public class WCSkueReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{

    /**
     * reduce
     */
    protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {
        int count = 0 ;
        for(IntWritable iw : values){
            count = count + iw.get() ;
        }
        context.write(key,new IntWritable(count));


    }
}
</code></pre><hr>
<p>//解决数据倾斜问题：<br>    public class WCSkueApp2 {</p>
<pre><code>    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;);
        Job job = Job.getInstance(conf);


//        设置作业的各种属性
        job.setJobName(&quot;MaxTempApp&quot;);    //作业名称
        job.setJarByClass(WCSkueApp2.class); //搜索类路径
        job.setInputFormatClass(TextInputFormat.class);//设置输入格式类

        //添加输入路径
        FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00000&quot;));
        FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00001&quot;));
        FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00002&quot;));
        FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00003&quot;));
        //设置输出路径
        org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(&quot;e:/mr/skew/out2&quot;));


        job.setMapperClass(WCSkueMapper2.class);         //mapper类
        job.setReducerClass(WCSkueReducer2.class);       //reduce类

        job.setNumReduceTasks(4);


        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(IntWritable.class);

        job.setOutputKeyClass(Text.class);          //设置输出类型
        job.setOutputValueClass(IntWritable.class);

        job.waitForCompletion(true);

    }
}
</code></pre><hr>
<pre><code>public class WCSkueMapper2 extends Mapper &lt;LongWritable, Text, Text, IntWritable&gt;{
    public WCSkueMapper2(){
        System.out.println(&quot;new WCMapper&quot;);
    }

    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        String[] arr = value.toString().split(&quot;\t&quot;);
        Text keyout= new Text();
        IntWritable valueout = new IntWritable();
        context.write(new Text(arr[0]),new IntWritable(Integer.parseInt(arr[1])));
    }
}
</code></pre><hr>
<pre><code>public class WCSkueReducer2 extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{

    /**
     * reduce
     */
    protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {
        int count = 0 ;
        for(IntWritable iw : values){
            count = count + iw.get() ;
        }
        context.write(key,new IntWritable(count));


    }
}
</code></pre><hr>
<p>上面这个就是通过2次mr解决数据倾斜问题    但是不用那么麻烦的去切分了直接使用Keyvalueinputformat即可实现：如下代码：<br>    //解决数据倾斜问题：<br>    public class WCSkueApp2 {</p>
<pre><code>    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;);
        Job job = Job.getInstance(conf);


//        设置作业的各种属性
        job.setJobName(&quot;MaxTempApp&quot;);    //作业名称
        job.setJarByClass(WCSkueApp2.class); //搜索类路径
        job.setInputFormatClass(KeyValueTextInputFormat.class);//设置输入格式类

        //添加输入路径
        FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00000&quot;));
        FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00001&quot;));
        FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00002&quot;));
        FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00003&quot;));
        //设置输出路径
        org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(&quot;e:/mr/skew/out2&quot;));


        job.setMapperClass(WCSkueMapper2.class);         //mapper类
        job.setReducerClass(WCSkueReducer2.class);       //reduce类

        job.setNumReduceTasks(4);


        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(IntWritable.class);

        job.setOutputKeyClass(Text.class);          //设置输出类型
        job.setOutputValueClass(IntWritable.class);

        job.waitForCompletion(true);

    }
}
</code></pre><hr>
<pre><code>public class WCSkueMapper2 extends Mapper &lt;Text, Text, Text, IntWritable&gt;{
    public WCSkueMapper2(){
        System.out.println(&quot;new WCMapper&quot;);
    }

    protected void map(Text key, Text value, Context context) throws IOException, InterruptedException {
        context.write(key,new IntWritable(Integer.parseInt(value.toString())));
    }
}
</code></pre><hr>
<pre><code>public class WCSkueReducer2 extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{

    /**
     * reduce
     */
    protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {
        int count = 0 ;
        for(IntWritable iw : values){
            count = count + iw.get() ;
        }
        context.write(key,new IntWritable(count));
    }
}
</code></pre><hr>
<hr>
<hr>
<h1 id="链条式编程"><a href="#链条式编程" class="headerlink" title="链条式编程"></a>链条式编程</h1><p><img src="https://i.imgur.com/VwHIXax.png" alt=""></p>
<p>讲解上图：<br>首先通过MapMapper1来切割，然后通过MapMapper1过滤掉法轮功，真个是一个链条，在map端，执行完之后开始聚合，在reduce端，在聚合的时候，每个单词都有各自的个数，对产生的结果再次过滤，过滤掉少于5的单词，当然可以放到同一个map里面，但是如果变成模块方法，易于维护。更加方便使用。<br>m阶段可以有多个m但是R阶段只能有一个r。但是r阶段可以有多个m。</p>
<p>代码如下图：<br>//链条式job任务<br>    public class WCChainApp {</p>
<pre><code>    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;);

        Job job = Job.getInstance(conf);

//        设置作业的各种属性
        job.setJobName(&quot;MaxTempApp&quot;);    //作业名称
        job.setJarByClass(WCChainApp.class); //搜索类路径
        job.setInputFormatClass(TextInputFormat.class);//设置输入格式类

        //添加输入路径
        FileInputFormat.addInputPath(job, new Path(&quot;e:/mr/skew&quot;));
        //设置输出路径
        org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(&quot;e:mr/skew/out&quot;));

        //在map链条上添加一个mapper的环节
        ChainMapper.addMapper(job,WCMapMapper1.class,LongWritable.class,Text.class,Text.class,IntWritable.class,conf);
        ChainMapper.addMapper(job,WCMapMapper2.class,Text.class,IntWritable.class,Text.class,IntWritable.class,conf);

        //在reduce链条上设置reduce
        ChainReducer.setReducer(job,WCReducer.class,Text.class,IntWritable.class,Text.class,IntWritable.class,conf);
        ChainReducer.addMapper(job,WCReduceMapper1.class,Text.class,IntWritable.class,Text.class,IntWritable.class,conf);

        job.setNumReduceTasks(3);


        job.waitForCompletion(true);

    }
}
</code></pre><hr>
<pre><code>public class WCMapMapper1 extends Mapper &lt;LongWritable, Text, Text, IntWritable&gt;{

    protected   void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        Text keyout= new Text();
        IntWritable valueout = new IntWritable();
        String[] arr = value.toString().split(&quot; &quot;);
        for(String s:arr){
            keyout.set(s);
            valueout.set(1);
            context.write(keyout,valueout);
        }
    }
}
</code></pre><hr>
<pre><code>public class WCMapMapper2 extends Mapper &lt;Text, IntWritable, Text, IntWritable&gt;{

    protected void map(Text key, IntWritable value, Context context) throws IOException, InterruptedException {
        if(!key.toString().equals(&quot;falungong&quot;)){
            context.write(key , value);
        }

    }
}
</code></pre><hr>
<pre><code>//过滤单词个数
public class WCReduceMapper1 extends Mapper&lt;Text,IntWritable,Text,IntWritable&gt; {
    @Override
    protected void map(Text key, IntWritable value, Context context) throws IOException, InterruptedException {
        if(value.get() &gt; 5){
            context.write(key,value);
        }
    }
}
</code></pre><hr>
<pre><code>/**
 * Reducer
 */
public class WCReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{

    /**
     * reduce
     */
    protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {
        int count = 0 ;
        for(IntWritable iw : values){
            count = count + iw.get() ;
        }
        context.write(key,new IntWritable(count));


    }
}
</code></pre><hr>
<hr>
<hr>
<h2 id="FileInputFormat（读源码）"><a href="#FileInputFormat（读源码）" class="headerlink" title="FileInputFormat（读源码）"></a>FileInputFormat（读源码）</h2><pre><code>获取切片集合。
子类都要重写方法isSplittable();
负责创建RecordReader对象。
设置IO路径。
</code></pre><h2 id="RecordReader（读源码）"><a href="#RecordReader（读源码）" class="headerlink" title="RecordReader（读源码）"></a>RecordReader（读源码）</h2><pre><code>负责从InputSplit中读取KV对。
</code></pre><h2 id="jdbc笔记模板："><a href="#jdbc笔记模板：" class="headerlink" title="jdbc笔记模板："></a>jdbc笔记模板：</h2><pre><code>[写操作]
Class.forName(&quot;com.mysql.jdbc.Driver&quot;);
Connection conn = DriverMananger.getConnection(&quot;jdbc:mysql://localhost:3306/big4&quot;,&quot;root&quot;,&quot;root&quot;);
//预处理语句
PreparedStatement ppst = conn.preparedStatement(&quot;insert into test(id,name,age) values(?,?,?)&quot;);
//绑定参数
ppst.setInteger(1,1);
ppst.setInteger(2,&quot;tom&quot;);
ppst.setInteger(3,12);

ppst.executeUpdate();
ppst.close();
conn.close();


[读操作]
Class.forName(&quot;com.mysql.jdbc.Driver&quot;);
Connection conn = DriverMananger.getConnection(&quot;jdbc:mysql://localhost:3306/big4&quot;,&quot;root&quot;,&quot;root&quot;);

ppst = conn.preparedStatement(&quot;select id,name from test &quot;);
//结果集
ResultSet rs = ppst.executeQuery();
while(rs.next()){
    int id = rs.getInt(&quot;id&quot;);
    String name = rs.getInt(&quot;name&quot;);
}
rs.close();
conn.close();
</code></pre><p>看一下下源码：<br><img src="https://i.imgur.com/5ZrVZsW.png" alt=""><br>看一下写和读入都把结果集和预处理语句都已经封装进去了，剩下的只要填空就好了。</p>
<p>以下是从数据库中读入的代码模板：</p>
<h1 id="使用DBWritable向数据库从数据库中读取"><a href="#使用DBWritable向数据库从数据库中读取" class="headerlink" title="使用DBWritable向数据库从数据库中读取"></a>使用DBWritable向数据库从数据库中读取</h1><h2 id="1-准备数据库"><a href="#1-准备数据库" class="headerlink" title="1.准备数据库"></a>1.准备数据库</h2><pre><code>create database big4 ;
use big4 ;
create table words(id int primary key auto_increment , name varchar(20) , txt varchar(255));

insert into words(name,txt) values(&apos;tomas&apos;,&apos;hello world tom&apos;);
insert into words(txt) values(&apos;hello tom world&apos;);
insert into words(txt) values(&apos;world hello tom&apos;);
insert into words(txt) values(&apos;world tom hello&apos;);
</code></pre><h2 id="2-编写hadoop-MyDBWritable"><a href="#2-编写hadoop-MyDBWritable" class="headerlink" title="2.编写hadoop MyDBWritable."></a>2.编写hadoop MyDBWritable.</h2><pre><code>import org.apache.hadoop.mapreduce.lib.db.DBWritable;

import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;
import java.sql.PreparedStatement;
import java.sql.ResultSet;
import java.sql.SQLException;

/**
 * MyDBWritable
 */
public class MyDBWritable implements DBWritable,Writable {
    private int id ;
    private String name ;
    private String txt ;

    public int getId() {
        return id;
    }

    public void setId(int id) {
        this.id = id;
    }

    public String getName() {
        return name;
    }

    public void setName(String name) {
        this.name = name;
    }

    public String getTxt() {
        return txt;
    }

    public void setTxt(String txt) {
        this.txt = txt;
    }

    public void write(DataOutput out) throws IOException {
        out.writeInt(id);
        out.writeUTF(name);
        out.writeUTF(txt);
    }

    public void readFields(DataInput in) throws IOException {
        id = in.readInt();
        name = in.readUTF();
        txt = in.readUTF();
    }

    /**
     * 写入db
     */
    public void write(PreparedStatement ppst) throws SQLException {
        ppst.setInt(1,id);
        ppst.setString(2,name);
        ppst.setString(3,txt);
    }

    /**
     * 从db读取
     */
    public void readFields(ResultSet rs) throws SQLException {
        id = rs.getInt(1);
        name = rs.getString(2);
        txt = rs.getString(3);
    }
}
</code></pre><h2 id="3-WcMapper"><a href="#3-WcMapper" class="headerlink" title="3.WcMapper"></a>3.WcMapper</h2><pre><code>public class WCMapper extends Mapper&lt;LongWritable,MyDBWritable,Text,IntWritable&gt; {

    protected void map(LongWritable key, MyDBWritable value, Context context) throws IOException, InterruptedException {
        System.out.println(key);
        String line = value.getTxt();
        System.out.println(value.getId() + &quot;,&quot; + value.getName());
        String[] arr = line.split(&quot; &quot;);
        for(String s : arr){
            context.write(new Text(s),new IntWritable(1));
        }
    }
}
</code></pre><h2 id="4-WCReducer"><a href="#4-WCReducer" class="headerlink" title="4.WCReducer"></a>4.WCReducer</h2><pre><code>protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {
    int count = 0 ;
    for(IntWritable w : values){
        count = count + w.get() ;
    }
    context.write(key,new IntWritable(count));
}
</code></pre><h2 id="5-WCApp"><a href="#5-WCApp" class="headerlink" title="5.WCApp"></a>5.WCApp</h2><pre><code>public static void main(String[] args) throws Exception {
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf);

    //设置job的各种属性
    job.setJobName(&quot;MySQLApp&quot;);                        //作业名称
    job.setJarByClass(WCApp.class);                 //搜索类

    //配置数据库信息
    String driverclass = &quot;com.mysql.jdbc.Driver&quot; ;
    String url = &quot;jdbc:mysql://localhost:3306/big4&quot; ;
    String username= &quot;root&quot; ;
    String password = &quot;root&quot; ;
    //设置数据库配置
    DBConfiguration.configureDB(job.getConfiguration(),driverclass,url,username,password);
    //设置数据输入内容
    DBInputFormat.setInput(job,MyDBWritable.class,&quot;select id,name,txt from words&quot;,&quot;select count(*) from words&quot;);

    //设置输出路径
    FileOutputFormat.setOutputPath(job,new Path(&quot;e:/mr/sql/out&quot;));

    //设置分区类
    job.setMapperClass(WCMapper.class);             //mapper类
    job.setReducerClass(WCReducer.class);           //reducer类

    job.setNumReduceTasks(3);                       //reduce个数

    job.setMapOutputKeyClass(Text.class);           //
    job.setMapOutputValueClass(IntWritable.class);  //

    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);     //

    job.waitForCompletion(true);
}
</code></pre><h2 id="6-pom-xml增加mysql驱动"><a href="#6-pom-xml增加mysql驱动" class="headerlink" title="6.pom.xml增加mysql驱动"></a>6.pom.xml增加mysql驱动</h2><pre><code>&lt;dependency&gt;
    &lt;groupId&gt;mysql&lt;/groupId&gt;
    &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;
    &lt;version&gt;5.1.17&lt;/version&gt;
&lt;/dependency&gt;
</code></pre><h2 id="7-将mr的统计结果写入mysql数据库"><a href="#7-将mr的统计结果写入mysql数据库" class="headerlink" title="7.将mr的统计结果写入mysql数据库"></a>7.将mr的统计结果写入mysql数据库</h2><pre><code>a)准备表
    create table stats(word varchar(50),c int);
b)设置App的DBOutputFormat类
    com.it18zhang.hdfs.mr.mysql.WCApp
d)
e)
f)
</code></pre><hr>
<h1 id="mysql分页查询"><a href="#mysql分页查询" class="headerlink" title="mysql分页查询"></a>mysql分页查询</h1><p>如图所示吧：</p>
<p><img src="https://i.imgur.com/0hHJN3B.png" alt=""></p>
<p><img src="https://i.imgur.com/FAbkIff.png" alt=""></p>
<p><img src="https://i.imgur.com/9wCCeH2.png" alt=""></p>
<h1 id="使用DBWritable向数据库从数据库中写入"><a href="#使用DBWritable向数据库从数据库中写入" class="headerlink" title="使用DBWritable向数据库从数据库中写入"></a>使用DBWritable向数据库从数据库中写入</h1><pre><code>public class MyDBWritalbe implements DBWritable,Writable {
    private int id=0;
    private String name=&quot;&quot;;
    private String txt=&quot;&quot;;
    private  String word=&quot;&quot;;
    private int wordcount=0;

    public String getWord() {
        return word;
    }

    public void setWord(String word) {
        this.word = word;
    }

    public int getWordcount() {
        return wordcount;
    }

    public void setWordcount(int wordcount) {
        this.wordcount = wordcount;
    }

    public int getId() {
        return id;
    }

    public void setId(int id) {
        this.id = id;
    }

    public String getName() {
        return name;
    }

    public void setName(String name) {
        this.name = name;
    }

    public String getTxt() {
        return txt;
    }

    public void setTxt(String txt) {
        this.txt = txt;
    }

    public void write(DataOutput out) throws IOException {

        out.writeInt(id);
        out.writeUTF(name);
        out.writeUTF(txt);
        out.writeUTF(word);
        out.writeInt(wordcount);
    }
    public void readFields(DataInput in) throws IOException {
        id=in.readInt();
        name=in.readUTF();
        txt=in.readUTF();
        word=in.readUTF();
        wordcount=in.readInt();

    }
    //向数据库中写入DB
    public void write(PreparedStatement ppst) throws SQLException {
        //要求定制字段列表的时候先单词后个数。

        ppst.setString(1,word);
        ppst.setInt(2,wordcount);

    }
    //从DB中读出
    public void readFields(ResultSet rs) throws SQLException {
        id=rs.getInt(1);
        name=rs.getString(2);
        txt=rs.getString(3);

    }
}
</code></pre><hr>
<pre><code>public class WCApp {
    //
//    public static void main(String[] args) throws Exception {
//        Configuration conf = new Configuration();
//
//        Job job = Job.getInstance(conf);
//
////        设置作业的各种属性
//        job.setJobName(&quot;MySQLApp&quot;);    //作业名称
//        job.setJarByClass(WCApp.class); //搜索类路径
//
//        //配置数据库信息
//        String driverclass = &quot;com.mysql.jdbc.Driver&quot;;
//        String url = &quot;jdbc:mysql://localhost:3306/big4&quot;;
//        String usrname = &quot;root&quot;;
//        String password = &quot;root&quot;;
//        DBConfiguration.configureDB(conf,driverclass,url,usrname,password);
//        //设置数据输入内容
//        DBInputFormat.setInput(job,DBWritable.class,&quot;select id,name,txt from words&quot;,&quot;select count(*) from words&quot;);
//        //设置输出路径
//        FileOutputFormat.setOutputPath(job, new Path(&quot;d:/mr/sql/out&quot;));
//
//        job.setMapperClass(WCMapper.class);         //mapper类
//        job.setReducerClass(WCReducer.class);       //reduce类
//
//        job.setNumReduceTasks(3);
//
//
//        job.setMapOutputKeyClass(Text.class);
//        job.setMapOutputValueClass(IntWritable.class);
//
//        job.setOutputKeyClass(Text.class);          //设置输出类型
//        job.setOutputValueClass(IntWritable.class);
//
//        job.waitForCompletion(true);
//
//    }
//}
    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf);

        //设置job的各种属性
        job.setJobName(&quot;MySQLApp&quot;);                        //作业名称
        job.setJarByClass(WCApp.class);                 //搜索类

        //配置数据库信息
        String driverclass = &quot;com.mysql.jdbc.Driver&quot;;
        String url = &quot;jdbc:mysql://localhost:3306/big4&quot;;
        String username = &quot;root&quot;;
        String password = &quot;root&quot;;
        //设置数据库配置
        DBConfiguration.configureDB(job.getConfiguration(), driverclass, url, username, password);
        //设置数据输入内容
        DBInputFormat.setInput(job, MyDBWritalbe.class, &quot;select id,name,txt from words&quot;, &quot;select count(*) from words&quot;);
        //这里定制字段列表，先单词后个数
        DBOutputFormat.setOutput(job,&quot;stats&quot;,&quot;word&quot;,&quot;c&quot;);

        //设置分区类
        job.setMapperClass(WCMapper.class);             //mapper类
        job.setReducerClass(WCReducer.class);           //reducer类

        job.setNumReduceTasks(3);                       //reduce个数

        job.setMapOutputKeyClass(Text.class);           //
        job.setMapOutputValueClass(IntWritable.class);  //

        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);     //

        job.waitForCompletion(true);
    }
}
</code></pre><hr>
<pre><code>public class WCMapper extends Mapper&lt;LongWritable, MyDBWritalbe, Text, IntWritable&gt; {
    @Override
    protected void map(LongWritable key, MyDBWritalbe value, Context context) throws IOException, InterruptedException {
        System.out.println(key);
        String line = value.getTxt();
        System.out.println(value.getId() + &quot;,&quot; + value.getName());
        String[] arr = line.split(&quot; &quot;);
        for (String s : arr) {
            context.write(new Text(s), new IntWritable(1));
        }
    }
}
</code></pre><hr>
<pre><code>public class WCReducer extends Reducer&lt;Text, IntWritable, MyDBWritalbe, NullWritable&gt; {

    protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {
        int count = 0;
        for (IntWritable w : values) {
            count = count + w.get();
        }
        MyDBWritalbe keyout = new MyDBWritalbe();
        keyout.setWord(key.toString());
        keyout.setWordcount(count);
        context.write(keyout, NullWritable.get());
    }
}
</code></pre><hr>
<h1 id="在虚拟机中跑wordcount写入数据库问题："><a href="#在虚拟机中跑wordcount写入数据库问题：" class="headerlink" title="在虚拟机中跑wordcount写入数据库问题："></a>在虚拟机中跑wordcount写入数据库问题：</h1><h2 id="需要修改的地方："><a href="#需要修改的地方：" class="headerlink" title="需要修改的地方："></a>需要修改的地方：</h2><p>1.修改url</p>
<p><img src="https://i.imgur.com/8lkYD5v.png" alt=""></p>
<p>2.修改core-site.xml或者删除掉</p>
<p><img src="https://i.imgur.com/weFYMBJ.png" alt=""></p>
<p>3.在虚拟机中分发mysql-connector-java-5.1.7-bin.jar将之分发到每个节点的/soft/hadoop/share/hadoop/common/lib目录下。</p>
<p><img src="https://i.imgur.com/ou52Qhr.png" alt=""></p>
<p>4.在运行</p>
<pre><code>hadoop jar HdfsDemo.jar com.it18zhang.hdfs.mr.mysql.WCApp
</code></pre><h2 id="出现的问题："><a href="#出现的问题：" class="headerlink" title="出现的问题："></a>出现的问题：</h2><p>1.出现连接不上的报错：</p>
<p><img src="https://i.imgur.com/JgE2Q3e.png" alt=""></p>
<p>百度了一下完美解决，是由于mysql没有对所有用户开启权限导致：</p>
<p><a href="https://blog.csdn.net/OldTogether/article/details/79612627?utm_source=blogxgwz1" target="_blank" rel="noopener">https://blog.csdn.net/OldTogether/article/details/79612627?utm_source=blogxgwz1</a></p>
<p>2.出现一下问题：</p>
<p><img src="https://i.imgur.com/vgUGlEc.png" alt=""></p>
<p>原因是有一个s205的防火墙没有关掉导致的：</p>
<p><a href="https://blog.csdn.net/shirdrn/article/details/7280040" title="防火墙没关掉导致的" target="_blank" rel="noopener">https://blog.csdn.net/shirdrn/article/details/7280040</a></p>
<hr>
<p>完</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/10/17/Hadoop第七天/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/17/Hadoop第七天/" itemprop="url">Hadoop第七天</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-10-17T16:17:15+08:00">
                2018-10-17
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hadoop/" itemprop="url" rel="index">
                    <span itemprop="name">Hadoop</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="多输入问题"><a href="#多输入问题" class="headerlink" title="多输入问题"></a>多输入问题</h1><p>在IDEA里面代码：</p>
<p>首先明确一点InputInputformat也就是文本输入格式的输入是Longwritblele和Text，然后SequenceFileputFormat的输入是intwritble和text。</p>
<pre><code>public class WCApp {

    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;);

        Job job = Job.getInstance(conf);

//       设置作业的各种属性
        job.setJobName(&quot;WCAppMulti&quot;);    //作业名称
        job.setJarByClass(WCApp.class); //搜索类路径

        //多个输入
        MultipleInputs.addInputPath(job,new Path(&quot;file:///e:/mr/txt&quot;),TextInputFormat.class,WCTextMapper.class);
        MultipleInputs.addInputPath(job,new Path(&quot;file:///e:/mr/seq&quot;),SequenceFileInputFormat.class,WCSeqMapper.class);

        //设置输出
        FileOutputFormat.setOutputPath(job,new Path(args[0]));

        job.setReducerClass(WCReducer.class);//reducer类
        job.setNumReduceTasks(3);//reducer个数


        job.setOutputKeyClass(Text.class);          //设置输出类型
        job.setOutputValueClass(IntWritable.class);

        job.waitForCompletion(true);

    }
}
</code></pre><hr>
<pre><code>ublic class WCReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{

/**
 * reduce
 */
protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {
    int count = 0 ;
    for(IntWritable iw : values){
        count = count + iw.get() ;
    }
    String tno = Thread.currentThread().getName();
    System.out.println(tno + &quot;WCReducer:&quot; + key.toString() + &quot;=&quot; + count);
    context.write(key,new IntWritable(count));
}
</code></pre><p>}</p>
<hr>
<pre><code>public class WCTextMapper extends Mapper &lt;LongWritable, Text, Text, IntWritable&gt;{
    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        String[] arr = value.toString().split(&quot; &quot;);
        Text keyout= new Text();
        IntWritable valueout = new IntWritable();
        for(String s:arr){
            keyout.set(s);
            valueout.set(1);
            context.write(keyout,valueout);
        }
    }
}
</code></pre><hr>
<pre><code>public class WCTextMapper extends Mapper &lt;LongWritable, Text, Text, IntWritable&gt;{
    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        String[] arr = value.toString().split(&quot; &quot;);
        Text keyout= new Text();
        IntWritable valueout = new IntWritable();
        for(String s:arr){
            keyout.set(s);
            valueout.set(1);
            context.write(keyout,valueout);
        }
    }
}
</code></pre><hr>
<p>下图是配置本地的路径。要明确其中的arg[]也就是这个里配置的那个Program arguments的路径。因为只需要一个参数就可以了，因为在代码中已经写好了输入路径了。</p>
<p><img src="https://i.imgur.com/vO9wETa.png" alt=""></p>
<hr>
<hr>
<hr>
<h1 id="计数器"><a href="#计数器" class="headerlink" title="计数器"></a>计数器</h1><p> 日志目录：<br>/soft/hadoop/logs/userlogs</p>
<p>用户打印的日志。Hadoop输出的日志都是在外层的文件之中。userlogs是自己的日志打印。</p>
<h2 id="计数器-1"><a href="#计数器-1" class="headerlink" title="计数器"></a>计数器</h2><p>是其他的内容都不做更改，在Mapper和Reducer里面添加这个语句即可<br>    context.getCounter(“r”, “WCReducer.reduce”).increment(1);</p>
<p>然后扔到虚拟机里面去运行：<br>    hadoop jar HdfsDemo.jar com.it18zhang.hdfs.mr.WCApp hdfs://user/centos/data/ hdfs://user/centos/data/out</p>
<h1 id="单独配置2nn到独立节点"><a href="#单独配置2nn到独立节点" class="headerlink" title="单独配置2nn到独立节点"></a>单独配置2nn到独立节点</h1><p>配置core-site文件</p>
<pre><code>[hdfs-site.xml]
&lt;property&gt;
        &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;
        &lt;value&gt;s206:50090&lt;/value&gt;
&lt;/property&gt;
</code></pre><h1 id="跟踪运行器信息"><a href="#跟踪运行器信息" class="headerlink" title="跟踪运行器信息"></a>跟踪运行器信息</h1><p><img src="https://i.imgur.com/UVIcFDb.png" alt=""></p>
<p>添加一个工具类：</p>
<pre><code>public class Util {

public static String getInfo(Object o,String msg ){
    return getHostname() + &quot;:&quot; + getPID() + &quot;:&quot; + getTID()+&quot;:&quot;+getObjInfo(o) +msg;
}

//得到主机名
public static String getHostname() {
    try {
        return InetAddress.getLocalHost().getHostName();
    } catch (UnknownHostException e) {
        e.printStackTrace();
    }
    return  null;
}
    //获得当前程序的所在的进程ID。
    public static int getPID()  {
        String info = ManagementFactory.getRuntimeMXBean().getName();
        return Integer.parseInt(info.substring(0,info.indexOf(&quot;@&quot;)));
    }
    //返回当前线程ID，
    public static String getTID(){
    return Thread.currentThread().getName();
    }

    public static  String getObjInfo(Object o){
        String sname = o.getClass().getSimpleName();
        return sname + &quot;@&quot;+o.hashCode();

    }
}
</code></pre><p>然后在map和reduce阶段添加：</p>
<pre><code>//每执行一次，计数器对这个组+1
context.getCounter(&quot;m&quot;,Util.getInfo(this,&quot;map&quot;)).increment(1);
</code></pre><p>效果如下图<br><img src="https://i.imgur.com/XeGcZvS.png" alt=""></p>
<h1 id="全排序"><a href="#全排序" class="headerlink" title="全排序"></a>全排序</h1><pre><code>## 普通排序求最高年份温度 ##
</code></pre><p>代码如下：</p>
<pre><code>public class MaxTempApp {

    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;);

        Job job = Job.getInstance(conf);

//        设置作业的各种属性
        job.setJobName(&quot;MaxTempApp&quot;);    //作业名称
        job.setJarByClass(MaxTempApp.class); //搜索类路径
        job.setInputFormatClass(TextInputFormat.class);//设置输入格式类

        //添加输入路径
        FileInputFormat.addInputPath(job, new Path(args[0]));
        //设置输出路径
        org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(args[1]));


        job.setMapperClass(MaxTempMapper.class);         //mapper类
        job.setReducerClass(MaxTempReducer.class);       //reduce类

        job.setNumReduceTasks(3);           //reduce个数


        job.setMapOutputKeyClass(IntWritable.class);
        job.setMapOutputValueClass(IntWritable.class);

        job.setOutputKeyClass(IntWritable.class);          //设置输出类型
        job.setOutputValueClass(IntWritable.class);

        job.waitForCompletion(true);

    }
}
</code></pre><hr>
<pre><code>public class MaxTempMapper extends Mapper&lt;LongWritable, Text, IntWritable, IntWritable&gt; {
    public MaxTempMapper() {
        System.out.println(&quot;new MaxTempMapper&quot;);
    }

    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        String line = value.toString();
        String arr[] = line.split(&quot; &quot;);
        context.write(new IntWritable(Integer.parseInt(arr[0])), new IntWritable((Integer.parseInt(arr[1]))));
    }
}
</code></pre><hr>
<pre><code>public class MaxTempReducer extends Reducer&lt;IntWritable, IntWritable, IntWritable, IntWritable&gt;{

    /**
     * reduce
     */
    protected void reduce(IntWritable key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {
            int max =Integer.MIN_VALUE;
            for(IntWritable iw : values){
                max = max &gt; iw.get() ? max : iw.get() ;
            }
                context.write(key,new IntWritable(max));

    }
}
</code></pre><hr>
<h2 id="全排序代码"><a href="#全排序代码" class="headerlink" title="全排序代码"></a>全排序代码</h2><p>上面代码产生的会是3个文件，文件内部各自排序，但是不是全排序的文件。全排序2种办法：</p>
<p>1、设置分区数是1，但是数据倾斜</p>
<ol start="2">
<li><p>在每个分区里设置0-30，30-60，60-100即可,然后每个分区返回0,1,2进入分成不同的区<br>代码如下图：<br> public class YearPartitioner extends Partitioner&lt;IntWritable, IntWritable&gt; {</p>
<pre><code>public int getPartition(IntWritable year, IntWritable temp, int parts) {
    int y = year.get() - 1970;
    if (y &lt; 33) {
        return 0;
    }
    if (y &gt; 33 &amp;&amp; y &lt; 66) {
        return 1;
    }
    else {
        return 2;
    }
}
</code></pre><p> }</p>
</li>
</ol>
<hr>
<pre><code>public class MaxTempReducer extends Reducer&lt;IntWritable, IntWritable, IntWritable, IntWritable&gt;{

    /**
     * reduce
     */
    protected void reduce(IntWritable key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {
            int max =Integer.MIN_VALUE;
            for(IntWritable iw : values){
                max = max &gt; iw.get() ? max : iw.get() ;
            }
                context.write(key,new IntWritable(max));

    }
}
</code></pre><hr>
<pre><code>public class MaxTempMapper extends Mapper&lt;LongWritable, Text, IntWritable, IntWritable&gt; {
    public MaxTempMapper() {
        System.out.println(&quot;new MaxTempMapper&quot;);
    }

    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        String line = value.toString();
        String arr[] = line.split(&quot; &quot;);
        context.write(new IntWritable(Integer.parseInt(arr[0])), new IntWritable((Integer.parseInt(arr[1]))));
    }
}
</code></pre><hr>
<pre><code>public class MaxTempApp {

    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;);

        Job job = Job.getInstance(conf);

//        设置作业的各种属性
        job.setJobName(&quot;MaxTempApp&quot;);    //作业名称
        job.setJarByClass(MaxTempApp.class); //搜索类路径
        job.setInputFormatClass(TextInputFormat.class);//设置输入格式类

        job.setPartitionerClass(YearPartitioner.class);

        //添加输入路径
        FileInputFormat.addInputPath(job, new Path(args[0]));
        //设置输出路径
        org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(args[1]));


        job.setMapperClass(MaxTempMapper.class);         //mapper类
        job.setReducerClass(MaxTempReducer.class);       //reduce类

        job.setNumReduceTasks(3);           //reduce个数


        job.setMapOutputKeyClass(IntWritable.class);
        job.setMapOutputValueClass(IntWritable.class);

        job.setOutputKeyClass(IntWritable.class);          //设置输出类型
        job.setOutputValueClass(IntWritable.class);

        job.waitForCompletion(true);

    }
}
</code></pre><hr>
<p>其实无非就是加一个partitioner的这个类而已。</p>
<hr>
<hr>
<hr>
<h1 id="全排序采样器"><a href="#全排序采样器" class="headerlink" title="全排序采样器"></a>全排序采样器</h1><p>1.定义1个reduce</p>
<p>2.自定义分区函数。<br>：        自行设置分解区间。</p>
<p>3.使用hadoop采样机制。</p>
<p>通过采样器生成分区文件，结合hadoop的TotalOrderPartitioner进行分区划分</p>
<p>TotalOrderPartitioner        //全排序分区类,读取外部生成的分区文件确定区间。</p>
<p>使用时采样代码在最后端,否则会出现错误。</p>
<p>//分区文件设置，设置的job的配置对象，不要是之前的conf.这里面的getconfiguration()是对最开始new的conf的拷贝，<br>TotalOrderPartitioner.setPartitionFile(job.getConfiguration(),new Path(“d:/mr/par.lst”));</p>
<h2 id="首先一段产生随机年份，温度的代码"><a href="#首先一段产生随机年份，温度的代码" class="headerlink" title="首先一段产生随机年份，温度的代码"></a>首先一段产生随机年份，温度的代码</h2><pre><code>public class PrepareTempData {
    @Test
    public void makeData() throws IOException {
        FileWriter fw = new FileWriter(&quot;e:/mr/temp.txt&quot;);
        for(int i=0;i&lt;6000;i++){
            int year=1970+ new Random().nextInt(100);
            int temp=-30 + new Random().nextInt(600);
            fw.write(&quot; &quot;+ year + &quot; &quot;+ temp + &quot;\r\n&quot; );
        }
            fw.close();
    }
}
</code></pre><hr>
<h2 id="全排序采样器代码"><a href="#全排序采样器代码" class="headerlink" title="全排序采样器代码"></a>全排序采样器代码</h2><pre><code>        public static void main(String[] args) throws Exception {
    Configuration conf = new Configuration();
    conf.set(&quot;fs.defaultFS&quot;, &quot;file:///&quot;);

    Job job = Job.getInstance(conf);

        设置作业的各种属性
    job.setJobName(&quot;MaxTempApp&quot;);    //作业名称
    job.setJarByClass(MaxTempApp.class); //搜索类路径
    job.setInputFormatClass(SequenceFileInputFormat.class);//设置输入格式类

    //添加输入路径
    FileInputFormat.addInputPath(job, new Path(args[0]));
    //设置输出路径
    org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(args[1]));

    job.setMapperClass(MaxTempMapper.class);         //mapper类
    job.setReducerClass(MaxTempReducer.class);       //reduce类


    job.setMapOutputKeyClass(IntWritable.class);
    job.setMapOutputValueClass(IntWritable.class);

    job.setOutputKeyClass(IntWritable.class);          //设置输出类型
    job.setOutputValueClass(IntWritable.class);

    //设置 全排序分区类
    job.setPartitionerClass(TotalOrderPartitioner.class);
    //将sample数据 写入分区文件
    TotalOrderPartitioner.setPartitionFile(conf, new Path(&quot;e:/mr/par.lst&quot;));

    //创建随机采样器对象
    InputSampler.Sampler&lt;IntWritable, IntWritable&gt; sampler = new InputSampler.RandomSampler&lt;IntWritable, IntWritable&gt;(0.1, 10000, 10);
    job.setNumReduceTasks(3);           //reduce个数
    InputSampler.writePartitionFile(job, sampler);


    job.waitForCompletion(true);

    }
}
</code></pre><hr>
<pre><code>public class MaxTempMapper extends Mapper&lt;IntWritable, IntWritable, IntWritable, IntWritable&gt; {
    public MaxTempMapper() {
        System.out.println(&quot;new MaxTempMapper&quot;);
    }

    @Override
    protected void map(IntWritable key, IntWritable value, Context context) throws IOException, InterruptedException {
        context.write(key,value);
    }
}
</code></pre><hr>
<pre><code>public class MaxTempReducer extends Reducer&lt;IntWritable, IntWritable, IntWritable, IntWritable&gt;{

    /**
     * reduce
     */
    protected void reduce(IntWritable key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {
            int max =Integer.MIN_VALUE;
            for(IntWritable iw : values){
                max = max &gt; iw.get() ? max : iw.get() ;
            }
                context.write(key,new IntWritable(max));

    }
}
</code></pre><hr>
<pre><code>public class YearPartitioner extends Partitioner&lt;IntWritable, IntWritable&gt; {

    public int getPartition(IntWritable year, IntWritable temp, int parts) {
        int y = year.get() - 1970;
        if (y &lt; 33) {
            return 0;
        }
        if (y &gt; 33 &amp;&amp; y &lt; 66) {
            return 1;
        }
        else {
            return 2;
        }
    }
}
</code></pre><hr>
<pre><code>全排序官方笔记:
1.定义1个reduce

2.自定义分区函数.
    自行设置分解区间。

3.使用hadoop采样机制。
    通过采样器生成分区文件，结合hadoop的TotalOrderPartitioner进行分区划分。
    TotalOrderPartitioner        //全排序分区类,读取外部生成的分区文件确定区间。

    使用时采样代码在最后端,否则会出现错误。

    //分区文件设置，设置的job的配置对象，不要是之前的conf.
    TotalOrderPartitioner.setPartitionFile(job.getConfiguration(),new Path(&quot;d:/mr/par.lst&quot;));
</code></pre><hr>
<hr>
<p><strong>## 全排序和部分排序二次排序在面试比例很重的 ##</strong><br>分区在map端，分组在reduce端。<br>二次排序就是把value和key组合到一起，然后value就是nullwritable这样子。合在一起成为一个combokey。<br>Combokey不但要可比较还要串行化，hadoop的串行化机制是writable ，串行就是写入到流离去，反串行就是读出来。<br>IntWritable LongWritable这些都是经过串行化实现的类，Writable有很多子类。 </p>
<h1 id="二次排序"><a href="#二次排序" class="headerlink" title="二次排序"></a>二次排序</h1><p>代码如下：</p>
<pre><code>/**
 * 自定义组合key
 */
public class ComboKey implements WritableComparable&lt;ComboKey&gt; {
    private int year;
    private int temp;

    public int getYear() {
        return year;
    }

    public void setYear(int year) {
        this.year = year;
    }

    public int getTemp() {
        return temp;
    }

    public void setTemp(int temp) {
        this.temp = temp;
    }

    /**
     * 对key进行比较实现
     */
    public int compareTo(ComboKey o) {
        int y0 = o.getYear();
        int t0 = o.getTemp();
        //年份相同(升序)
        if (year == y0) {
            //气温降序
            return -(temp - t0);
        } else {
            return year - y0;
        }
    }

    /**
     * 串行化过程
     */
    public void write(DataOutput out) throws IOException {
        //年份
        out.writeInt(year);
        //气温
        out.writeInt(temp);
    }

    public void readFields(DataInput in) throws IOException {
        year = in.readInt();
        temp = in.readInt();
    }
}
</code></pre><hr>
<pre><code>/**
 *ComboKeyComparator
 */
public class ComboKeyComparator extends WritableComparator {

    protected ComboKeyComparator() {
        super(ComboKey.class, true);
    }

    public int compare(WritableComparable a, WritableComparable b) {
        ComboKey k1 = (ComboKey) a;
        ComboKey k2 = (ComboKey) b;
        return k1.compareTo(k2);
    }
}
</code></pre><hr>
<pre><code>    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        conf.set(&quot;fs.defaultFS&quot;, &quot;file:///&quot;);

        Job job = Job.getInstance(conf);

//        设置作业的各种属性
        job.setJobName(&quot;SecondarySortApp&quot;);    //作业名称
        job.setJarByClass(MaxTempApp.class); //搜索类路径
        job.setInputFormatClass(TextInputFormat.class);//设置输入格式类

        //添加输入路径
        FileInputFormat.addInputPath(job, new Path(args[0]));
        //设置输出路径
        org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(args[1]));

        job.setMapperClass(MaxTempMapper.class);         //mapper类
        job.setReducerClass(MaxTempReducer.class);       //reduce类

        //设置map输出类型
        job.setMapOutputKeyClass(ComboKey.class);
        job.setMapOutputValueClass(NullWritable.class);

        //设置Reduceoutput类型
        job.setOutputKeyClass(IntWritable.class);          //设置输出类型
        job.setOutputValueClass(IntWritable.class);

        //设置分区类
        job.setPartitionerClass(YearPartitioner.class);
        //设置分组对比器。
        job.setGroupingComparatorClass(YearGroupComparator.class);
        //设置排序对比器，听说不写那个ComboKeyComparator不设置也可以
        job.setSortComparatorClass(ComboKeyComparator.class);
        //reduce个数
        job.setNumReduceTasks(3);
        job.waitForCompletion(true);

    }
}
</code></pre><hr>
<pre><code>public class MaxTempMapper extends Mapper&lt;LongWritable, Text, ComboKey, NullWritable&gt; {
    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        String line = value.toString();
        String[] arr = line.split(&quot; &quot;);

        ComboKey keyout = new ComboKey();
        keyout.setYear(Integer.parseInt(arr[0]));
        keyout.setTemp(Integer.parseInt(arr[1]));
        context.write(keyout, NullWritable.get());
    }
  }
</code></pre><hr>
<pre><code>/**
 * Reducer
 */
public class MaxTempReducer extends Reducer&lt;ComboKey, NullWritable, IntWritable, IntWritable&gt;{
    protected void reduce(ComboKey key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException {
            int year = key.getYear();
            int temp = key.getTemp();
            context.write(new IntWritable(year),new IntWritable(temp));
    }
}
</code></pre><hr>
<pre><code>public class YearGroupComparator extends WritableComparator {

    protected YearGroupComparator() {
        super(ComboKey.class, true);
    }

    public int compare(WritableComparable a, WritableComparable b) {
        ComboKey k1 = (ComboKey) a ;
        ComboKey k2 = (ComboKey) b ;
        return k1.getYear() - k2.getYear() ;
    }
}
</code></pre><hr>
<pre><code>public class YearPartitioner extends Partitioner&lt;ComboKey,NullWritable&gt; {

    public int getPartition(ComboKey key, NullWritable nullWritable, int numPartitions) {
        int year = key.getYear();
        return year % numPartitions;
    }
}
</code></pre><hr>
<hr>
<hr>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/10/11/Hadoop第六天之Yarn作业提交/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/11/Hadoop第六天之Yarn作业提交/" itemprop="url">Hadoop第六天之Yarn作业提交</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-10-11T15:48:32+08:00">
                2018-10-11
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hadoop/" itemprop="url" rel="index">
                    <span itemprop="name">Hadoop</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="本地模式job提交流程"><a href="#本地模式job提交流程" class="headerlink" title="本地模式job提交流程"></a>本地模式job提交流程</h2><pre><code>mr.Job = new Job();
job.setxxx();
JobSubmitter.提交
LocalJobRunner.Job();
start();
</code></pre><h2 id="hdfs-write"><a href="#hdfs-write" class="headerlink" title="hdfs write"></a>hdfs write</h2><pre><code>packet,
</code></pre><h2 id="hdfs-切片计算方式"><a href="#hdfs-切片计算方式" class="headerlink" title="hdfs 切片计算方式"></a>hdfs 切片计算方式</h2><pre><code>getFormatMinSplitSize() = 1

//最小值(&gt;=1)                            1                        0
long minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));

//最大值(&lt;= Long.Max , mapreduce.input.fileinputformat.split.maxsize=)
long maxSize = getMaxSplitSize(job);

//得到block大小
long blockSize = file.getBlockSize();

//minSplit maxSplit blockSize
//Math.max(minSize, Math.min(maxSize, blockSize));
</code></pre><p>LF : Line feed,换行符<br>private static final byte CR = ‘\r’;<br>private static final byte LF = ‘\n’;</p>
<h2 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h2><pre><code>1.Windows
    源文件大小:82.8k
    源文件类型:txt
    压缩性能比较
                |    DeflateCodec    GzipCodec    BZip2Codec    Lz4Codec    SnappyCodec    |结论
    ------------|-------------------------------------------------------------------|----------------------
    压缩时间(ms)|    450                7            196            44            不支持        |Gzip &gt; Lz4 &gt; BZip2 &gt; Deflate
    ------------|-------------------------------------------------------------------|----------------------
    解压时间(ms)|    444                66            85            33                        |lz4  &gt; gzip &gt; bzip2 &gt; Deflate
    ------------|-------------------------------------------------------------------|----------------------
    占用空间(k)    |    19k                19k            17k            31k            不支持        |Bzip &gt; Deflate = Gzip &gt; Lz4
                |                                                                    |

2.CentOS
    源文件大小:82.8k
    源文件类型:txt
                |    DeflateCodec    GzipCodec    BZip2Codec    Lz4Codec    LZO        SnappyCodec    |结论
    ------------|---------------------------------------------------------------------------|----------------------
    压缩时间(ms)|    944                77            261            53            77        不支持        |Gzip &gt; Lz4 &gt; BZip2 &gt; Deflate
    ------------|---------------------------------------------------------------------------|----------------------
    解压时间(ms)|    67                66            106            52            73                    |lz4  &gt; gzip &gt; Deflate&gt; lzo &gt; bzip2 
    ------------|---------------------------------------------------------------------------|----------------------
    占用空间(k)    |    19k                19k            17k            31k            34k                    |Bzip &gt; Deflate = Gzip &gt; Lz4 &gt; lzo
</code></pre><h2 id="远程调试"><a href="#远程调试" class="headerlink" title="远程调试"></a>远程调试</h2><pre><code>1.设置服务器java vm的-agentlib:jdwp选项.
[server]
//windwos
//set JAVA_OPTS=%JAVA_OPTS% -agentlib:jdwp=transport=dt_socket,address=8888,server=y,suspend=n

//linux
export HADOOP_CLIENT_OPTS=-agentlib:jdwp=transport=dt_socket,address=8888,server=y,suspend=y

2.在server启动java程序
    hadoop jar HdfsDemo.jar com.it18zhang.hdfs.mr.compress.TestCompress

3.server会暂挂在8888.
    Listening ...

4.客户端通过远程调试连接到远程主机的8888.

5.客户端就可以调试了。
</code></pre><p>hadoop jar<br>java </p>
<p>hadoop jar com.it18zhang.hdfs.mr.compress.TestCompress</p>
<p>export HADOOP_CLIENT_OPTS=-agentlib:jdwp=transport=dt_socket,address=8888,server=y,suspend=y</p>
<h2 id="在pom-xml中引入新的插件-maven-antrun-plugin-实现文件的复制"><a href="#在pom-xml中引入新的插件-maven-antrun-plugin-实现文件的复制" class="headerlink" title="在pom.xml中引入新的插件(maven-antrun-plugin),实现文件的复制."></a>在pom.xml中引入新的插件(maven-antrun-plugin),实现文件的复制.</h2><pre><code>[pom.xml]
&lt;project&gt;
    ...
    &lt;build&gt;
        &lt;plugins&gt;
            ...
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-antrun-plugin&lt;/artifactId&gt;
                &lt;version&gt;1.8&lt;/version&gt;
                &lt;executions&gt;
                    &lt;execution&gt;
                        &lt;phase&gt;package&lt;/phase&gt;
                        &lt;goals&gt;
                            &lt;goal&gt;run&lt;/goal&gt;
                        &lt;/goals&gt;
                        &lt;configuration&gt;
                            &lt;tasks&gt;
                                &lt;echo&gt;---------开始复制jar包到共享目录下----------&lt;/echo&gt;
                                &lt;delete file=&quot;D:\downloads\bigdata\data\HdfsDemo-1.0-SNAPSHOT.jar&quot;&gt;&lt;/delete&gt;
                                &lt;copy file=&quot;target/HdfsDemo-1.0-SNAPSHOT.jar&quot; toFile=&quot;D:\downloads\bigdata\data\HdfsDemo.jar&quot;&gt;
                                &lt;/copy&gt;
                            &lt;/tasks&gt;
                        &lt;/configuration&gt;
                    &lt;/execution&gt;
                &lt;/executions&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;
    ...
&lt;/project&gt;
</code></pre><h2 id="在centos上使用yum安装snappy压缩库文件"><a href="#在centos上使用yum安装snappy压缩库文件" class="headerlink" title="在centos上使用yum安装snappy压缩库文件"></a>在centos上使用yum安装snappy压缩库文件</h2><pre><code>[google snappy]
$&gt;sudo yum search snappy                #查看是否有snappy库
$&gt;sudo yum install -y snappy.x86_64        #安装snappy压缩解压缩库
</code></pre><h2 id="库文件"><a href="#库文件" class="headerlink" title="库文件"></a>库文件</h2><pre><code>windows    :dll(dynamic linked library)
linux    :so(shared object)
</code></pre><h2 id="LZO"><a href="#LZO" class="headerlink" title="LZO"></a>LZO</h2><pre><code>1.在pom.xml引入lzo依赖
    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
    &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;
             xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
             xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
        &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
        &lt;groupId&gt;com.it18zhang&lt;/groupId&gt;
        &lt;artifactId&gt;HdfsDemo&lt;/artifactId&gt;
        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
        &lt;packaging&gt;jar&lt;/packaging&gt;
        &lt;build&gt;
            &lt;plugins&gt;
                &lt;plugin&gt;
                    &lt;groupId&gt;org.apache.maven.plugins
                    &lt;/groupId&gt;
                    &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
                    &lt;configuration&gt;
                        &lt;source&gt;1.8&lt;/source&gt;
                        &lt;target&gt;1.8&lt;/target&gt;
                    &lt;/configuration&gt;
                &lt;/plugin&gt;
                &lt;plugin&gt;
                    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                    &lt;artifactId&gt;maven-antrun-plugin&lt;/artifactId&gt;
                    &lt;version&gt;1.8&lt;/version&gt;
                    &lt;executions&gt;
                        &lt;execution&gt;
                            &lt;phase&gt;package&lt;/phase&gt;
                            &lt;goals&gt;
                                &lt;goal&gt;run&lt;/goal&gt;
                            &lt;/goals&gt;
                            &lt;configuration&gt;
                                &lt;tasks&gt;
                                    &lt;echo&gt;---------开始复制jar包到共享目录下----------&lt;/echo&gt;
                                    &lt;delete file=&quot;D:\downloads\bigdata\data\HdfsDemo-1.0-SNAPSHOT.jar&quot;&gt;&lt;/delete&gt;
                                    &lt;copy file=&quot;target/HdfsDemo-1.0-SNAPSHOT.jar&quot; toFile=&quot;D:\downloads\bigdata\data\HdfsDemo.jar&quot;&gt;
                                    &lt;/copy&gt;
                                &lt;/tasks&gt;
                            &lt;/configuration&gt;
                        &lt;/execution&gt;
                    &lt;/executions&gt;
                &lt;/plugin&gt;
            &lt;/plugins&gt;
        &lt;/build&gt;
        &lt;dependencies&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
                &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;
                &lt;version&gt;2.7.3&lt;/version&gt;
            &lt;/dependency&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
                &lt;artifactId&gt;hadoop-yarn-common&lt;/artifactId&gt;
                &lt;version&gt;2.7.3&lt;/version&gt;
            &lt;/dependency&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
                &lt;artifactId&gt;hadoop-yarn-client&lt;/artifactId&gt;
                &lt;version&gt;2.7.3&lt;/version&gt;
            &lt;/dependency&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
                &lt;artifactId&gt;hadoop-yarn-server-resourcemanager&lt;/artifactId&gt;
                &lt;version&gt;2.7.3&lt;/version&gt;
            &lt;/dependency&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;org.anarres.lzo&lt;/groupId&gt;
                &lt;artifactId&gt;lzo-hadoop&lt;/artifactId&gt;
                &lt;version&gt;1.0.0&lt;/version&gt;
                &lt;scope&gt;compile&lt;/scope&gt;
            &lt;/dependency&gt;

            &lt;dependency&gt;
                &lt;groupId&gt;junit&lt;/groupId&gt;
                &lt;artifactId&gt;junit&lt;/artifactId&gt;
                &lt;version&gt;4.11&lt;/version&gt;
            &lt;/dependency&gt;

        &lt;/dependencies&gt;
    &lt;/project&gt;

2.在centos上安装lzo库
    $&gt;sudo yum -y install lzo

3.使用mvn命令下载工件中的所有依赖
    进入pom.xml所在目录，运行cmd：
    mvn -DoutputDirectory=./lib -DgroupId=com.it18zhang -DartifactId=HdfsDemo -Dversion=1.0-SNAPSHOT dependency:copy-dependencies

4.在lib下存放依赖所有的第三方jar

5.找出lzo-hadoop.jar + lzo-core.jar复制到hadoop的响应目录下。
    $&gt;cp lzo-hadoop.jar lzo-core.jar /soft/hadoop/shared/hadoop/common/lib

6.执行远程程序即可。
</code></pre><h2 id="修改maven使用aliyun镜像。"><a href="#修改maven使用aliyun镜像。" class="headerlink" title="修改maven使用aliyun镜像。"></a>修改maven使用aliyun镜像。</h2><pre><code>[maven/conf/settings.xml]
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;settings xmlns=&quot;http://maven.apache.org/SETTINGS/1.0.0&quot;
          xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
          xsi:schemaLocation=&quot;http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd&quot;&gt;
  &lt;pluginGroups&gt;
  &lt;/pluginGroups&gt;

  &lt;proxies&gt;
  &lt;/proxies&gt;

&lt;servers&gt;
    &lt;server&gt;
        &lt;id&gt;releases&lt;/id&gt;
        &lt;username&gt;admin&lt;/username&gt;
        &lt;password&gt;admin123&lt;/password&gt;
    &lt;/server&gt;
    &lt;server&gt;
        &lt;id&gt;snapshots&lt;/id&gt;
        &lt;username&gt;admin&lt;/username&gt;
        &lt;password&gt;admin123&lt;/password&gt;
    &lt;/server&gt;
    &lt;server&gt;
        &lt;id&gt;Tomcat7&lt;/id&gt;
        &lt;username&gt;tomcat&lt;/username&gt;
        &lt;password&gt;tomcat&lt;/password&gt;
    &lt;/server&gt;
&lt;/servers&gt;

&lt;mirrors&gt;
     &lt;mirror&gt;
        &lt;id&gt;nexus-aliyun&lt;/id&gt;
        &lt;mirrorOf&gt;*&lt;/mirrorOf&gt;
        &lt;name&gt;Nexus aliyun&lt;/name&gt;
        &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt;
    &lt;/mirror&gt; 
&lt;/mirrors&gt;
&lt;/settings&gt;
</code></pre><h2 id="文件格式-SequenceFile"><a href="#文件格式-SequenceFile" class="headerlink" title="文件格式:SequenceFile"></a>文件格式:SequenceFile</h2><pre><code>1.SequenceFile
    Key-Value对方式。

2.不是文本文件，是二进制文件。

3.可切割
    因为有同步点。
    reader.sync(pos);    //定位到pos之后的第一个同步点。
    writer.sync();        //写入同步点

4.压缩方式
    不压缩
    record压缩            //只压缩value
    块压缩                //按照多个record形成一个block.
</code></pre><h2 id="文件格式-MapFile"><a href="#文件格式-MapFile" class="headerlink" title="文件格式:MapFile"></a>文件格式:MapFile</h2><pre><code>1.Key-value
2.key按升序写入(可重复)。
3.mapFile对应一个目录，目录下有index和data文件,都是序列文件。
4.index文件划分key区间,用于快速定位。
</code></pre><h2 id="自定义分区函数"><a href="#自定义分区函数" class="headerlink" title="自定义分区函数"></a>自定义分区函数</h2><pre><code>1.定义分区类
    public class MyPartitioner extends Partitioner&lt;Text, IntWritable&gt;{
        public int getPartition(Text text, IntWritable intWritable, int numPartitions) {
            return 0;
        }
    }
2.程序中配置使用分区类
    job.setPartitionerClass(MyPartitioner.class);
</code></pre><h2 id="combinerd（合成）-继承了Reducer-任何的Reducer类都能被他使用"><a href="#combinerd（合成）-继承了Reducer-任何的Reducer类都能被他使用" class="headerlink" title="combinerd（合成） 继承了Reducer 任何的Reducer类都能被他使用"></a>combinerd（合成） 继承了Reducer 任何的Reducer类都能被他使用</h2><p> Map端的Reducer  预先化简<br>1，为了减少网络带宽 将Map端发出的的数据进行聚合 并不是所有的都可以用combiner<br>2,combiner </p>
<hr>
<p>切片个数是四个，mapper就需要也是4个 </p>
<p>下图是一个客户端的写入过程：首先形成一个管线，因为有很多节点，节点形成管线，现在本地构建2个队列，一个发送队列sendQueue，一个是确认队列ackQueue，客户端吧消息封装成Packet放到队列里面，而且结束后会放一个空包进去，发送的话，是以管线的方式发给A，A在发给B，B在发给C，在发送的过程中有一个校验的过程chunk(4+512)就是4个校验位+512个数据字节，一个Packet是64K+33个头的字节，在确认队列里面是一个编号seqNo如果在丢列中ABC中都回执，就吧seqNo在ackQueue中移除。发送如果失败，就吧管线重新建立，吧没法送的包返回到队列之前的一个容错机制。<br><img src="https://i.imgur.com/43B2Ebp.png" alt=""></p>
<p>输入格式定位文件，在输入格式中间有一个交互reader，在Map和Reduce都有这么一些方法，在Mapper和Reducer中有回调方法，就是本来使用的是子类的方法，但是子类中调用有父类的的run方法，然后run方法里面有reduce方法也就又调用了子类的方法，这叫做回调 。Mapper是通过reader读取下一条数据传进来的。Map和Reduce中间有一个shuffer的混洗过程，网络间分发数据的过程， Map里面先分区，分区个数等于Reduce的个数，所以有几个reduce就有几个分区，默认情况下，Map分区通过哈希算法来分区的，对K哈希。只要K相同，一定会被哈希到一个分区里面，分区也就是桶的概念，也就是只要K相同进入到同一个桶里面，只要是同一个分区就是进入到同一个reduce，默认情况下inputformat的格式，他的K就是偏移量。如果是图片格式的话就可能是别的。只有在文本输入的时候才是偏移量，当然也可以自己定义Inputformat格式。在mr计算之前切片就已经算好了，产生的切片文件和配置XML文件放到了临时目录下了。<br><img src="https://i.imgur.com/7qjZLGJ.png" alt=""></p>
<p>看下图，首先客户端运行作业，第一步run job，第二部走ResourceManager，是Yarn层面，负责资源调度，所有集群的资源都属于ResourceManager，第二部get new application，首先得到应用的ID，第三部copyt job resources，复制切片信息和xml配置信息和jar包到HDFS，第四部提交给资源管理器，然后资源管理器start container，然后通过节点管理器启动一个MRAPPMaster（应用管理程序），第六步初始化Job，第七部检索切片信息，第八步请求资源管理器去分配一个资源列表，第九步启动节点管理器，通过节点管理器来启动一个JVM虚拟机，在虚拟机里面启动yarn子进程，第十部通过子进程读取切片信息，然后在运行MapTast或者ReduceTask<br><img src="https://i.imgur.com/lqVQLT1.png" alt=""></p>
<p>客户端提取一个作业，然后联系资源管理器，为了得到id,在完全分布式下客户端client叫job，一旦提交就叫做application了，先请求，然后得到了appid返回给了客户端，第三部将作业信息上传到HDFS的临时目录，存档切片信息和jar包，然后找到节点管理器，然后NM孵化一个MRAPPMaster，然后检索HDFS文件系统，获取到有多少个要执行的并发任务，拿到任务之后，要求资源管理器分配一个集合，哪些节点可以用。 然后MRAppmaster在联系其他的NM，让NM去开启Yarn子进程，子进程在去检索HDFS信息，在开启Map和Reduce</p>
<p><img src="https://i.imgur.com/bn7A9H6.png" alt=""></p>
<h2 id="hdfs-切片计算方式-1"><a href="#hdfs-切片计算方式-1" class="headerlink" title="hdfs 切片计算方式"></a>hdfs 切片计算方式</h2><pre><code>getFormatMinSplitSize() = 1

//最小值(&gt;=1)                            1                        0
long minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));

//最大值(&lt;= Long.Max , mapreduce.input.fileinputformat.split.maxsize=)
long maxSize = getMaxSplitSize(job);

//得到block大小
long blockSize = file.getBlockSize();

//minSplit maxSplit blockSize
//Math.max(minSize, Math.min(maxSize, blockSize));
</code></pre><p>在计算切片的时候一般块大小不设置，是最好的，一般就是128M，采用本地优先策略，一个快大小是128m如果一旦把他边变1m那么切片大小就会变成1M，一个，然后128M的块大小会被分发128份切片分别传输给MR去计算，这样的话，就会增加网络间的传输，增加网络负载。所以128m是最合适的</p>
<hr>
<p>CR=‘\r’回车符<br>LF+’\n’换行符</p>
<p>windows系统里面是\r\n。回车带换行。<br>linux系统只有一个\n</p>
<p>切片是定义大方向的，阅读器recorderreader是控制细节的。切片定义任务数量，开几个map但是不一定是每个map都有任务。</p>
<p><strong>切片问题</strong>是物理设置，但是是逻辑读取。</p>
<p><img src="https://i.imgur.com/tgAbsGx.png" alt=""></p>
<p>打个比方说上图这个东西比如说第一个例子，前面的设置物理划分，根据字节数划分成了4个切片，也就是有4个Mapper,然后四个切片开始读取，其中第一个物理切割到t但是实际上，由于切片读取到换行符所以，第一个Mapper也就是读取到了hello world tom，然后第二个，第二个偏移量是13+13.偏移量是13，在物理读取13个直接，但是由于Mapper检测不是行首，所以直接从下一行读取也在读取一整行。这个就是每个切片128MB的缩影，吧128M变成了13而已。然后其实Textinputformat下一个环节是。recoderreader。这个会先检测是否是行首，<br><img src="https://i.imgur.com/7qjZLGJ.png" alt="">      </p>
<h2 id="压缩问题"><a href="#压缩问题" class="headerlink" title="压缩问题"></a>压缩问题</h2><pre><code>package com.it18zhang.hdfs.mr.compress;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.io.IOUtils;
import org.apache.hadoop.io.compress.*;
import org.apache.hadoop.util.ReflectionUtils;
import org.junit.Test;

import java.io.FileInputStream;
import java.io.FileOutputStream;

/**
 * @Title:TestCompress
 * @Description:
 * @Auther: Eric Hunn
 * @Version:1.0
 * @Create 2018/10/14 19:39
 */
public class TestCompress {
    @Test
    public void deflateCompress() throws Exception {
        Class[] zipClasses = {
                DeflateCodec.class,
                GzipCodec.class,
                BZip2Codec.class,
                Lz4Codec.class,
        };

        for(Class c : zipClasses){
            zip(c);
        }
    }

    /*压缩测试
     *
     * */
    public void zip(Class codecClass) throws Exception {
        long start = System.currentTimeMillis();
        //实例化对象
        CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, new Configuration());
        //创建文件输出流，得到默认拓展名
        FileOutputStream fos = new FileOutputStream(&quot;e:/comp/b&quot; + codec.getDefaultExtension());
        //得到压缩流
        CompressionOutputStream zipOut = codec.createOutputStream(fos);
        IOUtils.copyBytes(new FileInputStream(&quot;e:/11111.txt/&quot;), zipOut, 1024);
        zipOut.close();
        System.out.println(codecClass.getSimpleName() + &quot; ：&quot;);
    }
}                                     
</code></pre><p>压缩一般是在文件Map输出的时候压缩，Map输出的时候内存不够一般要往磁盘里面溢出，在溢出的时候可以让他压缩溢出，这样reduce的时候就要解压缩。上面代码测试过lz4不行，但是视频上是可以的，暂时不知道哪里出错了。</p>
<pre><code>package com.it18zhang.hdfs.mr.compress;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.io.IOUtils;
import org.apache.hadoop.io.compress.*;
import org.apache.hadoop.util.ReflectionUtils;
import org.junit.Test;

import java.io.FileInputStream;
import java.io.FileOutputStream;

/**
 * @Title:TestCompress
 * @Description:
 * @Auther: Eric Hunn
 * @Version:1.0
 * @Create 2018/10/14 19:39
 */
public class TestCompress {
    @Test
    public void deflateCompress() throws Exception {
        Class[] zipClasses = {
                DeflateCodec.class,
                GzipCodec.class,
                BZip2Codec.class,
//                Lz4Codec.class,
//                SnappyCodec.class,
        };

        for(Class c : zipClasses){
            unzip(c);
        }
    }

    /*压缩测试
     *
     * */
    public void zip(Class codecClass) throws Exception {
        long start = System.currentTimeMillis();
        //实例化对象
        CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, new Configuration());
        //创建文件输出流，得到默认拓展名
        FileOutputStream fos = new FileOutputStream(&quot;e:/comp/b&quot; + codec.getDefaultExtension());
        //得到压缩流
        CompressionOutputStream zipOut = codec.createOutputStream(fos);
        IOUtils.copyBytes(new FileInputStream(&quot;e:/11111.txt/&quot;), zipOut, 1024);
        zipOut.close();
        System.out.println(codecClass.getSimpleName() + &quot; ：&quot;);
    }


    public void unzip(Class codecClass) throws Exception {
        long start = System.currentTimeMillis();
        //实例化对象
        CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, new Configuration());
        //创建文件输出流，得到默认拓展名
        FileInputStream fis = new FileInputStream(&quot;e:/comp/b&quot; + codec.getDefaultExtension());
        //得到压缩流
        CompressionInputStream zipIn = codec.createInputStream(fis);
        IOUtils.copyBytes(zipIn,new FileOutputStream(&quot;e:/comp/b&quot; + codec.getDefaultExtension() + &quot;.txt&quot;), 1024);
        zipIn.close();
        System.out.println(codecClass.getSimpleName() + &quot; ：&quot; + (System.currentTimeMillis() - start));
    }
}
</code></pre><h2 id="在集群上运"><a href="#在集群上运" class="headerlink" title="在集群上运"></a>在集群上运</h2><pre><code>package com.it18zhang.hdfs.mr.compress;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.io.IOUtils;
import org.apache.hadoop.io.compress.*;
import org.apache.hadoop.util.ReflectionUtils;

import java.io.FileInputStream;
import java.io.FileOutputStream;

/**
 * @Title:TestCompress
 * @Description:
 * @Auther: Eric Hunn
 * @Version:1.0
 * @Create 2018/10/14 19:39
 */
public class TestCompress {
    public static void main(String[] args) throws Exception {
        Class[] zipClasses = {
                DeflateCodec.class,
                GzipCodec.class,
                BZip2Codec.class,
//                Lz4Codec.class,
//                SnappyCodec.class,
        };

        for(Class c : zipClasses){
            zip(c);
        }
        System.out.println(&quot;==================================&quot;);
        for(Class c : zipClasses){
            unzip(c);
        }
    }

    /*压缩测试
     *
     * */
    public static void zip(Class codecClass) throws Exception {
        long start = System.currentTimeMillis();
        //实例化对象
        CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, new Configuration());
        //创建文件输出流，得到默认拓展名
        FileOutputStream fos = new FileOutputStream(&quot;/home/centos/zip/b&quot; + codec.getDefaultExtension());
        //得到压缩流
        CompressionOutputStream zipOut = codec.createOutputStream(fos);
        IOUtils.copyBytes(new FileInputStream(&quot;/home/centos/zip/a.txt&quot;), zipOut, 1024);
        zipOut.close();
        System.out.println(codecClass.getSimpleName() + &quot; ：&quot;);
    }


    public static void unzip(Class codecClass) throws Exception {
        long start = System.currentTimeMillis();
        //实例化对象
        CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, new Configuration());
        //创建文件输出流，得到默认拓展名
        FileInputStream fis = new FileInputStream(&quot;/home/centos/zip/b&quot; + codec.getDefaultExtension());
        //得到压缩流
        CompressionInputStream zipIn = codec.createInputStream(fis);
        IOUtils.copyBytes(zipIn,new FileOutputStream(&quot;/home/centos/zip/b&quot; + codec.getDefaultExtension() + &quot;.txt&quot;), 1024);
        zipIn.close();
        System.out.println(codecClass.getSimpleName() + &quot; ：&quot; + (System.currentTimeMillis() - start));
    }
}
</code></pre><h2 id="远程调试-1"><a href="#远程调试-1" class="headerlink" title="远程调试"></a>远程调试</h2><pre><code>1.设置服务器java vm的-agentlib:jdwp选项.
[server]
//windwos
//set JAVA_OPTS=%JAVA_OPTS% -agentlib:jdwp=transport=dt_socket,address=8888,server=y,suspend=n

//linux


2.在server启动java程序
    hadoop jar HdfsDemo.jar com.it18zhang.hdfs.mr.compress.TestCompress

3.server会暂挂在8888.
    Listening ...

4.客户端通过远程调试连接到远程主机的8888.

5.客户端就可以调试了。
</code></pre><hr>
<hr>
<h3 id="通过MapFile来写入"><a href="#通过MapFile来写入" class="headerlink" title="通过MapFile来写入"></a>通过MapFile来写入</h3><pre><code>    /*写操作
 * */
@Test
public void save() throws Exception {
    Configuration conf = new Configuration();
    conf.set(&quot;fs.defaultFS &quot;, &quot;file:///&quot;);
    FileSystem fs = FileSystem.get(conf);
    Path p = new Path(&quot;e:/seq/1.seq&quot;);
    MapFile.Writer writer = new MapFile.Writer(conf, fs, &quot;e:/map&quot;, IntWritable.class, Text.class);


    for (int i = 0; i &lt; 100000; i++) {
        writer.append(new IntWritable(i), new Text(&quot;tom&quot; + i));
    }
</code></pre><p>//        for(int i =0 ;i &lt; 10 ; i++){<br>//            writer.append(new IntWritable(i),new Text(“tom” + i));<br>//        }<br>        writer.close();<br>    }</p>
<hr>
<h3 id="通过MapFile来读取"><a href="#通过MapFile来读取" class="headerlink" title="通过MapFile来读取"></a>通过MapFile来读取</h3><pre><code>/*读取Mapfile文件
 * */
@Test
public void readMapfile() throws Exception {
    Configuration conf = new Configuration();
    conf.set(&quot;fs.defaultFS &quot;, &quot;file:///&quot;);
    FileSystem fs = FileSystem.get(conf);
    Path p = new Path(&quot;e:/seq/1.seq&quot;);
    MapFile.Reader reader = new MapFile.Reader(fs, &quot;e:/map&quot;, conf);
    IntWritable key = new IntWritable();
    Text value = new Text();
    while (reader.next(key, value)) {
        System.out.println(key.get() + &quot;:&quot; + value.toString());
    }
    reader.close();


} 
</code></pre><hr>
<p>Map的分区是哈希分区</p>
<p>combiner的好处是：map端的reduce在map端做集合，减少了shuffle量。统计每个单词hello 1发了十万次，不如发一个hello1在map端做聚合发过去hello十万，即刻。就是在map端口预先reduce化简的过程。不是所有的作业都可以把reduce做成combiner。最大值最小值都可以combiner但是平均值就不行了。</p>
<p>第六天的最后两个视频没有往下写代码，因为太多太杂了。主要讲解MR计数器，数据倾斜，Mapfile 序列化读取，压缩这些。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/10/05/Hadoop第五天01之hdfs写入剖析/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/05/Hadoop第五天01之hdfs写入剖析/" itemprop="url">Hadoop第五天01之hdfs写入剖析</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-10-05T16:31:19+08:00">
                2018-10-05
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hadoop/" itemprop="url" rel="index">
                    <span itemprop="name">Hadoop</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>一段HDFS写入流源码分析。</p>
<p><img src="https://i.imgur.com/17cBkFO.png" alt=""></p>
<p>首先这个得到了一个抽象类，这个FileSystemm是一个抽象类。</p>
<p><img src="https://i.imgur.com/Rlbd5VC.png" alt=""></p>
<p>这个抽象类有子类，左边有一个箭头，点击得到了一个这个抽象类的所有的子类，简单看下，因为Hadoop包类的hadfs这个，得到的子类是DistributedFileSystem这个分布式文件系统。</p>
<p><img src="https://i.imgur.com/bzQ91XF.png" alt=""></p>
<p>也可以吧鼠标放到fs上会显示返回的类</p>
<p><img src="https://i.imgur.com/JOfPZvk.png" alt=""></p>
<p>也可以在IDEA的右下角的类标签里面找到：</p>
<p><img src="https://i.imgur.com/WSz2duV.png" alt=""></p>
<p>也就是说返回了一个DistributedFIleSystem,</p>
<p>然后文件系统fs.create调用了create方法返回了一个FSDataOutputStream流，这个是一个HDFS数据输出流</p>
<p><img src="https://i.imgur.com/SKFBqkH.png" alt=""></p>
<p><img src="https://i.imgur.com/0aIVTzf.png" alt=""></p>
<p>单机F5单部进入第一个：</p>
<p><img src="https://i.imgur.com/nnxWOri.png" alt=""></p>
<p><img src="https://i.imgur.com/f434RJR.png" alt=""></p>
<p><img src="https://i.imgur.com/4Cf1ljy.png" alt=""></p>
<p>看这个，进到下一步，调用checkClosed()，方法，这个方法其实是继承FSOutput流里面的东西。  </p>
<p><img src="https://i.imgur.com/lHPzWs4.png" alt=""><br>看到这个里面out=DFSOutputStream。这个是被装饰的流。调用这个流的write方法</p>
<p>HDFS流是对DFS输出流的包装</p>
<p>进去这个是装饰模式。</p>
<p>在这个构造模式中也声明了字段。</p>
<p><img src="https://i.imgur.com/8CsurDw.png" alt=""></p>
<p>下一步：</p>
<p><img src="https://i.imgur.com/0f1RcbV.png" alt=""></p>
<p>调用了Close方法因为是继承    都是FSoutput流的子类。一个检查的方法，判断是否数组越界。</p>
<p>下面这个for是个循环，循环写入，。 </p>
<p><img src="https://i.imgur.com/MTsauqO.png" alt=""></p>
<p>然后下一步，进入到write1方法。</p>
<p><img src="https://i.imgur.com/nAIKdn5.png" alt=""></p>
<p>里面的buf是一个缓冲区，count是一个成员常量</p>
<p><img src="https://i.imgur.com/aLCcnkl.png" alt=""></p>
<p>上图的buf看注释，是一个缓冲区用哦过来存储数据，在他被checksum校验和之前</p>
<p>校验和就是检查是否是比之前设置的最小块大小大，而且还必须是512字节的倍数</p>
<p><img src="https://i.imgur.com/pAFducf.png" alt=""></p>
<p>上图点击进入到一个校验和类的里面看注释，校验和用于数据传输之前。</p>
<p><img src="https://i.imgur.com/E8qEJy0.png" alt=""></p>
<p>上图这两行，通过一个算法来实现校验和。通过CRC32算法类似于哈希算法。</p>
<p><img src="https://i.imgur.com/RJ1vXHv.png" alt=""></p>
<p>不管他，回到buf缓冲这个地方，单部进入</p>
<p><img src="https://i.imgur.com/RKkQGry.png" alt=""></p>
<p>首先在缓冲区进行一个判定</p>
<p><img src="https://i.imgur.com/qfwP1Yn.png" alt=""></p>
<p>拷贝本地数据到本地的一个buf，显示Localbuffer=4608,count=0.要拷贝的字节是4608。<br>如果拷贝的内容没有缓冲区大就考数据内容，要是比缓冲区大的话，就拷贝缓冲区大小的内容。</p>
<p><img src="https://i.imgur.com/rk8wX6W.png" alt=""></p>
<p>在单部进入到上图。</p>
<p><img src="https://i.imgur.com/PjqlOXa.png" alt=""></p>
<p>返回到代码。进入到源代码中，如上图</p>
<p>在单部进入到这个Close里面：如下图：<br><img src="https://i.imgur.com/97VYh5X.png" alt=""></p>
<p>这个close是out.close。out是调用父类的close方法，而且父类还是一个包装类，所以进入了包装的列的close方法。<br>再进入到close()方法。如下图：</p>
<p><img src="https://i.imgur.com/2ExMNXU.png" alt=""></p>
<p>out的本类HDFSOutputStream，然后这个又是一个装饰类，本质上是DFSOutputStream这个类的方法。</p>
<p>单部进入到Out里面，看一下调用的到底是谁的out。再单部进入到Out里面</p>
<p><img src="https://i.imgur.com/RG98Z9F.png" alt=""></p>
<p>看注释，关闭输出流并且    释放与之相关联的系统资源。<br>上图最终进入到了DFSOutputstream的close()方法里面了。</p>
<p><img src="https://i.imgur.com/dDhSzjg.png" alt=""></p>
<p>接上图，看到在这个close(）方法里面有一个closeImp（)方法。调用自己的关闭实现方法<br>还在这个类里面执行呢：继续在这个类里面往下走:如下图：</p>
<p><img src="https://i.imgur.com/7Vkorih.png" alt=""></p>
<p>这个里面有一个flushbuffer()方法， 在单部进入到这个方法里面。如下图：</p>
<p><img src="https://i.imgur.com/YSCeFmf.png" alt=""></p>
<p>看到这个图里面this和当前类不太一样说明这两个东西也是继承关系。</p>
<p>清理缓冲区两个参数，看注释：强制任何输出流字节被校验和并且被写入底层的输出流。</p>
<p>再单步进入：</p>
<p><img src="https://i.imgur.com/jfWYEI1.png" alt=""></p>
<p><img src="https://i.imgur.com/4iqESya.png" alt=""></p>
<p>看到如上图的内容。然后到了writeChecksumChunks这个方法里面，单部进入到这个里面：</p>
<p><img src="https://i.imgur.com/FM0MpyT.png" alt=""></p>
<p>对给定的数据小块来生成校验和，并且根据小块和校验和给底层的数据输出流。 </p>
<p>单部进入到这个sum.calculateChunckedSum方法里面。</p>
<p><img src="https://i.imgur.com/EVQOpOL.png" alt=""></p>
<p><img src="https://i.imgur.com/UKsSHTO.png" alt=""></p>
<p>下一步</p>
<p><img src="https://i.imgur.com/ifq7oZR.png" alt=""></p>
<p>上图吧数据写入了底层里面去了。</p>
<p>下一步</p>
<p><img src="https://i.imgur.com/WLEldHU.png" alt=""></p>
<p>下一步</p>
<p><img src="https://i.imgur.com/TN0fU7T.png" alt=""></p>
<p>下一步</p>
<p><img src="https://i.imgur.com/X3dHeJt.png" alt=""></p>
<p><img src="https://i.imgur.com/W5NCoWA.png" alt=""></p>
<p>上图就是将底层的内容打成包，再去通过网络传输。在包里面加编号和序号，也就是加顺序。</p>
<p><img src="https://i.imgur.com/INeugMK.png" alt=""></p>
<p>往下不写了太多了。现在的视频位置是hdfs写入剖析2的开头。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/10/04/Hadoop第四天之滚动日志-安全模式/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/04/Hadoop第四天之滚动日志-安全模式/" itemprop="url">Hadoop第四天之滚动日志-安全模式</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-10-04T08:56:35+08:00">
                2018-10-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/10/04/Hadoop第四天之最小块设置-指定副本数/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/04/Hadoop第四天之最小块设置-指定副本数/" itemprop="url">Hadoop第四天之最小块设置-指定副本数</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-10-04T08:21:11+08:00">
                2018-10-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/10/01/遇到未解决的问题/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/01/遇到未解决的问题/" itemprop="url">遇到未解决的问题</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-10-01T11:15:11+08:00">
                2018-10-01
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/问题/" itemprop="url" rel="index">
                    <span itemprop="name">问题</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>在eclipse中，ctrl+左键找到源码，为什么旁边在pakage explore中有有轮廓啊？</p>
<p><img src="https://i.imgur.com/oYjJ6In.png" alt=""></p>
<p><img src="https://i.imgur.com/IZVxlow.png" alt=""></p>
<p>点击两个箭头即可转换。</p>
<p>这个东西是在IDEA中怎么调出来的。</p>
<p><img src="https://i.imgur.com/fz08Y7d.png" alt=""></p>
<hr>
<p>克隆centos之后有时候出现这种情况</p>
<p><img src="https://i.imgur.com/G7CLxft.png" alt=""></p>
<hr>
<p>IDEA的使用问题：<br>这个是什么快捷键。</p>
<p><img src="https://i.imgur.com/hKrt9St.png" alt=""></p>
<hr>
<p>在增强虚拟机工具的时候按照视频的方法没有成功，然后通过挂载的方法实现了，但是hgfs文件夹和文件夹里面的内容都是root权限的，只能修改文件夹的所有者，但是不能修改里面文件的所有者。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/09/30/haoop第三天之脚本分析，单个进程启动/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/30/haoop第三天之脚本分析，单个进程启动/" itemprop="url">haoop第三天之脚本分析，单个进程启动</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-09-30T09:33:38+08:00">
                2018-09-30
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="ssh权限问题"><a href="#ssh权限问题" class="headerlink" title="ssh权限问题"></a>ssh权限问题</h2><pre><code>1.~/.ssh/authorized_keys
    644
2.$/.ssh
    700
3.root
</code></pre><h2 id="配置SSH"><a href="#配置SSH" class="headerlink" title="配置SSH"></a>配置SSH</h2><pre><code>生成密钥对$&gt;ssh-keygen -t rsa -P &apos;&apos; -f ~/.ssh/id_rsa
添加认证文件$&gt;cat ~/.ssh/id_rsa.pub &gt; ~/.ssh/authorized_keys
权限设置,文件和文件夹权限除了自己之外，别人不可写。$&gt;chmod 700 ~/.ssh$&gt;chmod 644 ~/.ssh/authorized_keys
</code></pre><h2 id="scp"><a href="#scp" class="headerlink" title="scp"></a>scp</h2><pre><code>远程复制.
</code></pre><h2 id="rsync"><a href="#rsync" class="headerlink" title="rsync"></a>rsync</h2><pre><code>远程同步,支持符号链接。
rsync -lr xxx xxx
</code></pre><h2 id="完全分布式"><a href="#完全分布式" class="headerlink" title="完全分布式"></a>完全分布式</h2><pre><code>1.配置文件
[core-site.xml]
fs.defaultFS=hdfs://s201:8020/

[hdfs-site.xml]
replication=1        //伪分布
replication=3        //完全分布


[mapred-site.xml]
mapreduce.framework.name=yarn

[yarn-site.xml]
rm.name=s201

[slaves]
s202
s203
s204

2.分发文件
    a)ssh
    openssh-server        //sshd
    openssh-clients        //ssh
    openssh                //ssh-keygen

    b)scp/rsync

3.格式化文件系统
    $&gt;hadoop namenode -format

4.启动hadoop所有进程
    //start-dfs.sh + start-yarn.sh
    $&gt;start-all.sh

5.xcall.sh jps
    /usr/local/bin/jps 
    /usr/local/bin/java

6.查看jps进程
    $&gt;xcall.sh jps

7.关闭centos的防火墙
    $&gt;sudo service firewalld stop        // &lt;=6.5    start/stop/status/restart
    $&gt;sudo systemctl stop firewalld        // 7.0 停止    start/stop/status/restart

    $&gt;sudo systemctl disable firewalld    //关闭
    $&gt;sudo systemctl enable firewalld    //启用


7.最终通过webui
    http://s201:50070/
</code></pre><h2 id="符号连接"><a href="#符号连接" class="headerlink" title="符号连接"></a>符号连接</h2><pre><code>1.修改符号连接的owner
    $&gt;chown -h centos:centos xxx        //-h:针对连接本身，而不是所指文件.

2.修改符号链接
    $&gt;ln -sfT index.html index            //覆盖原有的连接。
</code></pre><h2 id="hadoop模块"><a href="#hadoop模块" class="headerlink" title="hadoop模块"></a>hadoop模块</h2><pre><code>common        //
hdfs        //
mapreduce    //
yarn        //
</code></pre><h2 id="进程"><a href="#进程" class="headerlink" title="进程"></a>进程</h2><pre><code>[hdfs]start-dfs.sh
NameNode            NN
DataNode            DN
SecondaryNamenode    2NN

[yarn]start-yarn.sh
ResourceMananger    RM
NodeManager            NM
</code></pre><h2 id="脚本分析"><a href="#脚本分析" class="headerlink" title="脚本分析"></a>脚本分析</h2><pre><code>sbin/start-all.sh
--------------
    libexec/hadoop-config.sh
    start-dfs.sh
    start-yarn.sh

sbin/start-dfs.sh
--------------
    libexec/hadoop-config.sh
    sbin/hadoop-daemons.sh --config .. --hostname .. start namenode ...
    sbin/hadoop-daemons.sh --config .. --hostname .. start datanode ...
    sbin/hadoop-daemons.sh --config .. --hostname .. start sescondarynamenode ...
    sbin/hadoop-daemons.sh --config .. --hostname .. start zkfc ...            //


sbin/start-yarn.sh
--------------    
    libexec/yarn-config.sh
    bin/yarn-daemon.sh start resourcemanager
    bin/yarn-daemons.sh start nodemanager


sbin/hadoop-daemons.sh
----------------------
    libexec/hadoop-config.sh

    slaves

    hadoop-daemon.sh

sbin/hadoop-daemon.sh
-----------------------
    libexec/hadoop-config.sh
    bin/hdfs ....


sbin/yarn-daemon.sh
-----------------------
    libexec/yarn-config.sh
    bin/yarn


bin/hadoop
------------------------
    hadoop verion        //版本
    hadoop fs            //文件系统客户端.
    hadoop jar            //
    hadoop classpath
    hadoop checknative


bin/hdfs
------------------------
    dfs                        // === hadoop fs
    classpath          
    namenode -format   
    secondarynamenode  
    namenode           
    journalnode        
    zkfc               
    datanode           
    dfsadmin           
    haadmin            
    fsck               
    balancer           
    jmxget             
    mover              

    oiv                
    oiv_legacy         
    oev                
    fetchdt            
    getconf            
    groups             
    snapshotDiff       

    lsSnapshottableDir 

    portmap            
    nfs3               
    cacheadmin         
    crypto             
    storagepolicies    
    version 
</code></pre><h2 id="hdfs常用命令"><a href="#hdfs常用命令" class="headerlink" title="hdfs常用命令"></a>hdfs常用命令</h2><pre><code>$&gt;hdfs dfs -mkdir /user/centos/hadoop
$&gt;hdfs dfs -ls -r /user/centos/hadoop
$&gt;hdfs dfs -lsr /user/centos/hadoop
$&gt;hdfs dfs -put index.html /user/centos/hadoop
$&gt;hdfs dfs -get /user/centos/hadoop/index.html a.html
$&gt;hdfs dfs -rm -r -f /user/centos/hadoop
</code></pre><h2 id="no-route"><a href="#no-route" class="headerlink" title="no route "></a>no route </h2><pre><code>关闭防火墙。
$&gt;su root
$&gt;xcall.sh &quot;service firewalld stop&quot;
$&gt;xcall.sh &quot;systemctl disable firewalld&quot;
</code></pre><h2 id="hdfs"><a href="#hdfs" class="headerlink" title="hdfs"></a>hdfs</h2><pre><code>500G
1024G = 2T/4T
切割。


寻址时间:10ms左右
磁盘速率 : 100M /s

64M
128M            //让寻址时间占用读取时间的1%.

1ms
1 / 100


size = 181260798
block-0 : 134217728
block-1 :  47043070 
--------------------

b0.no : 1073741829
b1.no : 1073741830
</code></pre><h2 id="HA"><a href="#HA" class="headerlink" title="HA"></a>HA</h2><pre><code>high availability,高可用性。通常用几个9衡量。
99.999%
</code></pre><h2 id="SPOF"><a href="#SPOF" class="headerlink" title="SPOF:"></a>SPOF:</h2><pre><code>single point of failure,单点故障。
</code></pre><h2 id="secondarynamenode"><a href="#secondarynamenode" class="headerlink" title="secondarynamenode"></a>secondarynamenode</h2><h2 id="找到所有的配置文件"><a href="#找到所有的配置文件" class="headerlink" title="找到所有的配置文件"></a>找到所有的配置文件</h2><pre><code>1.tar开hadoop-2.7.3.tar.gz
hadoop-2.7.3\share\hadoop\common\hadoop-common-2.7.3.jar\core-default.xml
hadoop-2.7.3\share\hadoop\hdfs\hadoop-hdfs-2.7.3.jar\hdfs-default.xml
hadoop-2.7.3\share\hadoop\mapreduce\hadoop-mapreduce-client-core-2.7.3.jar\mapred-default.xml
hadoop-2.7.3\share\hadoop\yarn\hadoop-yarn-common-2.7.3.jar\yarn-site.xml
</code></pre><h2 id="本地模式"><a href="#本地模式" class="headerlink" title="本地模式"></a>本地模式</h2><pre><code>[core-site.xml]
fs.defaultFS=file:///            //默认值
</code></pre><h2 id="配置hadoop临时目录"><a href="#配置hadoop临时目录" class="headerlink" title="配置hadoop临时目录"></a>配置hadoop临时目录</h2><pre><code>1.配置[core-site.xml]文件
&lt;configuration&gt;
        &lt;property&gt;
                &lt;name&gt;fs.defaultFS&lt;/name&gt;
                &lt;value&gt;hdfs://s201/&lt;/value&gt;
        &lt;/property&gt;
        &lt;!--- 配置新的本地目录 --&gt;
        &lt;property&gt;
                &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
                &lt;value&gt;/home/centos/hadoop&lt;/value&gt;
        &lt;/property&gt;
&lt;/configuration&gt;


//以下属性均由hadoop.tmp.dir决定,在hdfs-site.xml文件中配置。
dfs.namenode.name.dir=file://${hadoop.tmp.dir}/dfs/name
dfs.datanode.data.dir=file://${hadoop.tmp.dir}/dfs/data
dfs.datanode.data.dir=file://${hadoop.tmp.dir}/dfs/data

dfs.namenode.checkpoint.dir=file://${hadoop.tmp.dir}/dfs/namesecondary
dfs.namenode.checkpoint.dir=file://${hadoop.tmp.dir}/dfs/namesecondary


2.分发core-site.xml文件
    $&gt;xsync core-site.xml

3.格式化文件系统,只对namenode的本地目录进行初始化。
    $&gt;hadoop namenode -format        //hdfs namenode -format

4.启动hadoop
    $&gt;start-dfs.sh
</code></pre><h2 id="使用xcall-sh在所有节点上创建jps符号连接，指向-soft-jdk-bin-jps"><a href="#使用xcall-sh在所有节点上创建jps符号连接，指向-soft-jdk-bin-jps" class="headerlink" title="使用xcall.sh在所有节点上创建jps符号连接，指向/soft/jdk/bin/jps"></a>使用xcall.sh在所有节点上创建jps符号连接，指向/soft/jdk/bin/jps</h2><pre><code>1.切换到root用户
    $&gt;su root
2.创建符号连接
    $&gt;xcall.sh &quot;ln -sfT /soft/jdk/bin/jps /usr/local/bin/jps&quot;
3.修改jps符号连接的owner
    $&gt;xcall.sh &quot;chown -h centos:centos /usr/local/bin/jps&quot;
4.查看所有主机上的java进程
    $&gt;xcall.sh jps
</code></pre><h2 id="在centos桌面版中安装eclipse"><a href="#在centos桌面版中安装eclipse" class="headerlink" title="在centos桌面版中安装eclipse"></a>在centos桌面版中安装eclipse</h2><pre><code>1.下载eclipse linux版
    eclipse-jee-mars-R-linux-gtk-x86_64.tar.gz
2.tar开到/soft下,
    $&gt;tar -xzvf eclipse-jee-mars-R-linux-gtk-x86_64.tar.gz -C /soft
3.启动eclipse
    $&gt;cd /soft/eclipse
    $&gt;./eclipse &amp;            //后台启动
4.创建桌面快捷方式
    $&gt;ln -s /soft/eclipse/eclipse ~/Desktop/eclipse
5.
</code></pre><h2 id="收集hadoop的所有jar包"><a href="#收集hadoop的所有jar包" class="headerlink" title="收集hadoop的所有jar包"></a>收集hadoop的所有jar包</h2><h2 id="使用hadoop客户端api访问hdfs"><a href="#使用hadoop客户端api访问hdfs" class="headerlink" title="使用hadoop客户端api访问hdfs"></a>使用hadoop客户端api访问hdfs</h2><pre><code>1.创建java项目
2.导入hadoop类库

3.
4.
5.
</code></pre><h2 id="网络拓扑"><a href="#网络拓扑" class="headerlink" title="网络拓扑"></a>网络拓扑</h2><pre><code>1.
2.
3.
4.
</code></pre><h2 id="作业"><a href="#作业" class="headerlink" title="作业"></a>作业</h2><pre><code>1.使用hadoop API递归输出整个文件系统
2.
</code></pre>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/09/29/Hadoop第二天之搭建/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/29/Hadoop第二天之搭建/" itemprop="url">Hadoop第二天之搭建</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-09-29T17:21:35+08:00">
                2018-09-29
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hadoop/" itemprop="url" rel="index">
                    <span itemprop="name">Hadoop</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="hadoop"><a href="#hadoop" class="headerlink" title="hadoop"></a>hadoop</h2><pre><code>1.独立模式(standalone|local)
    nothing!
    本地文件系统。
    不需要启用单独进程。
2.pesudo(伪分布模式)
    等同于完全分布式，只有一个节点。
    SSH:        //(Socket),
                //public + private
                //server : sshd ps -Af | grep sshd
                //clint     : ssh
                //ssh-keygen:生成公私秘钥。
                //authorized_keys 需要使用644
                //ssh 192.168.231.201  yes
    [配置文件]
        core-site.xml    //fs.defaultFS=hdfs://localhost/

        hdfs-site.xml    //replication=1

        mapred-site.xml    //
        yarn-site.xml    //


3.full distributed(完全分布式)
</code></pre><h2 id="让命令行提示符显式完整路径"><a href="#让命令行提示符显式完整路径" class="headerlink" title="让命令行提示符显式完整路径"></a>让命令行提示符显式完整路径</h2><pre><code>1.编辑profile文件，添加环境变量PS1
    [/etc/profile]
    export PS1=&apos;[\u@\h `pwd`]\$&apos;

2.source    
    $&gt;source /etc/profile
</code></pre><h2 id="配置hadoop，使用符号连接的方式，让三种配置形态共存。"><a href="#配置hadoop，使用符号连接的方式，让三种配置形态共存。" class="headerlink" title="配置hadoop，使用符号连接的方式，让三种配置形态共存。"></a>配置hadoop，使用符号连接的方式，让三种配置形态共存。</h2><pre><code>1.创建三个配置目录,内容等同于hadoop目录
    ${hadoop_home}/etc/local
    ${hadoop_home}/etc/pesudo
    ${hadoop_home}/etc/full

2.创建符号连接
    $&gt;ln -s 

3.对hdfs进行格式化
    $&gt;hadoop namenode -format

4.修改hadoop配置文件，手动指定JAVA_HOME环境变量
    [${hadoop_home}/etc/hadoop/hadoop-env.sh]
    ...
    export JAVA_HOME=/soft/jdk
    ...

5.启动hadoop的所有进程
    $&gt;start-all.sh

6.启动完成后，出现以下进程
    $&gt;jps
        33702 NameNode
        33792 DataNode
        33954 SecondaryNameNode

        29041 ResourceManager
        34191 NodeManager

7.查看hdfs文件系统
    $&gt;hdfs dfs -ls /

8.创建目录
    $&gt;hdfs dfs -mkdir -p /user/centos/hadoop

9.通过webui查看hadoop的文件系统
    http://localhost:50070/

10.停止hadoop所有进程
    $&gt;stop-all.sh

11.centos防火墙操作
    [cnetos 6.5之前的版本]
    $&gt;sudo service firewalld stop        //停止服务
    $&gt;sudo service firewalld start        //启动服务
    $&gt;sudo service firewalld status        //查看状态

    [centos7]
    $&gt;sudo systemctl enable firewalld.service    //&quot;开机启动&quot;启用
    $&gt;sudo systemctl disable firewalld.service    //&quot;开机自启&quot;禁用
    $&gt;sudo systemctl start firewalld.service    //启动防火墙
    $&gt;sudo systemctl stop firewalld.service        //停止防火墙
    $&gt;sudo systemctl status firewalld.service    //查看防火墙状态

    [开机自启]
    $&gt;sudo chkconfig firewalld    on                //&quot;开启自启&quot;启用
    $&gt;sudo chkconfig firewalld    off                //&quot;开启自启&quot;禁用
</code></pre><h2 id="hadoop的端口"><a href="#hadoop的端口" class="headerlink" title="hadoop的端口"></a>hadoop的端口</h2><pre><code>50070        //namenode http port
50075        //datanode http port
50090        //2namenode    http port

8020        //namenode rpc port
50010        //datanode rpc port
</code></pre><h2 id="hadoop四大模块"><a href="#hadoop四大模块" class="headerlink" title="hadoop四大模块"></a>hadoop四大模块</h2><pre><code>common
hdfs        //namenode + datanode + secondarynamenode

mapred
yarn        //resourcemanager + nodemanager
</code></pre><h2 id="启动脚本"><a href="#启动脚本" class="headerlink" title="启动脚本"></a>启动脚本</h2><pre><code>1.start-all.sh        //启动所有进程
2.stop-all.sh        //停止所有进程

3.start-dfs.sh        //
4.start-yarn.sh

[hdfs]  start-dfs.sh stop-dfs.sh
    NN
    DN
    2NN

[yarn] start-yarn.sh stop-yarn.sh
    RM
    NM
</code></pre><h2 id="修改主机名"><a href="#修改主机名" class="headerlink" title="修改主机名"></a>修改主机名</h2><pre><code>1./etc/hostname
    s201
2./etc/hosts
    127.0.0.1 localhost
    192.168.231.201 s201
    192.168.231.202 s202
    192.168.231.203 s203
    192.168.231.204 s204
</code></pre><h2 id="完全分布式"><a href="#完全分布式" class="headerlink" title="完全分布式"></a>完全分布式</h2><pre><code>1.克隆3台client(centos7)
    右键centos-7--&gt;管理-&gt;克隆-&gt; ... -&gt; 完整克隆
2.启动client
3.启用客户机共享文件夹。
4.修改hostname和ip地址文件
    [/etc/hostname]
    s202

    [/etc/sysconfig/network-scripts/ifcfg-ethxxxx]
    ...
    IPADDR=..

5.重启网络服务
    $&gt;sudo service network restart

6.修改/etc/resolv.conf文件
    nameserver 192.168.231.2

7.重复以上3 ~ 6过程.
</code></pre><h2 id="准备完全分布式主机的ssh"><a href="#准备完全分布式主机的ssh" class="headerlink" title="准备完全分布式主机的ssh"></a>准备完全分布式主机的ssh</h2><pre><code>1.删除所有主机上的/home/centos/.ssh/*

2.在s201主机上生成密钥对
    $&gt;ssh-keygen -t rsa -P &apos;&apos; -f ~/.ssh/id_rsa

3.将s201的公钥文件id_rsa.pub远程复制到202 ~ 204主机上。
  并放置/home/centos/.ssh/authorized_keys
    $&gt;scp id_rsa.pub centos@s201:/home/centos/.ssh/authorized_keys
    $&gt;scp id_rsa.pub centos@s202:/home/centos/.ssh/authorized_keys
    $&gt;scp id_rsa.pub centos@s203:/home/centos/.ssh/authorized_keys
    $&gt;scp id_rsa.pub centos@s204:/home/centos/.ssh/authorized_keys

4.配置完全分布式(${hadoop_home}/etc/hadoop/)
    [core-site.xml]
    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
    &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;
    &lt;configuration&gt;
            &lt;property&gt;
                    &lt;name&gt;fs.defaultFS&lt;/name&gt;
                    &lt;value&gt;hdfs://s201/&lt;/value&gt;
            &lt;/property&gt;
    &lt;/configuration&gt;

    [hdfs-site.xml]
    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
    &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;
    &lt;configuration&gt;
            &lt;property&gt;
                    &lt;name&gt;dfs.replication&lt;/name&gt;
                    &lt;value&gt;3&lt;/value&gt;
            &lt;/property&gt;
    &lt;/configuration&gt;

    [mapred-site.xml]
        不变

    [yarn-site.xml]
    &lt;?xml version=&quot;1.0&quot;?&gt;
    &lt;configuration&gt;
            &lt;property&gt;
                    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;
                    &lt;value&gt;s201&lt;/value&gt;
            &lt;/property&gt;
            &lt;property&gt;
                    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
                    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
            &lt;/property&gt;
    &lt;/configuration&gt;

    [slaves]
    s202
    s203
    s204

    [hadoop-env.sh]
    ...
    export JAVA_HOME=/soft/jdk
    ...

5.分发配置
    $&gt;cd /soft/hadoop/etc/
    $&gt;scp -r full centos@s202:/soft/hadoop/etc/
    $&gt;scp -r full centos@s203:/soft/hadoop/etc/
    $&gt;scp -r full centos@s204:/soft/hadoop/etc/

6.删除符号连接
    $&gt;cd /soft/hadoop/etc
    $&gt;rm hadoop
    $&gt;ssh s202 rm /soft/hadoop/etc/hadoop
    $&gt;ssh s203 rm /soft/hadoop/etc/hadoop
    $&gt;ssh s204 rm /soft/hadoop/etc/hadoop

7.创建符号连接
    $&gt;cd /soft/hadoop/etc/
    $&gt;ln -s full hadoop
    $&gt;ssh s202 ln -s /soft/hadoop/etc/full /soft/hadoop/etc/hadoop
    $&gt;ssh s203 ln -s /soft/hadoop/etc/full /soft/hadoop/etc/hadoop
    $&gt;ssh s204 ln -s /soft/hadoop/etc/full /soft/hadoop/etc/hadoop

8.删除临时目录文件
    $&gt;cd /tmp
    $&gt;rm -rf hadoop-centos
    $&gt;ssh s202 rm -rf /tmp/hadoop-centos
    $&gt;ssh s203 rm -rf /tmp/hadoop-centos
    $&gt;ssh s204 rm -rf /tmp/hadoop-centos

9.删除hadoop日志
    $&gt;cd /soft/hadoop/logs
    $&gt;rm -rf *
    $&gt;ssh s202 rm -rf /soft/hadoop/logs/*
    $&gt;ssh s203 rm -rf /soft/hadoop/logs/*
    $&gt;ssh s204 rm -rf /soft/hadoop/logs/*

10.格式化文件系统
    $&gt;hadoop namenode -format

11.启动hadoop进程
    $&gt;start-all.sh
</code></pre><h2 id="rsync"><a href="#rsync" class="headerlink" title="rsync"></a>rsync</h2><pre><code>四个机器均安装rsync命令。
远程同步.
$&gt;sudo yum install rsync
</code></pre><h2 id="将root用户实现无密登录"><a href="#将root用户实现无密登录" class="headerlink" title="将root用户实现无密登录"></a>将root用户实现无密登录</h2><pre><code>1.同
</code></pre><h2 id="编写脚本"><a href="#编写脚本" class="headerlink" title="编写脚本"></a>编写脚本</h2><pre><code>1.xcall.sh


2.xsync.sh
    xsync.sh /home/etc/a.txt
    rsync -lr /home/etc/a.txt centos@s202:/home/etc
</code></pre><hr>
<pre><code>netstat -anop    查看进程
</code></pre>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/touxiang.jpg"
                alt="Eric Hunn" />
            
              <p class="site-author-name" itemprop="name">Eric Hunn</p>
              <p class="site-description motion-element" itemprop="description">大数据|动漫迷|科技控</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">26</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">39</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/erichunn" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:erichunn.me@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://weibo.com/u/2944687303/home?wvr=5" target="_blank" title="Webibo">
                      
                        <i class="fa fa-fw fa-weibo"></i>Webibo</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://github.com/drakeet/" title="Drakeet" target="_blank">Drakeet</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      

      
	  
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=1311545319&auto=1&height=66"></iframe>

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Eric Hunn</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  

  

  

</body>
</html>
