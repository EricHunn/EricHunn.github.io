<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />





  <link rel="alternate" href="/atom.xml" title="无心是一首歌" type="application/atom+xml" />






<meta name="description" content="大数据|动漫迷|科技控">
<meta property="og:type" content="website">
<meta property="og:title" content="无心是一首歌">
<meta property="og:url" content="http://erichunn.github.io/index.html">
<meta property="og:site_name" content="无心是一首歌">
<meta property="og:description" content="大数据|动漫迷|科技控">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="无心是一首歌">
<meta name="twitter:description" content="大数据|动漫迷|科技控">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: 'undefined',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://erichunn.github.io/"/>





  <title>无心是一首歌</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?02fe19ac065353167cab1b453f2909ec";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">无心是一首歌</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-首页">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-关于我">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于我
          </a>
        </li>
      
        
        <li class="menu-item menu-item-标签">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-分类">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-归档">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-站点地图">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br />
            
            站点地图
          </a>
        </li>
      
        
        <li class="menu-item menu-item-腾讯公益">
          <a href="/404/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br />
            
            腾讯公益
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/11/19/Hbase第三天/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/19/Hbase第三天/" itemprop="url">Hbase第三天</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-11-19T16:56:58+08:00">
                2018-11-19
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/11/16/Hbase第一天/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/16/Hbase第一天/" itemprop="url">Hbase第一天</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-11-16T09:33:51+08:00">
                2018-11-16
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/11/16/Hbase第二天/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/16/Hbase第二天/" itemprop="url">Hbase第一天</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-11-16T09:33:51+08:00">
                2018-11-16
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>start-hbase.sh<br>    hbase-daemon.sh start master<br>    habse-daemon.sh    start regionserver</p>
<p>hbase的ha设置：<br>    直接打开S202或者s203的master进程即可，启动命令如上图。</p>
<p>hbase shell操作：<br>    $&gt;hbaes shell<br>    $hbase&gt;help</p>
<p>namespace 类似于Mysql库的概念</p>
<p>insert into<br>nosql: not only SQL<br>key-value<br>put用来放kv对。<br>在建表的时候没有指定列明，指定的是列族名，然后这个列族下的列是可以增减的。<br>help ‘put’</p>
<p><img src="https://i.imgur.com/5JUqC0b.png" alt=""></p>
<p>habase shell 操作：</p>
<pre><code>$&gt;hbase shell                                    //登陆shell终端
$hbase&gt;help                                        //    
$hbase&gt;help &apos;list_namespace&apos;                    //查看特定 的命令帮助
$hbase&gt;list_namespace                            //列出名字空间（数据库）
$hbase&gt;list_namespace_tables &apos;default&apos;            //列出名字空间
$hbase&gt;create_namespace &apos;ns1&apos;                    //创建名字空间
$hbase&gt;help &apos;create&apos;                            //
$hbase&gt;create &apos;ns1:t1&apos;,&apos;f1&apos;                        //创建表，指定空间下
$hbase&gt;put &apos;ns1:t1&apos;,&apos;row1&apos;,&apos;f1:id&apos;,100
$hbase&gt;put &apos;ns1:t1&apos;,&apos;row1&apos;,&apos;f1:name&apos;,tom
$hbase&gt;get &apos;ns1:t1&apos;,&apos;row1&apos;                        //指定查询row
$hbase&gt;scan &apos;ns1:t1&apos;                            //权标扫描扫描ns1列族的t1列
</code></pre><p>三级坐标定位，一个是列族，一个是row一个是时间戳如下图;</p>
<p><img src="https://i.imgur.com/ipxtMbm.png" alt=""></p>
<p><img src="https://i.imgur.com/uz0TL9q.png" alt=""></p>
<p><img src="https://i.imgur.com/3HmcLCa.png" alt=""></p>
<p>通过java api操作hbase:<br>    package com.it18zhang.hbasedemo;</p>
<pre><code>import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.TableName;
import org.apache.hadoop.hbase.client.Connection;
import org.apache.hadoop.hbase.client.ConnectionFactory;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.client.Table;
import org.apache.hadoop.hbase.util.Bytes;
import org.junit.Test;

eate 2018/11/17 11:56
 */
public class TestCRUD {
    @Test
    public void put() throws Exception {
        Configuration conf = HBaseConfiguration.create();
        Connection conn = ConnectionFactory.createConnection(conf);

        TableName tname = TableName.valueOf(&quot;ns1:t1&quot;);
        Table table = conn.getTable(tname);
        byte[] rowid = Bytes.toBytes(&quot;row2&quot;);
        byte[] f1 = Bytes.toBytes(&quot;f1&quot;);
        byte[] id = Bytes.toBytes(&quot;id&quot;);
        byte[] value = Bytes.toBytes(102);
        //创建put对象

        Put put = new Put(rowid);
        put.addColumn(f1, id, value);
        table.put(put);

    }
}
</code></pre><p>pom文件：</p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;
         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;groupId&gt;com.it18zhang&lt;/groupId&gt;
    &lt;artifactId&gt;HbaseDemo&lt;/artifactId&gt;
    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;junit&lt;/groupId&gt;
            &lt;artifactId&gt;junit&lt;/artifactId&gt;
            &lt;version&gt;4.11&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;
            &lt;artifactId&gt;hbase-client&lt;/artifactId&gt;
            &lt;version&gt;1.2.3&lt;/version&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;


&lt;/project&gt;
</code></pre><hr>
<p>hbase架构介绍：</p>
<p><img src="https://i.imgur.com/BaJwZ3z.png" alt=""></p>
<h1 id="关于区域服务器"><a href="#关于区域服务器" class="headerlink" title="关于区域服务器"></a>关于区域服务器</h1><p>  每个大表按照rowkey来切分，rowkey是可排序的，每个切分的被分派到每个区域分五期里面。key是有序的。而且每个取余服务器的表都是有范围的，所以在get的时候要指定rowkey，然后查询的时候定位到这个rowkey在哪个区域服务器上，因为所有的区域服务器信息都在meta表这个自带的表里面，叫元数据表。里面存储着rowkey。也就是每个区域的rowkey的范围。</p>
<p><img src="https://i.imgur.com/4nzxjz8.png" alt=""></p>
<p>看里面的内容hbase；namespace,,14….,74….<br>这个就是名字空间表，起始的位置，结束的rowkey位置。<br>前4大段，是指的是一大行的前4列。两个逗号意味着没有切割。就只是在一个区域服务器里面</p>
<p>再看下面的ns1:t1表的都是一行，然后3列不同的列，如果查询这部分直接联系s204,如果204挂了， 这个表就会更新，变成承接的新表的数据。这个ns1:t1的这个表就没有分割。column是列，info是列族，server是列</p>
<p>在hbase查询的时候是3级定位。时间戳，列族（列），rowkey（行）</p>
<p>看一下下面的这个目录：</p>
<p>hbase,数据，ns1名字空间，t1表，区域服务器,f1列族，然后列族下面就是文件了二进制存储，内容都在这个二进制文件里面。</p>
<p><img src="https://i.imgur.com/18bQKMj.png" alt=""></p>
<p>一个表可以有多个区域，一个区域服务器肯定有多个区域，由于有副本的存在所以一个区域可以在多个服务器上，区域服务器是来提供服务的，读写过程是由区域服务器完成，相当于数据节点，这2个都是完成数据的读写过程的。</p>
<p>在区域里面有个store在里面有MemStore。所以这些内容都是在内存当中，如果内存满了，就会被溢出到磁盘当中，Storefile就是在磁盘当中了，Hfile通过和底层交互</p>
<h1 id="hbase的写入过程："><a href="#hbase的写入过程：" class="headerlink" title="hbase的写入过程："></a>hbase的写入过程：</h1><p><img src="https://i.imgur.com/XfOpVug.png" alt=""></p>
<p>root这个地方写错了是老版本的，应该是meta表<br><img src="https://i.imgur.com/Imht8Oq.png" alt=""><br><img src="https://i.imgur.com/peKNm1w.png" alt=""></p>
<p>现在妖王一个表里写数据，需要先查询这个表在那个区域服务器上，要先查询hbase空间下的meta表，因为这个表里面标识了类似于一个目录的内容，标识那个区域子在那个服务器上，也就是rowkey的范围。这个meta表在哪里呢怎么找到呢？ meta表要通过zk来找到。因为zk集群在，所以没有zk的情况下hbase都起不来。</p>
<p>进入到hbase shell里面</p>
<p><img src="https://i.imgur.com/Rx4QHIF.png" alt=""></p>
<p>通过进入到shell里面发现这个meta表大概可能在这个里面，进去之后发现这个meta表在s203里面。如下图</p>
<p><img src="https://i.imgur.com/SHSVJwz.png" alt=""></p>
<p>所以实际上就是先联系zk然后通过zk找到他的位置在s203里面，这个meta表在s203里面</p>
<p>ta/hbase/meta/1588230740/info/da7a63e29d1c4fb588068ea9c187ae27</p>
<hr>
<h1 id="hbase基于hdfs"><a href="#hbase基于hdfs" class="headerlink" title="hbase基于hdfs"></a>hbase基于hdfs</h1><p>【表数据的存储结构目录构成】</p>
<pre><code>hdfs://s201:8020/hbase/data/${名字空间}/${表名}/区域名称/列族名称
</code></pre><p>相同列族的数据存放在一个文件中，</p>
<p>【WAL写前日志目录结构构成】</p>
<pre><code>hdfs://s201:8020/hbase/wals/s202,16020,1542534586029/s202%2C16020%2C1542534586029.default.1542541792199

hdfs://s201:8020/hbase/wals/${区域服务器名称}/主机名，端口号，时间戳/
</code></pre><h1 id="client端交互过程"><a href="#client端交互过程" class="headerlink" title="client端交互过程"></a>client端交互过程</h1><p>0.集群启动时，master负责分配区域到指定的区域服务器</p>
<p>1.联系zk找出meta表所在的区域服务器rs(regionserver)<br>        /meta/meta-region-server<br>    定位到所在的服务器</p>
<p>2.定位rowkey，找到对应的rs(regionserver)</p>
<p>3.缓存信息到本地，</p>
<p>4.联系regionserver</p>
<p>5.HRegionServe负责open-HRegion对象打开H区域服务器对象，为每个列族创建store实例，说明store是和列族对应的，store包涵多个storefile实例，他们是对hfile的轻量级封装，每个store还对应一个memstroe（用于内存储速度快），</p>
<p><img src="https://i.imgur.com/ks0t9JW.png" alt=""></p>
<hr>
<h1 id="在百万数据存储的时候："><a href="#在百万数据存储的时候：" class="headerlink" title="在百万数据存储的时候："></a>在百万数据存储的时候：</h1><p>关闭WALS</p>
<p><img src="https://i.imgur.com/ITwCty6.png" alt=""></p>
<p>代码如下：</p>
<pre><code>@Test
   public void biginsert() throws Exception {
       long start=System.currentTimeMillis();
       Configuration conf = HBaseConfiguration.create();
       Connection conn = ConnectionFactory.createConnection(conf);
       TableName tname = TableName.valueOf(&quot;ns1:t1&quot;);
       HTable table = (HTable) conn.getTable(tname);
       //不要自动清理缓冲区
       table.setAutoFlushTo(false);

       for (int i = 0; i &lt; 1000000; i++) {
           Put put = new Put(Bytes.toBytes(&quot;row&quot; + i));
           //关闭写前日志
           put.setWriteToWAL(false);
           put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;), Bytes.toBytes(i));
           put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;), Bytes.toBytes(&quot;tom&quot; + i));
           put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;), Bytes.toBytes(i % 100));
           table.put(put);
           if (i % 2000 == 0) {
               table.flushCommits();
           }
       }
       table.flushCommits();
       System.out.println(System.currentTimeMillis()-start);

   }
</code></pre><hr>
<p>hbase shell命令：</p>
<p>要想删除表，先要禁用表。</p>
<pre><code>$hbase&gt;flush &apos;ns1:t1&apos;        //清理内存数据到磁盘
$hbase&gt;count &apos;ns1:t1&apos;        //统计函数
$hbase&gt;disable &apos;ns1:t1&apos;        //删除表之前要禁用表
$hbase&gt;drop &apos;ns1:t1&apos;        //删除表 
$hbase&gt;count &apos;hbase:meta&apos;     //查看元数据表
</code></pre><p><img src="https://i.imgur.com/AAzzOmp.png" alt=""></p>
<hr>
<h1 id="格式化代码，设置固定数字格式"><a href="#格式化代码，设置固定数字格式" class="headerlink" title="格式化代码，设置固定数字格式"></a>格式化代码，设置固定数字格式</h1><pre><code> @Test
    public void formatNum(){
        DecimalFormat format =new DecimalFormat();
        format.applyPattern(&quot;0000000&quot;);
//        format.applyPattern(&quot;###,###,00&quot;);
        System.out.println(format.format(8));
    }
</code></pre><hr>
<p>为什么要格式化rowid，因为rowid1008和8相比较8比1008还要大，所以要格式化一下。</p>
<p>经过格式化rowid的代码：</p>
<pre><code> @Test
public void biginsert() throws Exception {

    DecimalFormat format =new DecimalFormat();
    format.applyPattern(&quot;0000000&quot;);
    System.out.println(format.format(8));

    long start=System.currentTimeMillis();
    Configuration conf = HBaseConfiguration.create();
    Connection conn = ConnectionFactory.createConnection(conf);
    TableName tname = TableName.valueOf(&quot;ns1:t1&quot;);
    HTable table = (HTable) conn.getTable(tname);
    //不要自动清理缓冲区
    table.setAutoFlushTo(false);

    for (int i = 0; i &lt; 10000; i++) {
        Put put = new Put(Bytes.toBytes(&quot;row&quot; + format.format(i)));
        //关闭写前日志
        put.setWriteToWAL(false);
        put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;), Bytes.toBytes(i));
        put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;), Bytes.toBytes(&quot;tom&quot; + i));
        put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;), Bytes.toBytes(i % 100));
        table.put(put);
        if (i % 2000 == 0) {
            table.flushCommits();
        }
    }
    table.flushCommits();
    System.out.println(System.currentTimeMillis()-start);

}
</code></pre><hr>
<h1 id="flush命令"><a href="#flush命令" class="headerlink" title="flush命令"></a>flush命令</h1><pre><code>$hbase:flush：清理内存数据到磁盘
</code></pre><h1 id="region拆分切割"><a href="#region拆分切割" class="headerlink" title="region拆分切割"></a>region拆分切割</h1><p><img src="https://i.imgur.com/v0N169Q.png" alt=""><br><img src="https://i.imgur.com/XQesWs6.png" alt=""></p>
<p>hbase默认切割文件是10G，超过切割。</p>
<pre><code>$hbase&gt;count &apos;ns1:t1&apos;        //统计函数
</code></pre><p><img src="https://i.imgur.com/sjQ5dq7.png" alt=""></p>
<pre><code>切割而一旦完成会保留原来的表结构，但是原来的数据内容已经没有了被删除了 
</code></pre><p><img src="https://i.imgur.com/UhuexPk.png" alt=""></p>
<p><img src="https://i.imgur.com/0NfLQSJ.png" alt=""></p>
<p><img src="https://i.imgur.com/oJTAFPt.png" alt=""></p>
<hr>
<h1 id="hbase和hadoop的ha集成"><a href="#hbase和hadoop的ha集成" class="headerlink" title="hbase和hadoop的ha集成"></a>hbase和hadoop的ha集成</h1><p>1.在hbase-env.sh文件添加hadoop配置文件目录到hbase_classpath环境变量[/soft/hbase/conf/hbase-env]并且分发。</p>
<pre><code>export HBASE_CLASSPATH=$HBASE_CLASSPATH:/soft/hadoop/    etc/hadoop
</code></pre><p>2.在hbase/conf目录下创建到hadoop的hdfs-site.xml的符号链接</p>
<pre><code>    $&gt;ln -s /soft/hadoop/etc/hadoop/hdfs-site.xml
/soft/hbase/conf/hdfs-site.xml
</code></pre><p>3.修改Hbase-site.xml文件中hbase.rootdir的目录值<br>        /soft/hbase/conf/hbase-site.xml<br>4.将之都分发出去。</p>
<p>继承了之后及时关闭了s201的namenode，hbase的Hmaster也不会关闭，也就是形成了高可用状态。 </p>
<hr>
<h1 id="hbase手动移动区域"><a href="#hbase手动移动区域" class="headerlink" title="hbase手动移动区域"></a>hbase手动移动区域</h1><p>手动移动区域<br><img src="https://i.imgur.com/UayeavF.png" alt=""></p>
<p>手动强行合并hbase块<br><img src="https://i.imgur.com/CrvZAFo.png" alt=""><br><img src="https://i.imgur.com/JCFdxeV.png" alt=""></p>
<p>手动切割：</p>
<h1 id="拆分风暴："><a href="#拆分风暴：" class="headerlink" title="拆分风暴："></a>拆分风暴：</h1><p><img src="https://i.imgur.com/wP6mfUJ.png" alt=""></p>
<p>在达到10G以后，几个区域同时增长，同时到达10G临界点，同时切割额，服务器工作瞬间增大。每个都在切割，就叫切割风暴，我们要避免这种情况，通过使用手动切割，避免拆分风暴。</p>
<hr>
<p>代码操作增删改查<br>    package com.it18zhang.hbasedemo;</p>
<pre><code>import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.*;
import org.apache.hadoop.hbase.client.*;
import org.apache.hadoop.hbase.util.Bytes;
import org.junit.Test;

import java.io.IOException;
import java.text.DecimalFormat;
import java.util.Iterator;
import java.util.Map;
import java.util.NavigableMap;

/**
 * @Title:TestCRUD
 * @Description:
 * @Auther: Eric Hunn
 * @Version:1.0
 * @Create 2018/11/17 11:56
 */
public class TestCRUD {
    @Test
    public void put() throws Exception {
        Configuration conf = HBaseConfiguration.create();
        Connection conn = ConnectionFactory.createConnection(conf);

        TableName tname = TableName.valueOf(&quot;ns1:t1&quot;);
        Table table = conn.getTable(tname);
        byte[] rowid = Bytes.toBytes(&quot;row2&quot;);
        byte[] f1 = Bytes.toBytes(&quot;f1&quot;);
        byte[] id = Bytes.toBytes(&quot;id&quot;);
        byte[] value = Bytes.toBytes(102);
        //创建put对象

        Put put = new Put(rowid);
        put.addColumn(f1, id, value);
        table.put(put);

    }

    @Test
    public void biginsert() throws Exception {

        DecimalFormat format = new DecimalFormat();
        format.applyPattern(&quot;0000000&quot;);
        System.out.println(format.format(8));

        long start = System.currentTimeMillis();
        Configuration conf = HBaseConfiguration.create();
        Connection conn = ConnectionFactory.createConnection(conf);
        TableName tname = TableName.valueOf(&quot;ns1:t1&quot;);
        HTable table = (HTable) conn.getTable(tname);
        //不要自动清理缓冲区
        table.setAutoFlushTo(false);

        for (int i = 0; i &lt; 10000; i++) {
            Put put = new Put(Bytes.toBytes(&quot;row&quot; + format.format(i)));
            //关闭写前日志
            put.setWriteToWAL(false);
            put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;), Bytes.toBytes(i));
            put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;), Bytes.toBytes(&quot;tom&quot; + i));
            put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;), Bytes.toBytes(i % 100));
            table.put(put);
            if (i % 2000 == 0) {
                table.flushCommits();
            }
        }
        table.flushCommits();
        System.out.println(System.currentTimeMillis() - start);

    }

    @Test
    public void formatNum() {
        DecimalFormat format = new DecimalFormat();
        format.applyPattern(&quot;0000000&quot;);
//        format.applyPattern(&quot;###,###,00&quot;);
        System.out.println(format.format(8));
    }

    @Test
    public void createNamespace() throws Exception {
        //创建conf对象
        Configuration conf = HBaseConfiguration.create();
        //通过工厂创建连接对象
        Connection conn = ConnectionFactory.createConnection(conf);
        Admin admin = conn.getAdmin();
        NamespaceDescriptor nsd = NamespaceDescriptor.create(&quot;ns2&quot;).build();
        admin.createNamespace(nsd);

        NamespaceDescriptor[] ns = admin.listNamespaceDescriptors();
        for (NamespaceDescriptor n : ns) {
            System.out.println(n.getName());
        }

    }

    @Test
    public void listNamespaces() throws Exception {
        //创建conf对象
        Configuration conf = HBaseConfiguration.create();
        //通过工厂创建连接对象
        Connection conn = ConnectionFactory.createConnection(conf);
        Admin admin = conn.getAdmin();
        NamespaceDescriptor[] ns = admin.listNamespaceDescriptors();
        for (NamespaceDescriptor n : ns) {
            System.out.println(n.getName());
        }
    }

    @Test
    public void createTables() throws Exception {
        //创建conf对象
        Configuration conf = HBaseConfiguration.create();
        //通过工厂创建连接对象
        Connection conn = ConnectionFactory.createConnection(conf);
        Admin admin = conn.getAdmin();
        //创建表名对象
        TableName tbn = TableName.valueOf(&quot;ns2:t2&quot;);
        //创建表描述符对象
        HTableDescriptor tbl = new HTableDescriptor(tbn);
        //在表描述符中添加列族创建列族描述符
        HColumnDescriptor col = new HColumnDescriptor(&quot;f1&quot;);
        tbl.addFamily(col);

        admin.createTable(tbl);
        System.out.println(&quot;over&quot;);
    }

    @Test
    public void disableTable() throws Exception {
        Configuration conf = HBaseConfiguration.create();
        Connection conn = ConnectionFactory.createConnection(conf);
        Admin admin = conn.getAdmin();
        admin.disableTable(TableName.valueOf(&quot;ns2:t2&quot;));
    }

    @Test
    public void dropTable() throws Exception {
        Configuration conf = HBaseConfiguration.create();
        Connection conn = ConnectionFactory.createConnection(conf);
        Admin admin = conn.getAdmin();
        admin.disableTable(TableName.valueOf(&quot;ns2:t2&quot;));
        admin.deleteTable(TableName.valueOf(&quot;ns2:t2&quot;));
    }

    @Test
    public void deleteData() throws IOException {

        Configuration conf = HBaseConfiguration.create();
        Connection conn = ConnectionFactory.createConnection(conf);
        TableName tname = TableName.valueOf(&quot;ns1:t1&quot;);
        Table table = conn.getTable(tname);
        Delete del = new Delete(Bytes.toBytes(&quot;row0000001&quot;));
        del.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;));
        del.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;));

        table.delete(del);
        System.out.println(&quot;over&quot;);

    }

    @Test
    public void scanall() throws IOException {
        Configuration conf = HBaseConfiguration.create();
        Connection conn = ConnectionFactory.createConnection(conf);
        TableName tname = TableName.valueOf(&quot;ns1:t1&quot;);
        Table table = conn.getTable(tname);
        Scan scan = new Scan();
        ResultScanner rs = table.getScanner(scan);
        Iterator&lt;Result&gt; it = rs.iterator();
        while (it.hasNext()) {
            Result r = it.next();//理解这个r是对一整行的封装
            byte[] value = r.getValue(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;));
            System.out.println(Bytes.toString(value));
        }
    }


    @Test
    public void scan() throws IOException {
        Configuration conf = HBaseConfiguration.create();
        Connection conn = ConnectionFactory.createConnection(conf);
        TableName tname = TableName.valueOf(&quot;ns1:t1&quot;);
        Table table = conn.getTable(tname);
        Scan scan = new Scan();
        scan.setStartRow(Bytes.toBytes(&quot;row0005000&quot;));
        scan.setStopRow(Bytes.toBytes(&quot;row0008000&quot;));
        ResultScanner rs = table.getScanner(scan);
        Iterator&lt;Result&gt; it = rs.iterator();
        while (it.hasNext()) {
            Result r = it.next();//理解这个r是对一整行的封装
            byte[] value = r.getValue(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;));
            System.out.println(Bytes.toString(value));
        }
    }

    //动态获取所有的列和列族，动态遍历，每个列和值的集合，由于强行getvalue转换成string类型，所以难免出现乱码情况
    @Test
    public void scan2() throws IOException {
        Configuration conf = HBaseConfiguration.create();
        Connection conn = ConnectionFactory.createConnection(conf);
        TableName tname = TableName.valueOf(&quot;ns1:t1&quot;);
        Table table = conn.getTable(tname);
        Scan scan = new Scan();
        ResultScanner rs = table.getScanner(scan);
        Iterator&lt;Result&gt; it = rs.iterator();
        while (it.hasNext()) {
            Result r = it.next();//理解这个r是对一整行的封装
            Map&lt;byte[], byte[]&gt; map = r.getFamilyMap(Bytes.toBytes(&quot;f1&quot;));
            for (Map.Entry&lt;byte[], byte[]&gt; entrySet : map.entrySet()) {
                String col = Bytes.toString(entrySet.getKey());
                String val = Bytes.toString(entrySet.getValue());
                System.out.println(col + &quot;:&quot; + val + &quot;,&quot;);
            }
            System.out.println();
        }
    }

    @Test
    public void scan3() throws IOException {
        Configuration conf = HBaseConfiguration.create();
        Connection conn = ConnectionFactory.createConnection(conf);
        TableName tname = TableName.valueOf(&quot;ns1:t1&quot;);
        Table table = conn.getTable(tname);
        Scan scan = new Scan();
        ResultScanner rs = table.getScanner(scan);
        Iterator&lt;Result&gt; it = rs.iterator();
        while (it.hasNext()) {
            Result r = it.next();//理解这个r是对一整行的封装
            //得到一行的所有map，key=f1,value=Map&lt;Col,Map&lt;Timestamp,value&gt;&gt;这个结构

            NavigableMap&lt;byte[], NavigableMap&lt;byte[], NavigableMap&lt;Long, byte[]&gt;&gt;&gt; map = r.getMap();
            for (Map.Entry&lt;byte[], NavigableMap&lt;byte[], NavigableMap&lt;Long, byte[]&gt;&gt;&gt; entry : map.entrySet()) {
                //得到列族
                String f = Bytes.toString(entry.getKey());
                NavigableMap&lt;byte[], NavigableMap&lt;Long, byte[]&gt;&gt; colDataMap = entry.getValue();
                for (Map.Entry&lt;byte[], NavigableMap&lt;Long, byte[]&gt;&gt; ets : colDataMap.entrySet()) {
                    String c = Bytes.toString(ets.getKey());
                    Map&lt;Long, byte[]&gt; tsValueMap = ets.getValue();
                    for (Map.Entry&lt;Long, byte[]&gt; e : tsValueMap.entrySet()) {
                        Long ts = e.getKey();
                        String value = Bytes.toString(e.getValue());
                        System.out.println(f + &quot;:&quot; + c + &quot;:&quot; + ts + &quot;=&quot; + value + &quot;,&quot;);
                    }
                }
            }
        }
    }
}
</code></pre>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/11/15/Zookeeper第二天/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/15/Zookeeper第二天/" itemprop="url">Zookeeper第二天</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-11-15T16:45:25+08:00">
                2018-11-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="leader推选过程-最小号选举法"><a href="#leader推选过程-最小号选举法" class="headerlink" title="leader推选过程(最小号选举法)"></a>leader推选过程(最小号选举法)</h2><pre><code>1.所有节点在同一目录下创建临时序列节点。
2.节点下会生成/xxx/xx000000001等节点。
3.序号最小的节点就是leader，其余就是follower.
4.每个节点观察小于自己节点的主机。(注册观察者)
5.如果leader挂了，对应znode删除了。
6.观察者收到通知。
</code></pre><p><img src="https://i.imgur.com/O6wXdY8.png" alt=""></p>
<h2 id="配置完全分布式zk集群"><a href="#配置完全分布式zk集群" class="headerlink" title="配置完全分布式zk集群"></a>配置完全分布式zk集群</h2><pre><code>1.挑选3台主机
    s201 ~ s203
2.每台机器都安装zk
    tar
    环境变量

3.配置zk配置文件
    s201 ~ s203
    [/soft/zk/conf/zoo.cfg]
    ...
    dataDir=/home/centos/zookeeper

    server.1=s201:2888:3888
    server.2=s202:2888:3888 
    server.3=s203:2888:3888

4.在每台主机的/home/centos/zookeeper中添加myid,内容分别是1,2,3
    [s201]
    $&gt;echo 1 &gt; /home/centos/zookeeper/myid
    [s202]
    $&gt;echo 2 &gt; /home/centos/zookeeper/myid
    [s203]
    $&gt;echo 3 &gt; /home/centos/zookeeper/myid

5.启动服务器集群 
    $&gt;zkServer.sh start
    ...

6.查看每台服务器的状态
    $&gt;zkServer.sh status

7.修改zk的log目录

    vi /soft/zk/conf/log4j.properties
</code></pre><p>修改如下：</p>
<p><img src="https://i.imgur.com/xaRGDXr.png" alt=""></p>
<pre><code>8.创建log目录：
    xcall.sh &quot;mkdir /home/centos/zookeeper/log&quot;
</code></pre><h2 id="部署细节"><a href="#部署细节" class="headerlink" title="部署细节"></a>部署细节</h2><pre><code>1.在jn节点分别启动jn进程
    $&gt;hadoop-daemon.sh start journalnode

2.启动jn之后，在两个NN之间进行disk元数据同步
    a)如果是全新集群，先format文件系统,只需要在一个nn上执行。
        [s201]
        $&gt;hadoop namenode -format

    b)如果将非HA集群转换成HA集群，复制原NN的metadata到另一个nn.
        1.步骤一
            [s201]
            $&gt;scp -r /home/centos/hadoop/dfs centos@s206:/home/centos/hadoop/

        2.步骤二
            在新的nn(未格式化的nn)上运行一下命令，实现待命状态引导。
            [s206]
            $&gt;hdfs namenode -bootstrapStandby        //需要s201为启动状态,提示是否格式化,选择N.

        3)在一个NN上执行以下命令，完成edit日志到jn节点的传输。
            $&gt;hdfs namenode -initializeSharedEdits
            #查看s202,s203是否有edit数据.

        4)启动所有节点.
            [s201]
            $&gt;hadoop-daemon.sh start namenode        //启动名称节点
            $&gt;hadoop-daemons.sh start datanode        //启动所有数据节点

            [s206]
            $&gt;hadoop-daemon.sh start namenode        //启动名称节点
</code></pre><h2 id="HA管理"><a href="#HA管理" class="headerlink" title="HA管理"></a>HA管理</h2><pre><code>$&gt;hdfs haadmin -transitionToActive nn1                //切成激活态
$&gt;hdfs haadmin -transitionToStandby nn1                //切成待命态
$&gt;hdfs haadmin -transitionToActive --forceactive nn2//强行激活
$&gt;hdfs haadmin -failover nn1 nn2                    //模拟容灾演示,从nn1切换到nn2
</code></pre><h2 id="完全0开始部署hadoop-HDFS的HA集群，使用zk实现自动容灾"><a href="#完全0开始部署hadoop-HDFS的HA集群，使用zk实现自动容灾" class="headerlink" title="完全0开始部署hadoop HDFS的HA集群，使用zk实现自动容灾"></a>完全0开始部署hadoop HDFS的HA集群，使用zk实现自动容灾</h2><pre><code>1.停掉hadoop的所有进程

2.删除所有节点的日志和本地数据.
    删除/home/centos/hadoop下的所有和
        /home/centos/journal下的所有

3.改换hadoop符号连接为ha

4.登录每台JN节点主机，启动JN进程.
    [s202-s204]
    $&gt;hadoop-daemon.sh start journalnode

5.登录其中一个NN,格式化文件系统(s201)
    $&gt;hadoop namenode -format

6.复制201目录的下nn的元数据到s206
    $&gt;scp -r ~/hadoop/* centos@s206:/home/centos/hadoop

7.在未格式化的NN(s206)节点上做standby引导.
    7.1)需要保证201的NN启动
        $&gt;hadoop-daemon.sh start namenode

    7.2)登录到s206节点，做standby引导.
        $&gt;hdfs namenode -bootstrapStandby

    7.3)登录201，将s201的edit日志初始化到JN节点。
        $&gt;hdfs namenode -initializeSharedEdits

8.启动所有数据节点.
    $&gt;hadoop-daemons.sh start datanode

9.登录到206,启动NN
    $&gt;hadoop-daemon.sh start namenode

10.查看webui
    http://s201:50070/
    http://s206:50070/

11.自动容灾
    11.1)介绍
        自动容灾引入两个组件，zk quarum + zk容灾控制器(ZKFC)。



        运行NN的主机还要运行ZKFC进程，主要负责:
        a.健康监控
        b.session管理
        c.选举
    11.2部署容灾
        a.停止所有进程
            $&gt;stop-all.sh

        b.配置hdfs-site.xml，启用自动容灾.
            [hdfs-site.xml]
                &lt;property&gt;
                    &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;
                    &lt;value&gt;true&lt;/value&gt;
                &lt;/property&gt;

        c.配置core-site.xml，指定zk的连接地址.
            &lt;property&gt;
                &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;
                &lt;value&gt;s201:2181,s202:2181,s203:2181&lt;/value&gt;
            &lt;/property&gt;

        d.分发以上两个文件到所有节点。

12.登录其中的一台NN(s201),在ZK中初始化HA状态
    $&gt;hdfs zkfc -formatZK

13.启动hdfs进程.
    $&gt;start-dfs.sh

14.测试自动容在(206是活跃节点)
    $&gt;kill -9
</code></pre><h2 id="配置RM的HA自动容灾"><a href="#配置RM的HA自动容灾" class="headerlink" title="配置RM的HA自动容灾"></a>配置RM的HA自动容灾</h2><pre><code>1.配置yarn-site.xml
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;
        &lt;value&gt;true&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;
        &lt;value&gt;cluster1&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;
        &lt;value&gt;rm1,rm2&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;
        &lt;value&gt;s201&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;
        &lt;value&gt;s206&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt;
        &lt;value&gt;s201:8088&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt;
        &lt;value&gt;s206:8088&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;
        &lt;value&gt;s201:2181,s202:2181,s203:2181&lt;/value&gt;
    &lt;/property&gt;    

2.使用管理命令
    //查看状态
    $&gt;yarn rmadmin -getServiceState rm1
    //切换状态到standby
    $&gt;yarn rmadmin -transitionToStandby rm1

3.启动yarn集群
    $&gt;start-yarn.sh

4.hadoop没有启动两个resourcemanager,需要手动启动另外一个
    $&gt;yarn-daemon.sh start resourcemanager

5.查看webui

6.做容灾模拟.
    kill -9
</code></pre><h2 id="hive的注意事项"><a href="#hive的注意事项" class="headerlink" title="hive的注意事项"></a>hive的注意事项</h2><pre><code>如果配置hadoop HA之前，搭建了Hive的话，在HA之后，需要调整路径信息.
主要是修改mysql中的dbs,tbls等相关表。
</code></pre><h2 id="Hbase"><a href="#Hbase" class="headerlink" title="Hbase"></a>Hbase</h2><pre><code>hadoop数据库，分布式可伸缩大型数据存储。
用户对随机、实时读写数据。
十亿行 x 百万列。
版本化、非关系型数据库。
</code></pre><h2 id="Feature"><a href="#Feature" class="headerlink" title="Feature"></a>Feature</h2><pre><code>Linear and modular scalability.                    //线性模块化扩展方式。
Strictly consistent reads and writes.            //严格一致性读写
Automatic and configurable sharding of tables    //自动可配置表切割
Automatic failover support between RegionServers.    //区域服务器之间自动容在
Convenient base classes for backing Hadoop MapReduce jobs with Apache HBase tables.        //
Easy to use Java API for client access.            //java API
Block cache and Bloom Filters for real-time queries    //块缓存和布隆过滤器用于实时查询 
Query predicate push down via server side Filters    //通过服务器端过滤器实现查询预测
Thrift gateway and a REST-ful Web service that supports XML, Protobuf, and binary data encoding options    //
Extensible jruby-based (JIRB) shell                    //
Support for exporting metrics via the Hadoop metrics subsystem to files or Ganglia; or via JMX            //可视化
面向列数据库。
</code></pre><h2 id="hbase存储机制"><a href="#hbase存储机制" class="headerlink" title="hbase存储机制"></a>hbase存储机制</h2><pre><code>面向列存储，table是按row排序。
</code></pre><h2 id="搭建hbase集群"><a href="#搭建hbase集群" class="headerlink" title="搭建hbase集群"></a>搭建hbase集群</h2><pre><code>0.选择安装的主机
    s201 ~ s204
1.jdk
    略
2.hadoop
    略
3.tar 
    略
4.环境变量
    略

5.验证安装是否成功
    $&gt;hbase version

5.配置hbase模式
    5.1)本地模式
        [hbase/conf/hbase-env.sh]
        EXPORT JAVA_HOME=/soft/jdk

        [hbase/conf/hbase-site.xml]
        ...
        &lt;property&gt;
            &lt;name&gt;hbase.rootdir&lt;/name&gt;
            &lt;value&gt;file:/home/hadoop/HBase/HFiles&lt;/value&gt;
        &lt;/property&gt;

    5.2)伪分布式
        [hbase/conf/hbase-env.sh]
        EXPORT JAVA_HOME=/soft/jdk

        [hbase/conf/hbase-site.xml]
        &lt;property&gt;
            &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;
            &lt;value&gt;true&lt;/value&gt;
        &lt;/property
        &lt;property&gt;
            &lt;name&gt;hbase.rootdir&lt;/name&gt;
            &lt;value&gt;hdfs://localhost:8030/hbase&lt;/value&gt;
        &lt;/property&gt;

    5.3)完全分布式(必做)
        [hbase/conf/hbase-env.sh]
        export JAVA_HOME=/soft/jdk
        export HBASE_MANAGES_ZK=false

        [hbse-site.xml]
        &lt;!-- 使用完全分布式 --&gt;
        &lt;property&gt;
            &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;
            &lt;value&gt;true&lt;/value&gt;
        &lt;/property&gt;

        &lt;!-- 指定hbase数据在hdfs上的存放路径 --&gt;
        &lt;property&gt;
            &lt;name&gt;hbase.rootdir&lt;/name&gt;
            &lt;value&gt;hdfs://s201:8020/hbase&lt;/value&gt;
        &lt;/property&gt;
        &lt;!-- 配置zk地址 --&gt;
        &lt;property&gt;
            &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;
            &lt;value&gt;s201:2181,s202:2181,s203:2181&lt;/value&gt;
        &lt;/property&gt;
        &lt;!-- zk的本地目录 --&gt;
        &lt;property&gt;
            &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;
            &lt;value&gt;/home/centos/zookeeper&lt;/value&gt;
        &lt;/property&gt;

6.配置regionservers
    [hbase/conf/regionservers]
    s202
    s203
    s204

7.启动hbase集群(s201)
    $&gt;start-hbase.sh

8.登录hbase的webui
    http://s201:16010
</code></pre>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/11/12/SSM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/12/SSM/" itemprop="url">SSM</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-11-12T15:39:39+08:00">
                2018-11-12
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="SSM"><a href="#SSM" class="headerlink" title="SSM"></a>SSM</h2><pre><code>Spring        //业务层框架
Spring MVC    //开发web程序应用的模块model + view + controller
Mybatis        //持久化。jdbc , mysql
</code></pre><h2 id="mybatis"><a href="#mybatis" class="headerlink" title="mybatis"></a>mybatis</h2><pre><code>ibatis.
</code></pre><h2 id="体验"><a href="#体验" class="headerlink" title="体验"></a>体验</h2><pre><code>1.创建项目和模块
2.添加pom.xml
    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
    &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;
             xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
             xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
        &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

        &lt;groupId&gt;com.it18zhang&lt;/groupId&gt;
        &lt;artifactId&gt;mybatisdemo&lt;/artifactId&gt;
        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
        &lt;dependencies&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;org.mybatis&lt;/groupId&gt;
                &lt;artifactId&gt;mybatis&lt;/artifactId&gt;
                &lt;version&gt;3.2.1&lt;/version&gt;
            &lt;/dependency&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;mysql&lt;/groupId&gt;
                &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;
                &lt;version&gt;5.1.17&lt;/version&gt;
            &lt;/dependency&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;junit&lt;/groupId&gt;
                &lt;artifactId&gt;junit&lt;/artifactId&gt;
                &lt;version&gt;4.11&lt;/version&gt;
            &lt;/dependency&gt;
        &lt;/dependencies&gt;
    &lt;/project&gt;

3.添加配置
    [resoucecs/mybatis-config.xml]
    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;
    &lt;!DOCTYPE configuration
      PUBLIC &quot;-//mybatis.org//DTD Config 3.0//EN&quot;
      &quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot;&gt;
    &lt;configuration&gt;
      &lt;environments default=&quot;development&quot;&gt;
        &lt;environment id=&quot;development&quot;&gt;
          &lt;transactionManager type=&quot;JDBC&quot;/&gt;
          &lt;dataSource type=&quot;POOLED&quot;&gt;
            &lt;property name=&quot;driver&quot; value=&quot;${driver}&quot;/&gt;
            &lt;property name=&quot;url&quot; value=&quot;${url}&quot;/&gt;
            &lt;property name=&quot;username&quot; value=&quot;${username}&quot;/&gt;
            &lt;property name=&quot;password&quot; value=&quot;${password}&quot;/&gt;
          &lt;/dataSource&gt;
        &lt;/environment&gt;
      &lt;/environments&gt;
      &lt;mappers&gt;
        &lt;mapper resource=&quot;org/mybatis/example/BlogMapper.xml&quot;/&gt;
      &lt;/mappers&gt;
    &lt;/configuration&gt;

4.创建库和表
    mysql&gt;create database mybatis ;
    mysql&gt;use mybatis ;
    mysql&gt;create table users(id int primary key auto_increment , name varchar(20) ,age int) ;
    mysql&gt;desc users ;

5.测试连接
    package com.it18zhang.mybatisdemo;

    import org.apache.ibatis.io.Resources;
    import org.apache.ibatis.session.SqlSession;
    import org.apache.ibatis.session.SqlSessionFactory;
    import org.apache.ibatis.session.SqlSessionFactoryBuilder;

    import java.io.IOException;
    import java.io.InputStream;

    /**
     *
     */
    public class App {
        public static void main(String[] args) {
            try {
                //指定配置文件的路径(类路径)
                String resource = &quot;mybatis-config.xml&quot;;
                //加载文件
                InputStream inputStream = Resources.getResourceAsStream(resource);

                //创建会话工厂Builder,相当于连接池
                SqlSessionFactory sf = new SqlSessionFactoryBuilder().build(inputStream);

                //通过sf开启会话，相当于打开连接。
                SqlSession s = sf.openSession();
                System.out.println(s);

            } catch (Exception e) {
                e.printStackTrace();
            }
        }
    }

6.编写mapper文件。
    a)创建User类，和users对应
        public class User {
            private Integer id ;
            private String name ;
            private int age ;
            //get/set
        }

    b)创建UserMapper.xml,存放在resources/目录下
        包名[resources/UserMapper.xml]
        &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;
        &lt;!DOCTYPE mapper
                PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot;
                &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;
        &lt;!-- 定义名字空间 --&gt;
        &lt;mapper namespace=&quot;users&quot;&gt;
            &lt;!-- 定义insert语句 --&gt;
            &lt;insert id=&quot;insert&quot;&gt;
              insert into users(name,age) values(#{name},#{age})
            &lt;/insert&gt;
        &lt;/mapper&gt;

7.在resources/mybatis-config.xml文件中引入mapper的xml文件.
    [resources/mybatis-config.xml]
    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;
    &lt;!DOCTYPE configuration
            PUBLIC &quot;-//mybatis.org//DTD Config 3.0//EN&quot;
            &quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot;&gt;
    &lt;configuration&gt;
        &lt;environments default=&quot;development&quot;&gt;
            &lt;environment id=&quot;development&quot;&gt;
                &lt;transactionManager type=&quot;JDBC&quot;/&gt;
                &lt;dataSource type=&quot;POOLED&quot;&gt;
                    &lt;property name=&quot;driver&quot; value=&quot;com.mysql.jdbc.Driver&quot;/&gt;
                    &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/mybatis&quot;/&gt;
                    &lt;property name=&quot;username&quot; value=&quot;root&quot;/&gt;
                    &lt;property name=&quot;password&quot; value=&quot;root&quot;/&gt;
                &lt;/dataSource&gt;
            &lt;/environment&gt;
        &lt;/environments&gt;
        &lt;!-- *****引入映射文件(新增部分)***** --&gt;
        &lt;mappers&gt;
            &lt;mapper resource=&quot;UserMapper.xml&quot;/&gt;
        &lt;/mappers&gt;
    &lt;/configuration&gt;

8.编写单元测试，实现插入.
    [test/com.it18zhang.mybatis.test.TestCRUD.java]
    /**
     * insert
     */
    @Test
    public void insert() throws Exception {
        //指定配置文件的路径(类路径)
        String resource = &quot;mybatis-config.xml&quot;;
        //加载文件
        InputStream inputStream = Resources.getResourceAsStream(resource);

        //创建会话工厂Builder,相当于连接池
        SqlSessionFactory sf = new SqlSessionFactoryBuilder().build(inputStream);

        //通过sf开启会话，相当于打开连接。
        SqlSession s = sf.openSession();
        User u = new User();

        u.setName(&quot;jerry&quot;);
        u.setAge(2);
        s.insert(&quot;users.insert&quot;, u);
        s.commit();
        s.close();
    }

9.完成update-selectOne-selectAll操作。
    9.1)编写UserMapper.xml，添加相应的元素
        &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;
        &lt;!DOCTYPE mapper
                PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot;
                &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;
        &lt;mapper namespace=&quot;users&quot;&gt;
            &lt;insert id=&quot;insert&quot;&gt;
              insert into users(name,age) values(#{name},#{age})
            &lt;/insert&gt;
            &lt;update id=&quot;update&quot;&gt;
                update users set name = #{name} , age = #{age} where id = #{id}
            &lt;/update&gt;

            &lt;!-- selectOne --&gt;
            &lt;select id=&quot;selectOne&quot; parameterType=&quot;int&quot; resultType=&quot;com.it18zhang.mybatisdemo.domain.User&quot;&gt;
                select * from users where id = #{id}
            &lt;/select&gt;

            &lt;!-- selectAll --&gt;
            &lt;select id=&quot;selectAll&quot; resultType=&quot;com.it18zhang.mybatisdemo.domain.User&quot;&gt;
                select * from users
            &lt;/select&gt;
        &lt;/mapper&gt;

    9.2)编写测试程序
        /**
         * Created by Administrator on 2017/4/6.
         */
        public class TestCRUD {

            /**
             * insert
             */
            @Test
            public void insert() throws Exception {
                //指定配置文件的路径(类路径)
                String resource = &quot;mybatis-config.xml&quot;;
                //加载文件
                InputStream inputStream = Resources.getResourceAsStream(resource);

                //创建会话工厂Builder,相当于连接池
                SqlSessionFactory sf = new SqlSessionFactoryBuilder().build(inputStream);

                //通过sf开启会话，相当于打开连接。
                SqlSession s = sf.openSession();
                User u = new User();

                u.setName(&quot;jerry&quot;);
                u.setAge(2);
                s.insert(&quot;users.insert&quot;, u);
                s.commit();
                s.close();
            }

            /**
             * update
             */
            @Test
            public void update() throws Exception {
                String resource = &quot;mybatis-config.xml&quot;;
                InputStream inputStream = Resources.getResourceAsStream(resource);
                SqlSessionFactory sf = new SqlSessionFactoryBuilder().build(inputStream);
                SqlSession s = sf.openSession();
                User u = new User();
                u.setId(1);
                u.setName(&quot;tomas&quot;);
                u.setAge(32);
                s.update(&quot;users.update&quot;, u);
                s.commit();
                s.close();
            }

            /**
             * selectOne
             */
            @Test
            public void selectOne() throws Exception {
                String resource = &quot;mybatis-config.xml&quot;;
                InputStream inputStream = Resources.getResourceAsStream(resource);
                SqlSessionFactory sf = new SqlSessionFactoryBuilder().build(inputStream);
                SqlSession s = sf.openSession();
                User user = s.selectOne(&quot;users.selectOne&quot;,1);
                System.out.println(user.getName());
                s.commit();
                s.close();
            }

            /**
             * selectOne
             */
            @Test
            public void selectAll() throws Exception {
                String resource = &quot;mybatis-config.xml&quot;;
                InputStream inputStream = Resources.getResourceAsStream(resource);
                SqlSessionFactory sf = new SqlSessionFactoryBuilder().build(inputStream);
                SqlSession s = sf.openSession();
                List&lt;User&gt; users = s.selectList(&quot;users.selectAll&quot;);
                for(User uu : users){
                    System.out.println(uu.getName() + &quot;,&quot; + uu.getAge());
                }
                s.commit();
                s.close();
            }
        }
</code></pre><h1 id="SSM第二天"><a href="#SSM第二天" class="headerlink" title="SSM第二天"></a>SSM第二天</h1><p>三大框架的整合：</p>
<p>数据库层：<br>mybatis数据持久化层：dao在交互mybatis<br>dao:数据访问层：数据访问对象，存放简单的增删改查。每个表都有这个业务<br>service层：业务层，和dao交互，对dao的方法进行组合调用。形成套餐<br>springmvc展现层：做网页，做web开发的，springmvc应该和业务层交互，在springmvc里面的controller层和业务层交互，展现的东西就是jsp做web开发做网站的，做web应用的。通过web架构来实现。springmvc就是web架构里面的一个。</p>
<p><img src="https://i.imgur.com/ygpfvcz.png" alt=""></p>
<h2 id="复杂应用"><a href="#复杂应用" class="headerlink" title="复杂应用"></a>复杂应用</h2><pre><code>1.准备数据
    sql.sql

2.创建java类.
    [Order.java]
    public class Order {
        private Integer id ;
        private String orderNo ;
        //简历关联关系
        private User user ;

        //get/set
    }

    [Item.java]
    public class Item {
        private Integer id;
        private String itemName;
        //订单项和订单之间的关联关系
        private Order order;
        //get/set
    }

3.创建Order映射文件
    [resource/OrderMapper.xml]
    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;
    &lt;!DOCTYPE mapper
            PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot;
            &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;
    &lt;mapper namespace=&quot;orders&quot;&gt;
        &lt;insert id=&quot;insert&quot;&gt;
          insert into orders(orderno,uid) values(#{orderNo},#{user.id})
        &lt;/insert&gt;

        &lt;!-- findById --&gt;
        &lt;select id=&quot;selectOne&quot; parameterType=&quot;int&quot; resultMap=&quot;RM_Order&quot;&gt;
          select
            o.id oid ,
            o.orderno oorderno ,
            o.uid uid ,
            u.name uname ,
            u.age uage
          from orders o
            left outer join users u on o.uid = u.id where o.id = #{id}
        &lt;/select&gt;
        &lt;!-- findAll --&gt;
        &lt;select id=&quot;selectAll&quot; resultMap=&quot;RM_Order&quot;&gt;
          select
            o.id oid ,
            o.orderno oorderno ,
            o.uid uid ,
            u.name uname ,
            u.age uage
          from orders o
            left outer join users u on o.uid = u.id
        &lt;/select&gt;
        &lt;!-- 自定义结果映射 --&gt;
        &lt;resultMap id=&quot;RM_Order&quot; type=&quot;com.it18zhang.mybatisdemo.domain.Order&quot;&gt;
            &lt;id property=&quot;id&quot; column=&quot;oid&quot;/&gt;
            &lt;result property=&quot;orderNo&quot; column=&quot;oorderno&quot;/&gt;
            &lt;association property=&quot;user&quot; javaType=&quot;com.it18zhang.mybatisdemo.domain.User&quot;&gt;
                &lt;id property=&quot;id&quot; column=&quot;uid&quot; /&gt;
                &lt;result property=&quot;name&quot; column=&quot;uname&quot; /&gt;
                &lt;result property=&quot;age&quot; column=&quot;uage&quot; /&gt;
            &lt;/association&gt;
        &lt;/resultMap&gt;
    &lt;/mapper&gt;

4.修改配置文件,添加映射。
    [resource/mybatis-config.xml]
    &lt;!-- 引入映射文件 --&gt;
    &lt;mappers&gt;
        &lt;mapper resource=&quot;*Mapper.xml&quot;/&gt;
    &lt;/mappers&gt;

5.测试类
    public class TestOrder {

        /**
         * insert
         */
        @Test
        public void insert() throws Exception {
            String resource = &quot;mybatis-config.xml&quot;;
            InputStream inputStream = Resources.getResourceAsStream(resource);
            SqlSessionFactory sf = new SqlSessionFactoryBuilder().build(inputStream);
            SqlSession s = sf.openSession();

            User u = new User();
            u.setId(2);

            Order o = new Order();
            o.setOrderNo(&quot;No005&quot;);
            o.setUser(u);

            s.insert(&quot;orders.insert&quot;,o);
            s.commit();
            s.close();
        }

        @Test
        public void selectOne() throws Exception {
            String resource = &quot;mybatis-config.xml&quot;;
            InputStream inputStream = Resources.getResourceAsStream(resource);
            SqlSessionFactory sf = new SqlSessionFactoryBuilder().build(inputStream);
            SqlSession s = sf.openSession();
            Order order = s.selectOne(&quot;orders.selectOne&quot;,1);
            System.out.println(order.getOrderNo());
            s.commit();
            s.close();
        }

        @Test
        public void selectAll() throws Exception {
            String resource = &quot;mybatis-config.xml&quot;;
            InputStream inputStream = Resources.getResourceAsStream(resource);
            SqlSessionFactory sf = new SqlSessionFactoryBuilder().build(inputStream);
            SqlSession s = sf.openSession();
            List&lt;Order&gt; list = s.selectList(&quot;orders.selectAll&quot;);
            for(Order o : list){
                System.out.println(o.getOrderNo() + &quot; : &quot; + o.getUser().getName());
            }
            s.commit();
            s.close();
        }
    }
</code></pre><h2 id="配置一对多"><a href="#配置一对多" class="headerlink" title="配置一对多"></a>配置一对多</h2><pre><code>1.在User中增加orders集合。
    public class User {
        ...
        private List&lt;Order&gt; orders ;
        //get/set
    }

2.改造UserMapper.xml
</code></pre><h2 id="组合多对一和一对多关联关系到一个实体-Order-中"><a href="#组合多对一和一对多关联关系到一个实体-Order-中" class="headerlink" title="组合多对一和一对多关联关系到一个实体(Order)中"></a>组合多对一和一对多关联关系到一个实体(Order)中</h2><pre><code>1.关系
    Order(*) -&gt; (1)User
    Order(1) -&gt; (*)Item

2.Order.java
    class Order{
        ...
        List&lt;Item&gt; items ;
        //get/set    
    }
2&apos;.修改配置文件增加别名
    [resources/mybatis-config.xml]
    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;
    &lt;!DOCTYPE configuration
            PUBLIC &quot;-//mybatis.org//DTD Config 3.0//EN&quot;
            &quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot;&gt;
    &lt;configuration&gt;
        &lt;typeAliases&gt;
            &lt;typeAlias type=&quot;com.it18zhang.mybatisdemo.domain.User&quot; alias=&quot;_User&quot;/&gt;
            &lt;typeAlias type=&quot;com.it18zhang.mybatisdemo.domain.Order&quot; alias=&quot;_Order&quot;/&gt;
            &lt;typeAlias type=&quot;com.it18zhang.mybatisdemo.domain.Item&quot; alias=&quot;_Item&quot;/&gt;
        &lt;/typeAliases&gt;
        &lt;environments default=&quot;development&quot;&gt;
            &lt;environment id=&quot;development&quot;&gt;
                &lt;transactionManager type=&quot;JDBC&quot;/&gt;
                &lt;dataSource type=&quot;POOLED&quot;&gt;
                    &lt;property name=&quot;driver&quot; value=&quot;com.mysql.jdbc.Driver&quot;/&gt;
                    &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/mybatis&quot;/&gt;
                    &lt;property name=&quot;username&quot; value=&quot;root&quot;/&gt;
                    &lt;property name=&quot;password&quot; value=&quot;root&quot;/&gt;
                &lt;/dataSource&gt;
            &lt;/environment&gt;
        &lt;/environments&gt;
        &lt;!-- 引入映射文件 --&gt;
        &lt;mappers&gt;
            &lt;mapper resource=&quot;UserMapper.xml&quot;/&gt;
            &lt;mapper resource=&quot;OrderMapper.xml&quot;/&gt;
        &lt;/mappers&gt;
    &lt;/configuration&gt;

3.OrderMapper.xml
    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;
    &lt;!DOCTYPE mapper
            PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot;
            &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;
    &lt;mapper namespace=&quot;orders&quot;&gt;
        &lt;insert id=&quot;insert&quot;&gt;
          insert into orders(orderno,uid) values(#{orderNo},#{user.id})
        &lt;/insert&gt;

        &lt;!-- findById --&gt;
        &lt;select id=&quot;selectOne&quot; parameterType=&quot;int&quot; resultMap=&quot;RM_Order&quot;&gt;
          select
            o.id      oid ,
            o.orderno oorderno ,
            o.uid     uid ,
            u.name    uname ,
            u.age     uage ,
            i.id      iid,
            i.itemname iitemname
          from orders o
            left outer join users u on o.uid = u.id
            left outer join items i on o.id = i.oid
          where o.id = #{id}
        &lt;/select&gt;
        &lt;!-- findAll --&gt;
        &lt;select id=&quot;selectAll&quot; resultMap=&quot;RM_Order&quot;&gt;
          select
            o.id      oid ,
            o.orderno oorderno ,
            o.uid     uid ,
            u.name    uname ,
            u.age     uage ,
            i.id      iid,
            i.itemname iitemname
          from orders o
            left outer join users u on o.uid = u.id
            left outer join items i on o.id = i.oid
        &lt;/select&gt;
        &lt;!-- 自定义结果映射 --&gt;
        &lt;resultMap id=&quot;RM_Order&quot; type=&quot;com.it18zhang.mybatisdemo.domain.Order&quot;&gt;
            &lt;id property=&quot;id&quot; column=&quot;oid&quot;/&gt;
            &lt;result property=&quot;orderNo&quot; column=&quot;oorderno&quot;/&gt;
            &lt;association property=&quot;user&quot; javaType=&quot;com.it18zhang.mybatisdemo.domain.User&quot;&gt;
                &lt;id property=&quot;id&quot; column=&quot;uid&quot; /&gt;
                &lt;result property=&quot;name&quot; column=&quot;uname&quot; /&gt;
                &lt;result property=&quot;age&quot; column=&quot;uage&quot; /&gt;
            &lt;/association&gt;
            &lt;collection property=&quot;items&quot; ofType=&quot;_Item&quot;&gt;
                &lt;id property=&quot;id&quot; column=&quot;iid&quot; /&gt;
                &lt;result property=&quot;itemName&quot; column=&quot;iitemname&quot; /&gt;
            &lt;/collection&gt;
        &lt;/resultMap&gt;
    &lt;/mapper&gt;

4.测试
    @Test
    public void selectOne() throws Exception {
        String resource = &quot;mybatis-config.xml&quot;;
        InputStream inputStream = Resources.getResourceAsStream(resource);
        SqlSessionFactory sf = new SqlSessionFactoryBuilder().build(inputStream);
        SqlSession s = sf.openSession();
        Order order = s.selectOne(&quot;orders.selectOne&quot;,1);
        System.out.println(order.getOrderNo() + order.getUser().getName());
        for(Item i : order.getItems()){
            System.out.println(i.getId() + &quot;:&quot; + i.getItemName());
        }
        s.commit();
        s.close();
    }
</code></pre><h2 id="改造项目"><a href="#改造项目" class="headerlink" title="改造项目"></a>改造项目</h2><pre><code>1.引入Util类
    package com.it18zhang.mybatisdemo.util;

    import org.apache.ibatis.io.Resources;
    import org.apache.ibatis.session.SqlSession;
    import org.apache.ibatis.session.SqlSessionFactory;
    import org.apache.ibatis.session.SqlSessionFactoryBuilder;

    import java.io.InputStream;

    /**
     * 工具类
     */
    public class Util {
        //
        private static SqlSessionFactory sf ;

        static{
            try {
                String resource = &quot;mybatis-config.xml&quot;;
                InputStream inputStream = Resources.getResourceAsStream(resource);
                sf = new SqlSessionFactoryBuilder().build(inputStream);
            } catch (Exception e) {
                e.printStackTrace();
            }
        }

        /**
         * 开启会话
         */
        public static SqlSession openSession(){
            return sf.openSession() ;
        }

        /**
         * 关闭会话
         */
        public static void closeSession(SqlSession s){
            if(s != null){
                s.close();
            }
        }


        /**
         * 关闭会话
         */
        public static void rollbackTx(SqlSession s) {
            if (s != null) {
                s.rollback();
            }
        }
    }

2.设计模板类DaoTemplate和回调MybatisCallback接口
    [DaoTemplate.java]
    package com.it18zhang.mybatisdemo.dao;

    import com.it18zhang.mybatisdemo.util.Util;
    import org.apache.ibatis.session.SqlSession;

    /**
     * 模板类
     */
    public class DaoTemplate {
        /**
         * 执行
         */
        public static Object execute(MybatisCallback cb){
            SqlSession s = null;
            try {
                s = Util.openSession();
                Object ret = cb.doInMybatis(s);
                s.commit();
                return ret ;
            } catch (Exception e) {
                Util.rollbackTx(s);
            } finally {
                Util.closeSession(s);
            }
            return null ;
        }
    }

    [MybatisCallback.java]
    package com.it18zhang.mybatisdemo.dao;

    import org.apache.ibatis.session.SqlSession;

    /**
     * 回调接口
     */
    public interface MybatisCallback {
        public Object doInMybatis(SqlSession s);
    }

3.通过模板类+回调接口实现UserDao.java
    [UserDao.java]
    package com.it18zhang.mybatisdemo.dao;

    import com.it18zhang.mybatisdemo.domain.User;
    import com.it18zhang.mybatisdemo.util.Util;
    import org.apache.ibatis.session.SqlSession;

    import java.util.List;

    /**
     * UserDao
     */
    public class UserDao {

        /**
         * 插入操作
         */
        public void insert(final User user){
            DaoTemplate.execute(new MybatisCallback() {
                public Object doInMybatis(SqlSession s) {
                    s.insert(&quot;users.insert&quot;,user);
                    return null ;
                }
            });
        }

        /**
         * 插入操作
         */
        public void update(final User user){
            DaoTemplate.execute(new MybatisCallback() {
                public Object doInMybatis(SqlSession s) {
                    s.update(&quot;users.update&quot;, user);
                    return null ;
                }
            });
        }

        public User selctOne(final Integer id){
            return (User)DaoTemplate.execute(new MybatisCallback() {
                public Object doInMybatis(SqlSession s) {
                    return s.selectOne(&quot;users.selectOne&quot;,id);
                }
            });
        }

        public List&lt;User&gt; selctAll(){
            return (List&lt;User&gt;)DaoTemplate.execute(new MybatisCallback() {
                public Object doInMybatis(SqlSession s) {
                    return s.selectList(&quot;users.selectAll&quot;);
                }
            });
        }
    }

4.App测试
    public static void main(String[] args) {
        UserDao dao = new UserDao();
        User u = dao.selctOne(1);
        System.out.println(u.getName());
    }
</code></pre>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/11/11/Avro-protobuf/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/11/Avro-protobuf/" itemprop="url">Avro&protobuf</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-11-11T19:52:10+08:00">
                2018-11-11
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Avro-Protobuf/" itemprop="url" rel="index">
                    <span itemprop="name">Avro&Protobuf</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="hive"><a href="#hive" class="headerlink" title="hive"></a>hive</h2><pre><code>数据倾斜.
$hive&gt;SET hive.optimize.skewjoin=true;
$hive&gt;SET hive.skewjoin.key=100000;
$hive&gt;SET hive.groupby.skewindata=true;
</code></pre><p>CREATE TABLE mydb.doc(line string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’ ;</p>
<p>select t.word,count(*) from (select explode(split(line,’ ‘)) word from doc ) t group by t.word ;</p>
<h2 id="java串行化"><a href="#java串行化" class="headerlink" title="java串行化"></a>java串行化</h2><h2 id="串行化系统"><a href="#串行化系统" class="headerlink" title="串行化系统"></a>串行化系统</h2><p>protobuf，协议缓冲区。</p>
<p>在Hadoop里面的代码很多是通过相关语言自动生成的。</p>
<p>hadoop底层的rpc都是自动生成的。</p>
<p>java也有串行化，但是hadoop不适用，因为相对来讲比protobuf和avro复杂而且性能差一些。</p>
<p>当吧对象写入磁盘文件或者用于网络传输的时候要把对象变成字节数组。</p>
<p>串行化从本质上来讲是数据格式，就是一种格式，吧对象变成字节数组，也就是说所有数据都要落实到字节数组上，都需要这样，不管是影音文件还是别的文件，通过串行化吧他边恒字节数组    </p>
<h1 id="关于Javabean"><a href="#关于Javabean" class="headerlink" title="关于Javabean:"></a>关于Javabean:</h1><p>标准javabean(pojo,plain old java object)</p>
<p>任何一个Java类也可以叫javabean.广义上。</p>
<p>狭义上的javabean：就是普通古老的java对象pojo:plain old java object </p>
<p>也就是私有的属性，getset方法，空的构造函数。比如person,学生，人。都是javabean。现实生活中的名词，在开发中都被定义成一个类，也就是说的javabean。</p>
<p>下面的代码就是一段javabean<br>    class Person{<br>        public Person(){<br>    }<br>    private String name;<br>    public  void setName(String name){<br>        this.name=name;<br>    }<br>    publc String genName(){<br>        return name; }<br>}</p>
<h2 id="google-protobuf"><a href="#google-protobuf" class="headerlink" title="google protobuf"></a>google protobuf</h2><pre><code>1.下载google protobuf.配置环境
    protoc-2.5.0-win32.zip
</code></pre><p><img src="https://i.imgur.com/7QVjPjY.png" alt=""></p>
<pre><code>1&apos;.pom.xml
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.google.protobuf&lt;/groupId&gt;
            &lt;artifactId&gt;protobuf-java&lt;/artifactId&gt;
            &lt;version&gt;2.5.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;junit&lt;/groupId&gt;
            &lt;artifactId&gt;junit&lt;/artifactId&gt;
            &lt;version&gt;4.11&lt;/version&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;

2.设计对象
    ...
3.描述对象
    package tutorial;
    option java_package = &quot;com.example.tutorial&quot;;
    option java_outer_classname = &quot;AddressBookProtos&quot;;
    //这个是一个javabean，在protobuf里面叫message
        message Person {
        required string name = 1;
        required int32 id = 2;
        optional string email = 3;
    //下面这个是一个phonetype的枚举类
        enum PhoneType {
            MOBILE = 0;
            HOME = 1;
            WORK = 2;
        }
    //这个也是一个javabean。
        message PhoneNumber {
            required string number = 1;
            optional PhoneType type = 2 [default = HOME];
        }
        repeated PhoneNumber phone = 4;
    }
    message AddressBook {
        repeated Person person = 1;
    }

4.编译描述
    cmd&gt;protoc --java_out . xxx.proto

5.导入源代码到项目中
    ...

6.使用对象
    public class TestProtoBuf {

        @Test
        public void write() throws Exception{
            AddressBookProtos.Person john = AddressBookProtos.Person.newBuilder()
                    .setId(12345)
                    .setName(&quot;tomas&quot;)
                    .setEmail(&quot;123@123.123&quot;)
                    .addPhone(AddressBookProtos.Person.PhoneNumber.newBuilder()
                            .setNumber(&quot;+351 999 999 999&quot;)
                            .setType(AddressBookProtos.Person.PhoneType.HOME)
                            .build())
                    .build();
            john.writeTo(new FileOutputStream(&quot;d:/prototbuf.data&quot;));
        }

        @Test
        public void read() throws Exception{
            AddressBookProtos.Person john = AddressBookProtos.Person.parseFrom(new FileInputStream(&quot;d:/prototbuf.data&quot;));
            System.out.println(john.getName());
        }
    }
</code></pre><p>上一段是用Protobuf进行编译和反编译的过程。然后下面这段图片是用java进行编译和反编译的过程：</p>
<p><img src="https://i.imgur.com/8rzRjOu.png" alt=""></p>
<h2 id="xml"><a href="#xml" class="headerlink" title="xml"></a>xml</h2><pre><code>&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;persons&gt;
    &lt;person id=&quot;&quot; name=&quot;&quot;&gt;
        &lt;age&gt;12&lt;/age&gt;
    &lt;/person&gt;
&lt;/person&gt;
</code></pre><h2 id="json"><a href="#json" class="headerlink" title="json"></a>json</h2><pre><code>[{
    &quot;id&quot; : 1,
    &quot;nmae&quot; : &quot;tom&quot;,
    &quot;age&quot; : 20
},
{
    &quot;id&quot; : 2,
    &quot;nmae&quot; : &quot;tomas&quot;,
    &quot;age&quot; : 30
}
]
</code></pre><h2 id="avro-doug-cutting"><a href="#avro-doug-cutting" class="headerlink" title="avro (doug cutting)"></a>avro (doug cutting)</h2><pre><code>1.数据串行化系统
2.自描述语言.
    数据结构和数据都存在文件中。跨语言。
    使用json格式存储数据。
3.可压缩 + 可切割。
4.使用avro
    a)定义schema
    b)编译schema，生成java类
        {    //名字空间是这个，然后类型，然后名字，然后是字段数组
            &quot;namespace&quot;: &quot;tutorialspoint.com&quot;,
            &quot;type&quot;: &quot;record&quot;,
            &quot;name&quot;: &quot;emp&quot;,
            &quot;fields&quot;: [
                {&quot;name&quot;: &quot;name&quot;, &quot;type&quot;: &quot;string&quot;},
                {&quot;name&quot;: &quot;id&quot;, &quot;type&quot;: &quot;int&quot;},
                {&quot;name&quot;: &quot;salary&quot;, &quot;type&quot;: &quot;int&quot;},
                {&quot;name&quot;: &quot;age&quot;, &quot;type&quot;: &quot;int&quot;},
                {&quot;name&quot;: &quot;address&quot;, &quot;type&quot;: &quot;string&quot;}
            ]
        } 
    c)使用java类
        cmd&gt;java -jar avro-tools-1.7.7.jar compile schema emp.avsc .

    d)单元测试
        package com.it18zhang.avrodemo.test;

        import org.apache.avro.Schema;
        import org.apache.avro.file.DataFileReader;
        import org.apache.avro.file.DataFileWriter;
        import org.apache.avro.generic.GenericData;
        import org.apache.avro.generic.GenericRecord;
        import org.apache.avro.io.DatumWriter;
        import org.apache.avro.specific.SpecificDatumReader;
        import org.apache.avro.specific.SpecificDatumWriter;
        import org.junit.Test;

        import java.io.File;
        import java.io.IOException;
        import java.util.Iterator;

        /**
         * Created by Administrator on 2017/3/23.
         */
        public class TestAvro {

        //    @Test
        //    public void write() throws Exception {
        //        //创建writer对象
        //        SpecificDatumWriter empDatumWriter = new SpecificDatumWriter&lt;Employee&gt;(Employee.class);
        //        //写入文件
        //        DataFileWriter&lt;Employee&gt; empFileWriter = new DataFileWriter&lt;Employee&gt;(empDatumWriter);
        //
        //        //创建对象
        //        Employee e1 = new Employee();
        //        e1.setName(&quot;tomas&quot;);
        //        e1.setAge(12);
        //
        //        //串行化数据到磁盘
        //        empFileWriter.create(e1.getSchema(), new File(&quot;d:/avro/data/e1.avro&quot;));
        //        empFileWriter.append(e1);
        //        empFileWriter.append(e1);
        //        empFileWriter.append(e1);
        //        empFileWriter.append(e1);
        //        //关闭流
        //        empFileWriter.close();
        //    }
        //
        //    @Test
        //    public void read() throws Exception {
        //        //创建writer对象
        //        SpecificDatumReader empDatumReader = new SpecificDatumReader&lt;Employee&gt;(Employee.class);
        //        //写入文件
        //        DataFileReader&lt;Employee&gt; dataReader = new DataFileReader&lt;Employee&gt;(new File(&quot;d:/avro/data/e1.avro&quot;)  ,empDatumReader);
        //        Iterator&lt;Employee&gt; it = dataReader.iterator();
        //        while(it.hasNext()){
        //            System.out.println(it.next().getName());
        //        }
        //    }

            /**
             * 直接使用schema文件进行读写，不需要编译
             */
            @Test
            public void writeInSchema() throws  Exception {
                //指定定义的avsc文件。
                Schema schema = new Schema.Parser().parse(new File(&quot;d:/avro/emp.avsc&quot;));

                //创建GenericRecord,相当于Employee
                GenericRecord e1 = new GenericData.Record(schema);
                //设置javabean属性
                e1.put(&quot;Name&quot;, &quot;ramu&quot;);
        //        e1.put(&quot;id&quot;, 001);
        //        e1.put(&quot;salary&quot;, 30000);
                e1.put(&quot;age&quot;, 25);
        //        e1.put(&quot;address&quot;, &quot;chennai&quot;);

                //
                DatumWriter&lt;GenericRecord&gt; empDatumWriter = new SpecificDatumWriter&lt;GenericRecord&gt;(GenericRecord.class);
                DataFileWriter&lt;GenericRecord&gt; empFileWriter = new DataFileWriter&lt;GenericRecord&gt;(empDatumWriter);
                empFileWriter.create(schema,new File(&quot;d:/avro/data/e2.avro&quot;)) ;
                empFileWriter.append(e1);
                empFileWriter.append(e1);
                empFileWriter.append(e1);
                empFileWriter.append(e1);
                empFileWriter.close();

            }

        }
</code></pre><p>看一下AVSC这个编译好的avro文件里面的是什么结构：</p>
<p>他其实是一个json格式的结构：</p>
<p><img src="https://i.imgur.com/dKcBw8b.png" alt=""></p>
<pre><code>非编译模式
---------------
    @Test
    public void writeInSchema() throws  Exception {
        //指定定义的avsc文件。
        Schema schema = new Schema.Parser().parse(new File(&quot;d:/avro/emp.avsc&quot;));

        //创建GenericRecord,相当于Employee
        GenericRecord e1 = new GenericData.Record(schema);
        //设置javabean属性
        e1.put(&quot;name&quot;, &quot;ramu&quot;);
//        e1.put(&quot;id&quot;, 001);
//        e1.put(&quot;salary&quot;, 30000);
        e1.put(&quot;age&quot;, 25);
//        e1.put(&quot;address&quot;, &quot;chennai&quot;);

        //
        DatumWriter w1 = new SpecificDatumWriter (schema);
        DataFileWriter w2 = new DataFileWriter(w1);
        w2.create(schema,new File(&quot;d:/avro/data/e2.avro&quot;)) ;
        w2.append(e1);
        w2.append(e1);
        w2.close();
    }

    /**
     * 反串行avro数据
     */
    @Test
    public void readInSchema() throws  Exception {
        //指定定义的avsc文件。
        Schema schema = new Schema.Parser().parse(new File(&quot;d:/avro/emp.avsc&quot;));

        GenericRecord e1 = new GenericData.Record(schema);
        DatumReader r1 = new SpecificDatumReader (schema);
        DataFileReader r2 = new DataFileReader(new File(&quot;d:/avro/data/e2.avro&quot;),r1);
        while(r2.hasNext()){
            GenericRecord rec = (GenericRecord)r2.next();
            System.out.println(rec.get(&quot;name&quot;));
        }
        r2.close();
    }
</code></pre>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/11/11/笔记重点总结/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/11/笔记重点总结/" itemprop="url">笔记重点总结</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-11-11T19:51:41+08:00">
                2018-11-11
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/笔记总结/" itemprop="url" rel="index">
                    <span itemprop="name">笔记总结</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>在Hadoop中重点是，全排序，二次排序。<br>在视频的最后有一天是讲解一个二次排序的实例，我没有细看。</p>
<p>在Hive中我认为重点的是关于查询的内容，因为在hive中一般都是加载数据然后查询，查询之后有一个自定义函数UDF的内容，这部分的内容看的不是很清晰。</p>
<p>在avro和Protoc里面主要讲解2中反编译的方法，主要是一个面向对象开发的一个过程，还是java编程不熟悉，然后代码跟着走了一遍但是肯定还是有问题，视频里面的老师是在一遍看外文的书籍，根据里面的代码模板来写的。在自己敲的过程中protoc的代码引入maven依赖包之后，还是没有能够将编译好的文件识别出来，不知道什么问题，但是avo就引入maven之后解决了这个问题。</p>
<p>在讲解zookeeper的时候我觉得在选举算法的时候他没有细致的讲解，<br>然后这边主要是zookeeper部署在hadoop集群和使用API控制zookeeper。在集群部署高可用的时候是201202203部署zk202203204部署journalnode节点，（journalnode节点已经快忘记了）。</p>
<p>容灾管理器进程ZKFC在配置的时候后来跟着yarn启动了起来。下面图是整个的进程图：</p>
<p><img src="https://i.imgur.com/osWW4ZM.png" alt=""></p>
<p>这个zookeeper的地方看的比较快，对于每个目录做什么的比较忘记，然后在配置的时候没有细究配置的意义。就是跟着走了一遍。但是最后配置了起来了。配置hdfs高可用的时候比较费时费力，配置内容比较多，配置yarn的时候比较简单，配置一下即可。</p>
<hr>
<p>在hbase安装的过程中，由于开始的时候设置了zookeeper的HA自动容灾，所以最开始的时候s206自动设置为active模式，要把s206设置为<br>standby模式才能在s201里面设置s201为HMaster。</p>
<p>也就是说要杀死掉s206里面的namenode或者说通过转换吧他变成standby然后s201设置为active模式。不然会导致出错</p>
<hr>
<p>几个端口2181 8080 50070  8020 16010</p>
<hr>
<p>在讲解hbase 第二天的动态遍历并没有看明白，也就是说Java的基础还是很薄弱的。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/11/11/心情日记———————2018-11-11/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/11/心情日记———————2018-11-11/" itemprop="url">心情日记———————2018.11.110</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-11-11T19:11:03+08:00">
                2018-11-11
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/11/09/人为什么要努力？——2018-11-09/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/09/人为什么要努力？——2018-11-09/" itemprop="url">心情日记——2018.11.09</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-11-09T22:38:08+08:00">
                2018-11-09
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/心情记/" itemprop="url" rel="index">
                    <span itemprop="name">心情记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>无意之间从知乎里面看见的题目是：人这一辈子为什么要努力？</p>
<p>泪目，与诸君共勉。</p>
<pre><code>奶奶五十岁才来的美国，身无分文，也没有一技之长，随身携带的只有长年辛勤劳作留下的一身病根。

那是九十年代初，她去了唐人街的扎花厂上班，每天工作十个小时以上，一年工作三百六十五天。

扎花按件算钱，她眼神虽然不好，却比谁扎得都快。

有人弹吉他磨出了茧，有人搬砖头磨出了茧，她被针头扎出了茧。

回国看我的时候，她给我带了费列罗巧克力，翘胡子薯片，Jif花生酱。

她给我买好看的小西装，给我买一斤几十元的黄螺。

她带我去动物园，游乐园，森林公园，带我去北京看长城，看天安门，看毛主席。

“奶奶，美国比北京还好吗？”

她用最朴实的语言向我描述纽约的繁华，告诉我美国好极了，一切都在等着我。

知乎上经常讨论富养女孩，我有一个男孩被富养的故事。

有一年，我的巧克力吃完了，薯片吃完了，花生酱吃完了，奶奶还是没有回来。

那是我第一个不愿意知道的生活真相，她中风了，瘫了半边身子。



奶奶移民美国时带去了三个未成年的儿子，二叔，三叔，四叔。

二叔和三叔在登陆美国的第二天就打工去了，年幼的四叔读了几年书后也离开了学校。

他们在亲戚家的餐馆打工，每天工作十二个小时，每周工作六天。

餐馆一年营业三百六十四天，只在感恩节那天歇业。

奶奶瘫痪后，叔叔们轮流回国，给我带好吃的，带我出去玩。

我喜欢打乒乓球，二叔给我买了乒乓球，乒乓球拍，乒乓球桌。

有一年流行四驱车，三叔给我买了一辆遥控越野车，助我碾压所有的小伙伴。

四叔年少风流，出门把妹的时候总不忘带上我，要把我培养成下一代情圣。

他们绝口不提在美国生存的艰辛，那是大人们的秘密，和我无关。



奶奶出国五年后，爸妈也去了美国。

怎一个落魄了得？夫妻俩连属于自己的房间都没有。

扎花已经被时代淘汰，只有重活可以干。我爸当过屠夫，货柜工人，货运司机。

细节不必赘述，无非就是 12小时 x 365天的陈词滥调。

后来，他们四兄弟聚首，开了一家小超市，一家餐馆，后来又开了第二家小超市，第二家餐馆。

钱赚得越来越多，老家伙们拼命工作的老毛病却没有得到丝毫缓解。



爸妈出国五年后，我也来了美国，看清了生活本来的面目。

我在纽约生活了几个月，带着半身不遂的奶奶看遍了世界之都的繁华。

在这之前，她几乎没有走出过唐人街的范围，以前对我描绘纽约时一半是转述，一半靠想象。

后来，我回到了父母的身边，结束了长达五年的骨肉分离。

我爸来车站接我，把我带到了一栋小别墅前，很得意地告诉我：“这是我们家，知道你要来，刚买的。”

我去过奶奶刚来美国时住过的那个阴暗破旧的小公寓楼，也去过爸妈栖身过的那个散发着霉味的地下室，我知道我在新家会有一张床，一个房间，但我没想到，我会有一个别墅，一个前院，一个后院，一整个铺垫好的未来。



人这一生为什么要努力？

奶奶的答案是我，爸妈的答案是我，我的答案是他们，以及把身家性命托付于我的媳妇和孩子。

对于我来说，长大了，责任多了，自然而然地想要努力，不是为了赚很多很多钱，而是为了过上自己想要的生活，过上奶奶和父母不曾拥有过的生活。他们替我吃完了所有的苦，帮我走了九十九步，我自己只需再走一步，哪敢迟疑？

从外曾祖父母算起，我们家族四代八十多口人已经在美国奋斗了半个多世纪。我们人人都在努力，我们一代好过一代。



如果你的人生起点不高，不曾有人为你走过人生百步中的任何一步，不打紧，你尽管努力，多出来的步数不会被浪费掉，总有你在乎的人用得着，而你迟早会遇到你在乎的人。

与诸君共勉。

顾宇的知乎回答索引
</code></pre>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/11/08/Hive第二天/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/08/Hive第二天/" itemprop="url">Hive第二天</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-11-08T16:17:04+08:00">
                2018-11-08
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hive/" itemprop="url" rel="index">
                    <span itemprop="name">Hive</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="hive"><a href="#hive" class="headerlink" title="hive"></a>hive</h2><pre><code>数据仓库,在线分析处理。
HiveQL,类似sql语言。
表,metadata-&gt;rdbms.
hive处理的数据是hdfs.
MR,聚合操作。
</code></pre><h1 id="hive的2个重点一个是分区一个是桶表"><a href="#hive的2个重点一个是分区一个是桶表" class="headerlink" title="hive的2个重点一个是分区一个是桶表"></a>hive的2个重点一个是分区一个是桶表</h1><h1 id="内部表-管理表-托管表"><a href="#内部表-管理表-托管表" class="headerlink" title="内部表,管理表,托管表"></a>内部表,管理表,托管表</h1><pre><code>hive,drop ,数据也删除
</code></pre><h1 id="外部表"><a href="#外部表" class="headerlink" title="外部表"></a>外部表</h1><pre><code>hive表结构。
</code></pre><h1 id="分区表"><a href="#分区表" class="headerlink" title="分区表"></a>分区表</h1><pre><code>目录.
where 缩小查询范围。
</code></pre><h1 id="bucket表"><a href="#bucket表" class="headerlink" title="bucket表"></a>bucket表</h1><pre><code>文件。
hash
clustered by &apos;&apos;
</code></pre><h1 id="join"><a href="#join" class="headerlink" title="join"></a>join</h1><pre><code>横向连接，也就是作外连接和右外连接
</code></pre><h1 id="union"><a href="#union" class="headerlink" title="union"></a>union</h1><pre><code>竖向连接。只能指定相匹配的字段

select id ,name from a1 union select id ,cid from a2;
</code></pre><h1 id="hive-union操作"><a href="#hive-union操作" class="headerlink" title="hive union操作"></a>hive union操作</h1><pre><code>select id,name from customers union select id,orderno from orders ;
$&gt;hive                            //hive --service cli 
$&gt;hive --servic hiveserver2        //启动hiveserver2，10000 [thriftServer]
$&gt;hive --service beeline        //beeline    
</code></pre><h1 id="hive使用jdbc协议实现远程访问"><a href="#hive使用jdbc协议实现远程访问" class="headerlink" title="hive使用jdbc协议实现远程访问"></a>hive使用jdbc协议实现远程访问</h1><h1 id="hive-1"><a href="#hive-1" class="headerlink" title="hive"></a>hive</h1><pre><code>$hive&gt;CREATE TABLE t3(id int,name string,age int) PARTITIONED BY (Year INT, Month INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;
</code></pre><h1 id="export"><a href="#export" class="headerlink" title="export"></a>export</h1><pre><code>$hive&gt;EXPORT TABLE customers TO &apos;/user/centos/tmp.txt&apos;;        //导出表结构+数据到hdfs目录。
看一下导出来的东西，是一个目录，包括表结构和表内容。
</code></pre><p><img src="https://i.imgur.com/pFsQ5fZ.png" alt=""></p>
<h1 id="order全排序"><a href="#order全排序" class="headerlink" title="/order全排序"></a>/order全排序</h1><pre><code>$hive&gt;select * from orders order by id asc ;
</code></pre><h1 id="sort-map端排序-本地有序。"><a href="#sort-map端排序-本地有序。" class="headerlink" title="sort,map端排序,本地有序。"></a>sort,map端排序,本地有序。</h1><pre><code>$hive&gt;select * from orders sort by id asc ;
</code></pre><h1 id="distribute-by-amp-cluster-by-amp-sort-by"><a href="#distribute-by-amp-cluster-by-amp-sort-by" class="headerlink" title="distribute by &amp; cluster by  &amp; sort by"></a>distribute by &amp; cluster by  &amp; sort by</h1><p>类似于mysql的group by,进行分区操作。</p>
<pre><code>//select cid , ... from orders distribute by cid sort by name ;            //注意顺序.
$hive&gt;select id,orderno,cid from orders distribute by cid sort by cid desc ;

//cluster by ===&gt;  distribute by cid sort by cid
</code></pre><p><strong>destribut by 和cluster by图解对比：</strong></p>
<p><img src="https://i.imgur.com/BQ7NHQU.png" alt=""></p>
<p>这个地方没有细讲，说主要应用还是group by 只要记住这个cluster是dirstribute by 和sort by 的组合极客</p>
<h2 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h2><pre><code>mysql&gt;select concat(&apos;tom&apos;,1000) ;
$hive&gt;select current_database(),current_user() ;
$hive&gt;tab                                //查看帮助
</code></pre><h2 id="设置作业参数"><a href="#设置作业参数" class="headerlink" title="设置作业参数"></a>设置作业参数</h2><pre><code>$hive&gt;set hive.exec.reducers.bytes.per.reducer=xxx            //设置reducetask的字节数。
$hive&gt;set hive.exec.reducers.max=0                            //设置reduce task的最大任务数
$hive&gt;set mapreduce.job.reduces=0                            //设置reducetask个数。
</code></pre><h2 id="动态分区-严格模式-非严格模式"><a href="#动态分区-严格模式-非严格模式" class="headerlink" title="动态分区-严格模式-非严格模式"></a>动态分区-严格模式-非严格模式</h2><pre><code>动态分区模式:strict-严格模式，插入时至少指定一个静态分区，nonstrict-非严格模式-可以不指定静态分区。
set hive.exec.dynamic.partition.mode=nonstrict            //设置非严格模式
$hive&gt;INSERT [OVERWRITE] into TABLE employees PARTITION (country, state) SELECT ..., se.cnty, se.st FROM staged_employees se WHERE se.cnty = &apos;US&apos;;
</code></pre><p><img src="https://i.imgur.com/8zgzuuO.png" alt=""></p>
<p>本来有一个已经分区好了的表，然后再建立一个表，比如也是相同的字段，然后新表没有分区要插进去老表，让他自动分区，就要用到动态分区，动态分区分为严格模式和非严格模式。如下图先使用了严格模式的情况，出错，说要至少指定一个分区也就是</p>
<pre><code>$hive&gt;INSERT [OVERWRITE] into TABLE employees PARTITION (country=&apos;US&apos;, state) SELECT ..., se.cnty, se.st FROM staged_employees;
</code></pre><p>但是严格模式还是要制定这个分区，所以用一下非严格模式先设置一下：</p>
<pre><code>set hive.exec.dynamic.partition.mode=nonstrict            //设置非严格模式
</code></pre><p>然后我们开始插入：</p>
<pre><code>$hive&gt;INSERT [OVERWRITE] into TABLE employees PARTITION (country, state) SELECT ..., se.cnty, se.st FROM staged_employees;
</code></pre><p>这样就实现了非严格模式下的动态分区</p>
<h1 id="hive事务处理在-gt-0-13-0之后支持行级事务。-（这个地方老师没讲清楚自己没搞出来）"><a href="#hive事务处理在-gt-0-13-0之后支持行级事务。-（这个地方老师没讲清楚自己没搞出来）" class="headerlink" title="hive事务处理在&gt;0.13.0之后支持行级事务。 #（这个地方老师没讲清楚自己没搞出来）"></a>hive事务处理在&gt;0.13.0之后支持行级事务。 #（这个地方老师没讲清楚自己没搞出来）</h1><pre><code>1.所有事务自动提交。
2.只支持orc格式。
3.使用bucket表。
4.配置hive参数，使其支持事务。
</code></pre><p>$hive&gt;SET hive.support.concurrency = true;<br>$hive&gt;SET hive.enforce.bucketing = true;<br>$hive&gt;SET hive.exec.dynamic.partition.mode = nonstrict;<br>$hive&gt;SET hive.txn.manager = org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;<br>$hive&gt;SET hive.compactor.initiator.on = true;<br>$hive&gt;SET hive.compactor.worker.threads = 1;</p>
<pre><code>5.使用事务性操作
    $&gt;CREATE TABLE tx(id int,name string,age int) CLUSTERED BY (id) INTO 3 BUCKETS ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; stored as orc TBLPROPERTIES (&apos;transactional&apos;=&apos;true&apos;);
</code></pre><p><img src="https://i.imgur.com/UWJd7M5.png" alt=""></p>
<hr>
<p>mysql支持这样的操作，左边选择的查询的列，并不出现在右边的分组当中这样可以查询，但是hive中不允许这样。</p>
<p><img src="https://i.imgur.com/vpfbmjD.png" alt=""></p>
<p>但是在hive当中只可以如下图查询：</p>
<p><img src="https://i.imgur.com/oOqn35d.png" alt=""></p>
<h2 id="聚合处理"><a href="#聚合处理" class="headerlink" title="聚合处理"></a>聚合处理</h2><pre><code>每个用户购买商品订单大于1的：这个按照cid分组的，查询每个分组里面的count(*)，也就是每个不同的cid 的

$hive&gt;select cid,count(*) c ,max(price) from orders group by cid having c &gt; 1 ; 
</code></pre><h1 id="wordcount"><a href="#wordcount" class="headerlink" title="wordcount"></a>wordcount</h1><p>创建新表:stats(word string,c int) ;<br>    将查询结果插入到指定表中。</p>
<p><img src="https://i.imgur.com/wxA99my.png" alt=""></p>
<p>按照下图这个切割方法，用一个split的函数切割，切割之后形成一个数组类型的东西</p>
<p><img src="https://i.imgur.com/NVQuyss.png" alt=""></p>
<p>explode这个函数传入的需要是一个经过split切割之后的数组，传入之后再去展开得到如上图所示的东西。但是切出来还是需要聚合，查询每个单词的个数</p>
<p>对下面这个SQL语句进行讲解。先看括号里面的最里面的括号，select explode(split(line,’ ‘)) as word from doc这句话都能理解，就是先切割成数组，然后对这个数据进行explode炸裂成单词，然后as word就是别名叫word，然后再把整个查询的as t别名t，然后select t.word,count(*) c from …group by t.word order by c desc limit2;就是对t的word字段进行分组，对分组的t的字段word和计数总和进行查询，然后按照降序排序，然后limit 2取最高的2个数。</p>
<pre><code>$hive&gt;select t.word,count(*) c from ((select explode(split(line, &apos; &apos;)) as word from doc) as t) group by t.word order by c desc limit 2 ;
</code></pre><h1 id="view-视图-虚表"><a href="#view-视图-虚表" class="headerlink" title="view:视图,虚表"></a>view:视图,虚表</h1><p>是一个虚拟的表，类似于对这个语句的封装，或者说起了一个别名，并不是真的表。</p>
<p>这个地方创建视图的时候不能</p>
<pre><code>$hive&gt;create view v1 as select a.*,b.*from customers a left outer join default.tt b on a.id = b.cid ;
</code></pre><p>上图这样是错误的，因为在a和b里面有相同的字段，要把相同的字段修改为不同的字段，如下图：</p>
<pre><code>//创建视图
$hive&gt;create view v1 as select a.id aid,a.name ,b.id bid , b.order from customers a left outer join default.tt b on a.id = b.cid ;
</code></pre><p>可以在续表的基础上在查询，如下所示：</p>
<pre><code>//查看视图
$hive&gt;show tables ;
$hive&gt;select * from v1 ;
</code></pre><p>view这个在hdfs上没有哦，而是存放在了mysql里面</p>
<h2 id="一个mysql里面的操作：-G-行转列"><a href="#一个mysql里面的操作：-G-行转列" class="headerlink" title="一个mysql里面的操作：\G 行转列"></a>一个mysql里面的操作：\G 行转列</h2><p>select * from tbls \G：结果如下：</p>
<p><img src="https://i.imgur.com/AbE1mvx.png" alt=""></p>
<h2 id="Map端连接"><a href="#Map端连接" class="headerlink" title="Map端连接"></a>Map端连接</h2><pre><code>$hive&gt;set hive.auto.convert.join=true            //设置自动转换连接,默认开启了。

//使用mapjoin连接暗示实现mapjoin
$hive&gt;select /*+ mapjoin(customers) */ a.*,b.* from customers a left outer join orders b on a.id = b.cid ;
</code></pre><h2 id="调优"><a href="#调优" class="headerlink" title="调优"></a>调优</h2><pre><code>1.explain
    使用explain查看查询计划
    hive&gt;explain [extended] select count(*) from customers ;
    hive&gt;explain select t.name , count(*) from (select a.name ,b.id,b.orderno from customers a ,orders b where a.id = b.cid) t group by t.name ;

    //设置limit优化测，避免全部查询.但是树上说因为是随机取样，可能会出问题。
    hive&gt;set hive.limit.optimize.enable=true

    //本地模式
    $hive&gt;set mapred.job.tracker=local;            //
    $hive&gt;set hive.exec.mode.local.auto=true    //自动本地模式,主要用于测试不用于实战


    //并行执行,同时执行不存在依赖关系的阶段。??
    $hive&gt;set hive.exec.parallel=true            //是自动设置好的

    //严格模式,
    $hive&gt;set hive.mapred.mode=strict            //手动设置之后。1.分区表必须指定分区进行查询，否则不让查询。
                                                //2.order by时必须使用limit子句。
                                                //3.不允许笛卡尔积.


    //设置MR的数量
    hive&gt; set hive.exec.reducers.bytes.per.reducer=750000000;    //设置reduce处理的字节数。

    //JVM重用
    $hive&gt;set mapreduce.job.jvm.numtasks=1        //-1没有限制，使用大量小文件。


    //UDF
    //User define function,用户自定义函数
    //current_database(),current_user();

    //显式所有函数
    $hive&gt;show functions;
    $hive&gt;select array(1,2,3) ;

    //显式指定函数帮助
    $hive&gt;desc function current_database();

    //表生成函数,多行函数。
    $hive&gt;explode(str,exp);            //按照exp切割str.
</code></pre><h2 id="自定义函数"><a href="#自定义函数" class="headerlink" title="自定义函数"></a>自定义函数</h2><p><img src="https://i.imgur.com/lvT1nkj.png" alt=""></p>
<p>上图带括号的都是函数，不带括号的是命令</p>
<pre><code>1.创建类，继承UDF
    package com.it18zhang.hivedemo.udf;

    import org.apache.hadoop.hive.ql.exec.Description;
    import org.apache.hadoop.hive.ql.exec.UDF;

    /**
     * 自定义hive函数
     */
    @Description(name = &quot;myadd&quot;,
            value = &quot;myadd(int a , int b) ==&gt; return a + b &quot;,
            extended = &quot;Example:\n&quot;
                    + &quot; myadd(1,1) ==&gt; 2 \n&quot;
                    + &quot; myadd(1,2,3) ==&gt; 6;&quot;)
    public class AddUDF extends UDF {

        public int evaluate(int a ,int b) {
            return a + b ;
        }

        public int evaluate(int a ,int b , int c) {
            return a + b + c;
        }
    }
2.打成jar包。
    cmd&gt;cd {classes所在目录}
    cmd&gt;jar cvf HiveDemo.jar -C x/x/x/x/classes/ .
3.添加jar包到hive的类路径
    //添加jar到类路径
    $&gt;cp /mnt/hgfs/downloads/bigdata/data/HiveDemo.jar /soft/hive/lib

3.重进入hive
    $&gt;....

4.创建临时函数
    //
    CREATE TEMPORARY FUNCTION myadd AS &apos;com.it18zhang.hivedemo.udf.AddUDF&apos;;

5.在查询中使用自定义函数
    $hive&gt;select myadd(1,2)  ;

6.定义日期函数
    1)定义类
    public class ToCharUDF extends UDF {
        /**
         * 取出服务器的当前系统时间 2017/3/21 16:53:55
         */
        public String evaluate() {
            Date date = new Date();
            SimpleDateFormat sdf = new SimpleDateFormat();
            sdf.applyPattern(&quot;yyyy/MM/dd hh:mm:ss&quot;);
            return sdf.format(date) ;
        }
        public String evaluate(Date date) {
            SimpleDateFormat sdf = new SimpleDateFormat();
            sdf.applyPattern(&quot;yyyy/MM/dd hh:mm:ss&quot;);
            return sdf.format(date) ;
        }

        public String evaluate(Date date,String frt) {
            SimpleDateFormat sdf = new SimpleDateFormat();
            sdf.applyPattern(frt);
            return sdf.format(date) ;
        }
    }

    2)导出jar包，通过命令添加到hive的类路径(不需要重进hive)。
        $hive&gt;add jar /mnt/hgfs/downloads/bigdata/data/HiveDemo-1.0-SNAPSHOT.jar

    3)注册函数
        $hive&gt;CREATE TEMPORARY FUNCTION to_char AS &apos;com.it18zhang.hivedemo.udf.ToCharUDF&apos;;
        $hive&gt;CREATE TEMPORARY FUNCTION to_date AS &apos;com.it18zhang.hivedemo.udf.ToCharUDF&apos;;
</code></pre><h2 id="定义Nvl函数-这个是在英文版的Hadoop权威指南上给的"><a href="#定义Nvl函数-这个是在英文版的Hadoop权威指南上给的" class="headerlink" title="定义Nvl函数(这个是在英文版的Hadoop权威指南上给的)"></a>定义Nvl函数(这个是在英文版的Hadoop权威指南上给的)</h2><pre><code>package com.it18zhang.hivedemo.udf;

import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;
import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;
import org.apache.hadoop.hive.ql.metadata.HiveException;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;

/**
 * 自定义null值处理函数
 */
public class Nvl extends GenericUDF {
    private GenericUDFUtils.ReturnObjectInspectorResolver returnOIResolver;
    private ObjectInspector[] argumentOIs;

    public ObjectInspector initialize(ObjectInspector[] arguments)
            throws UDFArgumentException {
        argumentOIs = arguments;
        //检查参数个数
        if (arguments.length != 2) {
            throw new UDFArgumentLengthException(
                    &quot;The operator &apos;NVL&apos; accepts 2 arguments.&quot;);
        }
        returnOIResolver = new GenericUDFUtils.ReturnObjectInspectorResolver(true);
        //检查参数类型
        if (!(returnOIResolver.update(arguments[0]) &amp;&amp; returnOIResolver
                .update(arguments[1]))) {
            throw new UDFArgumentTypeException(2,
                    &quot;The 1st and 2nd args of function NLV should have the same type, &quot;
                            + &quot;but they are different: \&quot;&quot; + arguments[0].getTypeName()
                            + &quot;\&quot; and \&quot;&quot; + arguments[1].getTypeName() + &quot;\&quot;&quot;);
        }
        return returnOIResolver.get();
    }

    public Object evaluate(DeferredObject[] arguments) throws HiveException {
        Object retVal = returnOIResolver.convertIfNecessary(arguments[0].get(), argumentOIs[0]);
        if (retVal == null) {
            retVal = returnOIResolver.convertIfNecessary(arguments[1].get(),
                    argumentOIs[1]);
        }
        return retVal;
    }

    public String getDisplayString(String[] children) {
        StringBuilder sb = new StringBuilder();
        sb.append(&quot;if &quot;);
        sb.append(children[0]);
        sb.append(&quot; is null &quot;);
        sb.append(&quot;returns&quot;);
        sb.append(children[1]);
        return sb.toString();
    }
}

2)添加jar到类路径
    ...
3)注册函数
    $hive&gt;CREATE TEMPORARY FUNCTION nvl AS &apos;org.apache.hadoop.hive.ql.udf.generic.GenericUDFNvl&apos;;
</code></pre><h1 id="一些课上用的PPT"><a href="#一些课上用的PPT" class="headerlink" title="一些课上用的PPT"></a>一些课上用的PPT</h1><p><img src="https://i.imgur.com/6OyT9i3.png" alt=""><br><img src="https://i.imgur.com/CLZNHSV.png" alt=""><br><img src="https://i.imgur.com/Rx5Hslt.png" alt=""><br><img src="https://i.imgur.com/fb0PfG0.png" alt=""><br><img src="https://i.imgur.com/P8kwdax.png" alt=""><br><img src="https://i.imgur.com/avoDPKV.png" alt=""><br><img src="https://i.imgur.com/OwHiTIn.png" alt=""><br><img src="https://i.imgur.com/x4qXG7v.png" alt=""><br><img src="https://i.imgur.com/mIQbSR1.png" alt=""><br><img src="https://i.imgur.com/2bMpEMd.png" alt=""><br><img src="https://i.imgur.com/bSF4lMN.png" alt=""><br><img src="https://i.imgur.com/C3IMpOQ.png" alt=""><br><img src="https://i.imgur.com/6xpn8Ul.png" alt=""></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/touxiang.jpg"
                alt="Eric Hunn" />
            
              <p class="site-author-name" itemprop="name">Eric Hunn</p>
              <p class="site-description motion-element" itemprop="description">大数据|动漫迷|科技控</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">39</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">93</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/erichunn" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:erichunn.me@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://weibo.com/u/2944687303/home?wvr=5" target="_blank" title="Webibo">
                      
                        <i class="fa fa-fw fa-weibo"></i>Webibo</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://github.com/drakeet/" title="Drakeet" target="_blank">Drakeet</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      

      
	  
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=1311545319&auto=1&height=66"></iframe>

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Eric Hunn</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  

  

  

</body>
</html>
