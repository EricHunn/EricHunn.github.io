<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />





  <link rel="alternate" href="/atom.xml" title="无心是一首歌" type="application/atom+xml" />






<meta name="description" content="大数据|动漫迷|科技控">
<meta property="og:type" content="website">
<meta property="og:title" content="无心是一首歌">
<meta property="og:url" content="http://erichunn.github.io/index.html">
<meta property="og:site_name" content="无心是一首歌">
<meta property="og:description" content="大数据|动漫迷|科技控">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="无心是一首歌">
<meta name="twitter:description" content="大数据|动漫迷|科技控">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: 'undefined',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://erichunn.github.io/"/>





  <title>无心是一首歌</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?02fe19ac065353167cab1b453f2909ec";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">无心是一首歌</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-首页">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-关于我">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于我
          </a>
        </li>
      
        
        <li class="menu-item menu-item-标签">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-分类">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-归档">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-站点地图">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br />
            
            站点地图
          </a>
        </li>
      
        
        <li class="menu-item menu-item-腾讯公益">
          <a href="/404/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br />
            
            腾讯公益
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/11/25/Kafka/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/25/Kafka/" itemprop="url">Kafka</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-11-25T15:37:41+08:00">
                2018-11-25
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="flume"><a href="#flume" class="headerlink" title="flume"></a>flume</h2><pre><code>收集日志、移动、聚合框架。
基于事件。
</code></pre><h2 id="agent"><a href="#agent" class="headerlink" title="agent"></a>agent</h2><pre><code>source        //接收数据,生产者
            //put()
            //NetcatSource
            //ExecSource,实时收集 tail -F xxx.txt
            //spooldir
            //seq
            //Stress
            //avroSource

channel        //暂存数据，缓冲区,
            //非永久性:MemoryChannel
            //永久性  :FileChannel,磁盘. 
            //SpillableMemoryChannel :Mem + FileChannel.Capacity

sink        //输出数据,消费者
            //从channel提取take()数据,write()destination.
            //HdfsSink
            //HbaseSink
            //avroSink
</code></pre><p>看一下一边都是存储到哪里，如果是电信的那种需要经常查询的就需要放到Hbase里面，如果是放到hdfs里面就只能是全表扫描了。hbase可以随机定位。瞬间定位。  </p>
<h2 id="JMS"><a href="#JMS" class="headerlink" title="JMS"></a>JMS</h2><pre><code>java message service,java消息服务。

queue        //只有能有一个消费者。P2P模式(点对点).
            //发布订阅(publish-subscribe,主题模式)，
</code></pre><h2 id="kafka"><a href="#kafka" class="headerlink" title="kafka"></a>kafka</h2><pre><code>分布式流处理平台。
在系统之间构建实时数据流管道。
以topic分类对记录进行存储
每个记录包含key-value+timestamp
每秒钟百万消息吞吐量。


producer            //消息生产者
consumer            //消息消费者
consumer group        //消费者组
kafka server        //broker,kafka服务器也叫broker    
topic                //主题,副本数,分区.
zookeeper            //hadoop namenoade + RM HA | hbase | kafka
</code></pre><h2 id="安装kafka"><a href="#安装kafka" class="headerlink" title="安装kafka"></a>安装kafka</h2><pre><code>0.选择s202 ~ s204三台主机安装kafka
1.准备zk
    略
2.jdk
    略
3.tar文件
4.环境变量
    略
5.配置kafka
    [kafka/config/server.properties]
    ...
    broker.id=202
    ...
    listeners=PLAINTEXT://:9092
    ...
    log.dirs=/home/centos/kafka/logs
    ...
    zookeeper.connect=s201:2181,s202:2181,s203:2181

6.分发server.properties，同时修改每个文件的broker.id

7.启动kafka服务器
    a)先启动zk
    b)启动kafka
        [s202 ~ s204]
        $&gt;bin/kafka-server-start.sh config/server.properties

    c)验证kafka服务器是否启动
        $&gt;netstat -anop | grep 9092

8.创建主题 
    $&gt;bin/kafka-topics.sh --create --zookeeper s201:2181 --replication-factor 3 --partitions 3 --topic test

9.查看主题列表
    $&gt;bin/kafka-topics.sh --list --zookeeper s201:2181

10.启动控制台生产者
    $&gt;bin/kafka-console-producer.sh --broker-list s202:9092 --topic test1

11.启动控制台消费者
    $&gt;bin/kafka-console-consumer.sh --bootstrap-server s202:9092 --topic test1 --from-beginning --zookeeper s202:2181

12.在生产者控制台输入hello world
</code></pre><h2 id="kafka集群在zk的配置"><a href="#kafka集群在zk的配置" class="headerlink" title="kafka集群在zk的配置"></a>kafka集群在zk的配置</h2><pre><code>/controller            ===&gt;    {&quot;version&quot;:1,&quot;brokerid&quot;:202,&quot;timestamp&quot;:&quot;1490926369148&quot;

/controller_epoch    ===&gt;    1

/brokers
/brokers/ids        //记载kfk集群每个服务器的信息
/brokers/ids/202    ===&gt;    {&quot;jmx_port&quot;:-1,&quot;timestamp&quot;:&quot;1490926370304&quot;,&quot;endpoints&quot;:[&quot;PLAINTEXT://s202:9092&quot;],&quot;host&quot;:&quot;s202&quot;,&quot;version&quot;:3,&quot;port&quot;:9092}            //每个节点的连接信息
/brokers/ids/203
/brokers/ids/204        


//每个主题下分区数据，主题是有分区的。
/brokers/topics/test/partitions/0/state ===&gt;{&quot;controller_epoch&quot;:1,&quot;leader&quot;:203,&quot;version&quot;:1,&quot;leader_epoch&quot;:0,&quot;isr&quot;:[203,204,202]}
/brokers/topics/test/partitions/1/state ===&gt;...
/brokers/topics/test/partitions/2/state ===&gt;...
</code></pre><p>leader是针对于主题来说的，是针对主题上的分区来说的。每个分区test主题下。controller也是kfk注册的他和broker是一个层级。说明在kfk集群里面s202是类似于Leader的身份。在</p>
<pre><code>/brokers/seqid        ===&gt; null

/admin
/admin/delete_topics/test        ===&gt;标记删除的主题

/isr_change_notification

/consumers/xxxx/
/config
</code></pre><p>下图是生产者没有连接zk别的都是有连接zk的</p>
<p><img src="https://i.imgur.com/Kq3MmBF.png" alt=""></p>
<h2 id="容错"><a href="#容错" class="headerlink" title="容错"></a>容错</h2><h2 id="创建主题"><a href="#创建主题" class="headerlink" title="创建主题"></a>创建主题</h2><pre><code>//2个副本5个分区，2乘以5对应了10个文件夹分配到3个节点所以s202里面有3个，
repliation_factor 2 partitions 5

$&gt;kafka-topic.sh --zookeeper s202:2181 --replication_factor 3 --partitions 4 --create --topic test3

2 x 5  = 10        //是个文件夹

[s202]
test2-1            //
test2-2            //
test2-3            //

[s203]
test2-0
test2-2
test2-3
test2-4

[s204]
test2-0
test2-1
test2-4
</code></pre><h2 id="重新布局分区和副本，手动再平衡"><a href="#重新布局分区和副本，手动再平衡" class="headerlink" title="重新布局分区和副本，手动再平衡"></a>重新布局分区和副本，手动再平衡</h2><pre><code>$&gt;kafka-topics.sh --alter --zookeeper s202:2181 --topic test2 --replica-assignment 203:204,203:204,203:204,203:204,203:204
</code></pre><h2 id="副本"><a href="#副本" class="headerlink" title="副本"></a>副本</h2><pre><code>broker存放消息以消息达到顺序存放。生产和消费都是副本感知的。
支持到n-1故障。每个分区都有leader，follow.
leader挂掉时，消息分区写入到本地log或者，向生产者发送消息确认回执之前，生产者向新的leader发送消息。

新leader的选举是通过isr进行，第一个注册的follower成为leader。
</code></pre><h2 id="kafka支持副本模式"><a href="#kafka支持副本模式" class="headerlink" title="kafka支持副本模式"></a>kafka支持副本模式</h2><pre><code>[同步复制]
    1.producer联系zk识别leader
    2.向leader发送消息
    3.leadr收到消息写入到本地log
    4.follower从leader pull消息
    5.follower向本地写入log
    6.follower向leader发送ack消息
    7.leader收到所有follower的ack消息
    8.leader向producer回传ack


[异步副本]
    和同步复制的区别在与leader写入本地log之后，
    直接向client回传ack消息，不需要等待所有follower复制完成。
</code></pre><h2 id="通过java-API实现消息生产者，发送消息"><a href="#通过java-API实现消息生产者，发送消息" class="headerlink" title="通过java API实现消息生产者，发送消息"></a>通过java API实现消息生产者，发送消息</h2><pre><code>package com.it18zhang.kafkademo.test;

import org.junit.Test;

import kafka.javaapi.producer.Producer;
import kafka.producer.KeyedMessage;
import kafka.producer.ProducerConfig;

import java.util.HashMap;
import java.util.Properties;

/**
 * Created by Administrator on 2017/3/31.
 */
public class TestProducer {
    @Test
    public void testSend(){
        Properties props = new Properties();
        //broker列表
        props.put(&quot;metadata.broker.list&quot;, &quot;s202:9092&quot;);
        //串行化
        props.put(&quot;serializer.class&quot;, &quot;kafka.serializer.StringEncoder&quot;);
        //
        props.put(&quot;request.required.acks&quot;, &quot;1&quot;);

        //创建生产者配置对象
        ProducerConfig config = new ProducerConfig(props);

        //创建生产者
        Producer&lt;String, String&gt; producer = new Producer&lt;String, String&gt;(config);

        KeyedMessage&lt;String, String&gt; msg = new KeyedMessage&lt;String, String&gt;(&quot;test3&quot;,&quot;100&quot; ,&quot;hello world tomas100&quot;);
        producer.send(msg);
        System.out.println(&quot;send over!&quot;);
    }
}
</code></pre><h2 id="消息消费者"><a href="#消息消费者" class="headerlink" title="消息消费者"></a>消息消费者</h2><pre><code>/**
 * 消费者
 */
@Test
public void testConumser(){
    //
    Properties props = new Properties();
    props.put(&quot;zookeeper.connect&quot;, &quot;s202:2181&quot;);
    props.put(&quot;group.id&quot;, &quot;g3&quot;);
    props.put(&quot;zookeeper.session.timeout.ms&quot;, &quot;500&quot;);
    props.put(&quot;zookeeper.sync.time.ms&quot;, &quot;250&quot;);
    props.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;);
    props.put(&quot;auto.offset.reset&quot;, &quot;smallest&quot;);
    //创建消费者配置对象
    ConsumerConfig config = new ConsumerConfig(props);
    //
    Map&lt;String, Integer&gt; map = new HashMap&lt;String, Integer&gt;();
    map.put(&quot;test3&quot;, new Integer(1));
    Map&lt;String, List&lt;KafkaStream&lt;byte[], byte[]&gt;&gt;&gt; msgs = Consumer.createJavaConsumerConnector(new ConsumerConfig(props)).createMessageStreams(map);
    List&lt;KafkaStream&lt;byte[], byte[]&gt;&gt; msgList = msgs.get(&quot;test3&quot;);
    for(KafkaStream&lt;byte[],byte[]&gt; stream : msgList){
        ConsumerIterator&lt;byte[],byte[]&gt; it = stream.iterator();
        while(it.hasNext()){
            byte[] message = it.next().message();
            System.out.println(new String(message));
        }
    }
}
</code></pre><h2 id="flume集成kafka"><a href="#flume集成kafka" class="headerlink" title="flume集成kafka"></a>flume集成kafka</h2><pre><code>1.KafkaSink
    [生产者]
    a1.sources = r1
    a1.sinks = k1
    a1.channels = c1

    a1.sources.r1.type=netcat
    a1.sources.r1.bind=localhost
    a1.sources.r1.port=8888

    a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink
    a1.sinks.k1.kafka.topic = test3
    a1.sinks.k1.kafka.bootstrap.servers = s202:9092
    a1.sinks.k1.kafka.flumeBatchSize = 20
    a1.sinks.k1.kafka.producer.acks = 1

    a1.channels.c1.type=memory

    a1.sources.r1.channels = c1
    a1.sinks.k1.channel = c1

2.KafkaSource
    [消费者]
    a1.sources = r1
    a1.sinks = k1
    a1.channels = c1

    a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
    a1.sources.r1.batchSize = 5000
    a1.sources.r1.batchDurationMillis = 2000
    a1.sources.r1.kafka.bootstrap.servers = s202:9092
    a1.sources.r1.kafka.topics = test3
    a1.sources.r1.kafka.consumer.group.id = g4

    a1.sinks.k1.type = logger

    a1.channels.c1.type=memory

    a1.sources.r1.channels = c1
    a1.sinks.k1.channel = c1

3.Channel
    生产者 + 消费者
    a1.sources = r1
    a1.sinks = k1
    a1.channels = c1

    a1.sources.r1.type = avro
    a1.sources.r1.bind = localhost
    a1.sources.r1.port = 8888

    a1.sinks.k1.type = logger

    a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel
    a1.channels.c1.kafka.bootstrap.servers = s202:9092
    a1.channels.c1.kafka.topic = test3
    a1.channels.c1.kafka.consumer.group.id = g6

    a1.sources.r1.channels = c1
    a1.sinks.k1.channel = c1
</code></pre>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/11/22/Hbase第四天/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/22/Hbase第四天/" itemprop="url">Hbase第四天</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-11-22T09:26:56+08:00">
                2018-11-22
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="hbase"><a href="#hbase" class="headerlink" title="hbase"></a>hbase</h2><pre><code>协处理器.
Observer            //触发器,基于事件激活的。
Endpoint            //存储过程,客户端调用。

RegionObserver        //system --&gt; user[加载顺序]

100

00-99

callerId - 201703 : hashcode % 100 = 00-99

01,139xxxx,138yyy,....
</code></pre><h2 id="热点"><a href="#热点" class="headerlink" title="热点"></a>热点</h2><pre><code>让数据均匀分散。
</code></pre><p>create ‘ns1:calllogs’ , SPLITS=&gt;[01,02,03,,…99,]</p>
<h2 id="rowkey"><a href="#rowkey" class="headerlink" title="rowkey"></a>rowkey</h2><pre><code>按照byte排序。
</code></pre><p>create table xxx(){</p>
<p>}</p>
<h2 id="rowkey-1"><a href="#rowkey-1" class="headerlink" title="rowkey"></a>rowkey</h2><pre><code>分区编号
xx,callerId,callTime,calleeId

startkey = xx,19626675332,
startkey = xx,19626675333,
</code></pre><h1 id="对通话记录表的设计：（具体在HBase的设计原则2）"><a href="#对通话记录表的设计：（具体在HBase的设计原则2）" class="headerlink" title="对通话记录表的设计：（具体在HBase的设计原则2）"></a>对通话记录表的设计：（具体在HBase的设计原则2）</h1><p><img src="https://i.imgur.com/Zt5A1OI.png" alt=""></p>
<p>对于首先创建的主叫的表是上面这张表，在查询主叫的时候只需要指定xx，主叫时间,时间片。即可。但是查询被叫的时候就不行了。几乎要全表扫描。用00,138xx,2017010101,139xxx这样查询。所以每次向这个表写记录的时候，我们都知道被叫是谁，如果知道被叫的话。我们可以在设计一张表，叫calleeLog,他的rowkey有calleid，time,callerid构成。被叫表存的value存的是主叫表rowkey里面的被叫。然后根据这个查到主叫表后面的内容。但是如果只想知道谁给你打的电话，所以在被叫表的rowkey里面加了一个callerid，如果想查询谁给你打了电话，就在被叫表rowkey里面加了一个callerid。主叫表的rowkey里面都是作为主叫出现的，被叫表里面的数据都是作为被叫出现的。主叫表的内容被叫表里没有，被叫表的内容主叫表也没有的。而且应该吧时长duration也放在里面。也就是说要把最经常使用的信息都编入rowkey里面去，能不查具体的value就尽量不查询具体的value，但是如果要查询具体的数据，什么基站，那个口啊，就是要查询主叫表里面的value的值。这个下图所示的也就是叫二次索引。在写入主叫表的时候也在往被叫表里面写入，用什么写入？也就是用协处理器来处理，怎么处理呢，就是在你写入主叫表的时候，协处理器立刻截获，然后重写里面的方法，往被叫表里面写入就可以了。所以这个就是一个电信HBase的设计原则。    </p>
<p><img src="https://i.imgur.com/QLxuFNg.png" alt=""></p>
<h2 id="通化记录"><a href="#通化记录" class="headerlink" title="通化记录"></a>通化记录</h2><pre><code>1.创建表
    create &apos;ns1:calllogs&apos;,&apos;f1&apos;

2.创建单元测试
    @Test
    public void put() throws Exception {
        Configuration conf = HBaseConfiguration.create();
        Connection conn = ConnectionFactory.createConnection(conf);
        TableName tname = TableName.valueOf(&quot;ns1:calllogs&quot;);
        Table table = conn.getTable(tname);

        String callerId = &quot;13845456767&quot; ;
        String calleeId = &quot;139898987878&quot; ;
        SimpleDateFormat sdf = new SimpleDateFormat();
        sdf.applyPattern(&quot;yyyyMMddHHmmss&quot;);
        String callTime = sdf.format(new Date());
        int duration = 100 ;
        DecimalFormat dff = new DecimalFormat();
        dff.applyPattern(&quot;00000&quot;);
        String durStr = dff.format(duration);

        //区域00-99
        int hash = (callerId + callTime.substring(0, 6)).hashCode();
        hash = (hash &amp; Integer.MAX_VALUE) % 100 ;

        //hash区域号
        DecimalFormat df = new DecimalFormat();
        df.applyPattern(&quot;00&quot;);
        String regNo = df.format(hash);

        //拼接rowkey
        //xx , callerid , time ,  direction, calleid  ,duration
        String rowkey = regNo + &quot;,&quot; + callerId + &quot;,&quot; + callTime + &quot;,&quot; + &quot;0,&quot; + calleeId + &quot;,&quot; + durStr  ;
        byte[] rowid = Bytes.toBytes(rowkey);
        Put put = new Put(rowid);
        put.addColumn(Bytes.toBytes(&quot;f1&quot;),Bytes.toBytes(&quot;callerPos&quot;),Bytes.toBytes(&quot;河北&quot;));
        put.addColumn(Bytes.toBytes(&quot;f1&quot;),Bytes.toBytes(&quot;calleePos&quot;),Bytes.toBytes(&quot;河南&quot;));
        //执行插入
        table.put(put);
        System.out.println(&quot;over&quot;);
    }

3.创建协处理器
    public class CalleeLogRegionObserver extends BaseRegionObserver{

        public void postPut(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Put put, WALEdit edit, Durability durability) throws IOException {
            super.postPut(e, put, edit, durability);
            //
            TableName callLogs = TableName.valueOf(&quot;calllogs&quot;);
            //得到当前的TableName对象
            TableName tableName = e.getEnvironment().getRegion().getRegionInfo().getTable();
            if(!callLogs.equals(tableName)){
                return  ;
            }

            //得到主叫的rowkey
            //xx , callerid , time ,  direction, calleid  ,duration
            //被叫:calleid,time,

            String rowkey = Bytes.toString(put.getRow());
            String[] arr = rowkey.split(&quot;,&quot;);

            String hash = Util.getRegNo(arr[4],arr[2]);
            //hash

            String newRowKey = hash + &quot;,&quot; + arr[4] + &quot;,&quot; + arr[2] + &quot;,1,&quot; + arr[1] + &quot;,&quot; +  arr[5] ;
            Put newPut = new Put(Bytes.toBytes(newRowKey));

            Table t = e.getEnvironment().getTable(tableName);

            t.put(newPut);
        }
    }

4.配置hbase-site.xml并分发分发jar包。
    &lt;property&gt;
        &lt;name&gt;hbase.coprocessor.region.classes&lt;/name&gt;
        &lt;value&gt;com.it18zhang.hbasedemo.coprocessor.CalleeLogRegionObserver&lt;/value&gt;
    &lt;/property&gt;

5.启动hbase集群.
</code></pre><h2 id="BloomFilter"><a href="#BloomFilter" class="headerlink" title="BloomFilter"></a>BloomFilter</h2><pre><code>布隆过滤器。
</code></pre><p><img src="https://i.imgur.com/c0wLF4w.png" alt=""></p>
<p>代码如下:</p>
<p><img src="https://i.imgur.com/Za6Pcr1.png" alt=""></p>
<h2 id="phonix"><a href="#phonix" class="headerlink" title="phonix"></a>phonix</h2><pre><code>1.安装phonix
    a)下载apache-phoenix-4.10.0-HBase-1.2-bin.tar.gz
    b)tar
    c)复制xxx-server.jar到hbase的lib目录，并且分发,删除以前的phonixjar包。
    d)重启hbase

2.使用phonix的命令行程序
    $&gt;phonix/bin/.sqlline.py s202    //连接的是zk服务器
    $phonix&gt;!tables
    $phonix&gt;!help                    //查看帮助
</code></pre><p>phoenix在创建表的时候要使用大量的协处理器，他是在建表时候不区分大小写的，而且hbase不可以识别得出来他的表，但是hbase shell里面建的表他能识别。</p>
<pre><code>2.SQL Client安装
    a)下载squirrel-sql-3.7.1-standard.jar
        该文件是安装文件，执行的安装程序。
        $&gt;jar -jar squirrel-sql-3.7.1-standard.jar
        $&gt;下一步...

    b)复制phoenix-4.10.0-HBase-1.2-client.jar到SQuerrel安装目录的lib下(c:\myprograms\squirrel)。

    c)启动SQuirrel(GUI)
        定位安装目录-&gt;执行squirrel-sql.bat

    d)打开GUI界面

    d)在左侧的边栏选中&quot;Drivers&quot;选项卡，
        点击 &quot;+&quot; -&gt;
        URL                : jdbc:phoenix:192.168.231.202
        Driverclass        : org.apache.phoenix.jdbc.PhoenixDriver    
        jdbc:phoenix: s202

    d)测试。

3.SQLLine客户端操作
    //建表
    $jdbc:phoenix&gt;create table IF NOT EXISTS test.Person (IDCardNum INTEGER not null primary key, Name varchar(20),Age INTEGER);

    //插入数据
    $jdbc:phoenix&gt;UPSERT INTO test.PERSON(IDCardNum , Name,Age) VALUES (1,&apos;tom&apos;,12);

    //删除数据
    $jdbc:phoenix&gt;delete from test.persion where idcardnum = 1 ;

    //更新数据
    //upsert into test.PERSON(IDCardNum , Name,Age) VALUES (1,&apos;tom&apos;,12);
</code></pre>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/11/21/Flume/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/21/Flume/" itemprop="url">Flume</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-11-21T21:41:16+08:00">
                2018-11-21
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="hbase"><a href="#hbase" class="headerlink" title="hbase"></a>hbase</h2><pre><code>NoSQL.
面向列族。
随机定位
实时读写。
分布式
可伸缩
HA
zookeeper
                        version(列族)
rowkey/famil+qualifier/timestamp = value

rowkey        //唯一性,散列性,定长,不要太长,加盐.

二次索引
byte[]
</code></pre><h2 id="hive"><a href="#hive" class="headerlink" title="hive"></a>hive</h2><pre><code>离线计算。
</code></pre><h2 id="MR-MapReduce"><a href="#MR-MapReduce" class="headerlink" title="MR:MapReduce"></a>MR:MapReduce</h2><pre><code>hadoop : 
DBWritable + WritableComparable :
</code></pre><h2 id="将hbase的表影射到hive上，使用hive的查询语句。"><a href="#将hbase的表影射到hive上，使用hive的查询语句。" class="headerlink" title="将hbase的表影射到hive上，使用hive的查询语句。"></a>将hbase的表影射到hive上，使用hive的查询语句。</h2><pre><code>CREATE TABLE mydb.t11(key string, name string) STORED BY &apos;org.apache.hadoop.hive.hbase.HBaseStorageHandler&apos; 
WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,cf:name&quot;)
TBLPROPERTIES(&quot;hbase.table.name&quot; = &quot;ns1:t11&quot;);    

select count(*) from mydb.t11 ;
</code></pre><h2 id="flume"><a href="#flume" class="headerlink" title="flume"></a>flume</h2><pre><code>收集、移动、聚合大量日志数据的服务。
基于流数据的架构，用于在线日志分析。

基于事件。
在生产和消费者之间启动协调作用。
提供了事务保证，确保消息一定被分发。
Source 多种
sink多种.

multihop        //多级跃点.可以从一个flume到另外一个flume

水平扩展:        //加节点，

竖直扩展        //增加硬件。
</code></pre><h2 id="Source"><a href="#Source" class="headerlink" title="Source"></a>Source</h2><pre><code>接受数据，类型有多种。
</code></pre><h2 id="Channel"><a href="#Channel" class="headerlink" title="Channel"></a>Channel</h2><pre><code>临时存放地，对Source中来的数据进行缓冲，直到sink消费掉。
</code></pre><h2 id="Sink"><a href="#Sink" class="headerlink" title="Sink"></a>Sink</h2><pre><code>从channel提取数据存放到中央化存储(hadoop / hbase)。
</code></pre><p><img src="https://i.imgur.com/DbIixF1.png" alt=""></p>
<p>实时产生的数据，</p>
<p>Flume的优点<br>以下是使用Flume的优点：<br>使用Apache Flume，我们可以将数据存储到任何集中存储中<br>（HBase，HDFS）。<br>当传入数据的速率超过可以写入数据的速率时<br>目的地，Flume充当数据生产者和数据生成者之间的中介<br>集中存储并在它们之间提供稳定的数据流。<br>Flume提供了上下文路由的功能。</p>
<ol>
<li>FLUME - 介绍App Flume<br>2<br>Flume中的交易是基于渠道的，其中两个交易（一个发件人<br>为每条消息维护一个接收器。它保证了可靠的信息<br>交货。<br>Flume具有可靠性，容错性，可扩展性，可管理性和可定制性。<br>水槽的特点<br>Flume的一些显着特征如下：<br>Flume将来自多个Web服务器的日志数据提取到集中存储（HDFS，<br>HBase）有效。<br>使用Flume，我们可以立即将来自多个服务器的数据导入Hadoop。<br>与日志文件一起，Flume还用于导入大量事件数据<br>由Facebook和Twitter等社交网站和电子商务制作<br>亚马逊和Flipkart等网站。<br>Flume支持大量源和目标类型。<br>Flume支持多跳流，扇入扇出流，上下文路由等。<br>水槽可以水平缩放</li>
</ol>
<h2 id="安装flume"><a href="#安装flume" class="headerlink" title="安装flume"></a>安装flume</h2><pre><code>1.下载
2.tar
3.环境变量
4.验证flume是否成功  
    $&gt;flume-ng version            //next generation.下一代.
</code></pre><h2 id="配置flume"><a href="#配置flume" class="headerlink" title="配置flume"></a>配置flume</h2><pre><code>1.创建配置文件
[/soft/flume/conf/hello.conf]
#声明三种组件
a1.sources = r1
a1.channels = c1
a1.sinks = k1

#定义source信息
a1.sources.r1.type=netcat
a1.sources.r1.bind=localhost
a1.sources.r1.port=8888

#定义sink信息
a1.sinks.k1.type=logger

#定义channel信息
a1.channels.c1.type=memory

#绑定在一起
a1.sources.r1.channels=c1
a1.sinks.k1.channel=c1

2.运行
    a)启动flume agent
        $&gt;bin/flume-ng agent -f ../conf/helloworld.conf -n a1 -Dflume.root.logger=INFO,console
    b)启动nc的客户端
        $&gt;nc localhost 8888
        $nc&gt;hello world

    c)在flume的终端输出hello world.
</code></pre><h2 id="安装nc"><a href="#安装nc" class="headerlink" title="安装nc"></a>安装nc</h2><pre><code>$&gt;sudo yum install nmap-ncat.x86_64
</code></pre><h2 id="清除仓库缓存"><a href="#清除仓库缓存" class="headerlink" title="清除仓库缓存"></a>清除仓库缓存</h2><pre><code>$&gt;修改ali.repo --&gt; ali.repo.bak文件。
$&gt;sudo yum clean all
$&gt;sudo yum makecache

#例如阿里基本源 
$&gt;sudo wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo 

#阿里epel源
$&gt;sudo wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo
</code></pre><h2 id="flume-source"><a href="#flume-source" class="headerlink" title="flume source"></a>flume source</h2><pre><code>1.netcat
    nc ..

2.exec
    实时日志收集,实时收集日志。
    a1.sources = r1
    a1.sinks = k1
    a1.channels = c1

    a1.sources.r1.type=exec
    a1.sources.r1.command=tail -F /home/centos/test.txt

    a1.sinks.k1.type=logger

    a1.channels.c1.type=memory

    a1.sources.r1.channels=c1
    a1.sinks.k1.channel=c1

3.批量收集
    监控一个文件夹，静态文件。
    收集完之后，会重命名文件成新文件。.compeleted.
    a)配置文件
        [spooldir_r.conf]
        a1.sources = r1
        a1.channels = c1
        a1.sinks = k1

        a1.sources.r1.type=spooldir
        a1.sources.r1.spoolDir=/home/centos/spool
        a1.sources.r1.fileHeader=true

        a1.sinks.k1.type=logger

        a1.channels.c1.type=memory

        a1.sources.r1.channels=c1
        a1.sinks.k1.channel=c1

    b)创建目录
        $&gt;mkdir ~/spool

    c)启动flume
        $&gt;bin/flume-ng agent -f ../conf/helloworld.conf -n a1 -Dflume.root.logger=INFO,console

4.序列source
    [seq]
    a1.sources = r1
    a1.channels = c1
    a1.sinks = k1

    a1.sources.r1.type=seq
    a1.sources.r1.totalEvents=1000

    a1.sinks.k1.type=logger

    a1.channels.c1.type=memory

    a1.sources.r1.channels=c1
    a1.sinks.k1.channel=c1

    [运行]
    $&gt;bin/flume-ng agent -f ../conf/helloworld.conf -n a1 -Dflume.root.logger=INFO,console

5.StressSource
    a1.sources = stresssource-1
    a1.channels = memoryChannel-1
    a1.sources.stresssource-1.type = org.apache.flume.source.StressSource
    a1.sources.stresssource-1.size = 10240
    a1.sources.stresssource-1.maxTotalEvents = 1000000
    a1.sources.stresssource-1.channels = memoryChannel-1
</code></pre><h2 id="flume-sink"><a href="#flume-sink" class="headerlink" title="flume sink"></a>flume sink</h2><pre><code>1.hdfs
    a1.sources = r1
    a1.channels = c1
    a1.sinks = k1

    a1.sources.r1.type = netcat
    a1.sources.r1.bind = localhost
    a1.sources.r1.port = 8888

    a1.sinks.k1.type = hdfs
    a1.sinks.k1.hdfs.path = /flume/events/%y-%m-%d/%H/%M/%S
    a1.sinks.k1.hdfs.filePrefix = events-

    #是否是产生新目录,每十分钟产生一个新目录,一般控制的目录方面。round是决定是否产生新文件的，滚动是决定是否产生新文件的。
    #2017-12-12 --&gt;
    #2017-12-12 --&gt;%H%M%S

    a1.sinks.k1.hdfs.round = true            
    a1.sinks.k1.hdfs.roundValue = 10
    a1.sinks.k1.hdfs.roundUnit = second

    a1.sinks.k1.hdfs.useLocalTimeStamp=true

    #是否产生新文件。
    a1.sinks.k1.hdfs.rollInterval=10
    a1.sinks.k1.hdfs.rollSize=10
    a1.sinks.k1.hdfs.rollCount=3

    a1.channels.c1.type=memory

    a1.sources.r1.channels = c1
    a1.sinks.k1.channel = c1

2.hive
    略

3.hbase
    a1.sources = r1
    a1.channels = c1
    a1.sinks = k1

    a1.sources.r1.type = netcat
    a1.sources.r1.bind = localhost
    a1.sources.r1.port = 8888

    a1.sinks.k1.type = hbase
    a1.sinks.k1.table = ns1:t12
    a1.sinks.k1.columnFamily = f1
    a1.sinks.k1.serializer = org.apache.flume.sink.hbase.RegexHbaseEventSerializer

    a1.channels.c1.type=memory

    a1.sources.r1.channels = c1
    a1.sinks.k1.channel = c1

4.kafka
</code></pre><p>数据进入到源里面去，最终进入通道里面，有很多通道c1,c2,c3，这个取决于通道选择器，ChannelProcessor，对事件进行处理，先经过一堆拦截器也是有很多种 。拦截器在各个文件前加东西，拦截之后再回到选择器。拦截器是典型的批处理，把加到东西流水线似的加到头文件里面。不管是什么对象，通过什么source进来的，都被转换成envent对象。在拦截器这个地方是链式技术。把事件放到通道里面，每个通道放置事件都是一个事务，保证能成功，</p>
<p>这个图是source到通道的图<br><img src="https://i.imgur.com/bzQZ9yK.png" alt=""></p>
<p>sink的图：</p>
<p><img src="https://i.imgur.com/sb3F2lg.png" alt=""></p>
<h2 id="使用avroSource和AvroSink实现跃点agent处理"><a href="#使用avroSource和AvroSink实现跃点agent处理" class="headerlink" title="使用avroSource和AvroSink实现跃点agent处理"></a>使用avroSource和AvroSink实现跃点agent处理</h2><pre><code>1.创建配置文件
    [avro_hop.conf]
    #a1
    a1.sources = r1
    a1.sinks= k1
    a1.channels = c1

    a1.sources.r1.type=netcat
    a1.sources.r1.bind=localhost
    a1.sources.r1.port=8888

    a1.sinks.k1.type = avro
    a1.sinks.k1.hostname=localhost
    a1.sinks.k1.port=9999

    a1.channels.c1.type=memory

    a1.sources.r1.channels = c1
    a1.sinks.k1.channel = c1

    #a2
    a2.sources = r2
    a2.sinks= k2
    a2.channels = c2

    a2.sources.r2.type=avro
    a2.sources.r2.bind=localhost
    a2.sources.r2.port=9999

    a2.sinks.k2.type = logger

    a2.channels.c2.type=memory

    a2.sources.r2.channels = c2
    a2.sinks.k2.channel = c2

2.启动a2
    $&gt;flume-ng agent -f /soft/flume/conf/avro_hop.conf -n a2 -Dflume.root.logger=INFO,console

3.验证a2
    $&gt;netstat -anop | grep 9999


4.启动a1
    $&gt;flume-ng agent -f /soft/flume/conf/avro_hop.conf -n a1

5.验证a1
    $&gt;netstat -anop | grep 8888
</code></pre><h2 id="channel"><a href="#channel" class="headerlink" title="channel"></a>channel</h2><pre><code>1.MemoryChannel
    略
2.FileChannel
</code></pre><p>a1.sources = r1<br>a1.sinks= k1<br>a1.channels = c1</p>
<p>a1.sources.r1.type=netcat<br>a1.sources.r1.bind=localhost<br>a1.sources.r1.port=8888</p>
<p>a1.sinks.k1.type=logger</p>
<pre><code>a1.channels.c1.type = file
a1.channels.c1.checkpointDir = /home/centos/flume/fc_check
a1.channels.c1.dataDirs = /home/centos/flume/fc_data
</code></pre><p>a1.sources.r1.channels=c1<br>a1.sinks.k1.channel=c1</p>
<h2 id="可溢出文件通道"><a href="#可溢出文件通道" class="headerlink" title="可溢出文件通道"></a>可溢出文件通道</h2><p>a1.channels = c1<br>a1.channels.c1.type = SPILLABLEMEMORY</p>
<p>#0表示禁用内存通道，等价于文件通道<br>a1.channels.c1.memoryCapacity = 0</p>
<p>#0,禁用文件通道，等价内存通道。<br>a1.channels.c1.overflowCapacity = 2000</p>
<p>a1.channels.c1.byteCapacity = 800000<br>a1.channels.c1.checkpointDir = /user/centos/flume/fc_check<br>a1.channels.c1.dataDirs = /user/centos/flume/fc_data</p>
<h2 id="创建Flume模块"><a href="#创建Flume模块" class="headerlink" title="创建Flume模块"></a>创建Flume模块</h2><pre><code>1.添加pom.xml
    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
    &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;
             xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
             xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
        &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
        &lt;groupId&gt;com.it18zhang&lt;/groupId&gt;
        &lt;artifactId&gt;FluemDemo&lt;/artifactId&gt;
        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
        &lt;dependencies&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;org.apache.flume&lt;/groupId&gt;
                &lt;artifactId&gt;flume-ng-core&lt;/artifactId&gt;
                &lt;version&gt;1.7.0&lt;/version&gt;
            &lt;/dependency&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;junit&lt;/groupId&gt;
                &lt;artifactId&gt;junit&lt;/artifactId&gt;
                &lt;version&gt;4.11&lt;/version&gt;
            &lt;/dependency&gt;
        &lt;/dependencies&gt;
    &lt;/project&gt;
</code></pre>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/11/21/面试理论知识/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/21/面试理论知识/" itemprop="url">面试理论知识</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-11-21T21:27:01+08:00">
                2018-11-21
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>flume：<br>Apache Flume是一种工具/服务/数据提取机制，用于收集聚合和<br>从各种传输大量的流数据，如日志文件，事件（等等）<br>源集中数据存储。</p>
<p>Flume是一种高度可靠，分布式和可配置的工具。 它主要是为了设计的<br>将流数据（日志数据）从各种Web服务器复制到HDFS。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/11/19/SSM第二天/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/19/SSM第二天/" itemprop="url">SSM第二天</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-11-19T22:35:23+08:00">
                2018-11-19
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="SSM第二天"><a href="#SSM第二天" class="headerlink" title="SSM第二天"></a>SSM第二天</h1><p>三大框架的整合：</p>
<p>数据库层：<br>mybatis数据持久化层：dao在交互mybatis<br>dao:数据访问层：数据访问对象，存放简单的增删改查。每个表都有这个业务<br>service层：业务层，和dao交互，对dao的方法进行组合调用。形成套餐<br>springmvc展现层：做网页，做web开发的，springmvc应该和业务层交互，在springmvc里面的controller层和业务层交互，展现的东西就是jsp做web开发做网站的，做web应用的。通过web架构来实现。springmvc就是web架构里面的一个。</p>
<p><img src="https://i.imgur.com/ygpfvcz.png" alt=""></p>
<h1 id="Mybais和数据库整合"><a href="#Mybais和数据库整合" class="headerlink" title="Mybais和数据库整合"></a>Mybais和数据库整合</h1><p><img src="https://i.imgur.com/M8K3Sqr.png" alt=""></p>
<p>1.在pom中添加依赖：</p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;
         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;groupId&gt;com.it18zhang&lt;/groupId&gt;
    &lt;artifactId&gt;SpringmybatisDemo&lt;/artifactId&gt;
    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;

    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;junit&lt;/groupId&gt;
            &lt;artifactId&gt;junit&lt;/artifactId&gt;
            &lt;version&gt;4.11&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.mybatis&lt;/groupId&gt;
            &lt;artifactId&gt;mybatis&lt;/artifactId&gt;
            &lt;version&gt;3.4.6&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;mysql&lt;/groupId&gt;
            &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;
            &lt;version&gt;5.1.17&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.mchange&lt;/groupId&gt;
            &lt;artifactId&gt;c3p0&lt;/artifactId&gt;
            &lt;version&gt;0.9.5.2&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework&lt;/groupId&gt;
            &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt;
            &lt;version&gt;4.3.3.RElEASE&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework&lt;/groupId&gt;
            &lt;artifactId&gt;spring-tx&lt;/artifactId&gt;
            &lt;version&gt;4.3.3.RELEASE&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;    
            &lt;groupId&gt;org.springframework&lt;/groupId&gt;
            &lt;artifactId&gt;spring-aop&lt;/artifactId&gt;
            &lt;version&gt;4.3.3.RELEASE&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
        &lt;groupId&gt;org.mybatis&lt;/groupId&gt;
        &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt;
        &lt;version&gt;1.1.0&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;/dependencies&gt;
&lt;/project&gt;
</code></pre><p>2.创建包</p>
<pre><code>com.it18zhang.springmybatis.dao
com.it18zhang.springmybatis.service
com.it18zhang.springmybatis.utils
</code></pre><p>3.配置beans.xml在source下</p>
<hr>
<p>坑爹的错误：</p>
<p><img src="https://i.imgur.com/pu7CMME.png" alt=""></p>
<p>在上图的情下，看见下面的mybtis-spring是依赖于mybatis3.1.0，而上面有一个跟他一模一样的包，所以到下面这块，下面的这个版本的包就看不出来了。所以如果有的类仅仅是在下面这个版本下才有的，需要把上面的第一个包先删除掉。因为它只显示第一个依赖的包</p>
<h2 id="复杂应用"><a href="#复杂应用" class="headerlink" title="复杂应用"></a>复杂应用</h2><pre><code>1.准备数据
    sql.sql

2.创建java类.
    [Order.java]
    public class Order {
        private Integer id ;
        private String orderNo ;
        //简历关联关系
        private User user ;

        //get/set
    }

    [Item.java]
    public class Item {
        private Integer id;
        private String itemName;
        //订单项和订单之间的关联关系
        private Order order;
        //get/set
    }

3.创建Order映射文件
    [resource/OrderMapper.xml]
    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;
    &lt;!DOCTYPE mapper
            PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot;
            &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;
    &lt;mapper namespace=&quot;orders&quot;&gt;
        &lt;insert id=&quot;insert&quot;&gt;
          insert into orders(orderno,uid) values(#{orderNo},#{user.id})
        &lt;/insert&gt;

        &lt;!-- findById --&gt;
        &lt;select id=&quot;selectOne&quot; parameterType=&quot;int&quot; resultMap=&quot;RM_Order&quot;&gt;
          select
            o.id oid ,
            o.orderno oorderno ,
            o.uid uid ,
            u.name uname ,
            u.age uage
          from orders o
            left outer join users u on o.uid = u.id where o.id = #{id}
        &lt;/select&gt;
        &lt;!-- findAll --&gt;
        &lt;select id=&quot;selectAll&quot; resultMap=&quot;RM_Order&quot;&gt;
          select
            o.id oid ,
            o.orderno oorderno ,
            o.uid uid ,
            u.name uname ,
            u.age uage
          from orders o
            left outer join users u on o.uid = u.id
        &lt;/select&gt;
        &lt;!-- 自定义结果映射 --&gt;
        &lt;resultMap id=&quot;RM_Order&quot; type=&quot;com.it18zhang.mybatisdemo.domain.Order&quot;&gt;
            &lt;id property=&quot;id&quot; column=&quot;oid&quot;/&gt;
            &lt;result property=&quot;orderNo&quot; column=&quot;oorderno&quot;/&gt;
            &lt;association property=&quot;user&quot; javaType=&quot;com.it18zhang.mybatisdemo.domain.User&quot;&gt;
                &lt;id property=&quot;id&quot; column=&quot;uid&quot; /&gt;
                &lt;result property=&quot;name&quot; column=&quot;uname&quot; /&gt;
                &lt;result property=&quot;age&quot; column=&quot;uage&quot; /&gt;
            &lt;/association&gt;
        &lt;/resultMap&gt;
    &lt;/mapper&gt;

4.修改配置文件,添加映射。
    [resource/mybatis-config.xml]
    &lt;!-- 引入映射文件 --&gt;
    &lt;mappers&gt;
        &lt;mapper resource=&quot;*Mapper.xml&quot;/&gt;
    &lt;/mappers&gt;

5.测试类
    public class TestOrder {

        /**
         * insert
         */
        @Test
        public void insert() throws Exception {
            String resource = &quot;mybatis-config.xml&quot;;
            InputStream inputStream = Resources.getResourceAsStream(resource);
            SqlSessionFactory sf = new SqlSessionFactoryBuilder().build(inputStream);
            SqlSession s = sf.openSession();

            User u = new User();
            u.setId(2);

            Order o = new Order();
            o.setOrderNo(&quot;No005&quot;);
            o.setUser(u);

            s.insert(&quot;orders.insert&quot;,o);
            s.commit();
            s.close();
        }

        @Test
        public void selectOne() throws Exception {
            String resource = &quot;mybatis-config.xml&quot;;
            InputStream inputStream = Resources.getResourceAsStream(resource);
            SqlSessionFactory sf = new SqlSessionFactoryBuilder().build(inputStream);
            SqlSession s = sf.openSession();
            Order order = s.selectOne(&quot;orders.selectOne&quot;,1);
            System.out.println(order.getOrderNo());
            s.commit();
            s.close();
        }

        @Test
        public void selectAll() throws Exception {
            String resource = &quot;mybatis-config.xml&quot;;
            InputStream inputStream = Resources.getResourceAsStream(resource);
            SqlSessionFactory sf = new SqlSessionFactoryBuilder().build(inputStream);
            SqlSession s = sf.openSession();
            List&lt;Order&gt; list = s.selectList(&quot;orders.selectAll&quot;);
            for(Order o : list){
                System.out.println(o.getOrderNo() + &quot; : &quot; + o.getUser().getName());
            }
            s.commit();
            s.close();
        }
    }
</code></pre><h2 id="配置一对多"><a href="#配置一对多" class="headerlink" title="配置一对多"></a>配置一对多</h2><pre><code>1.在User中增加orders集合。
    public class User {
        ...
        private List&lt;Order&gt; orders ;
        //get/set
    }

2.改造UserMapper.xml
</code></pre><h2 id="组合多对一和一对多关联关系到一个实体-Order-中"><a href="#组合多对一和一对多关联关系到一个实体-Order-中" class="headerlink" title="组合多对一和一对多关联关系到一个实体(Order)中"></a>组合多对一和一对多关联关系到一个实体(Order)中</h2><pre><code>1.关系
    Order(*) -&gt; (1)User
    Order(1) -&gt; (*)Item

2.Order.java
    class Order{
        ...
        List&lt;Item&gt; items ;
        //get/set    
    }
2&apos;.修改配置文件增加别名
    [resources/mybatis-config.xml]
    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;
    &lt;!DOCTYPE configuration
            PUBLIC &quot;-//mybatis.org//DTD Config 3.0//EN&quot;
            &quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot;&gt;
    &lt;configuration&gt;
        &lt;typeAliases&gt;
            &lt;typeAlias type=&quot;com.it18zhang.mybatisdemo.domain.User&quot; alias=&quot;_User&quot;/&gt;
            &lt;typeAlias type=&quot;com.it18zhang.mybatisdemo.domain.Order&quot; alias=&quot;_Order&quot;/&gt;
            &lt;typeAlias type=&quot;com.it18zhang.mybatisdemo.domain.Item&quot; alias=&quot;_Item&quot;/&gt;
        &lt;/typeAliases&gt;
        &lt;environments default=&quot;development&quot;&gt;
            &lt;environment id=&quot;development&quot;&gt;
                &lt;transactionManager type=&quot;JDBC&quot;/&gt;
                &lt;dataSource type=&quot;POOLED&quot;&gt;
                    &lt;property name=&quot;driver&quot; value=&quot;com.mysql.jdbc.Driver&quot;/&gt;
                    &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/mybatis&quot;/&gt;
                    &lt;property name=&quot;username&quot; value=&quot;root&quot;/&gt;
                    &lt;property name=&quot;password&quot; value=&quot;root&quot;/&gt;
                &lt;/dataSource&gt;
            &lt;/environment&gt;
        &lt;/environments&gt;
        &lt;!-- 引入映射文件 --&gt;
        &lt;mappers&gt;
            &lt;mapper resource=&quot;UserMapper.xml&quot;/&gt;
            &lt;mapper resource=&quot;OrderMapper.xml&quot;/&gt;
        &lt;/mappers&gt;
    &lt;/configuration&gt;

3.OrderMapper.xml
    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;
    &lt;!DOCTYPE mapper
            PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot;
            &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;
    &lt;mapper namespace=&quot;orders&quot;&gt;
        &lt;insert id=&quot;insert&quot;&gt;
          insert into orders(orderno,uid) values(#{orderNo},#{user.id})
        &lt;/insert&gt;

        &lt;!-- findById --&gt;
        &lt;select id=&quot;selectOne&quot; parameterType=&quot;int&quot; resultMap=&quot;RM_Order&quot;&gt;
          select
            o.id      oid ,
            o.orderno oorderno ,
            o.uid     uid ,
            u.name    uname ,
            u.age     uage ,
            i.id      iid,
            i.itemname iitemname
          from orders o
            left outer join users u on o.uid = u.id
            left outer join items i on o.id = i.oid
          where o.id = #{id}
        &lt;/select&gt;
        &lt;!-- findAll --&gt;
        &lt;select id=&quot;selectAll&quot; resultMap=&quot;RM_Order&quot;&gt;
          select
            o.id      oid ,
            o.orderno oorderno ,
            o.uid     uid ,
            u.name    uname ,
            u.age     uage ,
            i.id      iid,
            i.itemname iitemname
          from orders o
            left outer join users u on o.uid = u.id
            left outer join items i on o.id = i.oid
        &lt;/select&gt;
        &lt;!-- 自定义结果映射 --&gt;
        &lt;resultMap id=&quot;RM_Order&quot; type=&quot;com.it18zhang.mybatisdemo.domain.Order&quot;&gt;
            &lt;id property=&quot;id&quot; column=&quot;oid&quot;/&gt;
            &lt;result property=&quot;orderNo&quot; column=&quot;oorderno&quot;/&gt;
            &lt;association property=&quot;user&quot; javaType=&quot;com.it18zhang.mybatisdemo.domain.User&quot;&gt;
                &lt;id property=&quot;id&quot; column=&quot;uid&quot; /&gt;
                &lt;result property=&quot;name&quot; column=&quot;uname&quot; /&gt;
                &lt;result property=&quot;age&quot; column=&quot;uage&quot; /&gt;
            &lt;/association&gt;
            &lt;collection property=&quot;items&quot; ofType=&quot;_Item&quot;&gt;
                &lt;id property=&quot;id&quot; column=&quot;iid&quot; /&gt;
                &lt;result property=&quot;itemName&quot; column=&quot;iitemname&quot; /&gt;
            &lt;/collection&gt;
        &lt;/resultMap&gt;
    &lt;/mapper&gt;

4.测试
    @Test
    public void selectOne() throws Exception {
        String resource = &quot;mybatis-config.xml&quot;;
        InputStream inputStream = Resources.getResourceAsStream(resource);
        SqlSessionFactory sf = new SqlSessionFactoryBuilder().build(inputStream);
        SqlSession s = sf.openSession();
        Order order = s.selectOne(&quot;orders.selectOne&quot;,1);
        System.out.println(order.getOrderNo() + order.getUser().getName());
        for(Item i : order.getItems()){
            System.out.println(i.getId() + &quot;:&quot; + i.getItemName());
        }
        s.commit();
        s.close();
    }
</code></pre><h2 id="改造项目"><a href="#改造项目" class="headerlink" title="改造项目"></a>改造项目</h2><pre><code>1.引入Util类
    package com.it18zhang.mybatisdemo.util;

    import org.apache.ibatis.io.Resources;
    import org.apache.ibatis.session.SqlSession;
    import org.apache.ibatis.session.SqlSessionFactory;
    import org.apache.ibatis.session.SqlSessionFactoryBuilder;

    import java.io.InputStream;

    /**
     * 工具类
     */
    public class Util {
        //
        private static SqlSessionFactory sf ;

        static{
            try {
                String resource = &quot;mybatis-config.xml&quot;;
                InputStream inputStream = Resources.getResourceAsStream(resource);
                sf = new SqlSessionFactoryBuilder().build(inputStream);
            } catch (Exception e) {
                e.printStackTrace();
            }
        }

        /**
         * 开启会话
         */
        public static SqlSession openSession(){
            return sf.openSession() ;
        }

        /**
         * 关闭会话
         */
        public static void closeSession(SqlSession s){
            if(s != null){
                s.close();
            }
        }


        /**
         * 关闭会话
         */
        public static void rollbackTx(SqlSession s) {
            if (s != null) {
                s.rollback();
            }
        }
    }

2.设计模板类DaoTemplate和回调MybatisCallback接口
    [DaoTemplate.java]
    package com.it18zhang.mybatisdemo.dao;

    import com.it18zhang.mybatisdemo.util.Util;
    import org.apache.ibatis.session.SqlSession;

    /**
     * 模板类
     */
    public class DaoTemplate {
        /**
         * 执行
         */
        public static Object execute(MybatisCallback cb){
            SqlSession s = null;
            try {
                s = Util.openSession();
                Object ret = cb.doInMybatis(s);
                s.commit();
                return ret ;
            } catch (Exception e) {
                Util.rollbackTx(s);
            } finally {
                Util.closeSession(s);
            }
            return null ;
        }
    }

    [MybatisCallback.java]
    package com.it18zhang.mybatisdemo.dao;

    import org.apache.ibatis.session.SqlSession;

    /**
     * 回调接口
     */
    public interface MybatisCallback {
        public Object doInMybatis(SqlSession s);
    }

3.通过模板类+回调接口实现UserDao.java
    [UserDao.java]
    package com.it18zhang.mybatisdemo.dao;

    import com.it18zhang.mybatisdemo.domain.User;
    import com.it18zhang.mybatisdemo.util.Util;
    import org.apache.ibatis.session.SqlSession;

    import java.util.List;

    /**
     * UserDao
     */
    public class UserDao {

        /**
         * 插入操作
         */
        public void insert(final User user){
            DaoTemplate.execute(new MybatisCallback() {
                public Object doInMybatis(SqlSession s) {
                    s.insert(&quot;users.insert&quot;,user);
                    return null ;
                }
            });
        }

        /**
         * 插入操作
         */
        public void update(final User user){
            DaoTemplate.execute(new MybatisCallback() {
                public Object doInMybatis(SqlSession s) {
                    s.update(&quot;users.update&quot;, user);
                    return null ;
                }
            });
        }

        public User selctOne(final Integer id){
            return (User)DaoTemplate.execute(new MybatisCallback() {
                public Object doInMybatis(SqlSession s) {
                    return s.selectOne(&quot;users.selectOne&quot;,id);
                }
            });
        }

        public List&lt;User&gt; selctAll(){
            return (List&lt;User&gt;)DaoTemplate.execute(new MybatisCallback() {
                public Object doInMybatis(SqlSession s) {
                    return s.selectList(&quot;users.selectAll&quot;);
                }
            });
        }
    }

4.App测试
    public static void main(String[] args) {
        UserDao dao = new UserDao();
        User u = dao.selctOne(1);
        System.out.println(u.getName());
    }
</code></pre><hr>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/11/19/Hbase第三天/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/19/Hbase第三天/" itemprop="url">Hbase第三天</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-11-19T16:56:58+08:00">
                2018-11-19
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="复习Hbase第二天"><a href="#复习Hbase第二天" class="headerlink" title="复习Hbase第二天"></a>复习Hbase第二天</h1><p>随机定位+实时读写</p>
<p>nosql:not only sql数据库</p>
<p>key-value对形式的存储</p>
<p>key: rowkey/family+col/timstamp = value</p>
<p>rowkey  排序,byte[]</p>
<p>客户端先联系zk找到元数据表hbae:meta，存放了整个数据库的表和区域服务器信息，相当于目录，类似于名称节点。当找到了之后就可以定位到区域服务器，所以hbase数据读写和HRegionServer来交互，有很多regionServer构成了一个集群。数据先进入到写前日志，写前日志用于容错，用于恢复，所以在交互的时候client先交互HRegionServer然后在网Hlog里写入数据，然后在溢出之后写入HRegion，对于HRegion来说有个内存储MemStore在内存中存储数据，用来提高速度，MemStore达到一定值溢出到磁盘，所以还有一个StoreFile存储，用来和底层交互，底层就是Hfile。通过Hfile对象来跟HDFS交互，就找到了HDFS客户端DFSClient了，这个DFSClient就是hdfs范畴了，最终数据存储到HDFS里面了。</p>
<p>表的切割指的是切割表或者切割区域，按照rowkey来切分，因为rowkey是有序的，相当于建立索引，通过切割可以实现负载均衡，如果所有东西都在一个点就会出现热点问题。</p>
<p>hbase的增删改查是：<br>    put(rowkey).addColumn().<br>    put(Put)</p>
<pre><code>delete

get()

更新也是put

scan()
</code></pre><p>merge合并。</p>
<p>移动区域，目的减少某一个服务器的压力。可以任意配置区域所在地，由那个区域服务器承载。</p>
<p>切割风暴：达到10G之后同时到达临界点，同时切割，为了避免可以让这个10G的值变大再切割，也就是不让他自动切割了。可以手动切割避免。或者预切割来处理。 </p>
<p>hbase存储的荣誉量比较大，因为它存储的时候都是以kv的方式来存储，而key是三极坐标，rowkey，列，列族，时间戳+一个value，所以前三个值都要存放很多次。所以要求列族和列的名称和rowkey的名字不能太长。一旦过长，就会发现存储的时候被坐标占用了大量的空间，而value的很少，最好列和列族名字不要太长。</p>
<p>本天会涉及rowkey的设计问题。</p>
<h1 id="预先切割"><a href="#预先切割" class="headerlink" title="预先切割"></a>预先切割</h1><p>创建表时可以预先对表进行切割。</p>
<p>切割线就是rowkey</p>
<p> create ‘ns1:t2’,’f1’,SPLITES=&gt;[‘row3000’,’row6000]</p>
<h2 id="预先切割-1"><a href="#预先切割-1" class="headerlink" title="预先切割"></a>预先切割</h2><pre><code>创建表时，预先对表进行切割。
切割线是rowkey.
$hbase&gt;create &apos;ns1:t2&apos;,&apos;f1&apos;,SPLITS=&gt;[&apos;row3000&apos;,&apos;row6000&apos;]
</code></pre><h2 id="创建表时指定列族的版本数-该列族的所有列都具有相同数量版本"><a href="#创建表时指定列族的版本数-该列族的所有列都具有相同数量版本" class="headerlink" title="创建表时指定列族的版本数,该列族的所有列都具有相同数量版本"></a>创建表时指定列族的版本数,该列族的所有列都具有相同数量版本</h2><pre><code>$hbase&gt;create &apos;ns1:t3&apos;,{NAME=&gt;&apos;f1&apos;,VERSIONS=&gt;3}            //创建表时，指定列族的版本数。
$hbase&gt;get &apos;ns1:t3&apos;,&apos;row1&apos;,{COLUMN=&gt;&apos;f1&apos;,VERSIONS=&gt;4}    //检索的时候，查询多少个版本。
$hbase&gt;put &apos;ns1:t3&apos;,&apos;row1&apos;,&apos;f1:name&apos;,&apos;tom&apos;
</code></pre><p>关于查询的命令行：</p>
<p><img src="https://i.imgur.com/Qq9LDyE.png" alt=""></p>
<p>关于创建表的命令：</p>
<p><img src="https://i.imgur.com/r9yKelH.png" alt=""></p>
<pre><code>@Test
public void getWithVersions() throws IOException {
    Configuration conf = HBaseConfiguration.create();
    Connection conn = ConnectionFactory.createConnection(conf);
    TableName tname = TableName.valueOf(&quot;ns1:t3&quot;);
    Table table = conn.getTable(tname);
    Get get = new Get(Bytes.toBytes(&quot;row1&quot;));
    //检索所有版本
    get.setMaxVersions();
    Result r = table.get(get);
    List&lt;Cell&gt; cells = r.getColumnCells(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;));
    for(Cell c : cells){
        String f = Bytes.toString(c.getFamily());
        String col = Bytes.toString(c.getQualifier());
        long ts = c.getTimestamp();
        String val = Bytes.toString(c.getValue());
        System.out.println(f + &quot;/&quot; + col + &quot;/&quot; + ts + &quot;=&quot; + val);
    }
}
</code></pre><h1 id="原生扫描-专家"><a href="#原生扫描-专家" class="headerlink" title="原生扫描(专家)"></a>原生扫描(专家)</h1><h2 id="1-原生扫描"><a href="#1-原生扫描" class="headerlink" title="1.原生扫描"></a>1.原生扫描</h2><pre><code>$hbase&gt;scan &apos;ns1:t3&apos;,{COLUMN=&gt;&apos;f1&apos;,RAW=&gt;true,VERSIONS=&gt;10}        //包含标记了delete的数据
</code></pre><h2 id="2-删除数据"><a href="#2-删除数据" class="headerlink" title="2.删除数据"></a>2.删除数据</h2><pre><code>$hbase&gt;delete &apos;nd1:t3&apos;,&apos;row1&apos;,&apos;f1:name&apos;,148989875645            //删除数据，标记为删除.
                                                                //小于该删除时间的数据都作废。
</code></pre><h2 id="3-TTL"><a href="#3-TTL" class="headerlink" title="3.TTL"></a>3.TTL</h2><pre><code>time to live ,存活时间。
影响所有的数据，包括没有删除的数据。
超过该时间，原生扫描也扫不到数据。
$hbase&gt;create &apos;ns1:tx&apos; , {NAME=&gt;&apos;f1&apos;,TTL=&gt;10,VERSIONS}
</code></pre><h2 id="4-KEEP-DELETED-CELLS"><a href="#4-KEEP-DELETED-CELLS" class="headerlink" title="4.KEEP_DELETED_CELLS"></a>4.KEEP_DELETED_CELLS</h2><pre><code>删除key之后，数据是否还保留。
$hbase&gt;create &apos;ns1:tx&apos; , {NAME=&gt;&apos;f1&apos;,TTL=&gt;10,VERSIONS,KEEP_DELETED_CELLS=&gt;true}
</code></pre><p><img src="https://i.imgur.com/2WE38AX.png" alt=""></p>
<h1 id="缓存和批处理"><a href="#缓存和批处理" class="headerlink" title="缓存和批处理"></a>缓存和批处理</h1><p><img src="https://i.imgur.com/U8uyqrS.png" alt=""></p>
<pre><code>1.开启服务器端扫描器缓存
    a)表层面(全局)只需要配置一个属性即可

        &lt;property&gt;
            &lt;name&gt;hbase.client.scanner.caching&lt;/name&gt;
            &lt;!-- 整数最大值 --&gt;
            &lt;value&gt;2147483647&lt;/value&gt;
            &lt;source&gt;hbase-default.xml&lt;/source&gt;
        &lt;/property&gt;

    b)操作层面

        //设置量
        scan.setCaching(10);

2.
3.


cache row nums : 1000            //632
cache row nums : 5000            //423
cache row nums : 1                //7359
</code></pre><h1 id="扫描器缓存"><a href="#扫描器缓存" class="headerlink" title="扫描器缓存"></a>扫描器缓存</h1><pre><code>面向行级别的。

@Test
public void getScanCache() throws IOException {

    Configuration conf = HBaseConfiguration.create();
    Connection conn = ConnectionFactory.createConnection(conf);
    TableName tname = TableName.valueOf(&quot;ns1:t1&quot;);
    Scan scan = new Scan();
    scan.setCaching(5000);
    Table t = conn.getTable(tname);
    ResultScanner rs = t.getScanner(scan);
    long start = System.currentTimeMillis() ;
    Iterator&lt;Result&gt; it = rs.iterator();
    while(it.hasNext()){
        Result r = it.next();
        System.out.println(r.getColumnLatestCell(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;)));
    }
    System.out.println(System.currentTimeMillis() - start);
}
</code></pre><h1 id="批量扫描是面向列级别"><a href="#批量扫描是面向列级别" class="headerlink" title="批量扫描是面向列级别"></a>批量扫描是面向列级别</h1><p><img src="https://i.imgur.com/kzrUOiK.png" alt=""></p>
<pre><code>控制每次next()服务器端返回的列的个数。
scan.setBatch(5);                //每次next返回5列。
</code></pre><h2 id="测试缓存和批处理"><a href="#测试缓存和批处理" class="headerlink" title="测试缓存和批处理"></a>测试缓存和批处理</h2><pre><code> */
@Test
public void testBatchAndCaching() throws IOException {

    Configuration conf = HBaseConfiguration.create();
    Connection conn = ConnectionFactory.createConnection(conf);
    TableName tname = TableName.valueOf(&quot;ns1:t7&quot;);
    Scan scan = new Scan();
    scan.setCaching(2);
    scan.setBatch(3);
    Table t = conn.getTable(tname);
    ResultScanner rs = t.getScanner(scan);
    Iterator&lt;Result&gt; it = rs.iterator();
    while (it.hasNext()) {
        Result r = it.next();
        System.out.println(&quot;========================================&quot;);
        //得到一行的所有map,key=f1,value=Map&lt;Col,Map&lt;Timestamp,value&gt;&gt;
        NavigableMap&lt;byte[], NavigableMap&lt;byte[], NavigableMap&lt;Long, byte[]&gt;&gt;&gt; map = r.getMap();
        //
        for (Map.Entry&lt;byte[], NavigableMap&lt;byte[], NavigableMap&lt;Long, byte[]&gt;&gt;&gt; entry : map.entrySet()) {
            //得到列族
            String f = Bytes.toString(entry.getKey());
            Map&lt;byte[], NavigableMap&lt;Long, byte[]&gt;&gt; colDataMap = entry.getValue();
            for (Map.Entry&lt;byte[], NavigableMap&lt;Long, byte[]&gt;&gt; ets : colDataMap.entrySet()) {
                String c = Bytes.toString(ets.getKey());
                Map&lt;Long, byte[]&gt; tsValueMap = ets.getValue();
                for (Map.Entry&lt;Long, byte[]&gt; e : tsValueMap.entrySet()) {
                    Long ts = e.getKey();
                    String value = Bytes.toString(e.getValue());
                    System.out.print(f + &quot;/&quot; + c + &quot;/&quot; + ts + &quot;=&quot; + value + &quot;,&quot;);
                }
            }
        }
        System.out.println();
    }
}
</code></pre><p>先插入数据<br><img src="https://i.imgur.com/aRdSViP.png" alt=""></p>
<p><img src="https://i.imgur.com/jMwoiQ6.png" alt=""></p>
<p>代码运行结果：</p>
<p><img src="https://i.imgur.com/0RNQJci.png" alt=""></p>
<p>上面代码和上图对应的。设置2个cach和3个batch视频上说是2行3列，但是我觉得应该是2个列族3个列的这样子去输出。然后最后这一行还剩下2个输出2个，也就是3个输出，2个输出，3个输出，2个输出，。</p>
<p>========================================</p>
<h1 id="f1-id-1490595148588-1-f2-addr-1490595182150-hebei-f2-age-1490595174760-12"><a href="#f1-id-1490595148588-1-f2-addr-1490595182150-hebei-f2-age-1490595174760-12" class="headerlink" title="f1/id/1490595148588=1,f2/addr/1490595182150=hebei,f2/age/1490595174760=12,"></a>f1/id/1490595148588=1,f2/addr/1490595182150=hebei,f2/age/1490595174760=12,</h1><p>f2/id/1490595164473=1,f2/name/1490595169589=tom,</p>
<p>========================================</p>
<h1 id="f1-id-1490595196410-2-f1-name-1490595213090-tom2-1-f2-addr-1490595264734-tangshan"><a href="#f1-id-1490595196410-2-f1-name-1490595213090-tom2-1-f2-addr-1490595264734-tangshan" class="headerlink" title="f1/id/1490595196410=2,f1/name/1490595213090=tom2.1,f2/addr/1490595264734=tangshan,"></a>f1/id/1490595196410=2,f1/name/1490595213090=tom2.1,f2/addr/1490595264734=tangshan,</h1><p>f2/age/1490595253996=13,f2/id/1490595233568=2,f2/name/1490595241891=tom2.2,</p>
<p>========================================</p>
<h1 id="f1-age-1490595295427-14-f1-id-1490595281251-3-f1-name-1490595289587-tom3-1"><a href="#f1-age-1490595295427-14-f1-id-1490595281251-3-f1-name-1490595289587-tom3-1" class="headerlink" title="f1/age/1490595295427=14,f1/id/1490595281251=3,f1/name/1490595289587=tom3.1,"></a>f1/age/1490595295427=14,f1/id/1490595281251=3,f1/name/1490595289587=tom3.1,</h1><p>f2/addr/1490595343690=beijing,f2/age/1490595336300=14,f2/id/1490595310966=3,</p>
<p>========================================<br>f2/name/1490595327531=tom3.2,</p>
<h1 id="Filter过滤器"><a href="#Filter过滤器" class="headerlink" title="Filter过滤器"></a>Filter过滤器</h1><p><img src="https://i.imgur.com/9jcHOUJ.png" alt=""><br>远程服务器收到scan对象进行反序列化，恢复成scan对象进行过滤，对每个区域进行过滤，每个区域服务器有很多区域，每个区域里面有区域扫描器， RegionScanner，会使用区域过滤器。 </p>
<pre><code>1.RowFilter
    select * from ns1:t1 where rowkey &lt;= row100
/**

 /**
 * 测试RowFilter过滤器
 */
@Test
public void testRowFilter() throws IOException {

    Configuration conf = HBaseConfiguration.create();
    Connection conn = ConnectionFactory.createConnection(conf);
    TableName tname = TableName.valueOf(&quot;ns1:t1&quot;);
    Scan scan = new Scan();
    RowFilter rowFilter = new RowFilter(CompareFilter.CompareOp.LESS_OR_EQUAL, new BinaryComparator(Bytes.toBytes(&quot;row0100&quot;)));
    scan.setFilter(rowFilter);
    Table t = conn.getTable(tname);
    ResultScanner rs = t.getScanner(scan);
    Iterator&lt;Result&gt; it = rs.iterator();
    while (it.hasNext()) {
        Result r = it.next();
        System.out.println(Bytes.toString(r.getRow()));
    }
}

/**
 * 测试FamilyFilter过滤器
 */
@Test
public void testFamilyFilter() throws IOException {

    Configuration conf = HBaseConfiguration.create();
    Connection conn = ConnectionFactory.createConnection(conf);
    TableName tname = TableName.valueOf(&quot;ns1:t7&quot;);
    Scan scan = new Scan();
    FamilyFilter filter = new FamilyFilter(CompareFilter.CompareOp.LESS, new BinaryComparator(Bytes.toBytes(&quot;f2&quot;)));
    scan.setFilter(filter);
    Table t = conn.getTable(tname);
    ResultScanner rs = t.getScanner(scan);
    Iterator&lt;Result&gt; it = rs.iterator();
    while (it.hasNext()) {
        Result r = it.next();
        byte[] f1id = r.getValue(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;));
        byte[] f2id = r.getValue(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;id&quot;));
        System.out.println(f1id + &quot; : &quot; + f2id);
    }
}

/**
 * 测试QualifierFilter(列过滤器)
 */
@Test
public void testColFilter() throws IOException {

    Configuration conf = HBaseConfiguration.create();
    Connection conn = ConnectionFactory.createConnection(conf);
    TableName tname = TableName.valueOf(&quot;ns1:t7&quot;);
    Scan scan = new Scan();
    QualifierFilter colfilter = new QualifierFilter(CompareFilter.CompareOp.EQUAL, new BinaryComparator(Bytes.toBytes(&quot;id&quot;)));
    scan.setFilter(colfilter);
    Table t = conn.getTable(tname);
    ResultScanner rs = t.getScanner(scan);
    Iterator&lt;Result&gt; it = rs.iterator();
    while (it.hasNext()) {
        Result r = it.next();
        byte[] f1id = r.getValue(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;));
        byte[] f2id = r.getValue(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;id&quot;));
        byte[] f2name = r.getValue(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;name&quot;));
        System.out.println(f1id + &quot; : &quot; + f2id + &quot; : &quot; + f2name);
    }
}


/**
 * 测试ValueFilter(值过滤器)
 * 过滤value的值，含有指定的字符子串
 */
@Test
public void testValueFilter() throws IOException {

    Configuration conf = HBaseConfiguration.create();
    Connection conn = ConnectionFactory.createConnection(conf);
    TableName tname = TableName.valueOf(&quot;ns1:t7&quot;);
    Scan scan = new Scan();
    ValueFilter filter = new ValueFilter(CompareFilter.CompareOp.EQUAL, new SubstringComparator(&quot;to&quot;));
    scan.setFilter(filter);
    Table t = conn.getTable(tname);
    ResultScanner rs = t.getScanner(scan);
    Iterator&lt;Result&gt; it = rs.iterator();
    while (it.hasNext()) {
        Result r = it.next();
        byte[] f1id = r.getValue(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;));
        byte[] f2id = r.getValue(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;id&quot;));
        byte[] f1name = r.getValue(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;));
        byte[] f2name = r.getValue(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;name&quot;));
        System.out.println(f1id + &quot; : &quot; + f2id + &quot; : &quot; + Bytes.toString(f1name) + &quot; : &quot; + Bytes.toString(f2name));
    }
}

/**
 * 依赖列过滤器
 */
@Test
public void testDepFilter() throws IOException {

    Configuration conf = HBaseConfiguration.create();
    Connection conn = ConnectionFactory.createConnection(conf);
    TableName tname = TableName.valueOf(&quot;ns1:t7&quot;);
    Scan scan = new Scan();
    DependentColumnFilter filter = new DependentColumnFilter(Bytes.toBytes(&quot;f2&quot;),
            Bytes.toBytes(&quot;addr&quot;),
            true,
            CompareFilter.CompareOp.NOT_EQUAL,
            new BinaryComparator(Bytes.toBytes(&quot;beijing&quot;))
            );

    //ValueFilter filter = new ValueFilter(CompareFilter.CompareOp.EQUAL, new SubstringComparator(&quot;to&quot;));
    scan.setFilter(filter);
    Table t = conn.getTable(tname);
    ResultScanner rs = t.getScanner(scan);
    Iterator&lt;Result&gt; it = rs.iterator();
    while (it.hasNext()) {
        Result r = it.next();
        byte[] f1id = r.getValue(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;));
        byte[] f2id = r.getValue(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;id&quot;));
        byte[] f1name = r.getValue(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;));
        byte[] f2name = r.getValue(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;name&quot;));
        System.out.println(f1id + &quot; : &quot; + f2id + &quot; : &quot; + Bytes.toString(f1name) + &quot; : &quot; + Bytes.toString(f2name));
    }
}

/**
 * 单列值value过滤，
 *  
 */
@Test
public void testSingleColumValueFilter() throws IOException {

    Configuration conf = HBaseConfiguration.create();
    Connection conn = ConnectionFactory.createConnection(conf);
    TableName tname = TableName.valueOf(&quot;ns1:t7&quot;);
    Scan scan = new Scan();
    SingleColumnValueFilter filter = new SingleColumnValueFilter(Bytes.toBytes(&quot;f2&quot;,
            Bytes.toBytes(&quot;name&quot;),
            CompareFilter.CompareOp.NOT_EQUAL),
            new BinaryComparator(Bytes.toBytes(&quot;tom2.1&quot;)));

    //ValueFilter filter = new ValueFilter(CompareFilter.CompareOp.EQUAL, new SubstringComparator(&quot;to&quot;));
    scan.setFilter(filter);
    Table t = conn.getTable(tname);
    ResultScanner rs = t.getScanner(scan);
    Iterator&lt;Result&gt; it = rs.iterator();
    while (it.hasNext()) {
        Result r = it.next();
        byte[] f1id = r.getValue(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;));
        byte[] f2id = r.getValue(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;id&quot;));
        byte[] f1name = r.getValue(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;));
        byte[] f2name = r.getValue(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;name&quot;));
        System.out.println(f1id + &quot; : &quot; + f2id + &quot; : &quot; + Bytes.toString(f1name) + &quot; : &quot; + Bytes.toString(f2name));
    }
}
</code></pre><h2 id="复杂查询"><a href="#复杂查询" class="headerlink" title="复杂查询"></a>复杂查询</h2><pre><code>select * from t7 where ((age &lt;= 13) and (name like &apos;%t&apos;)

                                    or

                        (age &gt; 13) and (name like &apos;t%&apos;))
</code></pre><p><img src="https://i.imgur.com/Z5AcN4v.png" alt=""></p>
<p>指定列族，指定列，指定对比方式，指定值(小于用二进制比较，等于用正则表达式串对比器)</p>
<p><img src="https://i.imgur.com/6EGrJ4V.png" alt=""></p>
<h1 id="复杂查询实现方式-FilterList"><a href="#复杂查询实现方式-FilterList" class="headerlink" title="复杂查询实现方式:FilterList"></a>复杂查询实现方式:FilterList</h1><pre><code>@Test
public void testComboFilter() throws IOException {

    Configuration conf = HBaseConfiguration.create();
    Connection conn = ConnectionFactory.createConnection(conf);
    TableName tname = TableName.valueOf(&quot;ns1:t7&quot;);
    Scan scan = new Scan();

    //where ... f2:age &lt;= 13
    SingleColumnValueFilter ftl = new SingleColumnValueFilter(
            Bytes.toBytes(&quot;f2&quot;),
            Bytes.toBytes(&quot;age&quot;),
            CompareFilter.CompareOp.LESS_OR_EQUAL,
            new BinaryComparator(Bytes.toBytes(&quot;13&quot;))
    );

    //where ... f2:name like %t
    SingleColumnValueFilter ftr = new SingleColumnValueFilter(
            Bytes.toBytes(&quot;f2&quot;),
            Bytes.toBytes(&quot;name&quot;),
            CompareFilter.CompareOp.EQUAL,
            new RegexStringComparator(&quot;^t&quot;)//以t开头
    );
    //ft
    FilterList ft = new FilterList(FilterList.Operator.MUST_PASS_ALL);
    ft.addFilter(ftl);
    ft.addFilter(ftr);

    //where ... f2:age &gt; 13
    SingleColumnValueFilter fbl = new SingleColumnValueFilter(
            Bytes.toBytes(&quot;f2&quot;),
            Bytes.toBytes(&quot;age&quot;),
            CompareFilter.CompareOp.GREATER,
            new BinaryComparator(Bytes.toBytes(&quot;13&quot;))
    );

    //where ... f2:name like %t
    SingleColumnValueFilter fbr = new SingleColumnValueFilter(
            Bytes.toBytes(&quot;f2&quot;),
            Bytes.toBytes(&quot;name&quot;),
            CompareFilter.CompareOp.EQUAL,
            new RegexStringComparator(&quot;t$&quot;)//以t结尾
    );
    //ft
    FilterList fb = new FilterList(FilterList.Operator.MUST_PASS_ALL);
    fb.addFilter(fbl);
    fb.addFilter(fbr);


    FilterList fall = new FilterList(FilterList.Operator.MUST_PASS_ONE);
    fall.addFilter(ft);
    fall.addFilter(fb);

    scan.setFilter(fall);
    Table t = conn.getTable(tname);
    ResultScanner rs = t.getScanner(scan);
    Iterator&lt;Result&gt; it = rs.iterator();
    while (it.hasNext()) {
        Result r = it.next();
        byte[] f1id = r.getValue(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;));
        byte[] f2id = r.getValue(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;id&quot;));
        byte[] f1name = r.getValue(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;));
        byte[] f2name = r.getValue(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;name&quot;));
        System.out.println(f1id + &quot; : &quot; + f2id + &quot; : &quot; + Bytes.toString(f1name) + &quot; : &quot; + Bytes.toString(f2name));
    }
}
</code></pre><h1 id="计数器"><a href="#计数器" class="headerlink" title="计数器"></a>计数器</h1><pre><code>$hbase&gt;incr &apos;ns1:t8&apos;,&apos;row1&apos;,&apos;f1:click&apos;,1
$hbase&gt;get_counter &apos;ns1:t8&apos;,&apos;row1&apos;,&apos;f1:click&apos;    //得到计数器的值


[API编程]
@Test
public void testIncr() throws IOException {

    Configuration conf = HBaseConfiguration.create();
    Connection conn = ConnectionFactory.createConnection(conf);
    TableName tname = TableName.valueOf(&quot;ns1:t8&quot;);
    Table t = conn.getTable(tname);
    Increment incr = new Increment(Bytes.toBytes(&quot;row1&quot;));
    incr.addColumn(Bytes.toBytes(&quot;f1&quot;),Bytes.toBytes(&quot;daily&quot;),1);
    incr.addColumn(Bytes.toBytes(&quot;f1&quot;),Bytes.toBytes(&quot;weekly&quot;),10);
    incr.addColumn(Bytes.toBytes(&quot;f1&quot;),Bytes.toBytes(&quot;monthly&quot;),100);
    t.increment(incr);
}
</code></pre><h1 id="coprocessor"><a href="#coprocessor" class="headerlink" title="coprocessor"></a>coprocessor</h1><p>协处理器工作过程：</p>
<p><img src="https://i.imgur.com/lZeb49u.png" alt=""></p>
<pre><code>批处理的，等价于存储过程或者触发器

[Observer]
    观察者,类似于触发器，基于事件。发生动作时，回调相应方法。
    RegionObserver        //RegionServer区域观察者
    MasterObserver        //Master节点。
    WAlObserver            //

[Endpoint]
    终端,类似于存储过程。


1.加载
    [hbase-site.xml]
    &lt;property&gt;
        &lt;name&gt;hbase.coprocessor.region.classes&lt;/name&gt;
        &lt;value&gt;coprocessor.RegionObserverExample, coprocessor.AnotherCoprocessor&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;hbase.coprocessor.master.classes&lt;/name&gt;
        &lt;value&gt;coprocessor.MasterObserverExample&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;hbase.coprocessor.wal.classes&lt;/name&gt;
        &lt;value&gt;coprocessor.WALObserverExample, bar.foo.MyWALObserver&lt;/value&gt;
    &lt;/property&gt;

2.自定义观察者
    [MyRegionObserver]
    package com.it18zhang.hbasedemo.coprocessor;

    import org.apache.hadoop.hbase.Cell;
    import org.apache.hadoop.hbase.CoprocessorEnvironment;
    import org.apache.hadoop.hbase.client.Delete;
    import org.apache.hadoop.hbase.client.Durability;
    import org.apache.hadoop.hbase.client.Get;
    import org.apache.hadoop.hbase.client.Put;
    import org.apache.hadoop.hbase.coprocessor.BaseRegionObserver;
    import org.apache.hadoop.hbase.coprocessor.ObserverContext;
    import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;
    import org.apache.hadoop.hbase.regionserver.wal.WALEdit;
    import org.apache.hadoop.hbase.util.Bytes;

    import java.io.FileWriter;
    import java.io.IOException;
    import java.util.List;

    /**
     * 自定义区域观察者
     */
    public class MyRegionObserver extends BaseRegionObserver{

        private void outInfo(String str){
            try {
                FileWriter fw = new FileWriter(&quot;/home/centos/coprocessor.txt&quot;,true);
                fw.write(str + &quot;\r\n&quot;);
                fw.close();
            } catch (Exception e) {
                e.printStackTrace();
            }

        }
        public void start(CoprocessorEnvironment e) throws IOException {
            super.start(e);
            outInfo(&quot;MyRegionObserver.start()&quot;);
        }

        public void preOpen(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e) throws IOException {
            super.preOpen(e);
            outInfo(&quot;MyRegionObserver.preOpen()&quot;);
        }

        public void postOpen(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e) {
            super.postOpen(e);
            outInfo(&quot;MyRegionObserver.postOpen()&quot;);
        }

        @Override
        public void preGetOp(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Get get, List&lt;Cell&gt; results) throws IOException {
            super.preGetOp(e, get, results);
            String rowkey = Bytes.toString(get.getRow());
            outInfo(&quot;MyRegionObserver.preGetOp() : rowkey = &quot; + rowkey);
        }

        public void postGetOp(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Get get, List&lt;Cell&gt; results) throws IOException {
            super.postGetOp(e, get, results);
            String rowkey = Bytes.toString(get.getRow());
            outInfo(&quot;MyRegionObserver.postGetOp() : rowkey = &quot; + rowkey);
        }

        public void prePut(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Put put, WALEdit edit, Durability durability) throws IOException {
            super.prePut(e, put, edit, durability);
            String rowkey = Bytes.toString(put.getRow());
            outInfo(&quot;MyRegionObserver.prePut() : rowkey = &quot; + rowkey);
        }

        @Override
        public void postPut(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Put put, WALEdit edit, Durability durability) throws IOException {
            super.postPut(e, put, edit, durability);
            String rowkey = Bytes.toString(put.getRow());
            outInfo(&quot;MyRegionObserver.postPut() : rowkey = &quot; + rowkey);
        }

        @Override
        public void preDelete(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Delete delete, WALEdit edit, Durability durability) throws IOException {
            super.preDelete(e, delete, edit, durability);
            String rowkey = Bytes.toString(delete.getRow());
            outInfo(&quot;MyRegionObserver.preDelete() : rowkey = &quot; + rowkey);
        }

        @Override
        public void postDelete(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Delete delete, WALEdit edit, Durability durability) throws IOException {
            super.postDelete(e, delete, edit, durability);
            String rowkey = Bytes.toString(delete.getRow());
            outInfo(&quot;MyRegionObserver.postDelete() : rowkey = &quot; + rowkey);
        }
    }

2.注册协处理器并分发
    &lt;property&gt;
        &lt;name&gt;hbase.coprocessor.region.classes&lt;/name&gt;
        &lt;value&gt;com.it18zhang.hbasedemo.coprocessor.MyRegionObserver&lt;/value&gt;
    &lt;/property&gt;

3.导出jar包。

4.复制jar到共享目录，分发到jar到hbase集群的hbase lib目录下.
    [/soft/hbase/lib]
</code></pre>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/11/19/SSM第一天/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/19/SSM第一天/" itemprop="url">SSM第一天</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-11-19T15:39:39+08:00">
                2018-11-19
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="SSM"><a href="#SSM" class="headerlink" title="SSM"></a>SSM</h2><pre><code>Spring        //业务层框架
Spring MVC    //开发web程序应用的模块model + view + controller
Mybatis        //持久化。jdbc , mysql
</code></pre><h2 id="mybatis"><a href="#mybatis" class="headerlink" title="mybatis"></a>mybatis</h2><pre><code>ibatis.
</code></pre><h2 id="体验"><a href="#体验" class="headerlink" title="体验"></a>体验</h2><pre><code>1.创建项目和模块
2.添加pom.xml
    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
    &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;
             xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
             xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
        &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

        &lt;groupId&gt;com.it18zhang&lt;/groupId&gt;
        &lt;artifactId&gt;mybatisdemo&lt;/artifactId&gt;
        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
        &lt;dependencies&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;org.mybatis&lt;/groupId&gt;
                &lt;artifactId&gt;mybatis&lt;/artifactId&gt;
                &lt;version&gt;3.2.1&lt;/version&gt;
            &lt;/dependency&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;mysql&lt;/groupId&gt;
                &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;
                &lt;version&gt;5.1.17&lt;/version&gt;
            &lt;/dependency&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;junit&lt;/groupId&gt;
                &lt;artifactId&gt;junit&lt;/artifactId&gt;
                &lt;version&gt;4.11&lt;/version&gt;
            &lt;/dependency&gt;
        &lt;/dependencies&gt;
    &lt;/project&gt;

3.添加配置
    [resoucecs/mybatis-config.xml]
    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;
    &lt;!DOCTYPE configuration
      PUBLIC &quot;-//mybatis.org//DTD Config 3.0//EN&quot;
      &quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot;&gt;
    &lt;configuration&gt;
      &lt;environments default=&quot;development&quot;&gt;
        &lt;environment id=&quot;development&quot;&gt;
          &lt;transactionManager type=&quot;JDBC&quot;/&gt;
          &lt;dataSource type=&quot;POOLED&quot;&gt;
            &lt;property name=&quot;driver&quot; value=&quot;${driver}&quot;/&gt;
            &lt;property name=&quot;url&quot; value=&quot;${url}&quot;/&gt;
            &lt;property name=&quot;username&quot; value=&quot;${username}&quot;/&gt;
            &lt;property name=&quot;password&quot; value=&quot;${password}&quot;/&gt;
          &lt;/dataSource&gt;
        &lt;/environment&gt;
      &lt;/environments&gt;
      &lt;mappers&gt;
        &lt;mapper resource=&quot;org/mybatis/example/BlogMapper.xml&quot;/&gt;
      &lt;/mappers&gt;
    &lt;/configuration&gt;

4.创建库和表
    mysql&gt;create database mybatis ;
    mysql&gt;use mybatis ;
    mysql&gt;create table users(id int primary key auto_increment , name varchar(20) ,age int) ;
    mysql&gt;desc users ;

5.测试连接
    package com.it18zhang.mybatisdemo;

    import org.apache.ibatis.io.Resources;
    import org.apache.ibatis.session.SqlSession;
    import org.apache.ibatis.session.SqlSessionFactory;
    import org.apache.ibatis.session.SqlSessionFactoryBuilder;

    import java.io.IOException;
    import java.io.InputStream;

    /**
     *
     */
    public class App {
        public static void main(String[] args) {
            try {
                //指定配置文件的路径(类路径)
                String resource = &quot;mybatis-config.xml&quot;;
                //加载文件
                InputStream inputStream = Resources.getResourceAsStream(resource);

                //创建会话工厂Builder,相当于连接池
                SqlSessionFactory sf = new SqlSessionFactoryBuilder().build(inputStream);

                //通过sf开启会话，相当于打开连接。
                SqlSession s = sf.openSession();
                System.out.println(s);

            } catch (Exception e) {
                e.printStackTrace();
            }
        }
    }

6.编写mapper文件。
    a)创建User类，和users对应
        public class User {
            private Integer id ;
            private String name ;
            private int age ;
            //get/set
        }

    b)创建UserMapper.xml,存放在resources/目录下
        包名[resources/UserMapper.xml]
        &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;
        &lt;!DOCTYPE mapper
                PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot;
                &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;
        &lt;!-- 定义名字空间 --&gt;
        &lt;mapper namespace=&quot;users&quot;&gt;
            &lt;!-- 定义insert语句 --&gt;
            &lt;insert id=&quot;insert&quot;&gt;
              insert into users(name,age) values(#{name},#{age})
            &lt;/insert&gt;
        &lt;/mapper&gt;

7.在resources/mybatis-config.xml文件中引入mapper的xml文件.
    [resources/mybatis-config.xml]
    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;
    &lt;!DOCTYPE configuration
            PUBLIC &quot;-//mybatis.org//DTD Config 3.0//EN&quot;
            &quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot;&gt;
    &lt;configuration&gt;
        &lt;environments default=&quot;development&quot;&gt;
            &lt;environment id=&quot;development&quot;&gt;
                &lt;transactionManager type=&quot;JDBC&quot;/&gt;
                &lt;dataSource type=&quot;POOLED&quot;&gt;
                    &lt;property name=&quot;driver&quot; value=&quot;com.mysql.jdbc.Driver&quot;/&gt;
                    &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/mybatis&quot;/&gt;
                    &lt;property name=&quot;username&quot; value=&quot;root&quot;/&gt;
                    &lt;property name=&quot;password&quot; value=&quot;root&quot;/&gt;
                &lt;/dataSource&gt;
            &lt;/environment&gt;
        &lt;/environments&gt;
        &lt;!-- *****引入映射文件(新增部分)***** --&gt;
        &lt;mappers&gt;
            &lt;mapper resource=&quot;UserMapper.xml&quot;/&gt;
        &lt;/mappers&gt;
    &lt;/configuration&gt;

8.编写单元测试，实现插入.
    [test/com.it18zhang.mybatis.test.TestCRUD.java]
    /**
     * insert
     */
    @Test
    public void insert() throws Exception {
        //指定配置文件的路径(类路径)
        String resource = &quot;mybatis-config.xml&quot;;
        //加载文件
        InputStream inputStream = Resources.getResourceAsStream(resource);

        //创建会话工厂Builder,相当于连接池
        SqlSessionFactory sf = new SqlSessionFactoryBuilder().build(inputStream);

        //通过sf开启会话，相当于打开连接。
        SqlSession s = sf.openSession();
        User u = new User();

        u.setName(&quot;jerry&quot;);
        u.setAge(2);
        s.insert(&quot;users.insert&quot;, u);
        s.commit();
        s.close();
    }

9.完成update-selectOne-selectAll操作。
    9.1)编写UserMapper.xml，添加相应的元素
        &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;
        &lt;!DOCTYPE mapper
                PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot;
                &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;
        &lt;mapper namespace=&quot;users&quot;&gt;
            &lt;insert id=&quot;insert&quot;&gt;
              insert into users(name,age) values(#{name},#{age})
            &lt;/insert&gt;
            &lt;update id=&quot;update&quot;&gt;
                update users set name = #{name} , age = #{age} where id = #{id}
            &lt;/update&gt;

            &lt;!-- selectOne --&gt;
            &lt;select id=&quot;selectOne&quot; parameterType=&quot;int&quot; resultType=&quot;com.it18zhang.mybatisdemo.domain.User&quot;&gt;
                select * from users where id = #{id}
            &lt;/select&gt;

            &lt;!-- selectAll --&gt;
            &lt;select id=&quot;selectAll&quot; resultType=&quot;com.it18zhang.mybatisdemo.domain.User&quot;&gt;
                select * from users
            &lt;/select&gt;
        &lt;/mapper&gt;

    9.2)编写测试程序
        /**
         * Created by Administrator on 2017/4/6.
         */
        public class TestCRUD {

            /**
             * insert
             */
            @Test
            public void insert() throws Exception {
                //指定配置文件的路径(类路径)
                String resource = &quot;mybatis-config.xml&quot;;
                //加载文件
                InputStream inputStream = Resources.getResourceAsStream(resource);

                //创建会话工厂Builder,相当于连接池
                SqlSessionFactory sf = new SqlSessionFactoryBuilder().build(inputStream);

                //通过sf开启会话，相当于打开连接。
                SqlSession s = sf.openSession();
                User u = new User();

                u.setName(&quot;jerry&quot;);
                u.setAge(2);
                s.insert(&quot;users.insert&quot;, u);
                s.commit();
                s.close();
            }

            /**
             * update
             */
            @Test
            public void update() throws Exception {
                String resource = &quot;mybatis-config.xml&quot;;
                InputStream inputStream = Resources.getResourceAsStream(resource);
                SqlSessionFactory sf = new SqlSessionFactoryBuilder().build(inputStream);
                SqlSession s = sf.openSession();
                User u = new User();
                u.setId(1);
                u.setName(&quot;tomas&quot;);
                u.setAge(32);
                s.update(&quot;users.update&quot;, u);
                s.commit();
                s.close();
            }

            /**
             * selectOne
             */
            @Test
            public void selectOne() throws Exception {
                String resource = &quot;mybatis-config.xml&quot;;
                InputStream inputStream = Resources.getResourceAsStream(resource);
                SqlSessionFactory sf = new SqlSessionFactoryBuilder().build(inputStream);
                SqlSession s = sf.openSession();
                User user = s.selectOne(&quot;users.selectOne&quot;,1);
                System.out.println(user.getName());
                s.commit();
                s.close();
            }

            /**
             * selectOne
             */
            @Test
            public void selectAll() throws Exception {
                String resource = &quot;mybatis-config.xml&quot;;
                InputStream inputStream = Resources.getResourceAsStream(resource);
                SqlSessionFactory sf = new SqlSessionFactoryBuilder().build(inputStream);
                SqlSession s = sf.openSession();
                List&lt;User&gt; users = s.selectList(&quot;users.selectAll&quot;);
                for(User uu : users){
                    System.out.println(uu.getName() + &quot;,&quot; + uu.getAge());
                }
                s.commit();
                s.close();
            }
        }
</code></pre><hr>
<p>几个文件：<br>    mybatis-config.xml配置文件指定数据库连接信息<br>    usermapper.xml映射文件，指定sql语句<br>    javabean.<br>    TestCRUD:测试文件，通过API访问，首先加载资源，产生一个会话工厂，开启会话，打开连接，实例化对象，加载。</p>
<hr>
<h2 id="复杂应用"><a href="#复杂应用" class="headerlink" title="复杂应用"></a>复杂应用</h2><pre><code>1.准备数据
    sql.sql

2.创建java类.
    [Order.java]
    public class Order {
        private Integer id ;
        private String orderNo ;
        //简历关联关系，订单和客户之间是多对一关系
        private User user ;

        //get/set
    }

    [Item.java]
    public class Item {
        private Integer id;
        private String itemName;
        //订单项和订单之间的关联关系
        private Order order;
        //get/set
    }

3.创建Order映射文件
    [resource/OrderMapper.xml]
    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;
    &lt;!DOCTYPE mapper
            PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot;
            &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;
    &lt;mapper namespace=&quot;orders&quot;&gt;
        &lt;insert id=&quot;insert&quot;&gt;
          insert into orders(orderno,uid) values(#{orderNo},#{user.id})
        &lt;/insert&gt;

        &lt;!-- findById --&gt;
        &lt;select id=&quot;selectOne&quot; parameterType=&quot;int&quot; resultMap=&quot;RM_Order&quot;&gt;
          select
            o.id oid ,
            o.orderno oorderno ,
            o.uid uid ,
            u.name uname ,
            u.age uage
          from orders o
            left outer join users u on o.uid = u.id where o.id = #{id}
        &lt;/select&gt;
        &lt;!-- findAll --&gt;
        &lt;select id=&quot;selectAll&quot; resultMap=&quot;RM_Order&quot;&gt;
          select
            o.id oid ,
            o.orderno oorderno ,
            o.uid uid ,
            u.name uname ,
            u.age uage
          from orders o
            left outer join users u on o.uid = u.id
        &lt;/select&gt;
        &lt;!-- 自定义结果映射 --&gt;
        &lt;resultMap id=&quot;RM_Order&quot; type=&quot;com.it18zhang.mybatisdemo.domain.Order&quot;&gt;
            &lt;id property=&quot;id&quot; column=&quot;oid&quot;/&gt;
            &lt;result property=&quot;orderNo&quot; column=&quot;oorderno&quot;/&gt;
            &lt;association property=&quot;user&quot; javaType=&quot;com.it18zhang.mybatisdemo.domain.User&quot;&gt;
                &lt;id property=&quot;id&quot; column=&quot;uid&quot; /&gt;
                &lt;result property=&quot;name&quot; column=&quot;uname&quot; /&gt;
                &lt;result property=&quot;age&quot; column=&quot;uage&quot; /&gt;
            &lt;/association&gt;
        &lt;/resultMap&gt;
    &lt;/mapper&gt;

4.修改配置文件,添加映射。
    [resource/mybatis-config.xml]
    &lt;!-- 引入映射文件 --&gt;
    &lt;mappers&gt;
        &lt;mapper resource=&quot;*Mapper.xml&quot;/&gt;
    &lt;/mappers&gt;

5.测试类
    public class TestOrder {

        /**
         * insert
         */
        @Test
        public void insert() throws Exception {
            String resource = &quot;mybatis-config.xml&quot;;
            InputStream inputStream = Resources.getResourceAsStream(resource);
            SqlSessionFactory sf = new SqlSessionFactoryBuilder().build(inputStream);
            SqlSession s = sf.openSession();

            User u = new User();
            u.setId(2);

            Order o = new Order();
            o.setOrderNo(&quot;No005&quot;);
            o.setUser(u);

            s.insert(&quot;orders.insert&quot;,o);
            s.commit();
            s.close();
        }

        @Test
        public void selectOne() throws Exception {
            String resource = &quot;mybatis-config.xml&quot;;
            InputStream inputStream = Resources.getResourceAsStream(resource);
            SqlSessionFactory sf = new SqlSessionFactoryBuilder().build(inputStream);
            SqlSession s = sf.openSession();
            Order order = s.selectOne(&quot;orders.selectOne&quot;,1);
            System.out.println(order.getOrderNo());
            s.commit();
            s.close();
        }

        @Test
        public void selectAll() throws Exception {
            String resource = &quot;mybatis-config.xml&quot;;
            InputStream inputStream = Resources.getResourceAsStream(resource);
            SqlSessionFactory sf = new SqlSessionFactoryBuilder().build(inputStream);
            SqlSession s = sf.openSession();
            List&lt;Order&gt; list = s.selectList(&quot;orders.selectAll&quot;);
            for(Order o : list){
                System.out.println(o.getOrderNo() + &quot; : &quot; + o.getUser().getName());
            }
            s.commit();
            s.close();
        }
    }
</code></pre><h2 id="配置一对多"><a href="#配置一对多" class="headerlink" title="配置一对多"></a>配置一对多</h2><pre><code>1.在User中增加orders集合。
    public class User {
        ...
        private List&lt;Order&gt; orders ;
        //get/set
    }

2.改造UserMapper.xml
</code></pre><h2 id="组合多对一和一对多关联关系到一个实体-Order-中"><a href="#组合多对一和一对多关联关系到一个实体-Order-中" class="headerlink" title="组合多对一和一对多关联关系到一个实体(Order)中"></a>组合多对一和一对多关联关系到一个实体(Order)中</h2><pre><code>1.关系
    Order(*) -&gt; (1)User
    Order(1) -&gt; (*)Item

2.Order.java
    class Order{
        ...
        List&lt;Item&gt; items ;
        //get/set    
    }
2&apos;.修改配置文件增加别名
    [resources/mybatis-config.xml]
    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;
    &lt;!DOCTYPE configuration
            PUBLIC &quot;-//mybatis.org//DTD Config 3.0//EN&quot;
            &quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot;&gt;
    &lt;configuration&gt;
        &lt;typeAliases&gt;
            &lt;typeAlias type=&quot;com.it18zhang.mybatisdemo.domain.User&quot; alias=&quot;_User&quot;/&gt;
            &lt;typeAlias type=&quot;com.it18zhang.mybatisdemo.domain.Order&quot; alias=&quot;_Order&quot;/&gt;
            &lt;typeAlias type=&quot;com.it18zhang.mybatisdemo.domain.Item&quot; alias=&quot;_Item&quot;/&gt;
        &lt;/typeAliases&gt;
        &lt;environments default=&quot;development&quot;&gt;
            &lt;environment id=&quot;development&quot;&gt;
                &lt;transactionManager type=&quot;JDBC&quot;/&gt;
                &lt;dataSource type=&quot;POOLED&quot;&gt;
                    &lt;property name=&quot;driver&quot; value=&quot;com.mysql.jdbc.Driver&quot;/&gt;
                    &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/mybatis&quot;/&gt;
                    &lt;property name=&quot;username&quot; value=&quot;root&quot;/&gt;
                    &lt;property name=&quot;password&quot; value=&quot;root&quot;/&gt;
                &lt;/dataSource&gt;
            &lt;/environment&gt;
        &lt;/environments&gt;
        &lt;!-- 引入映射文件 --&gt;
        &lt;mappers&gt;
            &lt;mapper resource=&quot;UserMapper.xml&quot;/&gt;
            &lt;mapper resource=&quot;OrderMapper.xml&quot;/&gt;
        &lt;/mappers&gt;
    &lt;/configuration&gt;

3.OrderMapper.xml
    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;
    &lt;!DOCTYPE mapper
            PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot;
            &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;
    &lt;mapper namespace=&quot;orders&quot;&gt;
        &lt;insert id=&quot;insert&quot;&gt;
          insert into orders(orderno,uid) values(#{orderNo},#{user.id})
        &lt;/insert&gt;

        &lt;!-- findById --&gt;
        &lt;select id=&quot;selectOne&quot; parameterType=&quot;int&quot; resultMap=&quot;RM_Order&quot;&gt;
          select
            o.id      oid ,
            o.orderno oorderno ,
            o.uid     uid ,
            u.name    uname ,
            u.age     uage ,
            i.id      iid,
            i.itemname iitemname
          from orders o
            left outer join users u on o.uid = u.id
            left outer join items i on o.id = i.oid
          where o.id = #{id}
        &lt;/select&gt;
        &lt;!-- findAll --&gt;
        &lt;select id=&quot;selectAll&quot; resultMap=&quot;RM_Order&quot;&gt;
          select
            o.id      oid ,
            o.orderno oorderno ,
            o.uid     uid ,
            u.name    uname ,
            u.age     uage ,
            i.id      iid,
            i.itemname iitemname
          from orders o
            left outer join users u on o.uid = u.id
            left outer join items i on o.id = i.oid
        &lt;/select&gt;
        &lt;!-- 自定义结果映射 --&gt;
        &lt;resultMap id=&quot;RM_Order&quot; type=&quot;com.it18zhang.mybatisdemo.domain.Order&quot;&gt;
            &lt;id property=&quot;id&quot; column=&quot;oid&quot;/&gt;
            &lt;result property=&quot;orderNo&quot; column=&quot;oorderno&quot;/&gt;
            &lt;association property=&quot;user&quot; javaType=&quot;com.it18zhang.mybatisdemo.domain.User&quot;&gt;
                &lt;id property=&quot;id&quot; column=&quot;uid&quot; /&gt;
                &lt;result property=&quot;name&quot; column=&quot;uname&quot; /&gt;
                &lt;result property=&quot;age&quot; column=&quot;uage&quot; /&gt;
            &lt;/association&gt;
            &lt;collection property=&quot;items&quot; ofType=&quot;_Item&quot;&gt;
                &lt;id property=&quot;id&quot; column=&quot;iid&quot; /&gt;
                &lt;result property=&quot;itemName&quot; column=&quot;iitemname&quot; /&gt;
            &lt;/collection&gt;
        &lt;/resultMap&gt;
    &lt;/mapper&gt;

4.测试
    @Test
    public void selectOne() throws Exception {
        String resource = &quot;mybatis-config.xml&quot;;
        InputStream inputStream = Resources.getResourceAsStream(resource);
        SqlSessionFactory sf = new SqlSessionFactoryBuilder().build(inputStream);
        SqlSession s = sf.openSession();
        Order order = s.selectOne(&quot;orders.selectOne&quot;,1);
        System.out.println(order.getOrderNo() + order.getUser().getName());
        for(Item i : order.getItems()){
            System.out.println(i.getId() + &quot;:&quot; + i.getItemName());
        }
        s.commit();
        s.close();
    }
</code></pre><h2 id="改造项目"><a href="#改造项目" class="headerlink" title="改造项目"></a>改造项目</h2><pre><code>1.引入Util类
    package com.it18zhang.mybatisdemo.util;

    import org.apache.ibatis.io.Resources;
    import org.apache.ibatis.session.SqlSession;
    import org.apache.ibatis.session.SqlSessionFactory;
    import org.apache.ibatis.session.SqlSessionFactoryBuilder;

    import java.io.InputStream;

    /**
     * 工具类
     */
    public class Util {
        //
        private static SqlSessionFactory sf ;

        static{
            try {
                String resource = &quot;mybatis-config.xml&quot;;
                InputStream inputStream = Resources.getResourceAsStream(resource);
                sf = new SqlSessionFactoryBuilder().build(inputStream);
            } catch (Exception e) {
                e.printStackTrace();
            }
        }

        /**
         * 开启会话
         */
        public static SqlSession openSession(){
            return sf.openSession() ;
        }

        /**
         * 关闭会话
         */
        public static void closeSession(SqlSession s){
            if(s != null){
                s.close();
            }
        }


        /**
         * 关闭会话
         */
        public static void rollbackTx(SqlSession s) {
            if (s != null) {
                s.rollback();
            }
        }
    }

2.设计模板类DaoTemplate和回调MybatisCallback接口
    [DaoTemplate.java]
    package com.it18zhang.mybatisdemo.dao;

    import com.it18zhang.mybatisdemo.util.Util;
    import org.apache.ibatis.session.SqlSession;

    /**
     * 模板类
     */
    public class DaoTemplate {
        /**
         * 执行
         */
        public static Object execute(MybatisCallback cb){
            SqlSession s = null;
            try {
                s = Util.openSession();
                Object ret = cb.doInMybatis(s);
                s.commit();
                return ret ;
            } catch (Exception e) {
                Util.rollbackTx(s);
            } finally {
                Util.closeSession(s);
            }
            return null ;
        }
    }

    [MybatisCallback.java]
    package com.it18zhang.mybatisdemo.dao;

    import org.apache.ibatis.session.SqlSession;

    /**
     * 回调接口
     */
    public interface MybatisCallback {
        public Object doInMybatis(SqlSession s);
    }

3.通过模板类+回调接口实现UserDao.java
    [UserDao.java]
    package com.it18zhang.mybatisdemo.dao;

    import com.it18zhang.mybatisdemo.domain.User;
    import com.it18zhang.mybatisdemo.util.Util;
    import org.apache.ibatis.session.SqlSession;

    import java.util.List;

    /**
     * UserDao
     */
    public class UserDao {

        /**
         * 插入操作
         */
        public void insert(final User user){
            DaoTemplate.execute(new MybatisCallback() {
                public Object doInMybatis(SqlSession s) {
                    s.insert(&quot;users.insert&quot;,user);
                    return null ;
                }
            });
        }

        /**
         * 插入操作
         */
        public void update(final User user){
            DaoTemplate.execute(new MybatisCallback() {
                public Object doInMybatis(SqlSession s) {
                    s.update(&quot;users.update&quot;, user);
                    return null ;
                }
            });
        }

        public User selctOne(final Integer id){
            return (User)DaoTemplate.execute(new MybatisCallback() {
                public Object doInMybatis(SqlSession s) {
                    return s.selectOne(&quot;users.selectOne&quot;,id);
                }
            });
        }

        public List&lt;User&gt; selctAll(){
            return (List&lt;User&gt;)DaoTemplate.execute(new MybatisCallback() {
                public Object doInMybatis(SqlSession s) {
                    return s.selectList(&quot;users.selectAll&quot;);
                }
            });
        }
    }

4.App测试
    public static void main(String[] args) {
        UserDao dao = new UserDao();
        User u = dao.selctOne(1);
        System.out.println(u.getName());
    }
</code></pre>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/11/16/Hbase第二天/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/16/Hbase第二天/" itemprop="url">Hbase第二天</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-11-16T09:33:51+08:00">
                2018-11-16
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>start-hbase.sh<br>    hbase-daemon.sh start master<br>    habse-daemon.sh    start regionserver</p>
<p>hbase的ha设置：<br>    直接打开S202或者s203的master进程即可，启动命令如上图。</p>
<p>hbase shell操作：<br>    $&gt;hbaes shell<br>    $hbase&gt;help</p>
<p>namespace 类似于Mysql库的概念</p>
<p>insert into<br>nosql: not only SQL<br>key-value<br>put用来放kv对。<br>在建表的时候没有指定列明，指定的是列族名，然后这个列族下的列是可以增减的。<br>help ‘put’</p>
<p><img src="https://i.imgur.com/5JUqC0b.png" alt=""></p>
<p>habase shell 操作：</p>
<pre><code>$&gt;hbase shell                                    //登陆shell终端
$hbase&gt;help                                        //    
$hbase&gt;help &apos;list_namespace&apos;                    //查看特定 的命令帮助
$hbase&gt;list_namespace                            //列出名字空间（数据库）
$hbase&gt;list_namespace_tables &apos;default&apos;            //列出名字空间
$hbase&gt;create_namespace &apos;ns1&apos;                    //创建名字空间
$hbase&gt;help &apos;create&apos;                            //
$hbase&gt;create &apos;ns1:t1&apos;,&apos;f1&apos;                        //创建表，指定空间下
$hbase&gt;put &apos;ns1:t1&apos;,&apos;row1&apos;,&apos;f1:id&apos;,100
$hbase&gt;put &apos;ns1:t1&apos;,&apos;row1&apos;,&apos;f1:name&apos;,tom
$hbase&gt;get &apos;ns1:t1&apos;,&apos;row1&apos;                        //指定查询row
$hbase&gt;scan &apos;ns1:t1&apos;                            //权标扫描扫描ns1列族的t1列
</code></pre><p>三级坐标定位，一个是列族，一个是row一个是时间戳如下图;</p>
<p><img src="https://i.imgur.com/ipxtMbm.png" alt=""></p>
<p><img src="https://i.imgur.com/uz0TL9q.png" alt=""></p>
<p><img src="https://i.imgur.com/3HmcLCa.png" alt=""></p>
<p>通过java api操作hbase:<br>    package com.it18zhang.hbasedemo;</p>
<pre><code>import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.TableName;
import org.apache.hadoop.hbase.client.Connection;
import org.apache.hadoop.hbase.client.ConnectionFactory;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.client.Table;
import org.apache.hadoop.hbase.util.Bytes;
import org.junit.Test;

eate 2018/11/17 11:56
 */
public class TestCRUD {
    @Test
    public void put() throws Exception {
        Configuration conf = HBaseConfiguration.create();
        Connection conn = ConnectionFactory.createConnection(conf);

        TableName tname = TableName.valueOf(&quot;ns1:t1&quot;);
        Table table = conn.getTable(tname);
        byte[] rowid = Bytes.toBytes(&quot;row2&quot;);
        byte[] f1 = Bytes.toBytes(&quot;f1&quot;);
        byte[] id = Bytes.toBytes(&quot;id&quot;);
        byte[] value = Bytes.toBytes(102);
        //创建put对象

        Put put = new Put(rowid);
        put.addColumn(f1, id, value);
        table.put(put);

    }
}
</code></pre><p>pom文件：</p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;
         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;groupId&gt;com.it18zhang&lt;/groupId&gt;
    &lt;artifactId&gt;HbaseDemo&lt;/artifactId&gt;
    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;junit&lt;/groupId&gt;
            &lt;artifactId&gt;junit&lt;/artifactId&gt;
            &lt;version&gt;4.11&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;
            &lt;artifactId&gt;hbase-client&lt;/artifactId&gt;
            &lt;version&gt;1.2.3&lt;/version&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;


&lt;/project&gt;
</code></pre><hr>
<p>hbase架构介绍：</p>
<p><img src="https://i.imgur.com/BaJwZ3z.png" alt=""></p>
<h1 id="关于区域服务器"><a href="#关于区域服务器" class="headerlink" title="关于区域服务器"></a>关于区域服务器</h1><p>  每个大表按照rowkey来切分，rowkey是可排序的，每个切分的被分派到每个区域分五期里面。key是有序的。而且每个取余服务器的表都是有范围的，所以在get的时候要指定rowkey，然后查询的时候定位到这个rowkey在哪个区域服务器上，因为所有的区域服务器信息都在meta表这个自带的表里面，叫元数据表。里面存储着rowkey。也就是每个区域的rowkey的范围。</p>
<p><img src="https://i.imgur.com/4nzxjz8.png" alt=""></p>
<p>看里面的内容hbase；namespace,,14….,74….<br>这个就是名字空间表，起始的位置，结束的rowkey位置。<br>前4大段，是指的是一大行的前4列。两个逗号意味着没有切割。就只是在一个区域服务器里面</p>
<p>再看下面的ns1:t1表的都是一行，然后3列不同的列，如果查询这部分直接联系s204,如果204挂了， 这个表就会更新，变成承接的新表的数据。这个ns1:t1的这个表就没有分割。column是列，info是列族，server是列</p>
<p>在hbase查询的时候是3级定位。时间戳，列族（列），rowkey（行）</p>
<p>看一下下面的这个目录：</p>
<p>hbase,数据，ns1名字空间，t1表，区域服务器,f1列族，然后列族下面就是文件了二进制存储，内容都在这个二进制文件里面。</p>
<p><img src="https://i.imgur.com/18bQKMj.png" alt=""></p>
<p>一个表可以有多个区域，一个区域服务器肯定有多个区域，由于有副本的存在所以一个区域可以在多个服务器上，区域服务器是来提供服务的，读写过程是由区域服务器完成，相当于数据节点，这2个都是完成数据的读写过程的。</p>
<p>在区域里面有个store在里面有MemStore。所以这些内容都是在内存当中，如果内存满了，就会被溢出到磁盘当中，Storefile就是在磁盘当中了，Hfile通过和底层交互</p>
<h1 id="hbase的写入过程："><a href="#hbase的写入过程：" class="headerlink" title="hbase的写入过程："></a>hbase的写入过程：</h1><p><img src="https://i.imgur.com/XfOpVug.png" alt=""></p>
<p>root这个地方写错了是老版本的，应该是meta表<br><img src="https://i.imgur.com/Imht8Oq.png" alt=""><br><img src="https://i.imgur.com/peKNm1w.png" alt=""></p>
<p>现在妖王一个表里写数据，需要先查询这个表在那个区域服务器上，要先查询hbase空间下的meta表，因为这个表里面标识了类似于一个目录的内容，标识那个区域子在那个服务器上，也就是rowkey的范围。这个meta表在哪里呢怎么找到呢？ meta表要通过zk来找到。因为zk集群在，所以没有zk的情况下hbase都起不来。</p>
<p>进入到hbase shell里面</p>
<p><img src="https://i.imgur.com/Rx4QHIF.png" alt=""></p>
<p>通过进入到shell里面发现这个meta表大概可能在这个里面，进去之后发现这个meta表在s203里面。如下图</p>
<p><img src="https://i.imgur.com/SHSVJwz.png" alt=""></p>
<p>所以实际上就是先联系zk然后通过zk找到他的位置在s203里面，这个meta表在s203里面</p>
<p>ta/hbase/meta/1588230740/info/da7a63e29d1c4fb588068ea9c187ae27</p>
<hr>
<h1 id="hbase基于hdfs"><a href="#hbase基于hdfs" class="headerlink" title="hbase基于hdfs"></a>hbase基于hdfs</h1><p>【表数据的存储结构目录构成】</p>
<pre><code>hdfs://s201:8020/hbase/data/${名字空间}/${表名}/区域名称/列族名称
</code></pre><p>相同列族的数据存放在一个文件中，</p>
<p>【WAL写前日志目录结构构成】</p>
<pre><code>hdfs://s201:8020/hbase/wals/s202,16020,1542534586029/s202%2C16020%2C1542534586029.default.1542541792199

hdfs://s201:8020/hbase/wals/${区域服务器名称}/主机名，端口号，时间戳/
</code></pre><h1 id="client端交互过程"><a href="#client端交互过程" class="headerlink" title="client端交互过程"></a>client端交互过程</h1><p>0.集群启动时，master负责分配区域到指定的区域服务器</p>
<p>1.联系zk找出meta表所在的区域服务器rs(regionserver)<br>        /meta/meta-region-server<br>    定位到所在的服务器</p>
<p>2.定位rowkey，找到对应的rs(regionserver)</p>
<p>3.缓存信息到本地，</p>
<p>4.联系regionserver</p>
<p>5.HRegionServe负责open-HRegion对象打开H区域服务器对象，为每个列族创建store实例，说明store是和列族对应的，store包涵多个storefile实例，他们是对hfile的轻量级封装，每个store还对应一个memstroe（用于内存储速度快），</p>
<p><img src="https://i.imgur.com/ks0t9JW.png" alt=""></p>
<hr>
<h1 id="在百万数据存储的时候："><a href="#在百万数据存储的时候：" class="headerlink" title="在百万数据存储的时候："></a>在百万数据存储的时候：</h1><p>关闭WALS</p>
<p><img src="https://i.imgur.com/ITwCty6.png" alt=""></p>
<p>代码如下：</p>
<pre><code>@Test
   public void biginsert() throws Exception {
       long start=System.currentTimeMillis();
       Configuration conf = HBaseConfiguration.create();
       Connection conn = ConnectionFactory.createConnection(conf);
       TableName tname = TableName.valueOf(&quot;ns1:t1&quot;);
       HTable table = (HTable) conn.getTable(tname);
       //不要自动清理缓冲区
       table.setAutoFlushTo(false);

       for (int i = 0; i &lt; 1000000; i++) {
           Put put = new Put(Bytes.toBytes(&quot;row&quot; + i));
           //关闭写前日志
           put.setWriteToWAL(false);
           put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;), Bytes.toBytes(i));
           put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;), Bytes.toBytes(&quot;tom&quot; + i));
           put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;), Bytes.toBytes(i % 100));
           table.put(put);
           if (i % 2000 == 0) {
               table.flushCommits();
           }
       }
       table.flushCommits();
       System.out.println(System.currentTimeMillis()-start);

   }
</code></pre><hr>
<p>hbase shell命令：</p>
<p>要想删除表，先要禁用表。</p>
<pre><code>$hbase&gt;flush &apos;ns1:t1&apos;        //清理内存数据到磁盘
$hbase&gt;count &apos;ns1:t1&apos;        //统计函数
$hbase&gt;disable &apos;ns1:t1&apos;        //删除表之前要禁用表
$hbase&gt;drop &apos;ns1:t1&apos;        //删除表 
$hbase&gt;count &apos;hbase:meta&apos;     //查看元数据表
</code></pre><p><img src="https://i.imgur.com/AAzzOmp.png" alt=""></p>
<hr>
<h1 id="格式化代码，设置固定数字格式"><a href="#格式化代码，设置固定数字格式" class="headerlink" title="格式化代码，设置固定数字格式"></a>格式化代码，设置固定数字格式</h1><pre><code> @Test
    public void formatNum(){
        DecimalFormat format =new DecimalFormat();
        format.applyPattern(&quot;0000000&quot;);
//        format.applyPattern(&quot;###,###,00&quot;);
        System.out.println(format.format(8));
    }
</code></pre><hr>
<p>为什么要格式化rowid，因为rowid1008和8相比较8比1008还要大，所以要格式化一下。</p>
<p>经过格式化rowid的代码：</p>
<pre><code> @Test
public void biginsert() throws Exception {

    DecimalFormat format =new DecimalFormat();
    format.applyPattern(&quot;0000000&quot;);
    System.out.println(format.format(8));

    long start=System.currentTimeMillis();
    Configuration conf = HBaseConfiguration.create();
    Connection conn = ConnectionFactory.createConnection(conf);
    TableName tname = TableName.valueOf(&quot;ns1:t1&quot;);
    HTable table = (HTable) conn.getTable(tname);
    //不要自动清理缓冲区
    table.setAutoFlushTo(false);

    for (int i = 0; i &lt; 10000; i++) {
        Put put = new Put(Bytes.toBytes(&quot;row&quot; + format.format(i)));
        //关闭写前日志
        put.setWriteToWAL(false);
        put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;), Bytes.toBytes(i));
        put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;), Bytes.toBytes(&quot;tom&quot; + i));
        put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;), Bytes.toBytes(i % 100));
        table.put(put);
        if (i % 2000 == 0) {
            table.flushCommits();
        }
    }
    table.flushCommits();
    System.out.println(System.currentTimeMillis()-start);

}
</code></pre><hr>
<h1 id="flush命令"><a href="#flush命令" class="headerlink" title="flush命令"></a>flush命令</h1><pre><code>$hbase:flush：清理内存数据到磁盘
</code></pre><h1 id="region拆分切割"><a href="#region拆分切割" class="headerlink" title="region拆分切割"></a>region拆分切割</h1><p><img src="https://i.imgur.com/v0N169Q.png" alt=""><br><img src="https://i.imgur.com/XQesWs6.png" alt=""></p>
<p>hbase默认切割文件是10G，超过切割。</p>
<pre><code>$hbase&gt;count &apos;ns1:t1&apos;        //统计函数
</code></pre><p><img src="https://i.imgur.com/sjQ5dq7.png" alt=""></p>
<pre><code>切割而一旦完成会保留原来的表结构，但是原来的数据内容已经没有了被删除了 
</code></pre><p><img src="https://i.imgur.com/UhuexPk.png" alt=""></p>
<p><img src="https://i.imgur.com/0NfLQSJ.png" alt=""></p>
<p><img src="https://i.imgur.com/oJTAFPt.png" alt=""></p>
<hr>
<h1 id="hbase和hadoop的ha集成"><a href="#hbase和hadoop的ha集成" class="headerlink" title="hbase和hadoop的ha集成"></a>hbase和hadoop的ha集成</h1><p>1.在hbase-env.sh文件添加hadoop配置文件目录到hbase_classpath环境变量[/soft/hbase/conf/hbase-env]并且分发。</p>
<pre><code>export HBASE_CLASSPATH=$HBASE_CLASSPATH:/soft/hadoop/    etc/hadoop
</code></pre><p>2.在hbase/conf目录下创建到hadoop的hdfs-site.xml的符号链接</p>
<pre><code>    $&gt;ln -s /soft/hadoop/etc/hadoop/hdfs-site.xml
/soft/hbase/conf/hdfs-site.xml
</code></pre><p>3.修改Hbase-site.xml文件中hbase.rootdir的目录值<br>        /soft/hbase/conf/hbase-site.xml<br>4.将之都分发出去。</p>
<p>继承了之后及时关闭了s201的namenode，hbase的Hmaster也不会关闭，也就是形成了高可用状态。 </p>
<hr>
<h1 id="hbase手动移动区域"><a href="#hbase手动移动区域" class="headerlink" title="hbase手动移动区域"></a>hbase手动移动区域</h1><p>手动移动区域<br><img src="https://i.imgur.com/UayeavF.png" alt=""></p>
<p>手动强行合并hbase块<br><img src="https://i.imgur.com/CrvZAFo.png" alt=""><br><img src="https://i.imgur.com/JCFdxeV.png" alt=""></p>
<p>手动切割：</p>
<h1 id="拆分风暴："><a href="#拆分风暴：" class="headerlink" title="拆分风暴："></a>拆分风暴：</h1><p><img src="https://i.imgur.com/wP6mfUJ.png" alt=""></p>
<p>在达到10G以后，几个区域同时增长，同时到达10G临界点，同时切割额，服务器工作瞬间增大。每个都在切割，就叫切割风暴，我们要避免这种情况，通过使用手动切割，避免拆分风暴。</p>
<hr>
<p>代码操作增删改查<br>    package com.it18zhang.hbasedemo;</p>
<pre><code>import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.*;
import org.apache.hadoop.hbase.client.*;
import org.apache.hadoop.hbase.util.Bytes;
import org.junit.Test;

import java.io.IOException;
import java.text.DecimalFormat;
import java.util.Iterator;
import java.util.Map;
import java.util.NavigableMap;

/**
 * @Title:TestCRUD
 * @Description:
 * @Auther: Eric Hunn
 * @Version:1.0
 * @Create 2018/11/17 11:56
 */
public class TestCRUD {
    @Test
    public void put() throws Exception {
        Configuration conf = HBaseConfiguration.create();
        Connection conn = ConnectionFactory.createConnection(conf);

        TableName tname = TableName.valueOf(&quot;ns1:t1&quot;);
        Table table = conn.getTable(tname);
        byte[] rowid = Bytes.toBytes(&quot;row2&quot;);
        byte[] f1 = Bytes.toBytes(&quot;f1&quot;);
        byte[] id = Bytes.toBytes(&quot;id&quot;);
        byte[] value = Bytes.toBytes(102);
        //创建put对象

        Put put = new Put(rowid);
        put.addColumn(f1, id, value);
        table.put(put);

    }

    @Test
    public void biginsert() throws Exception {

        DecimalFormat format = new DecimalFormat();
        format.applyPattern(&quot;0000000&quot;);
        System.out.println(format.format(8));

        long start = System.currentTimeMillis();
        Configuration conf = HBaseConfiguration.create();
        Connection conn = ConnectionFactory.createConnection(conf);
        TableName tname = TableName.valueOf(&quot;ns1:t1&quot;);
        HTable table = (HTable) conn.getTable(tname);
        //不要自动清理缓冲区
        table.setAutoFlushTo(false);

        for (int i = 0; i &lt; 10000; i++) {
            Put put = new Put(Bytes.toBytes(&quot;row&quot; + format.format(i)));
            //关闭写前日志
            put.setWriteToWAL(false);
            put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;), Bytes.toBytes(i));
            put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;), Bytes.toBytes(&quot;tom&quot; + i));
            put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;), Bytes.toBytes(i % 100));
            table.put(put);
            if (i % 2000 == 0) {
                table.flushCommits();
            }
        }
        table.flushCommits();
        System.out.println(System.currentTimeMillis() - start);

    }

    @Test
    public void formatNum() {
        DecimalFormat format = new DecimalFormat();
        format.applyPattern(&quot;0000000&quot;);
//        format.applyPattern(&quot;###,###,00&quot;);
        System.out.println(format.format(8));
    }

    @Test
    public void createNamespace() throws Exception {
        //创建conf对象
        Configuration conf = HBaseConfiguration.create();
        //通过工厂创建连接对象
        Connection conn = ConnectionFactory.createConnection(conf);
        Admin admin = conn.getAdmin();
        NamespaceDescriptor nsd = NamespaceDescriptor.create(&quot;ns2&quot;).build();
        admin.createNamespace(nsd);

        NamespaceDescriptor[] ns = admin.listNamespaceDescriptors();
        for (NamespaceDescriptor n : ns) {
            System.out.println(n.getName());
        }

    }

    @Test
    public void listNamespaces() throws Exception {
        //创建conf对象
        Configuration conf = HBaseConfiguration.create();
        //通过工厂创建连接对象
        Connection conn = ConnectionFactory.createConnection(conf);
        Admin admin = conn.getAdmin();
        NamespaceDescriptor[] ns = admin.listNamespaceDescriptors();
        for (NamespaceDescriptor n : ns) {
            System.out.println(n.getName());
        }
    }

    @Test
    public void createTables() throws Exception {
        //创建conf对象
        Configuration conf = HBaseConfiguration.create();
        //通过工厂创建连接对象
        Connection conn = ConnectionFactory.createConnection(conf);
        Admin admin = conn.getAdmin();
        //创建表名对象
        TableName tbn = TableName.valueOf(&quot;ns2:t2&quot;);
        //创建表描述符对象
        HTableDescriptor tbl = new HTableDescriptor(tbn);
        //在表描述符中添加列族创建列族描述符
        HColumnDescriptor col = new HColumnDescriptor(&quot;f1&quot;);
        tbl.addFamily(col);

        admin.createTable(tbl);
        System.out.println(&quot;over&quot;);
    }

    @Test
    public void disableTable() throws Exception {
        Configuration conf = HBaseConfiguration.create();
        Connection conn = ConnectionFactory.createConnection(conf);
        Admin admin = conn.getAdmin();
        admin.disableTable(TableName.valueOf(&quot;ns2:t2&quot;));
    }

    @Test
    public void dropTable() throws Exception {
        Configuration conf = HBaseConfiguration.create();
        Connection conn = ConnectionFactory.createConnection(conf);
        Admin admin = conn.getAdmin();
        admin.disableTable(TableName.valueOf(&quot;ns2:t2&quot;));
        admin.deleteTable(TableName.valueOf(&quot;ns2:t2&quot;));
    }

    @Test
    public void deleteData() throws IOException {

        Configuration conf = HBaseConfiguration.create();
        Connection conn = ConnectionFactory.createConnection(conf);
        TableName tname = TableName.valueOf(&quot;ns1:t1&quot;);
        Table table = conn.getTable(tname);
        Delete del = new Delete(Bytes.toBytes(&quot;row0000001&quot;));
        del.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;));
        del.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;));

        table.delete(del);
        System.out.println(&quot;over&quot;);

    }

    @Test
    public void scanall() throws IOException {
        Configuration conf = HBaseConfiguration.create();
        Connection conn = ConnectionFactory.createConnection(conf);
        TableName tname = TableName.valueOf(&quot;ns1:t1&quot;);
        Table table = conn.getTable(tname);
        Scan scan = new Scan();
        ResultScanner rs = table.getScanner(scan);
        Iterator&lt;Result&gt; it = rs.iterator();
        while (it.hasNext()) {
            Result r = it.next();//理解这个r是对一整行的封装
            byte[] value = r.getValue(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;));
            System.out.println(Bytes.toString(value));
        }
    }


    @Test
    public void scan() throws IOException {
        Configuration conf = HBaseConfiguration.create();
        Connection conn = ConnectionFactory.createConnection(conf);
        TableName tname = TableName.valueOf(&quot;ns1:t1&quot;);
        Table table = conn.getTable(tname);
        Scan scan = new Scan();
        scan.setStartRow(Bytes.toBytes(&quot;row0005000&quot;));
        scan.setStopRow(Bytes.toBytes(&quot;row0008000&quot;));
        ResultScanner rs = table.getScanner(scan);
        Iterator&lt;Result&gt; it = rs.iterator();
        while (it.hasNext()) {
            Result r = it.next();//理解这个r是对一整行的封装
            byte[] value = r.getValue(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;));
            System.out.println(Bytes.toString(value));
        }
    }

    //动态获取所有的列和列族，动态遍历，每个列和值的集合，由于强行getvalue转换成string类型，所以难免出现乱码情况
    @Test
    public void scan2() throws IOException {
        Configuration conf = HBaseConfiguration.create();
        Connection conn = ConnectionFactory.createConnection(conf);
        TableName tname = TableName.valueOf(&quot;ns1:t1&quot;);
        Table table = conn.getTable(tname);
        Scan scan = new Scan();
        ResultScanner rs = table.getScanner(scan);
        Iterator&lt;Result&gt; it = rs.iterator();
        while (it.hasNext()) {
            Result r = it.next();//理解这个r是对一整行的封装
            Map&lt;byte[], byte[]&gt; map = r.getFamilyMap(Bytes.toBytes(&quot;f1&quot;));
            for (Map.Entry&lt;byte[], byte[]&gt; entrySet : map.entrySet()) {
                String col = Bytes.toString(entrySet.getKey());
                String val = Bytes.toString(entrySet.getValue());
                System.out.println(col + &quot;:&quot; + val + &quot;,&quot;);
            }
            System.out.println();
        }
    }

    @Test
    public void scan3() throws IOException {
        Configuration conf = HBaseConfiguration.create();
        Connection conn = ConnectionFactory.createConnection(conf);
        TableName tname = TableName.valueOf(&quot;ns1:t1&quot;);
        Table table = conn.getTable(tname);
        Scan scan = new Scan();
        ResultScanner rs = table.getScanner(scan);
        Iterator&lt;Result&gt; it = rs.iterator();
        while (it.hasNext()) {
            Result r = it.next();//理解这个r是对一整行的封装
            //得到一行的所有map，key=f1,value=Map&lt;Col,Map&lt;Timestamp,value&gt;&gt;这个结构

            NavigableMap&lt;byte[], NavigableMap&lt;byte[], NavigableMap&lt;Long, byte[]&gt;&gt;&gt; map = r.getMap();
            for (Map.Entry&lt;byte[], NavigableMap&lt;byte[], NavigableMap&lt;Long, byte[]&gt;&gt;&gt; entry : map.entrySet()) {
                //得到列族
                String f = Bytes.toString(entry.getKey());
                NavigableMap&lt;byte[], NavigableMap&lt;Long, byte[]&gt;&gt; colDataMap = entry.getValue();
                for (Map.Entry&lt;byte[], NavigableMap&lt;Long, byte[]&gt;&gt; ets : colDataMap.entrySet()) {
                    String c = Bytes.toString(ets.getKey());
                    Map&lt;Long, byte[]&gt; tsValueMap = ets.getValue();
                    for (Map.Entry&lt;Long, byte[]&gt; e : tsValueMap.entrySet()) {
                        Long ts = e.getKey();
                        String value = Bytes.toString(e.getValue());
                        System.out.println(f + &quot;:&quot; + c + &quot;:&quot; + ts + &quot;=&quot; + value + &quot;,&quot;);
                    }
                }
            }
        }
    }
}
</code></pre>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/11/16/Hbase第一天/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/16/Hbase第一天/" itemprop="url">Hbase第一天</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-11-16T09:33:51+08:00">
                2018-11-16
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://erichunn.github.io/2018/11/15/Zookeeper第二天/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eric Hunn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无心是一首歌">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/15/Zookeeper第二天/" itemprop="url">Zookeeper第二天</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-11-15T16:45:25+08:00">
                2018-11-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="leader推选过程-最小号选举法"><a href="#leader推选过程-最小号选举法" class="headerlink" title="leader推选过程(最小号选举法)"></a>leader推选过程(最小号选举法)</h2><pre><code>1.所有节点在同一目录下创建临时序列节点。
2.节点下会生成/xxx/xx000000001等节点。
3.序号最小的节点就是leader，其余就是follower.
4.每个节点观察小于自己节点的主机。(注册观察者)
5.如果leader挂了，对应znode删除了。
6.观察者收到通知。
</code></pre><p><img src="https://i.imgur.com/O6wXdY8.png" alt=""></p>
<h2 id="配置完全分布式zk集群"><a href="#配置完全分布式zk集群" class="headerlink" title="配置完全分布式zk集群"></a>配置完全分布式zk集群</h2><pre><code>1.挑选3台主机
    s201 ~ s203
2.每台机器都安装zk
    tar
    环境变量

3.配置zk配置文件
    s201 ~ s203
    [/soft/zk/conf/zoo.cfg]
    ...
    dataDir=/home/centos/zookeeper

    server.1=s201:2888:3888
    server.2=s202:2888:3888 
    server.3=s203:2888:3888

4.在每台主机的/home/centos/zookeeper中添加myid,内容分别是1,2,3
    [s201]
    $&gt;echo 1 &gt; /home/centos/zookeeper/myid
    [s202]
    $&gt;echo 2 &gt; /home/centos/zookeeper/myid
    [s203]
    $&gt;echo 3 &gt; /home/centos/zookeeper/myid

5.启动服务器集群 
    $&gt;zkServer.sh start
    ...

6.查看每台服务器的状态
    $&gt;zkServer.sh status

7.修改zk的log目录

    vi /soft/zk/conf/log4j.properties
</code></pre><p>修改如下：</p>
<p><img src="https://i.imgur.com/xaRGDXr.png" alt=""></p>
<pre><code>8.创建log目录：
    xcall.sh &quot;mkdir /home/centos/zookeeper/log&quot;
</code></pre><h2 id="部署细节"><a href="#部署细节" class="headerlink" title="部署细节"></a>部署细节</h2><pre><code>1.在jn节点分别启动jn进程
    $&gt;hadoop-daemon.sh start journalnode

2.启动jn之后，在两个NN之间进行disk元数据同步
    a)如果是全新集群，先format文件系统,只需要在一个nn上执行。
        [s201]
        $&gt;hadoop namenode -format

    b)如果将非HA集群转换成HA集群，复制原NN的metadata到另一个nn.
        1.步骤一
            [s201]
            $&gt;scp -r /home/centos/hadoop/dfs centos@s206:/home/centos/hadoop/

        2.步骤二
            在新的nn(未格式化的nn)上运行一下命令，实现待命状态引导。
            [s206]
            $&gt;hdfs namenode -bootstrapStandby        //需要s201为启动状态,提示是否格式化,选择N.

        3)在一个NN上执行以下命令，完成edit日志到jn节点的传输。
            $&gt;hdfs namenode -initializeSharedEdits
            #查看s202,s203是否有edit数据.

        4)启动所有节点.
            [s201]
            $&gt;hadoop-daemon.sh start namenode        //启动名称节点
            $&gt;hadoop-daemons.sh start datanode        //启动所有数据节点

            [s206]
            $&gt;hadoop-daemon.sh start namenode        //启动名称节点
</code></pre><h2 id="HA管理"><a href="#HA管理" class="headerlink" title="HA管理"></a>HA管理</h2><pre><code>$&gt;hdfs haadmin -transitionToActive nn1                //切成激活态
$&gt;hdfs haadmin -transitionToStandby nn1                //切成待命态
$&gt;hdfs haadmin -transitionToActive --forceactive nn2//强行激活
$&gt;hdfs haadmin -failover nn1 nn2                    //模拟容灾演示,从nn1切换到nn2
</code></pre><h2 id="完全0开始部署hadoop-HDFS的HA集群，使用zk实现自动容灾"><a href="#完全0开始部署hadoop-HDFS的HA集群，使用zk实现自动容灾" class="headerlink" title="完全0开始部署hadoop HDFS的HA集群，使用zk实现自动容灾"></a>完全0开始部署hadoop HDFS的HA集群，使用zk实现自动容灾</h2><pre><code>1.停掉hadoop的所有进程

2.删除所有节点的日志和本地数据.
    删除/home/centos/hadoop下的所有和
        /home/centos/journal下的所有

3.改换hadoop符号连接为ha

4.登录每台JN节点主机，启动JN进程.
    [s202-s204]
    $&gt;hadoop-daemon.sh start journalnode

5.登录其中一个NN,格式化文件系统(s201)
    $&gt;hadoop namenode -format

6.复制201目录的下nn的元数据到s206
    $&gt;scp -r ~/hadoop/* centos@s206:/home/centos/hadoop

7.在未格式化的NN(s206)节点上做standby引导.
    7.1)需要保证201的NN启动
        $&gt;hadoop-daemon.sh start namenode

    7.2)登录到s206节点，做standby引导.
        $&gt;hdfs namenode -bootstrapStandby

    7.3)登录201，将s201的edit日志初始化到JN节点。
        $&gt;hdfs namenode -initializeSharedEdits

8.启动所有数据节点.
    $&gt;hadoop-daemons.sh start datanode

9.登录到206,启动NN
    $&gt;hadoop-daemon.sh start namenode

10.查看webui
    http://s201:50070/
    http://s206:50070/

11.自动容灾
    11.1)介绍
        自动容灾引入两个组件，zk quarum + zk容灾控制器(ZKFC)。



        运行NN的主机还要运行ZKFC进程，主要负责:
        a.健康监控
        b.session管理
        c.选举
    11.2部署容灾
        a.停止所有进程
            $&gt;stop-all.sh

        b.配置hdfs-site.xml，启用自动容灾.
            [hdfs-site.xml]
                &lt;property&gt;
                    &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;
                    &lt;value&gt;true&lt;/value&gt;
                &lt;/property&gt;

        c.配置core-site.xml，指定zk的连接地址.
            &lt;property&gt;
                &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;
                &lt;value&gt;s201:2181,s202:2181,s203:2181&lt;/value&gt;
            &lt;/property&gt;

        d.分发以上两个文件到所有节点。

12.登录其中的一台NN(s201),在ZK中初始化HA状态
    $&gt;hdfs zkfc -formatZK

13.启动hdfs进程.
    $&gt;start-dfs.sh

14.测试自动容在(206是活跃节点)
    $&gt;kill -9
</code></pre><h2 id="配置RM的HA自动容灾"><a href="#配置RM的HA自动容灾" class="headerlink" title="配置RM的HA自动容灾"></a>配置RM的HA自动容灾</h2><pre><code>1.配置yarn-site.xml
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;
        &lt;value&gt;true&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;
        &lt;value&gt;cluster1&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;
        &lt;value&gt;rm1,rm2&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;
        &lt;value&gt;s201&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;
        &lt;value&gt;s206&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt;
        &lt;value&gt;s201:8088&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt;
        &lt;value&gt;s206:8088&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;
        &lt;value&gt;s201:2181,s202:2181,s203:2181&lt;/value&gt;
    &lt;/property&gt;    

2.使用管理命令
    //查看状态
    $&gt;yarn rmadmin -getServiceState rm1
    //切换状态到standby
    $&gt;yarn rmadmin -transitionToStandby rm1

3.启动yarn集群
    $&gt;start-yarn.sh

4.hadoop没有启动两个resourcemanager,需要手动启动另外一个
    $&gt;yarn-daemon.sh start resourcemanager

5.查看webui

6.做容灾模拟.
    kill -9
</code></pre><h2 id="hive的注意事项"><a href="#hive的注意事项" class="headerlink" title="hive的注意事项"></a>hive的注意事项</h2><pre><code>如果配置hadoop HA之前，搭建了Hive的话，在HA之后，需要调整路径信息.
主要是修改mysql中的dbs,tbls等相关表。
</code></pre><h2 id="Hbase"><a href="#Hbase" class="headerlink" title="Hbase"></a>Hbase</h2><pre><code>hadoop数据库，分布式可伸缩大型数据存储。
用户对随机、实时读写数据。
十亿行 x 百万列。
版本化、非关系型数据库。
</code></pre><h2 id="Feature"><a href="#Feature" class="headerlink" title="Feature"></a>Feature</h2><pre><code>Linear and modular scalability.                    //线性模块化扩展方式。
Strictly consistent reads and writes.            //严格一致性读写
Automatic and configurable sharding of tables    //自动可配置表切割
Automatic failover support between RegionServers.    //区域服务器之间自动容在
Convenient base classes for backing Hadoop MapReduce jobs with Apache HBase tables.        //
Easy to use Java API for client access.            //java API
Block cache and Bloom Filters for real-time queries    //块缓存和布隆过滤器用于实时查询 
Query predicate push down via server side Filters    //通过服务器端过滤器实现查询预测
Thrift gateway and a REST-ful Web service that supports XML, Protobuf, and binary data encoding options    //
Extensible jruby-based (JIRB) shell                    //
Support for exporting metrics via the Hadoop metrics subsystem to files or Ganglia; or via JMX            //可视化
面向列数据库。
</code></pre><h2 id="hbase存储机制"><a href="#hbase存储机制" class="headerlink" title="hbase存储机制"></a>hbase存储机制</h2><pre><code>面向列存储，table是按row排序。
</code></pre><h2 id="搭建hbase集群"><a href="#搭建hbase集群" class="headerlink" title="搭建hbase集群"></a>搭建hbase集群</h2><pre><code>0.选择安装的主机
    s201 ~ s204
1.jdk
    略
2.hadoop
    略
3.tar 
    略
4.环境变量
    略

5.验证安装是否成功
    $&gt;hbase version

5.配置hbase模式
    5.1)本地模式
        [hbase/conf/hbase-env.sh]
        EXPORT JAVA_HOME=/soft/jdk

        [hbase/conf/hbase-site.xml]
        ...
        &lt;property&gt;
            &lt;name&gt;hbase.rootdir&lt;/name&gt;
            &lt;value&gt;file:/home/hadoop/HBase/HFiles&lt;/value&gt;
        &lt;/property&gt;

    5.2)伪分布式
        [hbase/conf/hbase-env.sh]
        EXPORT JAVA_HOME=/soft/jdk

        [hbase/conf/hbase-site.xml]
        &lt;property&gt;
            &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;
            &lt;value&gt;true&lt;/value&gt;
        &lt;/property
        &lt;property&gt;
            &lt;name&gt;hbase.rootdir&lt;/name&gt;
            &lt;value&gt;hdfs://localhost:8030/hbase&lt;/value&gt;
        &lt;/property&gt;

    5.3)完全分布式(必做)
        [hbase/conf/hbase-env.sh]
        export JAVA_HOME=/soft/jdk
        export HBASE_MANAGES_ZK=false

        [hbse-site.xml]
        &lt;!-- 使用完全分布式 --&gt;
        &lt;property&gt;
            &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;
            &lt;value&gt;true&lt;/value&gt;
        &lt;/property&gt;

        &lt;!-- 指定hbase数据在hdfs上的存放路径 --&gt;
        &lt;property&gt;
            &lt;name&gt;hbase.rootdir&lt;/name&gt;
            &lt;value&gt;hdfs://s201:8020/hbase&lt;/value&gt;
        &lt;/property&gt;
        &lt;!-- 配置zk地址 --&gt;
        &lt;property&gt;
            &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;
            &lt;value&gt;s201:2181,s202:2181,s203:2181&lt;/value&gt;
        &lt;/property&gt;
        &lt;!-- zk的本地目录 --&gt;
        &lt;property&gt;
            &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;
            &lt;value&gt;/home/centos/zookeeper&lt;/value&gt;
        &lt;/property&gt;

6.配置regionservers
    [hbase/conf/regionservers]
    s202
    s203
    s204

7.启动hbase集群(s201)
    $&gt;start-hbase.sh

8.登录hbase的webui
    http://s201:16010
</code></pre>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/touxiang.jpg"
                alt="Eric Hunn" />
            
              <p class="site-author-name" itemprop="name">Eric Hunn</p>
              <p class="site-description motion-element" itemprop="description">大数据|动漫迷|科技控</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">43</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">99</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/erichunn" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:erichunn.me@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://weibo.com/u/2944687303/home?wvr=5" target="_blank" title="Webibo">
                      
                        <i class="fa fa-fw fa-weibo"></i>Webibo</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://github.com/drakeet/" title="Drakeet" target="_blank">Drakeet</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      

      
	  
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=1311545319&auto=1&height=66"></iframe>

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Eric Hunn</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  

  

  

</body>
</html>
