<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>无心是一首歌</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://erichunn.github.io/"/>
  <updated>2018-11-09T14:39:47.344Z</updated>
  <id>http://erichunn.github.io/</id>
  
  <author>
    <name>Eric Hunn</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>心情日记——2018.11.09</title>
    <link href="http://erichunn.github.io/2018/11/09/%E5%BF%83%E6%83%85%E6%97%A5%E8%AE%B0%E2%80%94%E2%80%942018-11-09/"/>
    <id>http://erichunn.github.io/2018/11/09/心情日记——2018-11-09/</id>
    <published>2018-11-09T14:38:08.000Z</published>
    <updated>2018-11-09T14:39:47.344Z</updated>
    
    <content type="html"><![CDATA[<p>无意之间从知乎里面看见的题目是：人这一辈子为什么要努力？</p><p>泪目，与诸君共勉。</p><pre><code>奶奶五十岁才来的美国，身无分文，也没有一技之长，随身携带的只有长年辛勤劳作留下的一身病根。那是九十年代初，她去了唐人街的扎花厂上班，每天工作十个小时以上，一年工作三百六十五天。扎花按件算钱，她眼神虽然不好，却比谁扎得都快。有人弹吉他磨出了茧，有人搬砖头磨出了茧，她被针头扎出了茧。回国看我的时候，她给我带了费列罗巧克力，翘胡子薯片，Jif花生酱。她给我买好看的小西装，给我买一斤几十元的黄螺。她带我去动物园，游乐园，森林公园，带我去北京看长城，看天安门，看毛主席。“奶奶，美国比北京还好吗？”她用最朴实的语言向我描述纽约的繁华，告诉我美国好极了，一切都在等着我。知乎上经常讨论富养女孩，我有一个男孩被富养的故事。有一年，我的巧克力吃完了，薯片吃完了，花生酱吃完了，奶奶还是没有回来。那是我第一个不愿意知道的生活真相，她中风了，瘫了半边身子。奶奶移民美国时带去了三个未成年的儿子，二叔，三叔，四叔。二叔和三叔在登陆美国的第二天就打工去了，年幼的四叔读了几年书后也离开了学校。他们在亲戚家的餐馆打工，每天工作十二个小时，每周工作六天。餐馆一年营业三百六十四天，只在感恩节那天歇业。奶奶瘫痪后，叔叔们轮流回国，给我带好吃的，带我出去玩。我喜欢打乒乓球，二叔给我买了乒乓球，乒乓球拍，乒乓球桌。有一年流行四驱车，三叔给我买了一辆遥控越野车，助我碾压所有的小伙伴。四叔年少风流，出门把妹的时候总不忘带上我，要把我培养成下一代情圣。他们绝口不提在美国生存的艰辛，那是大人们的秘密，和我无关。奶奶出国五年后，爸妈也去了美国。怎一个落魄了得？夫妻俩连属于自己的房间都没有。扎花已经被时代淘汰，只有重活可以干。我爸当过屠夫，货柜工人，货运司机。细节不必赘述，无非就是 12小时 x 365天的陈词滥调。后来，他们四兄弟聚首，开了一家小超市，一家餐馆，后来又开了第二家小超市，第二家餐馆。钱赚得越来越多，老家伙们拼命工作的老毛病却没有得到丝毫缓解。爸妈出国五年后，我也来了美国，看清了生活本来的面目。我在纽约生活了几个月，带着半身不遂的奶奶看遍了世界之都的繁华。在这之前，她几乎没有走出过唐人街的范围，以前对我描绘纽约时一半是转述，一半靠想象。后来，我回到了父母的身边，结束了长达五年的骨肉分离。我爸来车站接我，把我带到了一栋小别墅前，很得意地告诉我：“这是我们家，知道你要来，刚买的。”我去过奶奶刚来美国时住过的那个阴暗破旧的小公寓楼，也去过爸妈栖身过的那个散发着霉味的地下室，我知道我在新家会有一张床，一个房间，但我没想到，我会有一个别墅，一个前院，一个后院，一整个铺垫好的未来。人这一生为什么要努力？奶奶的答案是我，爸妈的答案是我，我的答案是他们，以及把身家性命托付于我的媳妇和孩子。对于我来说，长大了，责任多了，自然而然地想要努力，不是为了赚很多很多钱，而是为了过上自己想要的生活，过上奶奶和父母不曾拥有过的生活。他们替我吃完了所有的苦，帮我走了九十九步，我自己只需再走一步，哪敢迟疑？从外曾祖父母算起，我们家族四代八十多口人已经在美国奋斗了半个多世纪。我们人人都在努力，我们一代好过一代。如果你的人生起点不高，不曾有人为你走过人生百步中的任何一步，不打紧，你尽管努力，多出来的步数不会被浪费掉，总有你在乎的人用得着，而你迟早会遇到你在乎的人。与诸君共勉。顾宇的知乎回答索引</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;无意之间从知乎里面看见的题目是：人这一辈子为什么要努力？&lt;/p&gt;
&lt;p&gt;泪目，与诸君共勉。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;奶奶五十岁才来的美国，身无分文，也没有一技之长，随身携带的只有长年辛勤劳作留下的一身病根。

那是九十年代初，她去了唐人街的扎花厂上班，每天工作十个小时
      
    
    </summary>
    
      <category term="心情记" scheme="http://erichunn.github.io/categories/%E5%BF%83%E6%83%85%E8%AE%B0/"/>
    
    
      <category term="心情记" scheme="http://erichunn.github.io/tags/%E5%BF%83%E6%83%85%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Hive第二天</title>
    <link href="http://erichunn.github.io/2018/11/08/Hive%E7%AC%AC%E4%BA%8C%E5%A4%A9/"/>
    <id>http://erichunn.github.io/2018/11/08/Hive第二天/</id>
    <published>2018-11-08T08:17:04.000Z</published>
    <updated>2018-11-09T12:15:36.875Z</updated>
    
    <content type="html"><![CDATA[<h2 id="hive"><a href="#hive" class="headerlink" title="hive"></a>hive</h2><pre><code>数据仓库,在线分析处理。HiveQL,类似sql语言。表,metadata-&gt;rdbms.hive处理的数据是hdfs.MR,聚合操作。</code></pre><h1 id="hive的2个重点一个是分区一个是桶表"><a href="#hive的2个重点一个是分区一个是桶表" class="headerlink" title="hive的2个重点一个是分区一个是桶表"></a>hive的2个重点一个是分区一个是桶表</h1><h1 id="内部表-管理表-托管表"><a href="#内部表-管理表-托管表" class="headerlink" title="内部表,管理表,托管表"></a>内部表,管理表,托管表</h1><pre><code>hive,drop ,数据也删除</code></pre><h1 id="外部表"><a href="#外部表" class="headerlink" title="外部表"></a>外部表</h1><pre><code>hive表结构。</code></pre><h1 id="分区表"><a href="#分区表" class="headerlink" title="分区表"></a>分区表</h1><pre><code>目录.where 缩小查询范围。</code></pre><h1 id="bucket表"><a href="#bucket表" class="headerlink" title="bucket表"></a>bucket表</h1><pre><code>文件。hashclustered by &apos;&apos;</code></pre><h1 id="join"><a href="#join" class="headerlink" title="join"></a>join</h1><pre><code>横向连接，也就是作外连接和右外连接</code></pre><h1 id="union"><a href="#union" class="headerlink" title="union"></a>union</h1><pre><code>竖向连接。只能指定相匹配的字段select id ,name from a1 union select id ,cid from a2;</code></pre><h1 id="hive-union操作"><a href="#hive-union操作" class="headerlink" title="hive union操作"></a>hive union操作</h1><pre><code>select id,name from customers union select id,orderno from orders ;$&gt;hive                            //hive --service cli $&gt;hive --servic hiveserver2        //启动hiveserver2，10000 [thriftServer]$&gt;hive --service beeline        //beeline    </code></pre><h1 id="hive使用jdbc协议实现远程访问"><a href="#hive使用jdbc协议实现远程访问" class="headerlink" title="hive使用jdbc协议实现远程访问"></a>hive使用jdbc协议实现远程访问</h1><h1 id="hive-1"><a href="#hive-1" class="headerlink" title="hive"></a>hive</h1><pre><code>$hive&gt;CREATE TABLE t3(id int,name string,age int) PARTITIONED BY (Year INT, Month INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;</code></pre><h1 id="export"><a href="#export" class="headerlink" title="export"></a>export</h1><pre><code>$hive&gt;EXPORT TABLE customers TO &apos;/user/centos/tmp.txt&apos;;        //导出表结构+数据到hdfs目录。看一下导出来的东西，是一个目录，包括表结构和表内容。</code></pre><p><img src="https://i.imgur.com/pFsQ5fZ.png" alt=""></p><h1 id="order全排序"><a href="#order全排序" class="headerlink" title="/order全排序"></a>/order全排序</h1><pre><code>$hive&gt;select * from orders order by id asc ;</code></pre><h1 id="sort-map端排序-本地有序。"><a href="#sort-map端排序-本地有序。" class="headerlink" title="sort,map端排序,本地有序。"></a>sort,map端排序,本地有序。</h1><pre><code>$hive&gt;select * from orders sort by id asc ;</code></pre><h1 id="distribute-by-amp-cluster-by-amp-sort-by"><a href="#distribute-by-amp-cluster-by-amp-sort-by" class="headerlink" title="distribute by &amp; cluster by  &amp; sort by"></a>distribute by &amp; cluster by  &amp; sort by</h1><p>类似于mysql的group by,进行分区操作。</p><pre><code>//select cid , ... from orders distribute by cid sort by name ;            //注意顺序.$hive&gt;select id,orderno,cid from orders distribute by cid sort by cid desc ;//cluster by ===&gt;  distribute by cid sort by cid</code></pre><p><strong>destribut by 和cluster by图解对比：</strong></p><p><img src="https://i.imgur.com/BQ7NHQU.png" alt=""></p><p>这个地方没有细讲，说主要应用还是group by 只要记住这个cluster是dirstribute by 和sort by 的组合极客</p><h2 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h2><pre><code>mysql&gt;select concat(&apos;tom&apos;,1000) ;$hive&gt;select current_database(),current_user() ;$hive&gt;tab                                //查看帮助</code></pre><h2 id="设置作业参数"><a href="#设置作业参数" class="headerlink" title="设置作业参数"></a>设置作业参数</h2><pre><code>$hive&gt;set hive.exec.reducers.bytes.per.reducer=xxx            //设置reducetask的字节数。$hive&gt;set hive.exec.reducers.max=0                            //设置reduce task的最大任务数$hive&gt;set mapreduce.job.reduces=0                            //设置reducetask个数。</code></pre><h2 id="动态分区-严格模式-非严格模式"><a href="#动态分区-严格模式-非严格模式" class="headerlink" title="动态分区-严格模式-非严格模式"></a>动态分区-严格模式-非严格模式</h2><pre><code>动态分区模式:strict-严格模式，插入时至少指定一个静态分区，nonstrict-非严格模式-可以不指定静态分区。set hive.exec.dynamic.partition.mode=nonstrict            //设置非严格模式$hive&gt;INSERT [OVERWRITE] into TABLE employees PARTITION (country, state) SELECT ..., se.cnty, se.st FROM staged_employees se WHERE se.cnty = &apos;US&apos;;</code></pre><p><img src="https://i.imgur.com/8zgzuuO.png" alt=""></p><p>本来有一个已经分区好了的表，然后再建立一个表，比如也是相同的字段，然后新表没有分区要插进去老表，让他自动分区，就要用到动态分区，动态分区分为严格模式和非严格模式。如下图先使用了严格模式的情况，出错，说要至少指定一个分区也就是</p><pre><code>$hive&gt;INSERT [OVERWRITE] into TABLE employees PARTITION (country=&apos;US&apos;, state) SELECT ..., se.cnty, se.st FROM staged_employees;</code></pre><p>但是严格模式还是要制定这个分区，所以用一下非严格模式先设置一下：</p><pre><code>set hive.exec.dynamic.partition.mode=nonstrict            //设置非严格模式</code></pre><p>然后我们开始插入：</p><pre><code>$hive&gt;INSERT [OVERWRITE] into TABLE employees PARTITION (country, state) SELECT ..., se.cnty, se.st FROM staged_employees;</code></pre><p>这样就实现了非严格模式下的动态分区</p><h1 id="hive事务处理在-gt-0-13-0之后支持行级事务。-（这个地方老师没讲清楚自己没搞出来）"><a href="#hive事务处理在-gt-0-13-0之后支持行级事务。-（这个地方老师没讲清楚自己没搞出来）" class="headerlink" title="hive事务处理在&gt;0.13.0之后支持行级事务。 #（这个地方老师没讲清楚自己没搞出来）"></a>hive事务处理在&gt;0.13.0之后支持行级事务。 #（这个地方老师没讲清楚自己没搞出来）</h1><pre><code>1.所有事务自动提交。2.只支持orc格式。3.使用bucket表。4.配置hive参数，使其支持事务。</code></pre><p>$hive&gt;SET hive.support.concurrency = true;<br>$hive&gt;SET hive.enforce.bucketing = true;<br>$hive&gt;SET hive.exec.dynamic.partition.mode = nonstrict;<br>$hive&gt;SET hive.txn.manager = org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;<br>$hive&gt;SET hive.compactor.initiator.on = true;<br>$hive&gt;SET hive.compactor.worker.threads = 1;</p><pre><code>5.使用事务性操作    $&gt;CREATE TABLE tx(id int,name string,age int) CLUSTERED BY (id) INTO 3 BUCKETS ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; stored as orc TBLPROPERTIES (&apos;transactional&apos;=&apos;true&apos;);</code></pre><p><img src="https://i.imgur.com/UWJd7M5.png" alt=""></p><hr><p>mysql支持这样的操作，左边选择的查询的列，并不出现在右边的分组当中这样可以查询，但是hive中不允许这样。</p><p><img src="https://i.imgur.com/vpfbmjD.png" alt=""></p><p>但是在hive当中只可以如下图查询：</p><p><img src="https://i.imgur.com/oOqn35d.png" alt=""></p><h2 id="聚合处理"><a href="#聚合处理" class="headerlink" title="聚合处理"></a>聚合处理</h2><pre><code>每个用户购买商品订单大于1的：这个按照cid分组的，查询每个分组里面的count(*)，也就是每个不同的cid 的$hive&gt;select cid,count(*) c ,max(price) from orders group by cid having c &gt; 1 ; </code></pre><h1 id="wordcount"><a href="#wordcount" class="headerlink" title="wordcount"></a>wordcount</h1><p>创建新表:stats(word string,c int) ;<br>    将查询结果插入到指定表中。</p><p><img src="https://i.imgur.com/wxA99my.png" alt=""></p><p>按照下图这个切割方法，用一个split的函数切割，切割之后形成一个数组类型的东西</p><p><img src="https://i.imgur.com/NVQuyss.png" alt=""></p><p>explode这个函数传入的需要是一个经过split切割之后的数组，传入之后再去展开得到如上图所示的东西。但是切出来还是需要聚合，查询每个单词的个数</p><p>对下面这个SQL语句进行讲解。先看括号里面的最里面的括号，select explode(split(line,’ ‘)) as word from doc这句话都能理解，就是先切割成数组，然后对这个数据进行explode炸裂成单词，然后as word就是别名叫word，然后再把整个查询的as t别名t，然后select t.word,count(*) c from …group by t.word order by c desc limit2;就是对t的word字段进行分组，对分组的t的字段word和计数总和进行查询，然后按照降序排序，然后limit 2取最高的2个数。</p><pre><code>$hive&gt;select t.word,count(*) c from ((select explode(split(line, &apos; &apos;)) as word from doc) as t) group by t.word order by c desc limit 2 ;</code></pre><h1 id="view-视图-虚表"><a href="#view-视图-虚表" class="headerlink" title="view:视图,虚表"></a>view:视图,虚表</h1><p>是一个虚拟的表，类似于对这个语句的封装，或者说起了一个别名，并不是真的表。</p><p>这个地方创建视图的时候不能</p><pre><code>$hive&gt;create view v1 as select a.*,b.*from customers a left outer join default.tt b on a.id = b.cid ;</code></pre><p>上图这样是错误的，因为在a和b里面有相同的字段，要把相同的字段修改为不同的字段，如下图：</p><pre><code>//创建视图$hive&gt;create view v1 as select a.id aid,a.name ,b.id bid , b.order from customers a left outer join default.tt b on a.id = b.cid ;</code></pre><p>可以在续表的基础上在查询，如下所示：</p><pre><code>//查看视图$hive&gt;show tables ;$hive&gt;select * from v1 ;</code></pre><p>view这个在hdfs上没有哦，而是存放在了mysql里面</p><h2 id="一个mysql里面的操作：-G-行转列"><a href="#一个mysql里面的操作：-G-行转列" class="headerlink" title="一个mysql里面的操作：\G 行转列"></a>一个mysql里面的操作：\G 行转列</h2><p>select * from tbls \G：结果如下：</p><p><img src="https://i.imgur.com/AbE1mvx.png" alt=""></p><h2 id="Map端连接"><a href="#Map端连接" class="headerlink" title="Map端连接"></a>Map端连接</h2><pre><code>$hive&gt;set hive.auto.convert.join=true            //设置自动转换连接,默认开启了。//使用mapjoin连接暗示实现mapjoin$hive&gt;select /*+ mapjoin(customers) */ a.*,b.* from customers a left outer join orders b on a.id = b.cid ;</code></pre><h2 id="调优"><a href="#调优" class="headerlink" title="调优"></a>调优</h2><pre><code>1.explain    使用explain查看查询计划    hive&gt;explain [extended] select count(*) from customers ;    hive&gt;explain select t.name , count(*) from (select a.name ,b.id,b.orderno from customers a ,orders b where a.id = b.cid) t group by t.name ;    //设置limit优化测，避免全部查询.但是树上说因为是随机取样，可能会出问题。    hive&gt;set hive.limit.optimize.enable=true    //本地模式    $hive&gt;set mapred.job.tracker=local;            //    $hive&gt;set hive.exec.mode.local.auto=true    //自动本地模式,主要用于测试不用于实战    //并行执行,同时执行不存在依赖关系的阶段。??    $hive&gt;set hive.exec.parallel=true            //是自动设置好的    //严格模式,    $hive&gt;set hive.mapred.mode=strict            //手动设置之后。1.分区表必须指定分区进行查询，否则不让查询。                                                //2.order by时必须使用limit子句。                                                //3.不允许笛卡尔积.    //设置MR的数量    hive&gt; set hive.exec.reducers.bytes.per.reducer=750000000;    //设置reduce处理的字节数。    //JVM重用    $hive&gt;set mapreduce.job.jvm.numtasks=1        //-1没有限制，使用大量小文件。    //UDF    //User define function,用户自定义函数    //current_database(),current_user();    //显式所有函数    $hive&gt;show functions;    $hive&gt;select array(1,2,3) ;    //显式指定函数帮助    $hive&gt;desc function current_database();    //表生成函数,多行函数。    $hive&gt;explode(str,exp);            //按照exp切割str.</code></pre><h2 id="自定义函数"><a href="#自定义函数" class="headerlink" title="自定义函数"></a>自定义函数</h2><p><img src="https://i.imgur.com/lvT1nkj.png" alt=""></p><p>上图带括号的都是函数，不带括号的是命令</p><pre><code>1.创建类，继承UDF    package com.it18zhang.hivedemo.udf;    import org.apache.hadoop.hive.ql.exec.Description;    import org.apache.hadoop.hive.ql.exec.UDF;    /**     * 自定义hive函数     */    @Description(name = &quot;myadd&quot;,            value = &quot;myadd(int a , int b) ==&gt; return a + b &quot;,            extended = &quot;Example:\n&quot;                    + &quot; myadd(1,1) ==&gt; 2 \n&quot;                    + &quot; myadd(1,2,3) ==&gt; 6;&quot;)    public class AddUDF extends UDF {        public int evaluate(int a ,int b) {            return a + b ;        }        public int evaluate(int a ,int b , int c) {            return a + b + c;        }    }2.打成jar包。    cmd&gt;cd {classes所在目录}    cmd&gt;jar cvf HiveDemo.jar -C x/x/x/x/classes/ .3.添加jar包到hive的类路径    //添加jar到类路径    $&gt;cp /mnt/hgfs/downloads/bigdata/data/HiveDemo.jar /soft/hive/lib3.重进入hive    $&gt;....4.创建临时函数    //    CREATE TEMPORARY FUNCTION myadd AS &apos;com.it18zhang.hivedemo.udf.AddUDF&apos;;5.在查询中使用自定义函数    $hive&gt;select myadd(1,2)  ;6.定义日期函数    1)定义类    public class ToCharUDF extends UDF {        /**         * 取出服务器的当前系统时间 2017/3/21 16:53:55         */        public String evaluate() {            Date date = new Date();            SimpleDateFormat sdf = new SimpleDateFormat();            sdf.applyPattern(&quot;yyyy/MM/dd hh:mm:ss&quot;);            return sdf.format(date) ;        }        public String evaluate(Date date) {            SimpleDateFormat sdf = new SimpleDateFormat();            sdf.applyPattern(&quot;yyyy/MM/dd hh:mm:ss&quot;);            return sdf.format(date) ;        }        public String evaluate(Date date,String frt) {            SimpleDateFormat sdf = new SimpleDateFormat();            sdf.applyPattern(frt);            return sdf.format(date) ;        }    }    2)导出jar包，通过命令添加到hive的类路径(不需要重进hive)。        $hive&gt;add jar /mnt/hgfs/downloads/bigdata/data/HiveDemo-1.0-SNAPSHOT.jar    3)注册函数        $hive&gt;CREATE TEMPORARY FUNCTION to_char AS &apos;com.it18zhang.hivedemo.udf.ToCharUDF&apos;;        $hive&gt;CREATE TEMPORARY FUNCTION to_date AS &apos;com.it18zhang.hivedemo.udf.ToDateUDF&apos;;</code></pre><h2 id="定义Nvl函数"><a href="#定义Nvl函数" class="headerlink" title="定义Nvl函数"></a>定义Nvl函数</h2><pre><code>package com.it18zhang.hivedemo.udf;import org.apache.hadoop.hive.ql.exec.UDFArgumentException;import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;import org.apache.hadoop.hive.ql.metadata.HiveException;import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;import org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils;import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;/** * 自定义null值处理函数 */public class Nvl extends GenericUDF {    private GenericUDFUtils.ReturnObjectInspectorResolver returnOIResolver;    private ObjectInspector[] argumentOIs;    public ObjectInspector initialize(ObjectInspector[] arguments)            throws UDFArgumentException {        argumentOIs = arguments;        //检查参数个数        if (arguments.length != 2) {            throw new UDFArgumentLengthException(                    &quot;The operator &apos;NVL&apos; accepts 2 arguments.&quot;);        }        returnOIResolver = new GenericUDFUtils.ReturnObjectInspectorResolver(true);        //检查参数类型        if (!(returnOIResolver.update(arguments[0]) &amp;&amp; returnOIResolver                .update(arguments[1]))) {            throw new UDFArgumentTypeException(2,                    &quot;The 1st and 2nd args of function NLV should have the same type, &quot;                            + &quot;but they are different: \&quot;&quot; + arguments[0].getTypeName()                            + &quot;\&quot; and \&quot;&quot; + arguments[1].getTypeName() + &quot;\&quot;&quot;);        }        return returnOIResolver.get();    }    public Object evaluate(DeferredObject[] arguments) throws HiveException {        Object retVal = returnOIResolver.convertIfNecessary(arguments[0].get(), argumentOIs[0]);        if (retVal == null) {            retVal = returnOIResolver.convertIfNecessary(arguments[1].get(),                    argumentOIs[1]);        }        return retVal;    }    public String getDisplayString(String[] children) {        StringBuilder sb = new StringBuilder();        sb.append(&quot;if &quot;);        sb.append(children[0]);        sb.append(&quot; is null &quot;);        sb.append(&quot;returns&quot;);        sb.append(children[1]);        return sb.toString();    }}2)添加jar到类路径    ...3)注册函数    $hive&gt;CREATE TEMPORARY FUNCTION nvl AS &apos;org.apache.hadoop.hive.ql.udf.generic.GenericUDFNvl&apos;;</code></pre><h1 id="一些课上用的PPT"><a href="#一些课上用的PPT" class="headerlink" title="一些课上用的PPT"></a>一些课上用的PPT</h1><p><img src="https://i.imgur.com/6OyT9i3.png" alt=""><br><img src="https://i.imgur.com/CLZNHSV.png" alt=""><br><img src="https://i.imgur.com/Rx5Hslt.png" alt=""><br><img src="https://i.imgur.com/fb0PfG0.png" alt=""><br><img src="https://i.imgur.com/P8kwdax.png" alt=""><br><img src="https://i.imgur.com/avoDPKV.png" alt=""><br><img src="https://i.imgur.com/OwHiTIn.png" alt=""><br><img src="https://i.imgur.com/x4qXG7v.png" alt=""><br><img src="https://i.imgur.com/mIQbSR1.png" alt=""><br><img src="https://i.imgur.com/2bMpEMd.png" alt=""><br><img src="https://i.imgur.com/bSF4lMN.png" alt=""><br><img src="https://i.imgur.com/C3IMpOQ.png" alt=""><br><img src="https://i.imgur.com/6xpn8Ul.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;hive&quot;&gt;&lt;a href=&quot;#hive&quot; class=&quot;headerlink&quot; title=&quot;hive&quot;&gt;&lt;/a&gt;hive&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;数据仓库,在线分析处理。
HiveQL,类似sql语言。
表,metadata-&amp;gt;rdbms.
hiv
      
    
    </summary>
    
      <category term="Hive" scheme="http://erichunn.github.io/categories/Hive/"/>
    
      <category term="内部表" scheme="http://erichunn.github.io/categories/Hive/%E5%86%85%E9%83%A8%E8%A1%A8/"/>
    
      <category term="外部表" scheme="http://erichunn.github.io/categories/Hive/%E5%86%85%E9%83%A8%E8%A1%A8/%E5%A4%96%E9%83%A8%E8%A1%A8/"/>
    
      <category term="托管表" scheme="http://erichunn.github.io/categories/Hive/%E5%86%85%E9%83%A8%E8%A1%A8/%E5%A4%96%E9%83%A8%E8%A1%A8/%E6%89%98%E7%AE%A1%E8%A1%A8/"/>
    
      <category term="分区" scheme="http://erichunn.github.io/categories/Hive/%E5%86%85%E9%83%A8%E8%A1%A8/%E5%A4%96%E9%83%A8%E8%A1%A8/%E6%89%98%E7%AE%A1%E8%A1%A8/%E5%88%86%E5%8C%BA/"/>
    
      <category term="分桶" scheme="http://erichunn.github.io/categories/Hive/%E5%86%85%E9%83%A8%E8%A1%A8/%E5%A4%96%E9%83%A8%E8%A1%A8/%E6%89%98%E7%AE%A1%E8%A1%A8/%E5%88%86%E5%8C%BA/%E5%88%86%E6%A1%B6/"/>
    
      <category term="join" scheme="http://erichunn.github.io/categories/Hive/%E5%86%85%E9%83%A8%E8%A1%A8/%E5%A4%96%E9%83%A8%E8%A1%A8/%E6%89%98%E7%AE%A1%E8%A1%A8/%E5%88%86%E5%8C%BA/%E5%88%86%E6%A1%B6/join/"/>
    
      <category term="union" scheme="http://erichunn.github.io/categories/Hive/%E5%86%85%E9%83%A8%E8%A1%A8/%E5%A4%96%E9%83%A8%E8%A1%A8/%E6%89%98%E7%AE%A1%E8%A1%A8/%E5%88%86%E5%8C%BA/%E5%88%86%E6%A1%B6/join/union/"/>
    
      <category term="export" scheme="http://erichunn.github.io/categories/Hive/%E5%86%85%E9%83%A8%E8%A1%A8/%E5%A4%96%E9%83%A8%E8%A1%A8/%E6%89%98%E7%AE%A1%E8%A1%A8/%E5%88%86%E5%8C%BA/%E5%88%86%E6%A1%B6/join/union/export/"/>
    
      <category term="order" scheme="http://erichunn.github.io/categories/Hive/%E5%86%85%E9%83%A8%E8%A1%A8/%E5%A4%96%E9%83%A8%E8%A1%A8/%E6%89%98%E7%AE%A1%E8%A1%A8/%E5%88%86%E5%8C%BA/%E5%88%86%E6%A1%B6/join/union/export/order/"/>
    
      <category term="sort" scheme="http://erichunn.github.io/categories/Hive/%E5%86%85%E9%83%A8%E8%A1%A8/%E5%A4%96%E9%83%A8%E8%A1%A8/%E6%89%98%E7%AE%A1%E8%A1%A8/%E5%88%86%E5%8C%BA/%E5%88%86%E6%A1%B6/join/union/export/order/sort/"/>
    
      <category term="动态分区" scheme="http://erichunn.github.io/categories/Hive/%E5%86%85%E9%83%A8%E8%A1%A8/%E5%A4%96%E9%83%A8%E8%A1%A8/%E6%89%98%E7%AE%A1%E8%A1%A8/%E5%88%86%E5%8C%BA/%E5%88%86%E6%A1%B6/join/union/export/order/sort/%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA/"/>
    
    
      <category term="Hive" scheme="http://erichunn.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive第一天</title>
    <link href="http://erichunn.github.io/2018/11/06/Hive%E7%AC%AC%E4%B8%80%E5%A4%A9/"/>
    <id>http://erichunn.github.io/2018/11/06/Hive第一天/</id>
    <published>2018-11-06T08:35:55.000Z</published>
    <updated>2018-11-07T13:13:48.789Z</updated>
    
    <content type="html"><![CDATA[<h2 id="hive"><a href="#hive" class="headerlink" title="hive"></a>hive</h2><pre><code>在hadoop处理结构化数据的数据仓库。不是:    关系数据库        不是OLTP        实时查询和行级更新。</code></pre><h2 id="hive特点"><a href="#hive特点" class="headerlink" title="hive特点"></a>hive特点</h2><pre><code>hive存储数据结构(schema)在数据库中,处理的数据进入hdfs.OLAPHQL / HiveQL</code></pre><h2 id="hive安装"><a href="#hive安装" class="headerlink" title="hive安装"></a>hive安装</h2><pre><code>1.下载hive2.1-tar.gz2.tar开    $&gt;tar -xzvf hive-2.1.0.tar.gz -C /soft    //tar开    $&gt;cd /soft/hive-2.1.0                    //    $&gt;ln -s hive-2.1.0 hive                    //符号连接3.配置环境变量    [/etc/profile]    HIVE_HOME=/soft/hive    PATH=...:$HIVE_HOME/bin4.验证hive安装成功    $&gt;hive --v5.配置hive,使用win7的mysql存放hive的元数据.    a)复制mysql驱动程序到hive的lib目录下。        ...    b)配置hive-site.xml        复制hive-default.xml.template为hive-site.xml        修改连接信息为mysql链接地址，将${system:...字样替换成具体路径。        [hive/conf/hive-site.xml]        &lt;property&gt;            &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;            &lt;value&gt;root&lt;/value&gt;            &lt;description&gt;password to use against metastore database&lt;/description&gt;        &lt;/property&gt;        &lt;property&gt;            &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;            &lt;value&gt;root&lt;/value&gt;            &lt;description&gt;Username to use against metastore database&lt;/description&gt;        &lt;/property&gt;        &lt;property&gt;            &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;            &lt;value&gt;jdbc:mysql://192.168.231.1:3306/hive2&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;            &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;            &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;            &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;        &lt;/property&gt;    6)在msyql中创建存放hive信息的数据库        mysql&gt;create database hive2 ;    6)初始化hive的元数据(表结构)到mysql中。        $&gt;cd /soft/hive/bin        $&gt;schematool -dbType mysql -initSchema</code></pre><h1 id="hive命令行操作"><a href="#hive命令行操作" class="headerlink" title="hive命令行操作"></a>hive命令行操作</h1><h2 id="1-创建hive的数据库"><a href="#1-创建hive的数据库" class="headerlink" title="1.创建hive的数据库"></a>1.创建hive的数据库</h2><pre><code>$hive&gt;hive --version                //$hive&gt;hive --help                    //$hive&gt;create database mydb2 ;                //$hive&gt;show databases ;$hive&gt;use mydb2 ;$hive&gt;create table mydb2.t(id int,name string,age int);$hive&gt;drop table t ;$hive&gt;drop table mydb2.t ;$hive&gt;select * from mydb2.t ;        //查看指定库的表$hive&gt;exit ;                        //退出$&gt;hive                                //hive --service cli$&gt;hive                                //hive --service cli</code></pre><h2 id="2-通过远程jdbc方式连接到hive数据仓库"><a href="#2-通过远程jdbc方式连接到hive数据仓库" class="headerlink" title="2.通过远程jdbc方式连接到hive数据仓库"></a>2.通过远程jdbc方式连接到hive数据仓库</h2><pre><code>1.启动hiveserver2服务器，监听端口10000    $&gt;hive --service hiveserver2 &amp;2.通过beeline命令行连接到hiveserver2    $&gt;beeline                                            //进入beeline命令行(于hive --service beeline)    $beeline&gt;!help                                        //查看帮助    $beeline&gt;!quit                                        //退出    $beeline&gt;!connect jdbc:hive2://localhost:10000/mydb2//连接到hibve数据    $beeline&gt;show databases ;    $beeline&gt;use mydb2 ;    $beeline&gt;show tables;                                //显式表</code></pre><h2 id="使用Hive-jdbc驱动程序采用jdbc方式访问远程数据仓库"><a href="#使用Hive-jdbc驱动程序采用jdbc方式访问远程数据仓库" class="headerlink" title="使用Hive-jdbc驱动程序采用jdbc方式访问远程数据仓库"></a>使用Hive-jdbc驱动程序采用jdbc方式访问远程数据仓库</h2><pre><code>1.创建java模块2.引入maven3.添加hive-jdbc依赖    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;    &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;             xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;             xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;        &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;        &lt;groupId&gt;com.it18zhang&lt;/groupId&gt;        &lt;artifactId&gt;HiveDemo&lt;/artifactId&gt;        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;        &lt;dependencies&gt;            &lt;dependency&gt;                &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;                &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt;                &lt;version&gt;2.1.0&lt;/version&gt;            &lt;/dependency&gt;        &lt;/dependencies&gt;    &lt;/project&gt;4.App    package com.it18zhang.hivedemo;    import java.sql.Connection;    import java.sql.DriverManager;    import java.sql.ResultSet;    import java.sql.Statement;    /**     * 使用jdbc方式连接到hive数据仓库，数据仓库需要开启hiveserver2服务。     */    public class App {        public static void main(String[] args) throws  Exception {            Class.forName(&quot;org.apache.hive.jdbc.HiveDriver&quot;);            Connection conn = DriverManager.getConnection(&quot;jdbc:hive2://192.168.231.201:10000/mydb2&quot;);            Statement st = conn.createStatement();            ResultSet rs = st.executeQuery(&quot;select id , name ,age from t&quot;);            while(rs.next()){                System.out.println(rs.getInt(1) + &quot;,&quot; + rs.getString(2)) ;            }            rs.close();            st.close();            conn.close();        }    }</code></pre><h1 id="hive中表"><a href="#hive中表" class="headerlink" title="hive中表"></a>hive中表</h1><h2 id="1-managed-table"><a href="#1-managed-table" class="headerlink" title="1.managed table"></a>1.managed table</h2><pre><code>托管表。删除表时，数据也删除了。</code></pre><h2 id="2-external-table"><a href="#2-external-table" class="headerlink" title="2.external table"></a>2.external table</h2><pre><code>外部表。删除表时，数据不删。</code></pre><h1 id="hive命令"><a href="#hive命令" class="headerlink" title="hive命令"></a>hive命令</h1><pre><code>//创建表,external 外部表$hive&gt;CREATE external TABLE IF NOT EXISTS t2(id int,name string,age int)COMMENT &apos;xx&apos; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; STORED AS TEXTFILE ; //查看表数据$hive&gt;desc t2 ;$hive&gt;desc formatted t2 ;//加载数据到hive表$hive&gt;load data local inpath &apos;/home/centos/customers.txt&apos; into table t2 ;    //local上传文件$hive&gt;load data inpath &apos;/user/centos/customers.txt&apos; [overwrite] into table t2 ;    //移动文件//复制表mysql&gt;create table tt as select * from users ;        //携带数据和表结构mysql&gt;create table tt like users ;            //不带数据，只有表结构hive&gt;create table tt as select * from users ;    hive&gt;create table tt like users ;    //count()查询要转成mr$hive&gt;select count(*) from t2 ;$hive&gt;select id,name from t2 ;//$hive&gt;select * from t2 order by id desc ;                //MR//启用/禁用表$hive&gt;ALTER TABLE t2 ENABLE NO_DROP;    //不允许删除$hive&gt;ALTER TABLE t2 DISABLE NO_DROP;    //允许删除</code></pre><h1 id="分区表"><a href="#分区表" class="headerlink" title="分区表"></a>分区表</h1><pre><code>优化手段之一，从目录的层面控制搜索数据的范围。//创建分区表.$hive&gt;CREATE TABLE t3(id int,name string,age int) PARTITIONED BY (Year INT, Month INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;//显式表的分区信息$hive&gt;SHOW PARTITIONS t3;//添加分区,创建目录$hive&gt; alter table t3 add partition (year=2014, month=12) partition (year=2014,month=11);//删除分区hive&gt;ALTER TABLE employee_partitioned DROP IF EXISTS PARTITION (year=2014, month=11);//分区结构hive&gt;/user/hive/warehouse/mydb2.db/t3/year=2014/month=11hive&gt;/user/hive/warehouse/mydb2.db/t3/year=2014/month=12//加载数据到分区表hive&gt;load data local inpath &apos;/home/centos/customers.txt&apos; into table t3 partition(year=2014,month=11);//查询分区表hive&gt;select * from t3 where year = 2014 and month =11;</code></pre><p><img src="https://i.imgur.com/Ad6JAI7.png" alt=""></p><p>分区对应的是文件夹是目录，分桶对应的是文件，按照id分桶的意思就是按照id哈希不同的哈希进入不同的桶</p><pre><code>//创建桶表$hive&gt;CREATE TABLE t4(id int,name string,age int) CLUSTERED BY (id) INTO 3 BUCKETS ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;//加载数据不会进行分桶操作$hive&gt;load data local inpath &apos;/home/centos/customers.txt&apos; into table t4 ;//查询t3表数据插入到t4中。t3是分区表还多了年份和月份的2个字段所以要投影查询不能直接insert into t4 select * from t3;这是错的$hive&gt;insert into t4 select id,name,age from t3 ;</code></pre><p><img src="https://i.imgur.com/5Rpz8su.png" alt=""></p><pre><code>//桶表的数量如何设置?//评估数据量，保证每个桶的数据量block的2倍大小。//连接查询$hive&gt;CREATE TABLE customers(id int,name string,age int) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;$hive&gt;CREATE TABLE orders(id int,orderno string,price float,cid int) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;//加载数据到表//内连接查询hive&gt;select a.*,b.* from customers a , orders b where a.id = b.cid ;//左外hive&gt;select a.*,b.* from customers a left outer join orders b on a.id = b.cid ;hive&gt;select a.*,b.* from customers a right outer join orders b on a.id = b.cid ;hive&gt;select a.*,b.* from customers a full outer join orders b on a.id = b.cid ;//explode,炸裂,表生成函数。//使用hive实现单词统计//1.建表$hive&gt;CREATE TABLE doc(line string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;hive&quot;&gt;&lt;a href=&quot;#hive&quot; class=&quot;headerlink&quot; title=&quot;hive&quot;&gt;&lt;/a&gt;hive&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;在hadoop处理结构化数据的数据仓库。
不是:    关系数据库
        不是OLTP
     
      
    
    </summary>
    
      <category term="Hive" scheme="http://erichunn.github.io/categories/Hive/"/>
    
    
      <category term="Hive" scheme="http://erichunn.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop第十一天</title>
    <link href="http://erichunn.github.io/2018/11/01/Hadoop%E7%AC%AC%E5%8D%81%E4%B8%80%E5%A4%A9/"/>
    <id>http://erichunn.github.io/2018/11/01/Hadoop第十一天/</id>
    <published>2018-11-01T10:11:38.000Z</published>
    <updated>2018-11-01T10:17:09.635Z</updated>
    
    <content type="html"><![CDATA[<h2 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h2><pre><code>[customers.txt]1,tom,122,tom,133,tom,144,tom,15[orders.txt]1,no001,12.23,12,no001,12.23,13,no001,12.23,24,no001,12.23,25,no001,12.23,26,no001,12.23,37,no001,12.23,38,no001,12.23,39,no001,12.23,3</code></pre><h2 id="map端join"><a href="#map端join" class="headerlink" title="map端join"></a>map端join</h2><pre><code>1.创建Mapper    package com.it18zhang.hdfs.mr.mapjoin;    import org.apache.hadoop.conf.Configuration;    import org.apache.hadoop.fs.FSDataInputStream;    import org.apache.hadoop.fs.FileSystem;    import org.apache.hadoop.fs.Path;    import org.apache.hadoop.io.LongWritable;    import org.apache.hadoop.io.NullWritable;    import org.apache.hadoop.io.Text;    import org.apache.hadoop.mapreduce.Mapper;    import java.io.BufferedReader;    import java.io.IOException;    import java.io.InputStreamReader;    import java.util.HashMap;    import java.util.Map;    /**     * join操作，map端连接。     */    public class MapJoinMapper extends Mapper&lt;LongWritable,Text,Text,NullWritable&gt; {        private Map&lt;String,String&gt; allCustomers = new HashMap&lt;String,String&gt;();        //启动,初始化客户信息        protected void setup(Context context) throws IOException, InterruptedException {            try {                Configuration conf = context.getConfiguration();                FileSystem fs = FileSystem.get(conf);                FSDataInputStream fis = fs.open(new Path(&quot;file:///d:/mr/mapjoin/customers.txt&quot;));                //得到缓冲区阅读器                BufferedReader br = new BufferedReader(new InputStreamReader(fis));                String line = null ;                while((line = br.readLine()) != null){                    //得到cid                    String cid = line.substring(0,line.indexOf(&quot;,&quot;));                    allCustomers.put(cid,line);                }            } catch (Exception e) {                e.printStackTrace();            }        }        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {            //订单信息            String line = value.toString();            //提取customer id            String cid = line.substring(line.lastIndexOf(&quot;,&quot;) + 1);            //订单信息            String orderInfo = line.substring(0,line.lastIndexOf(&quot;,&quot;));            //连接customer + &quot;,&quot; + order            String customerInfo = allCustomers.get(cid);            context.write(new Text(customerInfo + &quot;,&quot; + orderInfo),NullWritable.get());        }    }2.创建App    package com.it18zhang.hdfs.mr.mapjoin;    import org.apache.hadoop.conf.Configuration;    import org.apache.hadoop.fs.Path;    import org.apache.hadoop.io.NullWritable;    import org.apache.hadoop.io.Text;    import org.apache.hadoop.mapreduce.Job;    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;    /**     *     */    public class MapJoinApp {        public static void main(String[] args) throws Exception {            Configuration conf = new Configuration();            conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;);            Job job = Job.getInstance(conf);            //设置job的各种属性            job.setJobName(&quot;MapJoinApp&quot;);                        //作业名称            job.setJarByClass(MapJoinApp.class);                 //搜索类            //添加输入路径            FileInputFormat.addInputPath(job,new Path(args[0]));            //设置输出路径            FileOutputFormat.setOutputPath(job,new Path(args[1]));            //没有reduce            job.setNumReduceTasks(0);            job.setMapperClass(MapJoinMapper.class);             //mapper类            job.setMapOutputKeyClass(Text.class);           //            job.setMapOutputValueClass(NullWritable.class);  //            job.waitForCompletion(true);        }    }</code></pre><h2 id="join端连接"><a href="#join端连接" class="headerlink" title="join端连接"></a>join端连接</h2><pre><code>1.自定义key    package com.it18zhang.hdfs.mr.mapjoin.reducejoin;    import org.apache.hadoop.io.WritableComparable;    import java.io.DataInput;    import java.io.DataOutput;    import java.io.IOException;    /**     */    public class ComboKey2 implements WritableComparable&lt;ComboKey2&gt; {        //0-customer 1-order        private int type ;        private int cid ;        private int oid ;        private String customerInfo = &quot;&quot; ;        private String orderInfo = &quot;&quot; ;        public int compareTo(ComboKey2 o) {            int type0 = o.type ;            int cid0= o.cid;            int oid0 = o.oid;            String customerInfo0 = o.customerInfo;            String orderInfo0 = o.orderInfo ;            //是否同一个customer的数据            if(cid == cid0){                //同一个客户的两个订单                if(type == type0){                    return oid - oid0 ;                }                //一个Customer + 他的order                else{                    if(type ==0)                        return -1 ;                    else                        return 1 ;                }            }            //cid不同            else{                return cid - cid0 ;            }        }        public void write(DataOutput out) throws IOException {            out.writeInt(type);            out.writeInt(cid);            out.writeInt(oid);            out.writeUTF(customerInfo);            out.writeUTF(orderInfo);        }        public void readFields(DataInput in) throws IOException {            this.type = in.readInt();            this.cid = in.readInt();            this.oid = in.readInt();            this.customerInfo = in.readUTF();            this.orderInfo = in.readUTF();        }    }2.自定义分区类    public class CIDPartitioner extends Partitioner&lt;ComboKey2,NullWritable&gt;{        public int getPartition(ComboKey2 key, NullWritable nullWritable, int numPartitions) {            return key.getCid() % numPartitions;        }    }3.创建Mapper    package com.it18zhang.hdfs.mr.mapjoin.reducejoin;    import org.apache.hadoop.io.LongWritable;    import org.apache.hadoop.io.NullWritable;    import org.apache.hadoop.io.Text;    import org.apache.hadoop.mapreduce.InputSplit;    import org.apache.hadoop.mapreduce.Mapper;    import org.apache.hadoop.mapreduce.lib.input.FileSplit;    import java.io.IOException;    /**     * mapper     */    public class ReduceJoinMapper extends Mapper&lt;LongWritable,Text,ComboKey2,NullWritable&gt; {        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {            //            String line = value.toString() ;            //判断是customer还是order            FileSplit split = (FileSplit)context.getInputSplit();            String path = split.getPath().toString();            //客户信息            ComboKey2 key2 = new ComboKey2();            if(path.contains(&quot;customers&quot;)){                String cid = line.substring(0,line.indexOf(&quot;,&quot;));                String custInfo = line ;                key2.setType(0);                key2.setCid(Integer.parseInt(cid));                key2.setCustomerInfo(custInfo);            }            //order info            else{                String cid = line.substring(line.lastIndexOf(&quot;,&quot;) + 1);                String oid = line.substring(0, line.indexOf(&quot;,&quot;));                String oinfo = line.substring(0, line.lastIndexOf(&quot;,&quot;));                key2.setType(1);                key2.setCid(Integer.parseInt(cid));                key2.setOid(Integer.parseInt(oid));                key2.setOrderInfo(oinfo);            }            context.write(key2,NullWritable.get());        }    }4.创建Reducer    package com.it18zhang.hdfs.mr.mapjoin.reducejoin;    import org.apache.hadoop.io.NullWritable;    import org.apache.hadoop.io.Text;    import org.apache.hadoop.mapreduce.Reducer;    import java.io.IOException;    import java.util.Iterator;    /**     * ReduceJoinReducer,reducer端连接实现。     */    public class ReduceJoinReducer extends Reducer&lt;ComboKey2,NullWritable,Text,NullWritable&gt; {        protected void reduce(ComboKey2 key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException {            Iterator&lt;NullWritable&gt; it = values.iterator();            it.next();            int type = key.getType();            int cid = key.getCid() ;            String cinfo = key.getCustomerInfo() ;            while(it.hasNext()){                it.next();                String oinfo = key.getOrderInfo();                context.write(new Text(cinfo + &quot;,&quot; + oinfo),NullWritable.get());            }        }    }5.创建排序对比器    package com.it18zhang.hdfs.mr.mapjoin.reducejoin;    import com.it18zhang.hdfs.maxtemp.allsort.secondarysort.ComboKey;    import org.apache.hadoop.io.WritableComparable;    import org.apache.hadoop.io.WritableComparator;    /**     * 组合Key排序对比器     */    public class ComboKey2Comparator extends WritableComparator {        protected ComboKey2Comparator() {            super(ComboKey2.class, true);        }        public int compare(WritableComparable a, WritableComparable b) {            ComboKey2 k1 = (ComboKey2) a;            ComboKey2 k2 = (ComboKey2) b;            return k1.compareTo(k2);        }    }6.分组对比器    package com.it18zhang.hdfs.mr.mapjoin.reducejoin;    import com.it18zhang.hdfs.maxtemp.allsort.secondarysort.ComboKey;    import org.apache.hadoop.io.WritableComparable;    import org.apache.hadoop.io.WritableComparator;    /**     * CID分组对比器     */    public class CIDGroupComparator extends WritableComparator{        protected CIDGroupComparator() {            super(ComboKey2.class, true);        }        public int compare(WritableComparable a, WritableComparable b) {            ComboKey2 k1 = (ComboKey2) a;            ComboKey2 k2 = (ComboKey2) b;            return k1.getCid() - k2.getCid();        }    }7.App    package com.it18zhang.hdfs.mr.mapjoin.reducejoin;    import com.it18zhang.hdfs.maxtemp.allsort.secondarysort.*;    import org.apache.hadoop.conf.Configuration;    import org.apache.hadoop.fs.Path;    import org.apache.hadoop.io.IntWritable;    import org.apache.hadoop.io.NullWritable;    import org.apache.hadoop.io.Text;    import org.apache.hadoop.mapreduce.Job;    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;    import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;    /**     *     */    public class ReduceJoinApp {        public static void main(String[] args) throws Exception {            Configuration conf = new Configuration();            conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;);            Job job = Job.getInstance(conf);            //设置job的各种属性            job.setJobName(&quot;ReduceJoinApp&quot;);                        //作业名称            job.setJarByClass(ReduceJoinApp.class);                 //搜索类            //添加输入路径            FileInputFormat.addInputPath(job,new Path(&quot;D:\\mr\\reducejoin&quot;));            //设置输出路径            FileOutputFormat.setOutputPath(job,new Path(&quot;D:\\mr\\reducejoin\\out&quot;));            job.setMapperClass(ReduceJoinMapper.class);             //mapper类            job.setReducerClass(ReduceJoinReducer.class);           //reducer类            //设置Map输出类型            job.setMapOutputKeyClass(ComboKey2.class);            //            job.setMapOutputValueClass(NullWritable.class);      //            //设置ReduceOutput类型            job.setOutputKeyClass(Text.class);            job.setOutputValueClass(NullWritable.class);         //            //设置分区类            job.setPartitionerClass(CIDPartitioner.class);            //设置分组对比器            job.setGroupingComparatorClass(CIDGroupComparator.class);            //设置排序对比器            job.setSortComparatorClass(ComboKey2Comparator.class);            job.setNumReduceTasks(2);                           //reduce个数            job.waitForCompletion(true);        }    }</code></pre><h2 id="hive"><a href="#hive" class="headerlink" title="hive"></a>hive</h2><pre><code>在hadoop处理结构化数据的数据仓库。不是:    关系数据库        不是OLTP        实时查询和行级更新。</code></pre><h2 id="hive特点"><a href="#hive特点" class="headerlink" title="hive特点"></a>hive特点</h2><pre><code>hive存储数据结构(schema)在数据库中,处理的数据进入hdfs.OLAPHQL / HiveQL</code></pre><h2 id="hive安装"><a href="#hive安装" class="headerlink" title="hive安装"></a>hive安装</h2><pre><code>1.下载hive2.1-tar.gz2.tar开    $&gt;tar -xzvf hive-2.1.0.tar.gz -C /soft    //tar开    $&gt;cd /soft/hive-2.1.0                    //    $&gt;ln -s hive-2.1.0 hive                    //符号连接3.配置环境变量    [/etc/profile]    HIVE_HOME=/soft/hive    PATH=...:$HIVE_HOME/bin4.验证hive安装成功    $&gt;hive --v5.配置hive,使用win7的mysql存放hive的元数据.    a)复制mysql驱动程序到hive的lib目录下。        ...    b)配置hive-site.xml        复制hive-default.xml.template为hive-site.xml        修改连接信息为mysql链接地址，将${system:...字样替换成具体路径。        [hive/conf/hive-site.xml]        &lt;property&gt;            &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;            &lt;value&gt;root&lt;/value&gt;            &lt;description&gt;password to use against metastore database&lt;/description&gt;        &lt;/property&gt;        &lt;property&gt;            &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;            &lt;value&gt;root&lt;/value&gt;            &lt;description&gt;Username to use against metastore database&lt;/description&gt;        &lt;/property&gt;        &lt;property&gt;            &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;            &lt;value&gt;jdbc:mysql://192.168.231.1:3306/hive2&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;            &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;            &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;            &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;        &lt;/property&gt;    6)在msyql中创建存放hive信息的数据库        mysql&gt;create database hive2 ;    6)初始化hive的元数据(表结构)到mysql中。        $&gt;cd /soft/hive/bin        $&gt;schematool -dbType mysql -initSchema</code></pre><h2 id="hive命令行操作"><a href="#hive命令行操作" class="headerlink" title="hive命令行操作"></a>hive命令行操作</h2><pre><code>1.创建hive的数据库    $hive&gt;hive --version                //    $hive&gt;hive --help                    //    $hive&gt;create database mydb2 ;                //    $hive&gt;show databases ;    $hive&gt;use mydb2 ;    $hive&gt;create table mydb2.t(id int,name string,age int);    $hive&gt;drop table t ;    $hive&gt;drop table mydb2.t ;    $hive&gt;select * from mydb2.t ;        //查看指定库的表    $hive&gt;exit ;                        //退出    $&gt;hive                                //hive --service cli    $&gt;hive                                //hive --service cli</code></pre><h2 id="通过远程jdbc方式连接到hive数据仓库"><a href="#通过远程jdbc方式连接到hive数据仓库" class="headerlink" title="通过远程jdbc方式连接到hive数据仓库"></a>通过远程jdbc方式连接到hive数据仓库</h2><pre><code>1.启动hiveserver2服务器，监听端口10000    $&gt;hive --service hiveserver2 &amp;2.通过beeline命令行连接到hiveserver2    $&gt;beeline                                            //进入beeline命令行(于hive --service beeline)    $beeline&gt;!help                                        //查看帮助    $beeline&gt;!quit                                        //退出    $beeline&gt;!connect jdbc:hive2://localhost:10000/mydb2//连接到hibve数据    $beeline&gt;show databases ;    $beeline&gt;use mydb2 ;    $beeline&gt;show tables;                                //显式表</code></pre><h2 id="使用Hive-jdbc驱动程序采用jdbc方式访问远程数据仓库"><a href="#使用Hive-jdbc驱动程序采用jdbc方式访问远程数据仓库" class="headerlink" title="使用Hive-jdbc驱动程序采用jdbc方式访问远程数据仓库"></a>使用Hive-jdbc驱动程序采用jdbc方式访问远程数据仓库</h2><pre><code>1.创建java模块2.引入maven3.添加hive-jdbc依赖    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;    &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;             xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;             xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;        &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;        &lt;groupId&gt;com.it18zhang&lt;/groupId&gt;        &lt;artifactId&gt;HiveDemo&lt;/artifactId&gt;        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;        &lt;dependencies&gt;            &lt;dependency&gt;                &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;                &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt;                &lt;version&gt;2.1.0&lt;/version&gt;            &lt;/dependency&gt;        &lt;/dependencies&gt;    &lt;/project&gt;4.App    package com.it18zhang.hivedemo;    import java.sql.Connection;    import java.sql.DriverManager;    import java.sql.ResultSet;    import java.sql.Statement;    /**     * 使用jdbc方式连接到hive数据仓库，数据仓库需要开启hiveserver2服务。     */    public class App {        public static void main(String[] args) throws  Exception {            Class.forName(&quot;org.apache.hive.jdbc.HiveDriver&quot;);            Connection conn = DriverManager.getConnection(&quot;jdbc:hive2://192.168.231.201:10000/mydb2&quot;);            Statement st = conn.createStatement();            ResultSet rs = st.executeQuery(&quot;select id , name ,age from t&quot;);            while(rs.next()){                System.out.println(rs.getInt(1) + &quot;,&quot; + rs.getString(2)) ;            }            rs.close();            st.close();            conn.close();        }    }</code></pre><h2 id="hive中表"><a href="#hive中表" class="headerlink" title="hive中表"></a>hive中表</h2><pre><code>1.managed table    托管表。    删除表时，数据也删除了。2.external table    外部表。    删除表时，数据不删。</code></pre><h2 id="hive命令"><a href="#hive命令" class="headerlink" title="hive命令"></a>hive命令</h2><pre><code>//创建表,external 外部表$hive&gt;CREATE external TABLE IF NOT EXISTS t2(id int,name string,age int)COMMENT &apos;xx&apos; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; STORED AS TEXTFILE ; //查看表数据$hive&gt;desc t2 ;$hive&gt;desc formatted t2 ;//加载数据到hive表$hive&gt;load data local inpath &apos;/home/centos/customers.txt&apos; into table t2 ;    //local上传文件$hive&gt;load data inpath &apos;/user/centos/customers.txt&apos; [overwrite] into table t2 ;    //移动文件//复制表mysql&gt;create table tt as select * from users ;        //携带数据和表结构mysql&gt;create table tt like users ;            //不带数据，只有表结构hive&gt;create table tt as select * from users ;    hive&gt;create table tt like users ;    //count()查询要转成mr$hive&gt;select count(*) from t2 ;$hive&gt;select id,name from t2 ;//$hive&gt;select * from t2 order by id desc ;                //MR//启用/禁用表$hive&gt;ALTER TABLE t2 ENABLE NO_DROP;    //不允许删除$hive&gt;ALTER TABLE t2 DISABLE NO_DROP;    //允许删除//分区表,优化手段之一，从目录的层面控制搜索数据的范围。//创建分区表.$hive&gt;CREATE TABLE t3(id int,name string,age int) PARTITIONED BY (Year INT, Month INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;//显式表的分区信息$hive&gt;SHOW PARTITIONS t3;//添加分区,创建目录$hive&gt;alter table t3 add partition (year=2014, month=12);//删除分区hive&gt;ALTER TABLE employee_partitioned DROP IF EXISTS PARTITION (year=2014, month=11);//分区结构hive&gt;/user/hive/warehouse/mydb2.db/t3/year=2014/month=11hive&gt;/user/hive/warehouse/mydb2.db/t3/year=2014/month=12//加载数据到分区表hive&gt;load data local inpath &apos;/home/centos/customers.txt&apos; into table t3 partition(year=2014,month=11);//创建桶表$hive&gt;CREATE TABLE t4(id int,name string,age int) CLUSTERED BY (id) INTO 3 BUCKETS ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;//加载数据不会进行分桶操作$hive&gt;load data local inpath &apos;/home/centos/customers.txt&apos; into table t4 ;//查询t3表数据插入到t4中。$hive&gt;insert into t4 select id,name,age from t3 ;//桶表的数量如何设置?//评估数据量，保证每个桶的数据量block的2倍大小。//连接查询$hive&gt;CREATE TABLE customers(id int,name string,age int) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;$hive&gt;CREATE TABLE orders(id int,orderno string,price float,cid int) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;//加载数据到表//内连接查询hive&gt;select a.*,b.* from customers a , orders b where a.id = b.cid ;//左外hive&gt;select a.*,b.* from customers a left outer join orders b on a.id = b.cid ;hive&gt;select a.*,b.* from customers a right outer join orders b on a.id = b.cid ;hive&gt;select a.*,b.* from customers a full outer join orders b on a.id = b.cid ;//explode,炸裂,表生成函数。//使用hive实现单词统计//1.建表$hive&gt;CREATE TABLE doc(line string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;数据&quot;&gt;&lt;a href=&quot;#数据&quot; class=&quot;headerlink&quot; title=&quot;数据&quot;&gt;&lt;/a&gt;数据&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;[customers.txt]
1,tom,12
2,tom,13
3,tom,14
4,tom,15

[orders.t
      
    
    </summary>
    
      <category term="Hadoop" scheme="http://erichunn.github.io/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="http://erichunn.github.io/tags/Hadoop/"/>
    
      <category term="join" scheme="http://erichunn.github.io/tags/join/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop第十天</title>
    <link href="http://erichunn.github.io/2018/10/28/Hadoop%E7%AC%AC%E5%8D%81%E5%A4%A9/"/>
    <id>http://erichunn.github.io/2018/10/28/Hadoop第十天/</id>
    <published>2018-10-28T02:31:16.000Z</published>
    <updated>2018-10-31T12:40:18.671Z</updated>
    
    <content type="html"><![CDATA[<h2 id="HA"><a href="#HA" class="headerlink" title="HA"></a>HA</h2><pre><code>3个JournalNode,edit log两个NN，active | standby2NN</code></pre><h2 id="ha的管理命令"><a href="#ha的管理命令" class="headerlink" title="ha的管理命令"></a>ha的管理命令</h2><pre><code>hdfs haadmin -getServiceState nn1        //查看服务hdfs haadmin -transitionToActive nn1    //激活hdfs haadmin -transitionToStandby nn2    //待命hdfs haadmin -failover nn1 nn2            //对调</code></pre><h2 id="数据仓库"><a href="#数据仓库" class="headerlink" title="数据仓库"></a>数据仓库</h2><pre><code>OLAP        //online analyze process,在线分析处理            //延迟性高.在于后期海量数据的查询统计hive是延迟性比较高的操作实时性不好都是批量计算</code></pre><h2 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h2><pre><code>OLTP        //online transaction process在线事务处理.            //实时性好。延迟性很低        mysql针对事务性处理保证ACI特性。        </code></pre><h2 id="HA-1"><a href="#HA-1" class="headerlink" title="HA"></a>HA</h2><pre><code>3个JournalNode,edit log两个NN，active | standby2NN</code></pre><h2 id="ha的管理命令-1"><a href="#ha的管理命令-1" class="headerlink" title="ha的管理命令"></a>ha的管理命令</h2><pre><code>hdfs haadmin -getServiceState nn1        //查看服务hdfs haadmin -transitionToActive nn1    //激活hdfs haadmin -transitionToStandby nn2    //待命hdfs haadmin -failover nn1 nn2            //对调</code></pre><h2 id="数据仓库-1"><a href="#数据仓库-1" class="headerlink" title="数据仓库"></a>数据仓库</h2><pre><code>OLAP        //online analyze process,在线分析处理            //延迟性高.</code></pre><h2 id="数据库-1"><a href="#数据库-1" class="headerlink" title="数据库"></a>数据库</h2><pre><code>OLTP        //online transaction process在线事务处理.            //实时性好。</code></pre><h2 id="jdbc"><a href="#jdbc" class="headerlink" title="jdbc"></a>jdbc</h2><pre><code>java database connection,java数据库连接。</code></pre><p>java数据库连接设计套接字编程吗：面向接口编程，依托mysql驱动程序和mysql操作， jdbc就是socket，musql就是serversocket。</p><p><img src="https://i.imgur.com/5s8oEhk.png" alt=""><br>0</p><pre><code>1.创建mysql数据库和表     create table users(id int primary key auto_increment , name varchar(20) , age int);2.idea中创建jdbcDemo模块</code></pre><h2 id="事务-transaction"><a href="#事务-transaction" class="headerlink" title="事务:transaction,"></a>事务:transaction,</h2><pre><code>和数据库之间的一组操作。特点.a        //atomic,原子性,不可分割.c        //consistent,不能破坏掉i        //isolate,隔离型.d        //durable.永久性</code></pre><h2 id="truncate"><a href="#truncate" class="headerlink" title="truncate"></a>truncate</h2><pre><code>截断表，类似于delete操作，速度快，数据无法回滚。在Mysql里面操作truncate table users ;</code></pre><h2 id="sql语句"><a href="#sql语句" class="headerlink" title="sql语句"></a>sql语句</h2><pre><code>1.2.3.</code></pre><h2 id="Transaction"><a href="#Transaction" class="headerlink" title="Transaction"></a>Transaction</h2><pre><code>commit            //提交rollback        //回滚savePoint        //保存点</code></pre><h1 id="插入10万条数据不用预处理语句"><a href="#插入10万条数据不用预处理语句" class="headerlink" title="插入10万条数据不用预处理语句"></a>插入10万条数据不用预处理语句</h1><pre><code>import org.junit.Test;import java.sql.Connection;import java.sql.DriverManager;import java.sql.Statement;//测试增删改查基本功能public class TestCRUD {    @Test    public void testStatement() throws Exception{        long start=System.currentTimeMillis();        //创建连接        String diverClass = &quot;com.mysql.jdbc.Driver&quot;;        String url = &quot;jdbc:mysql://localhost:3306/big4&quot;;        String username = &quot;root&quot;;        String password = &quot;root&quot;;        Class.forName(diverClass);        Connection conn = DriverManager.getConnection(url, username, password);        //关闭自动提交        conn.setAutoCommit(false);        //创建语句对象        Statement st = conn.createStatement();        for(int i=0;i&lt;100000;i++){            String sql = &quot;insert into users(name,age) values(&apos;tomas&quot; + i + &quot;&apos;,&quot; + (i % 100) + &quot;)&quot;;            st.execute(sql);        }        conn.commit();        st.close();        conn.close();        System.out.println(System.currentTimeMillis() - start);    }}</code></pre><h1 id="使用预处理语句："><a href="#使用预处理语句：" class="headerlink" title="使用预处理语句："></a>使用预处理语句：</h1><pre><code>import org.junit.Test;import java.sql.Connection;import java.sql.DriverManager;import java.sql.PreparedStatement;//测试增删改查基本功能public class TestCRUD {    @Test    public void testPreparedStatement() throws Exception{        long start=System.currentTimeMillis();        //创建连接        String diverClass = &quot;com.mysql.jdbc.Driver&quot;;        String url = &quot;jdbc:mysql://localhost:3306/big4&quot;;        String username = &quot;root&quot;;        String password = &quot;root&quot;;        Class.forName(diverClass);        Connection conn = DriverManager.getConnection(url, username, password);        //关闭自动提交        conn.setAutoCommit(false);        //创建语句对象        String sql = &quot;insert into users(name,age) value(?,?)&quot;;        PreparedStatement ppst = conn.prepareStatement(sql);        for(int i =0;i &lt; 10000;i++){                ppst.setString(1,&quot;tom&quot;+i);                ppst.setInt(2,i%100);                ppst.executeUpdate();//执行更新             }        conn.commit();        ppst.close();        conn.close();        System.out.println(System.currentTimeMillis() - start);    }}</code></pre><h2 id="上述代码加一个批处理："><a href="#上述代码加一个批处理：" class="headerlink" title="上述代码加一个批处理："></a>上述代码加一个批处理：</h2><pre><code>import org.junit.Test;import java.sql.Connection;import java.sql.DriverManager;import java.sql.PreparedStatement;//测试增删改查基本功能public class TestCRUD {    @Test    public void testPreparedStatement() throws Exception {        long start = System.currentTimeMillis();        //创建连接        String diverClass = &quot;com.mysql.jdbc.Driver&quot;;        String url = &quot;jdbc:mysql://localhost:3306/big4&quot;;        String username = &quot;root&quot;;        String password = &quot;root&quot;;        Class.forName(diverClass);        Connection conn = DriverManager.getConnection(url, username, password);        //关闭自动提交        conn.setAutoCommit(false);        //创建语句对象        String sql = &quot;insert into users(name,age) value(?,?)&quot;;        PreparedStatement ppst = conn.prepareStatement(sql);        for (int i = 0; i &lt; 10000; i++) {            ppst.setString(1, &quot;tom&quot; + i);            ppst.setInt(2, i % 100);            ppst.addBatch();//这个地方相当于一个缓冲区，相当于缓冲区里面放了2000个在传出去，减少了网络带宽速度会变快            if (i % 200 == 0) {                ppst.executeUpdate();//执行更新            }        }        ppst.addBatch();//最后不够2000在进行一个批处理        conn.commit();        ppst.close();        conn.close();        System.out.println(System.currentTimeMillis() - start);    }}</code></pre><p>这边的一个通过自回环传输一百万次插入，传输量巨大。<br>但是要是直接在Mysql端直接给个命令做一百万次插入，就很快。不用和服务器端交互很多的数据量。通过存储过程来写，存储过程就是一组sql语句。这些语句已经预先在数据库中存放了。无非就是循环了。要执行这些语句，就要发一条执行存储过程的指令，服务器已收到命令，就直接执行。 速度很快的。</p><p><img src="https://i.imgur.com/7TubkBX.png" alt=""></p><h1 id="100000条数据通过普通事务，预处理，批处理时间"><a href="#100000条数据通过普通事务，预处理，批处理时间" class="headerlink" title="100000条数据通过普通事务，预处理，批处理时间"></a>100000条数据通过普通事务，预处理，批处理时间</h1><pre><code>Statement                //46698PreparedStatent            //43338CallableStatement        //14385</code></pre><p><img src="https://i.imgur.com/YJmcnEF.png" alt=""></p><h1 id="mysql存储过程"><a href="#mysql存储过程" class="headerlink" title="mysql存储过程"></a>mysql存储过程</h1><p>msyql&gt;– 定义新的终止符,<strong>*</strong>不要带空格这个是注释<strong>*</strong><br>mysql&gt;delimiter //</p><p>mysql&gt;– 创建存储过程<br>mysql&gt;CREATE PROCEDURE simpleproc (OUT param1 INT)<br>        BEGIN<br>        SELECT COUNT(*) INTO param1 FROM users;        – into 是赋值方式之一<br>        END<br>        //</p><p>mysql&gt;– 查看存储过程的状态<br>mysql&gt;show procedure status //</p><p>mysql&gt;– 查看指定存储过程创建语句<br>mysql&gt;show create procedure simpleproc ;</p><p>mysql&gt;– 调用存储过程,@a在命令中定义变量<br>mysql&gt;call simpleproc(@a)</p><p>mysql&gt;– 删除存储过程<br>mysql&gt;show drop procedure simpleproc ;</p><p>mysql&gt;– 定义加法存储过程,set赋值语句 :=<br>mysql&gt;create procedure sp_add(in a int,in b int, out c int)<br>            begin<br>            set c := a + b ;<br>        end<br>        //</p><h2 id="java访问存储过程（调用的是上一步c-a-b这个过程）"><a href="#java访问存储过程（调用的是上一步c-a-b这个过程）" class="headerlink" title="java访问存储过程（调用的是上一步c=a+b这个过程）"></a>java访问存储过程（调用的是上一步c=a+b这个过程）</h2><pre><code>import org.junit.Test;import java.sql.*;/** * 测试基本操作 */public class TestCRUD {    /**     * 存储过程     */    @Test    public void testCallableStatement() throws Exception {        long start = System.currentTimeMillis();        //创建连接        String driverClass = &quot;com.mysql.jdbc.Driver&quot;;        String url = &quot;jdbc:mysql://localhost:3306/big4&quot;;        String username = &quot;root&quot;;        String password = &quot;root&quot;;        Class.forName(driverClass);        Connection conn = DriverManager.getConnection(url, username, password);        //关闭自动提交        conn.setAutoCommit(false);        //创建可调用语句，调用存储过程        CallableStatement cst = conn.prepareCall(&quot;{call sp_add(?,?,?)}&quot;);        cst.setInt(1,2);        //绑定参数        cst.setInt(2,3);        //注册输出参数类型        cst.registerOutParameter(3,Types.INTEGER);        cst.execute();        int sum = cst.getInt(3);        System.out.println(sum);        conn.commit();        conn.close();        System.out.println(System.currentTimeMillis() - start);    }}</code></pre><h2 id="百万数据插入，存储过程的性能"><a href="#百万数据插入，存储过程的性能" class="headerlink" title="百万数据插入，存储过程的性能"></a>百万数据插入，存储过程的性能</h2><pre><code>1.创建存储过程    mysql&gt;create procedure sp_batchinsert(in n int)        begin        DECLARE name0 varchar(20);    -- 定义在begin内部        DECLARE age0 int;        DECLARE i int default 0 ;            while i &lt; n do                set name0 := concat(&apos;tom&apos;,i) ;                set age0 := i % 100 ;                insert into users(name,age) values(name0,age0);                set i := i + 1 ;            end while ;        end         //2.java代码    @Test    public void testCallableStatement() throws Exception {        long start = System.currentTimeMillis();        //创建连接        String driverClass = &quot;com.mysql.jdbc.Driver&quot;;        String url = &quot;jdbc:mysql://localhost:3306/big4&quot;;        String username = &quot;root&quot;;        String password = &quot;root&quot;;        Class.forName(driverClass);        Connection conn = DriverManager.getConnection(url, username, password);        //关闭自动提交        conn.setAutoCommit(false);        //创建可调用语句，调用存储过程        CallableStatement cst = conn.prepareCall(&quot;{call sp_batchinsert(?)}&quot;);        cst.setInt(1,1000000);        //绑定参数        //注册输出参数类型        cst.execute();        conn.commit();        conn.close();        System.out.println(System.currentTimeMillis() - start);    }</code></pre><h1 id="mysql函数"><a href="#mysql函数" class="headerlink" title="mysql函数"></a>mysql函数</h1><p>这个是创建函数的SQL参考手册：</p><p><img src="https://i.imgur.com/1gvxZzF.png" alt=""></p><pre><code>1.函数和存储过程相似，只是多了返回值声明.2.创建函数    mysql&gt;create function sf_add(a int ,b int) returns int        begin            return a + b ;        end        //3.显式创建的函数    mysql&gt;show function status                --     mysql&gt;show function status like &apos;%add%&apos;    --     mysql&gt;select sf_add(1,2)                --4.java调用函数    @Test    public void testFunction() throws Exception {        long start = System.currentTimeMillis();        //创建连接        String driverClass = &quot;com.mysql.jdbc.Driver&quot;;        String url = &quot;jdbc:mysql://localhost:3306/big4&quot;;        String username = &quot;root&quot;;        String password = &quot;root&quot;;        Class.forName(driverClass);        Connection conn = DriverManager.getConnection(url, username, password);        //关闭自动提交        conn.setAutoCommit(false);        //创建可调用语句，调用存储过程        CallableStatement cst = conn.prepareCall(&quot;{? = call sf_add(?,?)}&quot;);        cst.setInt(2,100);//设置第二个参数是100第三个参数是200，第一个参数是输出。        cst.setInt(3,200);        cst.registerOutParameter(1,Types.INTEGER);        //注册输出参数类型        cst.execute();        System.out.println(cst.getInt(1));        conn.commit();        conn.close();        System.out.println(System.currentTimeMillis() - start);    }</code></pre><h2 id="MVCC"><a href="#MVCC" class="headerlink" title="MVCC"></a>MVCC</h2><pre><code>multiple version concurrent control,多版本并发控制。数据库的记录存储有多条，每一条都附加版本。所以可以实现隔离级别。是因为有MVCC多版本并发控制</code></pre><h2 id="事务的并发执行，容易出现的几个现象"><a href="#事务的并发执行，容易出现的几个现象" class="headerlink" title="事务的并发执行，容易出现的几个现象"></a>事务的并发执行，容易出现的几个现象</h2><pre><code>1.脏读    读未提交,一个事务读取了另外一个事务改写还没有提交的数据，如果另外一个    事务在稍后的时候回滚。2.不可重复读    一个事务进行相同条件查询连续的两次或者两次以上，每次结果都不同。这个是对数据进行了更改    有其他事务做了update操作。3.幻读    和(2)很像，其他事务做了insert操作.1时间查询到了10个，2时刻查询到了20个，是因为中间有做了插入操作</code></pre><h2 id="隔离级别（由于有MVCC）"><a href="#隔离级别（由于有MVCC）" class="headerlink" title="隔离级别（由于有MVCC）"></a>隔离级别（由于有MVCC）</h2><pre><code>为了避免出现哪种并发现象的。1        //read uncommitted    ,读未提交        导致脏读导致不可重复读2        //read committed    ,读已提交        这里避免了脏读4        //repeatable read    ,可以重复读        这里避免了不可重复读。还是可以让他插入更新，但是读取的是更新之前的值8        //serializable        ,串行化(悲观锁)        这里避免了换读，串行化不支持并发了就已经。</code></pre><h2 id="演示mysql事务隔离级别"><a href="#演示mysql事务隔离级别" class="headerlink" title="演示mysql事务隔离级别"></a>演示mysql事务隔离级别</h2><pre><code>1.开启mysql客户端    mysql&gt;2.关闭自动提交    mysql&gt;set autocommit 0 ;3.每次操作数据,都要开启事务，提交事务。</code></pre><h2 id="脏读现象"><a href="#脏读现象" class="headerlink" title="脏读现象"></a>脏读现象</h2><pre><code>[A]    1)mysql&gt;start transaction ;                                -- 开始事务    2)msyql&gt;update users set age = age + 1 where id = 1 ;    -- 更新数据,没有提交    6)mysql&gt;rollback ;                                        -- 回滚    7)mysql&gt;select * from users ;[B]    3)mysql&gt;set session transaction isolation level read uncommitted ;    -- 读未提交    4)msyql&gt;start transaction ;        -- 开始事务    5)mysql&gt;select * from users ;    -- 13</code></pre><h2 id="避免脏读"><a href="#避免脏读" class="headerlink" title="避免脏读"></a>避免脏读</h2><pre><code>[A]    1)mysql&gt;start transaction ;                                -- 开始事务    2)msyql&gt;update users set age = age + 1 where id = 1 ;    -- 更新数据,没有提交    6)mysql&gt;rollback ;                                        -- 回滚    7)mysql&gt;select * from users ;[B]    3)mysql&gt;set session transaction isolation level read committed ;    -- 读已提交    4)msyql&gt;start transaction ;        -- 开始事务    5)mysql&gt;select * from users ;    -- 13</code></pre><h2 id="测试不可重复读-隔离级别设置为读已提交不能避免不可重复读。"><a href="#测试不可重复读-隔离级别设置为读已提交不能避免不可重复读。" class="headerlink" title="测试不可重复读(隔离级别设置为读已提交不能避免不可重复读。)"></a>测试不可重复读(隔离级别设置为读已提交不能避免不可重复读。)</h2><pre><code>[A]    1)mysql&gt;commit ;    2)mysql&gt;set session transaction isolation level read committed ;    -- 读已提交    3)mysql&gt;start transaction ;                                            -- 开始事务    4)mysql&gt;select * from users    ;                                        -- 查询    9)mysql&gt;select * from users    ;[B]    5)mysql&gt;commit;    6)mysql&gt;start transaction ;        7)mysql&gt;update users set age = 15 where id = 1 ;                    -- 更新    8)mysql&gt;commit;</code></pre><h2 id="测试避免不可重复读-隔离级别设置为读已提交不能避免不可重复读。"><a href="#测试避免不可重复读-隔离级别设置为读已提交不能避免不可重复读。" class="headerlink" title="测试避免不可重复读(隔离级别设置为读已提交不能避免不可重复读。)"></a>测试避免不可重复读(隔离级别设置为读已提交不能避免不可重复读。)</h2><pre><code>[A]    1)mysql&gt;commit ;    2)mysql&gt;set session transaction isolation level repeatable read ;    -- 可以重复读    3)mysql&gt;start transaction ;                                            -- 开始事务    4)mysql&gt;select * from users    ;                                        -- 查询    9)mysql&gt;select * from users    ;[B]    5)mysql&gt;commit;    6)mysql&gt;start transaction ;        7)mysql&gt;update users set age = 15 where id = 1 ;                    -- 更新    8)mysql&gt;commit;</code></pre><h2 id="测试幻读-隔离级别设置为repeatable"><a href="#测试幻读-隔离级别设置为repeatable" class="headerlink" title="测试幻读(隔离级别设置为repeatable)"></a>测试幻读(隔离级别设置为repeatable)</h2><pre><code>[A]    1)mysql&gt;commit ;    2)mysql&gt;set session transaction isolation level serializable;        -- 串行化    3)mysql&gt;start transaction ;                                            -- 开始事务    4)mysql&gt;select * from users    ;                                        -- 查询    9)mysql&gt;select * from users    ;[B]    5)mysql&gt;commit;    6)mysql&gt;start transaction ;        7)mysql&gt;insert into users(name,age) values(&apos;tomas&apos;,13);                -- 更新    8)mysql&gt;commit;</code></pre><h2 id="ANSI-SQL"><a href="#ANSI-SQL" class="headerlink" title="ANSI SQL"></a>ANSI SQL</h2><pre><code>美国国家标准结构SQL组select * from users for update ;</code></pre><h2 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h2><pre><code>1.支持四种隔离级别。2.默认隔离级别是可以重复读。3.隔离级别是seriable,不支持并发写。</code></pre><h2 id="表级锁"><a href="#表级锁" class="headerlink" title="表级锁"></a>表级锁</h2><pre><code>LOCK TABLE t WRITE;    -- 加锁(表级锁,read)UNLOCK TABLES ;        -- 解除自己所有的所有表级锁表级锁只能通过命令来解锁。</code></pre><p><img src="https://i.imgur.com/cLufeUV.png" alt=""></p><h2 id="编程实现脏读现象"><a href="#编程实现脏读现象" class="headerlink" title="编程实现脏读现象"></a>编程实现脏读现象</h2><pre><code>package com.it18zhang.jdbcdemo.test;import org.junit.Test;import java.sql.*;/** * 测试隔离级别 */public class TestIsolationLevel {    /**     * 执行写，不提交     */    @Test    public void testA() throws  Exception{        //创建连接        String driverClass = &quot;com.mysql.jdbc.Driver&quot;;        String url = &quot;jdbc:mysql://localhost:3306/big4&quot;;        String username = &quot;root&quot;;        String password = &quot;root&quot;;        Class.forName(driverClass);        Connection conn = DriverManager.getConnection(url, username, password);        conn.setAutoCommit(false);        Statement st = conn.createStatement();        st.execute(&quot;update users set age = 80 where id = 1&quot;);        System.out.println(&quot;===============&quot;);        conn.commit();        conn.close();    }    /**     * 查询，查到别人没有提交的数据     */    @Test    public void testB() throws  Exception{        //创建连接        String driverClass = &quot;com.mysql.jdbc.Driver&quot;;        String url = &quot;jdbc:mysql://localhost:3306/big4&quot;;        String username = &quot;root&quot;;        String password = &quot;root&quot;;        Class.forName(driverClass);        Connection conn = DriverManager.getConnection(url, username, password);        //设置隔离级别读未提交==&gt;导致脏读        /************************** 设置隔离级别 ***************************************/        conn.setTransactionIsolation(Connection.TRANSACTION_READ_UNCOMMITTED);        conn.setAutoCommit(false);        Statement st = conn.createStatement();        ResultSet rs = st.executeQuery(&quot;select age from users where id = 1&quot;);        rs.next();        int age = rs.getInt(1) ;        System.out.println(age);        System.out.println(&quot;===============&quot;);        conn.commit();        conn.close();    }</code></pre><h2 id="共享读锁"><a href="#共享读锁" class="headerlink" title="共享读锁"></a>共享读锁</h2><h1 id="独占写锁"><a href="#独占写锁" class="headerlink" title="独占写锁"></a>独占写锁</h1><pre><code>一个事务写操作，另一个塞住。行级别的锁在独占锁这块，开启事务之后，资源存在征用的情况，如果A修改&gt;=2，b修改&lt;=3那么就出现先来先用的情况，没有提交事务之前，B一直要等待状态。然后事务是you原子性的，肯定就是非交集和交集的部分都不能更改。因为原子性。查询是由隔离级别控制。在查询的时候为了不让别人更新设置隔离级别为最高级别，就是8串行化，但是很难受，连接不关掉放到连接池里面，被下一个调用，还是串行化状态，不能写入，性能差。除了设置隔离级别，我们还可以加上一个select * from users for update。这样的话就代表仅仅是对这一个查询语句的时候执行独占写锁。这个时候不需要再设置隔离级别为8了。</code></pre><h2 id="SQL"><a href="#SQL" class="headerlink" title="SQL"></a>SQL</h2><pre><code>//insert into users(name,age,...) values(&apos;&apos;,12,..) ;                -- insertupdate users set name = &apos;xxx&apos;,age = xxx ,... where id = xxx ;    -- updatedelete from users where id = xxx                                -- delete-- 投影查询 projection.select id,name from users where ... order by limit xxx            --select-- 查询时直接上独占写锁select * from users for update ;    </code></pre><h2 id="连接查询"><a href="#连接查询" class="headerlink" title="连接查询"></a>连接查询</h2><pre><code>1.准备表[mysql.sql]drop table if exists customers; -- 删除表drop table if exists orders ;    -- 删除表create table customers(id int primary key auto_increment , name varchar(20) , age int);                    -- 创建customers表create table orders(id int primary key auto_increment , orderno varchar(20) , price float , cid int);    -- 创建orders表-- 插入数据insert into customers(name,age) values(&apos;tom&apos;,12);insert into customers(name,age) values(&apos;tomas&apos;,13);insert into customers(name,age) values(&apos;tomasLee&apos;,14);insert into customers(name,age) values(&apos;tomason&apos;,15);-- 插入订单数据insert into orders(orderno,price,cid) values(&apos;No001&apos;,12.25,1);insert into orders(orderno,price,cid) values(&apos;No002&apos;,12.30,1);insert into orders(orderno,price,cid) values(&apos;No003&apos;,12.25,2);insert into orders(orderno,price,cid) values(&apos;No004&apos;,12.25,2);insert into orders(orderno,price,cid) values(&apos;No005&apos;,12.25,2);insert into orders(orderno,price,cid) values(&apos;No006&apos;,12.25,3);insert into orders(orderno,price,cid) values(&apos;No007&apos;,12.25,3);insert into orders(orderno,price,cid) values(&apos;No008&apos;,12.25,3);insert into orders(orderno,price,cid) values(&apos;No009&apos;,12.25,3);insert into orders(orderno,price,cid) values(&apos;No0010&apos;,12.25,NULL);---执行SQL文件    source d:/SQL/mysql.sql2.查询--连接查询mysql&gt;-- 笛卡尔积查询,无连接条件查询mysql&gt;select a.*,b.* from customers a , orders b ; </code></pre><p><img src="https://i.imgur.com/4hC9GS4.png" alt=""></p><pre><code>mysql&gt;-- 内连接,查询符合条件的记录.mysql&gt;select a.*,b.* from customers a , orders b where a.id = b.cid ; </code></pre><p><img src="https://i.imgur.com/2bP8sx4.png" alt=""></p><pre><code>mysql&gt;-- 左外连接,查询符合条件的记录.mysql&gt;select a.*,b.* from customers a left outer join orders b on a.id = b.cid ; </code></pre><p><img src="https://i.imgur.com/XKWihaX.png" alt=""></p><pre><code>mysql&gt;-- 右外连接,查询符合条件的记录.mysql&gt;select a.*,b.* from customers a right outer join orders b on a.id = b.cid ; </code></pre><p><img src="https://i.imgur.com/LiR1yV0.png" alt=""></p><p><img src="https://i.imgur.com/6GdI0kM.png" alt=""></p><pre><code>mysql&gt;-- 全外连接,查询符合条件的记录(mysql不支持全外链接)mysql&gt;select a.*,b.* from customers a full outer join orders b on a.id = b.cid ; </code></pre><p>2.查询–分组</p><pre><code>             字段列表  表       条件        分组        组内条件       排序     分页mysql&gt;select ... from ... where ... group by ... having ...  order by ... limit ..mysql&gt;-- 去重查询mysql&gt;select distinct price,cid from orders ;mysql&gt;-- 条件查询mysql&gt;select price,cid from orders where price &gt; 12.27 ;mysql&gt;-- 聚集查询mysql&gt;select max(price) from orders ;mysql&gt;select min(price) from orders ;mysql&gt;select avg(price) from orders ;mysql&gt;select sum(price) from orders ;mysql&gt;select count(id) from orders ;mysql&gt;-- 分组查询mysql&gt;select max(price) from orders where cid is not null group by cid ;mysql&gt;-- 分组查询(组内过滤)mysql&gt;select cid ,orderno,max(price) as max_price,min(price) from orders where cid is not null group by cid having max_price &gt; 20 ;mysql&gt;-- 降序查询mysql&gt;select cid ,orderno,max(price) as max_price,min(price) from orders where cid is not null group by cid having max_price &gt; 20 order by max_price desc;mysql&gt;-- 模糊查询mysql&gt;select  * from customers where name like &apos;toma%&apos;        mysql&gt;select  * from customers where name not like &apos;toma%&apos;        mysql&gt;-- 范围查询mysql&gt;select  * from customers where id in (1,2,3)        mysql&gt;select  * from customers where id not in (1,2,3)        mysql&gt;-- between 1 and 10,闭区间mysql&gt;select  * from customers where id between 1 and 3 ;mysql&gt;select  * from customers where id &gt;= 1 and id &lt;= 3 ;mysql&gt;-- 嵌套子查询(查询没有订单的客户)mysql&gt;select  * from customers where id not in (select distinct cid from orders where cid is not null);mysql&gt;-- 嵌套子查询(查询订单数量&gt;2的客户)mysql&gt;select * from customers where id in (select cid from orders group by cid having count(cid) &gt; 2);mysql&gt;select * from customers where id in ( select t.cid from (select cid,count(*) as c from orders group by cid having c &gt; 2) as t);mysql&gt;--向已有表中添加列mysql&gt;--alter table orders add column area int;mysql&gt;--设置area字段的值mysql&gt;--update orders set area = 2 where id in(1,3,,6,7);mysql&gt;--设置area字段的值mysql&gt;--update orders set area = 1 where id not in(1,3,,6,7);mysql&gt;--设置area字段的值mysql&gt;--update orders set area = 1 where id not in(1,3,,6,7);稍微看一下下面这个SQL语句：mysq&gt;--select cid,area,count(price) from orders group by cid,area;//这句话的意思是对cid和area联合分组，什么意思：就是说要cid和area要都相同才能进入到同一个组里面去。mysq&gt;--select cid,area,count(price),max(price),min(price) from orders group by cid,area order by cid asc, area desc  ;//这句话的意思是对cid和area联合分组，什么意思：就是说要cid和area要都相同才能进入到同一个组里面去。然后这个cid升序area降序是在1里面对area降序，也就是说先对cid整体升序，然后对升序完之后相同的cid里面进行降序。</code></pre><p><img src="https://i.imgur.com/QbThVj2.png" alt=""></p><pre><code>mysql&gt;-- 嵌套子查询(查询客户id,客户name,订单数量,最贵订单，最便宜订单，平均订单价格 where 订单数量&gt;2的客户)mysql&gt;select a.id,a.name,b.c,b.max,b.min,b.avg       from customers a,((select cid,count(cid) c , max(price) max ,min(price) min,avg(price) avg from orders group by cid having c &gt; 2) as b)       where a.id = b.cid ;</code></pre><h2 id="hadoop"><a href="#hadoop" class="headerlink" title="hadoop"></a>hadoop</h2><pre><code>MR左外连接.</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;HA&quot;&gt;&lt;a href=&quot;#HA&quot; class=&quot;headerlink&quot; title=&quot;HA&quot;&gt;&lt;/a&gt;HA&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;3个JournalNode,edit log
两个NN，active | standby
2NN
&lt;/code&gt;&lt;/pre&gt;
      
    
    </summary>
    
      <category term="Hadoop" scheme="http://erichunn.github.io/categories/Hadoop/"/>
    
      <category term="Mysql" scheme="http://erichunn.github.io/categories/Hadoop/Mysql/"/>
    
    
      <category term="jdbc加强" scheme="http://erichunn.github.io/tags/jdbc%E5%8A%A0%E5%BC%BA/"/>
    
      <category term="事务操作" scheme="http://erichunn.github.io/tags/%E4%BA%8B%E5%8A%A1%E6%93%8D%E4%BD%9C/"/>
    
      <category term="批处理" scheme="http://erichunn.github.io/tags/%E6%89%B9%E5%A4%84%E7%90%86/"/>
    
      <category term="预处理" scheme="http://erichunn.github.io/tags/%E9%A2%84%E5%A4%84%E7%90%86/"/>
    
      <category term="自定义mysql存储过程" scheme="http://erichunn.github.io/tags/%E8%87%AA%E5%AE%9A%E4%B9%89mysql%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B/"/>
    
      <category term="mysql函数" scheme="http://erichunn.github.io/tags/mysql%E5%87%BD%E6%95%B0/"/>
    
      <category term="百万数据插入" scheme="http://erichunn.github.io/tags/%E7%99%BE%E4%B8%87%E6%95%B0%E6%8D%AE%E6%8F%92%E5%85%A5/"/>
    
      <category term="HA管理命令" scheme="http://erichunn.github.io/tags/HA%E7%AE%A1%E7%90%86%E5%91%BD%E4%BB%A4/"/>
    
      <category term="数据仓库" scheme="http://erichunn.github.io/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"/>
    
      <category term="数据库" scheme="http://erichunn.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
      <category term="mysql事务隔离级别" scheme="http://erichunn.github.io/tags/mysql%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB/"/>
    
      <category term="独占写锁" scheme="http://erichunn.github.io/tags/%E7%8B%AC%E5%8D%A0%E5%86%99%E9%94%81/"/>
    
      <category term="共享读锁" scheme="http://erichunn.github.io/tags/%E5%85%B1%E4%BA%AB%E8%AF%BB%E9%94%81/"/>
    
      <category term="脏读" scheme="http://erichunn.github.io/tags/%E8%84%8F%E8%AF%BB/"/>
    
      <category term="不可重复读" scheme="http://erichunn.github.io/tags/%E4%B8%8D%E5%8F%AF%E9%87%8D%E5%A4%8D%E8%AF%BB/"/>
    
      <category term="幻读" scheme="http://erichunn.github.io/tags/%E5%B9%BB%E8%AF%BB/"/>
    
      <category term="串行化" scheme="http://erichunn.github.io/tags/%E4%B8%B2%E8%A1%8C%E5%8C%96/"/>
    
      <category term="行级锁" scheme="http://erichunn.github.io/tags/%E8%A1%8C%E7%BA%A7%E9%94%81/"/>
    
      <category term="连接查询" scheme="http://erichunn.github.io/tags/%E8%BF%9E%E6%8E%A5%E6%9F%A5%E8%AF%A2/"/>
    
      <category term="模糊查询" scheme="http://erichunn.github.io/tags/%E6%A8%A1%E7%B3%8A%E6%9F%A5%E8%AF%A2/"/>
    
      <category term="外连接查询" scheme="http://erichunn.github.io/tags/%E5%A4%96%E8%BF%9E%E6%8E%A5%E6%9F%A5%E8%AF%A2/"/>
    
      <category term="笛卡尔积查询" scheme="http://erichunn.github.io/tags/%E7%AC%9B%E5%8D%A1%E5%B0%94%E7%A7%AF%E6%9F%A5%E8%AF%A2/"/>
    
      <category term="分组查询" scheme="http://erichunn.github.io/tags/%E5%88%86%E7%BB%84%E6%9F%A5%E8%AF%A2/"/>
    
      <category term="嵌套子查询" scheme="http://erichunn.github.io/tags/%E5%B5%8C%E5%A5%97%E5%AD%90%E6%9F%A5%E8%AF%A2/"/>
    
      <category term="条件查询" scheme="http://erichunn.github.io/tags/%E6%9D%A1%E4%BB%B6%E6%9F%A5%E8%AF%A2/"/>
    
      <category term="去重查询" scheme="http://erichunn.github.io/tags/%E5%8E%BB%E9%87%8D%E6%9F%A5%E8%AF%A2/"/>
    
      <category term="范围查询" scheme="http://erichunn.github.io/tags/%E8%8C%83%E5%9B%B4%E6%9F%A5%E8%AF%A2/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop第九天</title>
    <link href="http://erichunn.github.io/2018/10/25/Hadoop%E7%AC%AC%E4%B9%9D%E5%A4%A9/"/>
    <id>http://erichunn.github.io/2018/10/25/Hadoop第九天/</id>
    <published>2018-10-25T01:52:10.000Z</published>
    <updated>2018-10-27T09:06:13.510Z</updated>
    
    <content type="html"><![CDATA[<h1 id="复习："><a href="#复习：" class="headerlink" title="复习："></a>复习：</h1><p>1.链式job编程</p><pre><code>MR        //Mapper+ / Reduce Mapper*</code></pre><p>2.DBWritable</p><pre><code>和数据库交互。</code></pre><p>3.Sqoop</p><p>4.全排序</p><pre><code>对reduce输出的所有结果进行排序。</code></pre><p>5.二次排序</p><pre><code>对value进行排序。</code></pre><p>6.数据倾斜</p><pre><code>1.reduce2.自定义分区函数    数据结果错 + 二次job3.重新设计key    数据结果错 + 二次job</code></pre><h1 id="机架感知"><a href="#机架感知" class="headerlink" title="机架感知"></a>机架感知</h1><p>机架有一块网卡。可以连接很多主机，机架上每个刀片服务器主机都连接到这个机架网卡上的。</p><p>比如一台刀片服务器上有2个进程，2个进程之间通信需要走机架上的交换机吗。不需要直接走自回环网络即可。走Localhost即可。所以2个进程之间不需要网卡交互数据。2个进程之间的距离就是0。每个进程所在的主机节点，连接他们之间的网络通路的跃点数就是他们之间的距离</p><p>如果b中绿色进程和a中红色进程需要交互就需要通过交换机，距离就是2.怎么算的，就是从每个服务器到交换机之间的距离相加，就是1+1；</p><p>如果是2个机架也要通过一个交换机连接起来。左边机架a中绿色和右边机架红色通信。距离就是4.</p><p>同一个机房通信最多就是4。也就是在通过一个交换机。</p><p>Hadoop都有机架感知能力：是他里面的一种策略，都需人为维护。在默认情况下所有节点都在同一个机架上。/default-rack。<br><img src="https://i.imgur.com/jdRJXuc.png" alt=""><br>但是在临近节点数据量非常大的时候，也是不会使用的，就近原则只是一种策略，还有其他策略。</p><h2 id="fault-tolerance"><a href="#fault-tolerance" class="headerlink" title="fault tolerance"></a>fault tolerance</h2><pre><code>容错.针对业务。map或reduce任务失败，的这种错误。</code></pre><h2 id="fail-over"><a href="#fail-over" class="headerlink" title="fail over"></a>fail over</h2><pre><code>容灾.针对硬件故障。</code></pre><h2 id="master-slave"><a href="#master-slave" class="headerlink" title="master / slave"></a>master / slave</h2><pre><code>主(master,namenode)从(slave,datanode)结构.</code></pre><p> topology.node.switch.mapping.impl</p><h1 id="客户端请求Namenode来读取datanodes的过程"><a href="#客户端请求Namenode来读取datanodes的过程" class="headerlink" title="客户端请求Namenode来读取datanodes的过程"></a>客户端请求Namenode来读取datanodes的过程</h1><p><img src="https://i.imgur.com/XFcQNFV.png" alt=""><br>Namenode只存放文件系统的数据信息，而不存放各个节点的IP地址。存放块ID，块列表。</p><h2 id="可靠性"><a href="#可靠性" class="headerlink" title="可靠性"></a>可靠性</h2><pre><code>提供数据安全的能力。</code></pre><h2 id="可用性"><a href="#可用性" class="headerlink" title="可用性"></a>可用性</h2><pre><code>提供持续服务的能力。</code></pre><h2 id="默认的副本放置策略"><a href="#默认的副本放置策略" class="headerlink" title="默认的副本放置策略"></a>默认的副本放置策略</h2><pre><code>首选在本地机架的一个node存放副本,另一个副本在本地机架的另一个不同节点。最后一个副本在不同机架的不同节点上。</code></pre><p>hads oiv        //image data metadata.    离线镜像查看器<br>hads oev        //edit        编辑日志</p><p>镜像文件存放的是元信息，元信息包括块，块ID，块列表，是不是上下级关系，副本数，权限，块大小。除了数据内容本身内容之外的。</p><p>通过实现接口改变配置实现一个机架感知。</p><h2 id="自定义机架感知-优化hadoop集群一种方式"><a href="#自定义机架感知-优化hadoop集群一种方式" class="headerlink" title="自定义机架感知(优化hadoop集群一种方式)"></a>自定义机架感知(优化hadoop集群一种方式)</h2><pre><code>1.自定义实现类package com.it18zhang.hdfs.rackaware;import org.apache.hadoop.net.DNSToSwitchMapping;import java.io.FileWriter;import java.io.IOException;import java.util.ArrayList;import java.util.List;/*机架感知实现类吧203以下的机器设置为机架1，吧203以上的机架设置为机架2 */public class MyRackAware implements DNSToSwitchMapping {    public List&lt;String&gt; resolve(List&lt;String&gt; names) {        ArrayList&lt;String&gt; list = new ArrayList&lt;String&gt;();        //true表示是不是追加模式        FileWriter fw = null;        try {            fw = new FileWriter(&quot;/home/centos/rackaware.txt&quot;, true);            for (String str : names) {                fw.write(str + &quot;\r\n&quot;);                if (str.startsWith(&quot;192&quot;)) {                    //192.168.192.202                    String ip = str.substring(str.lastIndexOf(&quot;.&quot;) + 1);                    if (Integer.parseInt(ip) &lt;= 203) {                        list.add(&quot;/rack1/&quot; + ip);                    } else {                        list.add(&quot;/rack2&quot; + ip);                    }                } else if (str.startsWith(&quot;s&quot;)) {                    String ip = str.substring(1);                    if (Integer.parseInt(ip) &lt;= 203) {                        list.add(&quot;/rack1/&quot; + ip);                    } else {                        list.add(&quot;/rack2&quot; + ip);                    }                }            }        } catch (IOException e) {            e.printStackTrace();        }        return list;    }    public void reloadCachedMappings() {    }    public void reloadCachedMappings(List&lt;String&gt; names) {    }}2.配置core-site.xml    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;    &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;    &lt;configuration&gt;            &lt;property&gt;                    &lt;name&gt;fs.defaultFS&lt;/name&gt;                    &lt;value&gt;hdfs://192.168.231.201/&lt;/value&gt;            &lt;/property&gt;            &lt;property&gt;                    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;                    &lt;value&gt;/home/centos/hadoop&lt;/value&gt;            &lt;/property&gt;            &lt;property&gt;                    &lt;name&gt;topology.node.switch.mapping.impl&lt;/name&gt;                    &lt;value&gt;com.it18zhang.hdfs.rackaware.MyRackAware&lt;/value&gt;            &lt;/property&gt;    &lt;/configuration&gt;3.导出jar包4.复制jar到/soft/hadoop/shared/hadoop/common/lib（这个是hadoop的类路径）5.分发jar.(可以不做)    实际上不需要分发，只在名称节点上运行。6.重启名称节点    $&gt;hadoop-daemon.sh stop namenode    $&gt;hadoop-daemon.sh start namenode</code></pre><hr><p>在s202上传一个文件，最后得出来确实和副本存放策略一致：</p><pre><code>首选在本地机架的一个node存放副本,另一个副本在本地机架的另一个不同节点。最后一个副本在不同机架的不同节点上。</code></pre><p><img src="https://i.imgur.com/agPBlr6.png" alt=""></p><hr><h1 id="关于HDFS"><a href="#关于HDFS" class="headerlink" title="关于HDFS"></a>关于HDFS</h1><p>下面这一段是关于HDFS的讲解，HDFS包括namenode和datanode，名称节点和数据节点。名称节点只是保存名称信息。那么如果在HDFS中文件移动的话怎么办，就是名称节点改变目录来移动即可。1kb和1G的速度是一样的。在HDFS中移动只是改变路径而已，块根本就没变，只能重新删掉在put才是真正移动了块。这一节就是通过实验验证了这个问题。</p><p><img src="https://i.imgur.com/JozXTOc.png" alt=""></p><hr><h2 id="去IOE"><a href="#去IOE" class="headerlink" title="去IOE"></a>去IOE</h2><pre><code>IBM            //Oracle        //EMC            //</code></pre><h2 id="HA"><a href="#HA" class="headerlink" title="HA"></a>HA</h2><pre><code>1.NFS    网络共享存储设备。2.QJM    Quorum Journal Manager3.两个名称节点    active            //激活    standby            //待命</code></pre><p>active            //激活<br>deactive        //钝化</p><h2 id="SPOF"><a href="#SPOF" class="headerlink" title="SPOF"></a>SPOF</h2><pre><code>single point of failure,单点故障。</code></pre><h2 id="事务是个特性"><a href="#事务是个特性" class="headerlink" title="事务是个特性"></a>事务是个特性</h2><pre><code>a        //atomic    原子性c        //consistent一致性i        //isolate    隔离型d        //durable    永久性·</code></pre><h2 id="majority"><a href="#majority" class="headerlink" title="majority "></a>majority </h2><pre><code>大部分.</code></pre><h2 id="HA高可用配置"><a href="#HA高可用配置" class="headerlink" title="HA高可用配置"></a>HA高可用配置</h2><pre><code>high availability,高可用./home/centos/hadoop/dfs/data/current/BP-2100834435-192.168.231.201-1489328949370/current/finalized/subdir0/subdir0两个名称节点，一个active(激活态)，一个是standby(slave待命),slave节点维护足够多状态以便于容灾。和客户端交互的active节点,standby不交互.两个节点都和JN守护进程构成组的进行通信。数据节点配置两个名称节点，分别报告各自的信息。同一时刻只能有一个激活态名称节点。脑裂:两个节点都是激活态。为防止脑裂，JNs只允许同一时刻只有一个节点向其写数据。容灾发生时，成为active节点的namenode接管向jn的写入工作。</code></pre><h2 id="硬件资源"><a href="#硬件资源" class="headerlink" title="硬件资源"></a>硬件资源</h2><pre><code>名称节点:    硬件配置相同。JN节点    :    轻量级进程，至少3个节点,允许挂掉的节点数 (n - 1) / 2.            不需要再运行辅助名称节点。</code></pre><h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><h2 id="配置细节"><a href="#配置细节" class="headerlink" title="配置细节"></a>配置细节</h2><pre><code>0.s201和s206具有完全一致的配置，尤其是ssh.1.配置nameservice    [hdfs-site.xml]    &lt;property&gt;        &lt;name&gt;dfs.nameservices&lt;/name&gt;        &lt;value&gt;mycluster&lt;/value&gt;    &lt;/property&gt;2.dfs.ha.namenodes.[nameservice ID]    [hdfs-site.xml]    &lt;!-- myucluster下的名称节点两个id --&gt;    &lt;property&gt;        &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt;        &lt;value&gt;nn1,nn2&lt;/value&gt;    &lt;/property&gt;3.dfs.namenode.rpc-address.[nameservice ID].[name node ID]     [hdfs-site.xml]    配置每个nn的rpc地址。    &lt;property&gt;        &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt;        &lt;value&gt;s201:8020&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt;        &lt;value&gt;s206:8020&lt;/value&gt;    &lt;/property&gt;4.dfs.namenode.http-address.[nameservice ID].[name node ID]    配置webui端口    [hdfs-site.xml]    &lt;property&gt;        &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt;        &lt;value&gt;s201:50070&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt;        &lt;value&gt;s206:50070&lt;/value&gt;    &lt;/property&gt;5.dfs.namenode.shared.edits.dir    名称节点共享编辑目录.    [hdfs-site.xml]    &lt;property&gt;        &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;        &lt;value&gt;qjournal://s202:8485;s203:8485;s204:8485/mycluster&lt;/value&gt;    &lt;/property&gt;6.dfs.client.failover.proxy.provider.[nameservice ID]    java类，client使用它判断哪个节点是激活态。    [hdfs-site.xml]    &lt;property&gt;        &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt;        &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;    &lt;/property&gt;7.dfs.ha.fencing.methods    脚本列表或者java类，在容灾保护激活态的nn.    [hdfs-site.xml]    &lt;property&gt;        &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;        &lt;value&gt;                sshfence                shell(/bin/true)        &lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;        &lt;value&gt;/home/centos/.ssh/id_rsa&lt;/value&gt;    &lt;/property&gt;8.fs.defaultFS     配置hdfs文件系统名称服务。    [core-site.xml]    &lt;property&gt;        &lt;name&gt;fs.defaultFS&lt;/name&gt;        &lt;value&gt;hdfs://mycluster&lt;/value&gt;    &lt;/property&gt; 9.dfs.journalnode.edits.dir    配置JN存放edit的本地路径。    [hdfs-site.xml]    &lt;property&gt;        &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;        &lt;value&gt;/home/centos/hadoop/journal&lt;/value&gt;    &lt;/property&gt;</code></pre><h2 id="部署细节"><a href="#部署细节" class="headerlink" title="部署细节"></a>部署细节</h2><pre><code>1.在jn节点分别启动jn进程    $&gt;hadoop-daemon.sh start journalnode2.启动jn之后，在两个NN之间进行disk元数据同步    a)如果是全新集群，先format文件系统,只需要在一个nn上执行。        [s201]        $&gt;hadoop namenode -format</code></pre><p>格式化文件系统主要就是生成一个新的VERSION，里面内容就是生成一个名称空间ID集群ID时间块池ID</p><p><img src="https://i.imgur.com/9loeVXF.png" alt=""></p><pre><code>b)如果将非HA集群转换成HA集群，复制原NN的metadata到另一个nn.    1.步骤一        [s201]        $&gt;scp -r /home/centos/hadoop/dfs centos@s206:/home/centos/hadoop/    2.步骤二        在新的nn(未格式化的nn)上运行一下命令，实现待命状态引导。        [s206]        $&gt;hdfs namenode -bootstrapStandby        //需要s201为启动状态,提示是否格式化,选择N.    3)在一个NN上执行以下命令，完成edit日志到jn节点的传输。        $&gt;hdfs namenode -initializeSharedEdits        #查看s202,s203是否有edit数据.    4)启动所有节点.        [s201]        $&gt;hadoop-daemon.sh start namenode        //启动名称节点        $&gt;hadoop-daemons.sh start datanode        //启动所有数据节点        [s206]        $&gt;hadoop-daemon.sh start namenode        //启动名称节点</code></pre><h2 id="HA管理"><a href="#HA管理" class="headerlink" title="HA管理"></a>HA管理</h2><pre><code>$&gt;hdfs haadmin -transitionToActive nn1                //切成激活态$&gt;hdfs haadmin -transitionToStandby nn1                //切成待命态$&gt;hdfs haadmin -transitionToActive --forceactive nn2//强行激活$&gt;hdfs haadmin -failover nn1 nn2                    //模拟容灾演示,从nn1切换到nn2</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;复习：&quot;&gt;&lt;a href=&quot;#复习：&quot; class=&quot;headerlink&quot; title=&quot;复习：&quot;&gt;&lt;/a&gt;复习：&lt;/h1&gt;&lt;p&gt;1.链式job编程&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;MR        //Mapper+ / Reduce Mapper*
&lt;/co
      
    
    </summary>
    
      <category term="Hadoop" scheme="http://erichunn.github.io/categories/Hadoop/"/>
    
    
      <category term="机架感知" scheme="http://erichunn.github.io/tags/%E6%9C%BA%E6%9E%B6%E6%84%9F%E7%9F%A5/"/>
    
      <category term="机架感知实现" scheme="http://erichunn.github.io/tags/%E6%9C%BA%E6%9E%B6%E6%84%9F%E7%9F%A5%E5%AE%9E%E7%8E%B0/"/>
    
      <category term="HA" scheme="http://erichunn.github.io/tags/HA/"/>
    
      <category term="手动移动数据块" scheme="http://erichunn.github.io/tags/%E6%89%8B%E5%8A%A8%E7%A7%BB%E5%8A%A8%E6%95%B0%E6%8D%AE%E5%9D%97/"/>
    
      <category term="HDFS" scheme="http://erichunn.github.io/tags/HDFS/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop第八天</title>
    <link href="http://erichunn.github.io/2018/10/22/Hadoop%E7%AC%AC%E5%85%AB%E5%A4%A9/"/>
    <id>http://erichunn.github.io/2018/10/22/Hadoop第八天/</id>
    <published>2018-10-22T11:40:51.000Z</published>
    <updated>2018-10-24T11:12:28.389Z</updated>
    
    <content type="html"><![CDATA[<h1 id="二次排序链条化"><a href="#二次排序链条化" class="headerlink" title="二次排序链条化"></a>二次排序链条化</h1><p>分组不能解决数据倾斜问题，已经进入到了同一个reduce对象里面了，分一个组出来就进入同一个reduce方法里面。每次分组只是决定调用不调用同一个reduce方法。已经晚了，数据已经进入到了同一个reduce了，分区可以决定数据倾斜问题。</p><p>reduce分为方法和对象，有的时候说reduce是指的是方法，有的时候指的是对象，要分开对待。</p><p>单词统计做统计肯定不能哈希分区，因为哈希同一个值进入到同一个reduce对象利民区去了</p><h1 id="数据倾斜问题-随机分区-二次MR"><a href="#数据倾斜问题-随机分区-二次MR" class="headerlink" title="数据倾斜问题 随机分区 二次MR"></a>数据倾斜问题 随机分区 二次MR</h1><pre><code>1.2.3.4.</code></pre><p>如果正常按照wordcount来处理会分为</p><p>reduce不设置就是1，map没机会设置数量。但是也可以设置map，但是意义，以为mr提交的时候map的个数取决于切片的个数。</p><p>切片的计算公式：min block、maxsplit、 blocksize取中间值<br>等于blocksize。</p><p>现在文件如下，现在3个文件每个文件有一百万数据且有一百万个hello就对应了3个map。（这里没明白为什么3个切片也就是为啥3个map）</p><p>每个切片里，分区个数等于reduce个数，分区的个数取决于reduce个数，之前自己已经在WCAPP里面设置好了reduce个数。</p><p>我有3个map，4个reduce。那么每个map里面就有4个分区了。</p><p><strong>要解决哈希分区带来的数据倾斜问题，可以通过随机分区，然后在进行二次mr来解决这个问题</strong></p><p><img src="https://i.imgur.com/VoHpfBH.png" alt=""></p><p>组合combineor是对分区里面的预先聚合，下面的hello就不会发送一百万次，而是提前聚合了发送了hello 一百万次。这个样子。能解决目前的倾斜问题，但是不可能这么多数据，可能量很大</p><p>下面这种情况下，所有的hello根据哈希分区就进入到同一个分区，然后一百万个tom分4个分区</p><p>通过随机分区可以解决数据倾斜问题，但是会产生新的问题，就是说hello被分到4个不同的reduce里面，导致reduce1产生hello10reduce2产生hello13这种问题，所以通过二次mr来解决这个问题，让hello这个单词统计，进入到同一个reduce结果当中。所以也就是通过随即分区和二次mr解决</p><pre><code>[1.txt]1000000hello tom1hello tom2hello tom3hello tom4hello tom5hello tom6hello tom7hello tom8hello tom9hello tom10[2.txt]1000000hello tom11hello tom12hello tom13hello tom14hello tom15hello tom16hello tom17hello tom18hello tom19hello tom20[3.txt]1000000hello tom21hello tom22hello tom23hello tom24hello tom25hello tom26hello tom27hello tom28hello tom29hello tom30</code></pre><p>代码如下：<br>    //自定义分区函数<br>    public class RandomPartitioner extends Partitioner&lt;Text,IntWritable&gt; {<br>        @Override<br>        public int getPartition(Text text, IntWritable intWritable, int numPartitions) {<br>            return new Random().nextInt(numPartitions);<br>        }<br>    }</p><hr><pre><code>//解决数据倾斜问题：public class WCSkueApp {    public static void main(String[] args) throws Exception {        Configuration conf = new Configuration();        conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;);        Job job = Job.getInstance(conf);//        设置作业的各种属性        job.setJobName(&quot;MaxTempApp&quot;);    //作业名称        job.setJarByClass(WCSkueApp.class); //搜索类路径        job.setInputFormatClass(TextInputFormat.class);//设置输入格式类        //添加输入路径        FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew&quot;));        //设置输出路径        org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(&quot;e:/mr/skew/out&quot;));        //设置随机分区：        job.setPartitionerClass(RandomPartitioner.class);        job.setMapperClass(WCSkueMapper.class);         //mapper类        job.setReducerClass(WCSkueReducer.class);       //reduce类        job.setNumReduceTasks(4);        job.setMapOutputKeyClass(Text.class);        job.setMapOutputValueClass(IntWritable.class);        job.setOutputKeyClass(Text.class);          //设置输出类型        job.setOutputValueClass(IntWritable.class);        job.waitForCompletion(true);    }}</code></pre><hr><pre><code>public class WCSkueMapper extends Mapper &lt;LongWritable, Text, Text, IntWritable&gt;{    public WCSkueMapper(){        System.out.println(&quot;new WCMapper&quot;);    }    @Override    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {        String[] arr = value.toString().split(&quot; &quot;);        Text keyout= new Text();        IntWritable valueout = new IntWritable();        for(String s:arr){            keyout.set(s);            valueout.set(1);            context.write(keyout,valueout);        }    }}</code></pre><hr><pre><code>public class WCSkueReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{    /**     * reduce     */    protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {        int count = 0 ;        for(IntWritable iw : values){            count = count + iw.get() ;        }        context.write(key,new IntWritable(count));    }}</code></pre><hr><p>//解决数据倾斜问题：<br>    public class WCSkueApp2 {</p><pre><code>    public static void main(String[] args) throws Exception {        Configuration conf = new Configuration();        conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;);        Job job = Job.getInstance(conf);//        设置作业的各种属性        job.setJobName(&quot;MaxTempApp&quot;);    //作业名称        job.setJarByClass(WCSkueApp2.class); //搜索类路径        job.setInputFormatClass(TextInputFormat.class);//设置输入格式类        //添加输入路径        FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00000&quot;));        FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00001&quot;));        FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00002&quot;));        FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00003&quot;));        //设置输出路径        org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(&quot;e:/mr/skew/out2&quot;));        job.setMapperClass(WCSkueMapper2.class);         //mapper类        job.setReducerClass(WCSkueReducer2.class);       //reduce类        job.setNumReduceTasks(4);        job.setMapOutputKeyClass(Text.class);        job.setMapOutputValueClass(IntWritable.class);        job.setOutputKeyClass(Text.class);          //设置输出类型        job.setOutputValueClass(IntWritable.class);        job.waitForCompletion(true);    }}</code></pre><hr><pre><code>public class WCSkueMapper2 extends Mapper &lt;LongWritable, Text, Text, IntWritable&gt;{    public WCSkueMapper2(){        System.out.println(&quot;new WCMapper&quot;);    }    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {        String[] arr = value.toString().split(&quot;\t&quot;);        Text keyout= new Text();        IntWritable valueout = new IntWritable();        context.write(new Text(arr[0]),new IntWritable(Integer.parseInt(arr[1])));    }}</code></pre><hr><pre><code>public class WCSkueReducer2 extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{    /**     * reduce     */    protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {        int count = 0 ;        for(IntWritable iw : values){            count = count + iw.get() ;        }        context.write(key,new IntWritable(count));    }}</code></pre><hr><p>上面这个就是通过2次mr解决数据倾斜问题    但是不用那么麻烦的去切分了直接使用Keyvalueinputformat即可实现：如下代码：<br>    //解决数据倾斜问题：<br>    public class WCSkueApp2 {</p><pre><code>    public static void main(String[] args) throws Exception {        Configuration conf = new Configuration();        conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;);        Job job = Job.getInstance(conf);//        设置作业的各种属性        job.setJobName(&quot;MaxTempApp&quot;);    //作业名称        job.setJarByClass(WCSkueApp2.class); //搜索类路径        job.setInputFormatClass(KeyValueTextInputFormat.class);//设置输入格式类        //添加输入路径        FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00000&quot;));        FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00001&quot;));        FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00002&quot;));        FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00003&quot;));        //设置输出路径        org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(&quot;e:/mr/skew/out2&quot;));        job.setMapperClass(WCSkueMapper2.class);         //mapper类        job.setReducerClass(WCSkueReducer2.class);       //reduce类        job.setNumReduceTasks(4);        job.setMapOutputKeyClass(Text.class);        job.setMapOutputValueClass(IntWritable.class);        job.setOutputKeyClass(Text.class);          //设置输出类型        job.setOutputValueClass(IntWritable.class);        job.waitForCompletion(true);    }}</code></pre><hr><pre><code>public class WCSkueMapper2 extends Mapper &lt;Text, Text, Text, IntWritable&gt;{    public WCSkueMapper2(){        System.out.println(&quot;new WCMapper&quot;);    }    protected void map(Text key, Text value, Context context) throws IOException, InterruptedException {        context.write(key,new IntWritable(Integer.parseInt(value.toString())));    }}</code></pre><hr><pre><code>public class WCSkueReducer2 extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{    /**     * reduce     */    protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {        int count = 0 ;        for(IntWritable iw : values){            count = count + iw.get() ;        }        context.write(key,new IntWritable(count));    }}</code></pre><hr><hr><hr><h1 id="链条式编程"><a href="#链条式编程" class="headerlink" title="链条式编程"></a>链条式编程</h1><p><img src="https://i.imgur.com/VwHIXax.png" alt=""></p><p>讲解上图：<br>首先通过MapMapper1来切割，然后通过MapMapper1过滤掉法轮功，真个是一个链条，在map端，执行完之后开始聚合，在reduce端，在聚合的时候，每个单词都有各自的个数，对产生的结果再次过滤，过滤掉少于5的单词，当然可以放到同一个map里面，但是如果变成模块方法，易于维护。更加方便使用。<br>m阶段可以有多个m但是R阶段只能有一个r。但是r阶段可以有多个m。</p><p>代码如下图：<br>//链条式job任务<br>    public class WCChainApp {</p><pre><code>    public static void main(String[] args) throws Exception {        Configuration conf = new Configuration();        conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;);        Job job = Job.getInstance(conf);//        设置作业的各种属性        job.setJobName(&quot;MaxTempApp&quot;);    //作业名称        job.setJarByClass(WCChainApp.class); //搜索类路径        job.setInputFormatClass(TextInputFormat.class);//设置输入格式类        //添加输入路径        FileInputFormat.addInputPath(job, new Path(&quot;e:/mr/skew&quot;));        //设置输出路径        org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(&quot;e:mr/skew/out&quot;));        //在map链条上添加一个mapper的环节        ChainMapper.addMapper(job,WCMapMapper1.class,LongWritable.class,Text.class,Text.class,IntWritable.class,conf);        ChainMapper.addMapper(job,WCMapMapper2.class,Text.class,IntWritable.class,Text.class,IntWritable.class,conf);        //在reduce链条上设置reduce        ChainReducer.setReducer(job,WCReducer.class,Text.class,IntWritable.class,Text.class,IntWritable.class,conf);        ChainReducer.addMapper(job,WCReduceMapper1.class,Text.class,IntWritable.class,Text.class,IntWritable.class,conf);        job.setNumReduceTasks(3);        job.waitForCompletion(true);    }}</code></pre><hr><pre><code>public class WCMapMapper1 extends Mapper &lt;LongWritable, Text, Text, IntWritable&gt;{    protected   void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {        Text keyout= new Text();        IntWritable valueout = new IntWritable();        String[] arr = value.toString().split(&quot; &quot;);        for(String s:arr){            keyout.set(s);            valueout.set(1);            context.write(keyout,valueout);        }    }}</code></pre><hr><pre><code>public class WCMapMapper2 extends Mapper &lt;Text, IntWritable, Text, IntWritable&gt;{    protected void map(Text key, IntWritable value, Context context) throws IOException, InterruptedException {        if(!key.toString().equals(&quot;falungong&quot;)){            context.write(key , value);        }    }}</code></pre><hr><pre><code>//过滤单词个数public class WCReduceMapper1 extends Mapper&lt;Text,IntWritable,Text,IntWritable&gt; {    @Override    protected void map(Text key, IntWritable value, Context context) throws IOException, InterruptedException {        if(value.get() &gt; 5){            context.write(key,value);        }    }}</code></pre><hr><pre><code>/** * Reducer */public class WCReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{    /**     * reduce     */    protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {        int count = 0 ;        for(IntWritable iw : values){            count = count + iw.get() ;        }        context.write(key,new IntWritable(count));    }}</code></pre><hr><hr><hr><h2 id="FileInputFormat（读源码）"><a href="#FileInputFormat（读源码）" class="headerlink" title="FileInputFormat（读源码）"></a>FileInputFormat（读源码）</h2><pre><code>获取切片集合。子类都要重写方法isSplittable();负责创建RecordReader对象。设置IO路径。</code></pre><h2 id="RecordReader（读源码）"><a href="#RecordReader（读源码）" class="headerlink" title="RecordReader（读源码）"></a>RecordReader（读源码）</h2><pre><code>负责从InputSplit中读取KV对。</code></pre><h2 id="jdbc笔记模板："><a href="#jdbc笔记模板：" class="headerlink" title="jdbc笔记模板："></a>jdbc笔记模板：</h2><pre><code>[写操作]Class.forName(&quot;com.mysql.jdbc.Driver&quot;);Connection conn = DriverMananger.getConnection(&quot;jdbc:mysql://localhost:3306/big4&quot;,&quot;root&quot;,&quot;root&quot;);//预处理语句PreparedStatement ppst = conn.preparedStatement(&quot;insert into test(id,name,age) values(?,?,?)&quot;);//绑定参数ppst.setInteger(1,1);ppst.setInteger(2,&quot;tom&quot;);ppst.setInteger(3,12);ppst.executeUpdate();ppst.close();conn.close();[读操作]Class.forName(&quot;com.mysql.jdbc.Driver&quot;);Connection conn = DriverMananger.getConnection(&quot;jdbc:mysql://localhost:3306/big4&quot;,&quot;root&quot;,&quot;root&quot;);ppst = conn.preparedStatement(&quot;select id,name from test &quot;);//结果集ResultSet rs = ppst.executeQuery();while(rs.next()){    int id = rs.getInt(&quot;id&quot;);    String name = rs.getInt(&quot;name&quot;);}rs.close();conn.close();</code></pre><p>看一下下源码：<br><img src="https://i.imgur.com/5ZrVZsW.png" alt=""><br>看一下写和读入都把结果集和预处理语句都已经封装进去了，剩下的只要填空就好了。</p><p>以下是从数据库中读入的代码模板：</p><h1 id="使用DBWritable向数据库从数据库中读取"><a href="#使用DBWritable向数据库从数据库中读取" class="headerlink" title="使用DBWritable向数据库从数据库中读取"></a>使用DBWritable向数据库从数据库中读取</h1><h2 id="1-准备数据库"><a href="#1-准备数据库" class="headerlink" title="1.准备数据库"></a>1.准备数据库</h2><pre><code>create database big4 ;use big4 ;create table words(id int primary key auto_increment , name varchar(20) , txt varchar(255));insert into words(name,txt) values(&apos;tomas&apos;,&apos;hello world tom&apos;);insert into words(txt) values(&apos;hello tom world&apos;);insert into words(txt) values(&apos;world hello tom&apos;);insert into words(txt) values(&apos;world tom hello&apos;);</code></pre><h2 id="2-编写hadoop-MyDBWritable"><a href="#2-编写hadoop-MyDBWritable" class="headerlink" title="2.编写hadoop MyDBWritable."></a>2.编写hadoop MyDBWritable.</h2><pre><code>import org.apache.hadoop.mapreduce.lib.db.DBWritable;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import java.sql.PreparedStatement;import java.sql.ResultSet;import java.sql.SQLException;/** * MyDBWritable */public class MyDBWritable implements DBWritable,Writable {    private int id ;    private String name ;    private String txt ;    public int getId() {        return id;    }    public void setId(int id) {        this.id = id;    }    public String getName() {        return name;    }    public void setName(String name) {        this.name = name;    }    public String getTxt() {        return txt;    }    public void setTxt(String txt) {        this.txt = txt;    }    public void write(DataOutput out) throws IOException {        out.writeInt(id);        out.writeUTF(name);        out.writeUTF(txt);    }    public void readFields(DataInput in) throws IOException {        id = in.readInt();        name = in.readUTF();        txt = in.readUTF();    }    /**     * 写入db     */    public void write(PreparedStatement ppst) throws SQLException {        ppst.setInt(1,id);        ppst.setString(2,name);        ppst.setString(3,txt);    }    /**     * 从db读取     */    public void readFields(ResultSet rs) throws SQLException {        id = rs.getInt(1);        name = rs.getString(2);        txt = rs.getString(3);    }}</code></pre><h2 id="3-WcMapper"><a href="#3-WcMapper" class="headerlink" title="3.WcMapper"></a>3.WcMapper</h2><pre><code>public class WCMapper extends Mapper&lt;LongWritable,MyDBWritable,Text,IntWritable&gt; {    protected void map(LongWritable key, MyDBWritable value, Context context) throws IOException, InterruptedException {        System.out.println(key);        String line = value.getTxt();        System.out.println(value.getId() + &quot;,&quot; + value.getName());        String[] arr = line.split(&quot; &quot;);        for(String s : arr){            context.write(new Text(s),new IntWritable(1));        }    }}</code></pre><h2 id="4-WCReducer"><a href="#4-WCReducer" class="headerlink" title="4.WCReducer"></a>4.WCReducer</h2><pre><code>protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {    int count = 0 ;    for(IntWritable w : values){        count = count + w.get() ;    }    context.write(key,new IntWritable(count));}</code></pre><h2 id="5-WCApp"><a href="#5-WCApp" class="headerlink" title="5.WCApp"></a>5.WCApp</h2><pre><code>public static void main(String[] args) throws Exception {    Configuration conf = new Configuration();    Job job = Job.getInstance(conf);    //设置job的各种属性    job.setJobName(&quot;MySQLApp&quot;);                        //作业名称    job.setJarByClass(WCApp.class);                 //搜索类    //配置数据库信息    String driverclass = &quot;com.mysql.jdbc.Driver&quot; ;    String url = &quot;jdbc:mysql://localhost:3306/big4&quot; ;    String username= &quot;root&quot; ;    String password = &quot;root&quot; ;    //设置数据库配置    DBConfiguration.configureDB(job.getConfiguration(),driverclass,url,username,password);    //设置数据输入内容    DBInputFormat.setInput(job,MyDBWritable.class,&quot;select id,name,txt from words&quot;,&quot;select count(*) from words&quot;);    //设置输出路径    FileOutputFormat.setOutputPath(job,new Path(&quot;e:/mr/sql/out&quot;));    //设置分区类    job.setMapperClass(WCMapper.class);             //mapper类    job.setReducerClass(WCReducer.class);           //reducer类    job.setNumReduceTasks(3);                       //reduce个数    job.setMapOutputKeyClass(Text.class);           //    job.setMapOutputValueClass(IntWritable.class);  //    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(IntWritable.class);     //    job.waitForCompletion(true);}</code></pre><h2 id="6-pom-xml增加mysql驱动"><a href="#6-pom-xml增加mysql驱动" class="headerlink" title="6.pom.xml增加mysql驱动"></a>6.pom.xml增加mysql驱动</h2><pre><code>&lt;dependency&gt;    &lt;groupId&gt;mysql&lt;/groupId&gt;    &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;    &lt;version&gt;5.1.17&lt;/version&gt;&lt;/dependency&gt;</code></pre><h2 id="7-将mr的统计结果写入mysql数据库"><a href="#7-将mr的统计结果写入mysql数据库" class="headerlink" title="7.将mr的统计结果写入mysql数据库"></a>7.将mr的统计结果写入mysql数据库</h2><pre><code>a)准备表    create table stats(word varchar(50),c int);b)设置App的DBOutputFormat类    com.it18zhang.hdfs.mr.mysql.WCAppd)e)f)</code></pre><hr><h1 id="mysql分页查询"><a href="#mysql分页查询" class="headerlink" title="mysql分页查询"></a>mysql分页查询</h1><p>如图所示吧：</p><p><img src="https://i.imgur.com/0hHJN3B.png" alt=""></p><p><img src="https://i.imgur.com/FAbkIff.png" alt=""></p><p><img src="https://i.imgur.com/9wCCeH2.png" alt=""></p><h1 id="使用DBWritable向数据库从数据库中写入"><a href="#使用DBWritable向数据库从数据库中写入" class="headerlink" title="使用DBWritable向数据库从数据库中写入"></a>使用DBWritable向数据库从数据库中写入</h1><pre><code>public class MyDBWritalbe implements DBWritable,Writable {    private int id=0;    private String name=&quot;&quot;;    private String txt=&quot;&quot;;    private  String word=&quot;&quot;;    private int wordcount=0;    public String getWord() {        return word;    }    public void setWord(String word) {        this.word = word;    }    public int getWordcount() {        return wordcount;    }    public void setWordcount(int wordcount) {        this.wordcount = wordcount;    }    public int getId() {        return id;    }    public void setId(int id) {        this.id = id;    }    public String getName() {        return name;    }    public void setName(String name) {        this.name = name;    }    public String getTxt() {        return txt;    }    public void setTxt(String txt) {        this.txt = txt;    }    public void write(DataOutput out) throws IOException {        out.writeInt(id);        out.writeUTF(name);        out.writeUTF(txt);        out.writeUTF(word);        out.writeInt(wordcount);    }    public void readFields(DataInput in) throws IOException {        id=in.readInt();        name=in.readUTF();        txt=in.readUTF();        word=in.readUTF();        wordcount=in.readInt();    }    //向数据库中写入DB    public void write(PreparedStatement ppst) throws SQLException {        //要求定制字段列表的时候先单词后个数。        ppst.setString(1,word);        ppst.setInt(2,wordcount);    }    //从DB中读出    public void readFields(ResultSet rs) throws SQLException {        id=rs.getInt(1);        name=rs.getString(2);        txt=rs.getString(3);    }}</code></pre><hr><pre><code>public class WCApp {    ////    public static void main(String[] args) throws Exception {//        Configuration conf = new Configuration();////        Job job = Job.getInstance(conf);//////        设置作业的各种属性//        job.setJobName(&quot;MySQLApp&quot;);    //作业名称//        job.setJarByClass(WCApp.class); //搜索类路径////        //配置数据库信息//        String driverclass = &quot;com.mysql.jdbc.Driver&quot;;//        String url = &quot;jdbc:mysql://localhost:3306/big4&quot;;//        String usrname = &quot;root&quot;;//        String password = &quot;root&quot;;//        DBConfiguration.configureDB(conf,driverclass,url,usrname,password);//        //设置数据输入内容//        DBInputFormat.setInput(job,DBWritable.class,&quot;select id,name,txt from words&quot;,&quot;select count(*) from words&quot;);//        //设置输出路径//        FileOutputFormat.setOutputPath(job, new Path(&quot;d:/mr/sql/out&quot;));////        job.setMapperClass(WCMapper.class);         //mapper类//        job.setReducerClass(WCReducer.class);       //reduce类////        job.setNumReduceTasks(3);//////        job.setMapOutputKeyClass(Text.class);//        job.setMapOutputValueClass(IntWritable.class);////        job.setOutputKeyClass(Text.class);          //设置输出类型//        job.setOutputValueClass(IntWritable.class);////        job.waitForCompletion(true);////    }//}    public static void main(String[] args) throws Exception {        Configuration conf = new Configuration();        Job job = Job.getInstance(conf);        //设置job的各种属性        job.setJobName(&quot;MySQLApp&quot;);                        //作业名称        job.setJarByClass(WCApp.class);                 //搜索类        //配置数据库信息        String driverclass = &quot;com.mysql.jdbc.Driver&quot;;        String url = &quot;jdbc:mysql://localhost:3306/big4&quot;;        String username = &quot;root&quot;;        String password = &quot;root&quot;;        //设置数据库配置        DBConfiguration.configureDB(job.getConfiguration(), driverclass, url, username, password);        //设置数据输入内容        DBInputFormat.setInput(job, MyDBWritalbe.class, &quot;select id,name,txt from words&quot;, &quot;select count(*) from words&quot;);        //这里定制字段列表，先单词后个数        DBOutputFormat.setOutput(job,&quot;stats&quot;,&quot;word&quot;,&quot;c&quot;);        //设置分区类        job.setMapperClass(WCMapper.class);             //mapper类        job.setReducerClass(WCReducer.class);           //reducer类        job.setNumReduceTasks(3);                       //reduce个数        job.setMapOutputKeyClass(Text.class);           //        job.setMapOutputValueClass(IntWritable.class);  //        job.setOutputKeyClass(Text.class);        job.setOutputValueClass(IntWritable.class);     //        job.waitForCompletion(true);    }}</code></pre><hr><pre><code>public class WCMapper extends Mapper&lt;LongWritable, MyDBWritalbe, Text, IntWritable&gt; {    @Override    protected void map(LongWritable key, MyDBWritalbe value, Context context) throws IOException, InterruptedException {        System.out.println(key);        String line = value.getTxt();        System.out.println(value.getId() + &quot;,&quot; + value.getName());        String[] arr = line.split(&quot; &quot;);        for (String s : arr) {            context.write(new Text(s), new IntWritable(1));        }    }}</code></pre><hr><pre><code>public class WCReducer extends Reducer&lt;Text, IntWritable, MyDBWritalbe, NullWritable&gt; {    protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {        int count = 0;        for (IntWritable w : values) {            count = count + w.get();        }        MyDBWritalbe keyout = new MyDBWritalbe();        keyout.setWord(key.toString());        keyout.setWordcount(count);        context.write(keyout, NullWritable.get());    }}</code></pre><hr><h1 id="在虚拟机中跑wordcount写入数据库问题："><a href="#在虚拟机中跑wordcount写入数据库问题：" class="headerlink" title="在虚拟机中跑wordcount写入数据库问题："></a>在虚拟机中跑wordcount写入数据库问题：</h1><h2 id="需要修改的地方："><a href="#需要修改的地方：" class="headerlink" title="需要修改的地方："></a>需要修改的地方：</h2><p>1.修改url</p><p><img src="https://i.imgur.com/8lkYD5v.png" alt=""></p><p>2.修改core-site.xml或者删除掉</p><p><img src="https://i.imgur.com/weFYMBJ.png" alt=""></p><p>3.在虚拟机中分发mysql-connector-java-5.1.7-bin.jar将之分发到每个节点的/soft/hadoop/share/hadoop/common/lib目录下。</p><p><img src="https://i.imgur.com/ou52Qhr.png" alt=""></p><p>4.在运行</p><pre><code>hadoop jar HdfsDemo.jar com.it18zhang.hdfs.mr.mysql.WCApp</code></pre><h2 id="出现的问题："><a href="#出现的问题：" class="headerlink" title="出现的问题："></a>出现的问题：</h2><p>1.出现连接不上的报错：</p><p><img src="https://i.imgur.com/JgE2Q3e.png" alt=""></p><p>百度了一下完美解决，是由于mysql没有对所有用户开启权限导致：</p><p><a href="https://blog.csdn.net/OldTogether/article/details/79612627?utm_source=blogxgwz1" target="_blank" rel="noopener">https://blog.csdn.net/OldTogether/article/details/79612627?utm_source=blogxgwz1</a></p><p>2.出现一下问题：</p><p><img src="https://i.imgur.com/vgUGlEc.png" alt=""></p><p>原因是有一个s205的防火墙没有关掉导致的：</p><p><a href="https://blog.csdn.net/shirdrn/article/details/7280040" title="防火墙没关掉导致的" target="_blank" rel="noopener">https://blog.csdn.net/shirdrn/article/details/7280040</a></p><hr><p>完</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;二次排序链条化&quot;&gt;&lt;a href=&quot;#二次排序链条化&quot; class=&quot;headerlink&quot; title=&quot;二次排序链条化&quot;&gt;&lt;/a&gt;二次排序链条化&lt;/h1&gt;&lt;p&gt;分组不能解决数据倾斜问题，已经进入到了同一个reduce对象里面了，分一个组出来就进入同一个redu
      
    
    </summary>
    
      <category term="Hadoop" scheme="http://erichunn.github.io/categories/Hadoop/"/>
    
    
      <category term="二次排序链条化" scheme="http://erichunn.github.io/tags/%E4%BA%8C%E6%AC%A1%E6%8E%92%E5%BA%8F%E9%93%BE%E6%9D%A1%E5%8C%96/"/>
    
      <category term="数据倾斜" scheme="http://erichunn.github.io/tags/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop第七天</title>
    <link href="http://erichunn.github.io/2018/10/17/Hadoop%E7%AC%AC%E4%B8%83%E5%A4%A9/"/>
    <id>http://erichunn.github.io/2018/10/17/Hadoop第七天/</id>
    <published>2018-10-17T08:17:15.000Z</published>
    <updated>2018-11-02T13:58:54.355Z</updated>
    
    <content type="html"><![CDATA[<h1 id="多输入问题"><a href="#多输入问题" class="headerlink" title="多输入问题"></a>多输入问题</h1><p>在IDEA里面代码：</p><p>首先明确一点InputInputformat也就是文本输入格式的输入是Longwritblele和Text，然后SequenceFileputFormat的输入是intwritble和text。</p><pre><code>public class WCApp {    public static void main(String[] args) throws Exception {        Configuration conf = new Configuration();        conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;);        Job job = Job.getInstance(conf);//       设置作业的各种属性        job.setJobName(&quot;WCAppMulti&quot;);    //作业名称        job.setJarByClass(WCApp.class); //搜索类路径        //多个输入        MultipleInputs.addInputPath(job,new Path(&quot;file:///e:/mr/txt&quot;),TextInputFormat.class,WCTextMapper.class);        MultipleInputs.addInputPath(job,new Path(&quot;file:///e:/mr/seq&quot;),SequenceFileInputFormat.class,WCSeqMapper.class);        //设置输出        FileOutputFormat.setOutputPath(job,new Path(args[0]));        job.setReducerClass(WCReducer.class);//reducer类        job.setNumReduceTasks(3);//reducer个数        job.setOutputKeyClass(Text.class);          //设置输出类型        job.setOutputValueClass(IntWritable.class);        job.waitForCompletion(true);    }}</code></pre><hr><pre><code>ublic class WCReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{/** * reduce */protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {    int count = 0 ;    for(IntWritable iw : values){        count = count + iw.get() ;    }    String tno = Thread.currentThread().getName();    System.out.println(tno + &quot;WCReducer:&quot; + key.toString() + &quot;=&quot; + count);    context.write(key,new IntWritable(count));}</code></pre><p>}</p><hr><pre><code>public class WCTextMapper extends Mapper &lt;LongWritable, Text, Text, IntWritable&gt;{    @Override    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {        String[] arr = value.toString().split(&quot; &quot;);        Text keyout= new Text();        IntWritable valueout = new IntWritable();        for(String s:arr){            keyout.set(s);            valueout.set(1);            context.write(keyout,valueout);        }    }}</code></pre><hr><pre><code>public class WCTextMapper extends Mapper &lt;LongWritable, Text, Text, IntWritable&gt;{    @Override    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {        String[] arr = value.toString().split(&quot; &quot;);        Text keyout= new Text();        IntWritable valueout = new IntWritable();        for(String s:arr){            keyout.set(s);            valueout.set(1);            context.write(keyout,valueout);        }    }}</code></pre><hr><p>下图是配置本地的路径。要明确其中的arg[]也就是这个里配置的那个Program arguments的路径。因为只需要一个参数就可以了，因为在代码中已经写好了输入路径了。</p><p><img src="https://i.imgur.com/vO9wETa.png" alt=""></p><hr><hr><hr><h1 id="计数器"><a href="#计数器" class="headerlink" title="计数器"></a>计数器</h1><p> 日志目录：<br>/soft/hadoop/logs/userlogs</p><p>用户打印的日志。Hadoop输出的日志都是在外层的文件之中。userlogs是自己的日志打印。</p><h2 id="计数器-1"><a href="#计数器-1" class="headerlink" title="计数器"></a>计数器</h2><p>是其他的内容都不做更改，在Mapper和Reducer里面添加这个语句即可<br>    context.getCounter(“r”, “WCReducer.reduce”).increment(1);</p><p>然后扔到虚拟机里面去运行：<br>    hadoop jar HdfsDemo.jar com.it18zhang.hdfs.mr.WCApp hdfs://user/centos/data/ hdfs://user/centos/data/out</p><h1 id="单独配置2nn到独立节点"><a href="#单独配置2nn到独立节点" class="headerlink" title="单独配置2nn到独立节点"></a>单独配置2nn到独立节点</h1><p>配置core-site文件</p><pre><code>[hdfs-site.xml]&lt;property&gt;        &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;        &lt;value&gt;s206:50090&lt;/value&gt;&lt;/property&gt;</code></pre><h1 id="跟踪运行器信息"><a href="#跟踪运行器信息" class="headerlink" title="跟踪运行器信息"></a>跟踪运行器信息</h1><p><img src="https://i.imgur.com/UVIcFDb.png" alt=""></p><p>添加一个工具类：</p><pre><code>public class Util {public static String getInfo(Object o,String msg ){    return getHostname() + &quot;:&quot; + getPID() + &quot;:&quot; + getTID()+&quot;:&quot;+getObjInfo(o) +msg;}//得到主机名public static String getHostname() {    try {        return InetAddress.getLocalHost().getHostName();    } catch (UnknownHostException e) {        e.printStackTrace();    }    return  null;}    //获得当前程序的所在的进程ID。    public static int getPID()  {        String info = ManagementFactory.getRuntimeMXBean().getName();        return Integer.parseInt(info.substring(0,info.indexOf(&quot;@&quot;)));    }    //返回当前线程ID，    public static String getTID(){    return Thread.currentThread().getName();    }    public static  String getObjInfo(Object o){        String sname = o.getClass().getSimpleName();        return sname + &quot;@&quot;+o.hashCode();    }}</code></pre><p>然后在map和reduce阶段添加：</p><pre><code>//每执行一次，计数器对这个组+1context.getCounter(&quot;m&quot;,Util.getInfo(this,&quot;map&quot;)).increment(1);</code></pre><p>效果如下图<br><img src="https://i.imgur.com/XeGcZvS.png" alt=""></p><h1 id="全排序"><a href="#全排序" class="headerlink" title="全排序"></a>全排序</h1><pre><code>## 普通排序求最高年份温度 ##</code></pre><p>代码如下：</p><pre><code>public class MaxTempApp {    public static void main(String[] args) throws Exception {        Configuration conf = new Configuration();        conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;);        Job job = Job.getInstance(conf);//        设置作业的各种属性        job.setJobName(&quot;MaxTempApp&quot;);    //作业名称        job.setJarByClass(MaxTempApp.class); //搜索类路径        job.setInputFormatClass(TextInputFormat.class);//设置输入格式类        //添加输入路径        FileInputFormat.addInputPath(job, new Path(args[0]));        //设置输出路径        org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(args[1]));        job.setMapperClass(MaxTempMapper.class);         //mapper类        job.setReducerClass(MaxTempReducer.class);       //reduce类        job.setNumReduceTasks(3);           //reduce个数        job.setMapOutputKeyClass(IntWritable.class);        job.setMapOutputValueClass(IntWritable.class);        job.setOutputKeyClass(IntWritable.class);          //设置输出类型        job.setOutputValueClass(IntWritable.class);        job.waitForCompletion(true);    }}</code></pre><hr><pre><code>public class MaxTempMapper extends Mapper&lt;LongWritable, Text, IntWritable, IntWritable&gt; {    public MaxTempMapper() {        System.out.println(&quot;new MaxTempMapper&quot;);    }    @Override    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {        String line = value.toString();        String arr[] = line.split(&quot; &quot;);        context.write(new IntWritable(Integer.parseInt(arr[0])), new IntWritable((Integer.parseInt(arr[1]))));    }}</code></pre><hr><pre><code>public class MaxTempReducer extends Reducer&lt;IntWritable, IntWritable, IntWritable, IntWritable&gt;{    /**     * reduce     */    protected void reduce(IntWritable key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {            int max =Integer.MIN_VALUE;            for(IntWritable iw : values){                max = max &gt; iw.get() ? max : iw.get() ;            }                context.write(key,new IntWritable(max));    }}</code></pre><hr><h2 id="全排序代码"><a href="#全排序代码" class="headerlink" title="全排序代码"></a>全排序代码</h2><p>上面代码产生的会是3个文件，文件内部各自排序，但是不是全排序的文件。全排序2种办法：</p><p>1、设置分区数是1，但是数据倾斜</p><ol start="2"><li><p>在每个分区里设置0-30，30-60，60-100即可,然后每个分区返回0,1,2进入分成不同的区<br>代码如下图：<br> public class YearPartitioner extends Partitioner&lt;IntWritable, IntWritable&gt; {</p><pre><code>public int getPartition(IntWritable year, IntWritable temp, int parts) {    int y = year.get() - 1970;    if (y &lt; 33) {        return 0;    }    if (y &gt; 33 &amp;&amp; y &lt; 66) {        return 1;    }    else {        return 2;    }}</code></pre><p> }</p></li></ol><hr><pre><code>public class MaxTempReducer extends Reducer&lt;IntWritable, IntWritable, IntWritable, IntWritable&gt;{    /**     * reduce     */    protected void reduce(IntWritable key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {            int max =Integer.MIN_VALUE;            for(IntWritable iw : values){                max = max &gt; iw.get() ? max : iw.get() ;            }                context.write(key,new IntWritable(max));    }}</code></pre><hr><pre><code>public class MaxTempMapper extends Mapper&lt;LongWritable, Text, IntWritable, IntWritable&gt; {    public MaxTempMapper() {        System.out.println(&quot;new MaxTempMapper&quot;);    }    @Override    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {        String line = value.toString();        String arr[] = line.split(&quot; &quot;);        context.write(new IntWritable(Integer.parseInt(arr[0])), new IntWritable((Integer.parseInt(arr[1]))));    }}</code></pre><hr><pre><code>public class MaxTempApp {    public static void main(String[] args) throws Exception {        Configuration conf = new Configuration();        conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;);        Job job = Job.getInstance(conf);//        设置作业的各种属性        job.setJobName(&quot;MaxTempApp&quot;);    //作业名称        job.setJarByClass(MaxTempApp.class); //搜索类路径        job.setInputFormatClass(TextInputFormat.class);//设置输入格式类        job.setPartitionerClass(YearPartitioner.class);        //添加输入路径        FileInputFormat.addInputPath(job, new Path(args[0]));        //设置输出路径        org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(args[1]));        job.setMapperClass(MaxTempMapper.class);         //mapper类        job.setReducerClass(MaxTempReducer.class);       //reduce类        job.setNumReduceTasks(3);           //reduce个数        job.setMapOutputKeyClass(IntWritable.class);        job.setMapOutputValueClass(IntWritable.class);        job.setOutputKeyClass(IntWritable.class);          //设置输出类型        job.setOutputValueClass(IntWritable.class);        job.waitForCompletion(true);    }}</code></pre><hr><p>其实无非就是加一个partitioner的这个类而已。</p><hr><hr><hr><h1 id="全排序采样器"><a href="#全排序采样器" class="headerlink" title="全排序采样器"></a>全排序采样器</h1><p>1.定义1个reduce</p><p>2.自定义分区函数。<br>：        自行设置分解区间。</p><p>3.使用hadoop采样机制。</p><p>通过采样器生成分区文件，结合hadoop的TotalOrderPartitioner进行分区划分</p><p>TotalOrderPartitioner        //全排序分区类,读取外部生成的分区文件确定区间。</p><p>使用时采样代码在最后端,否则会出现错误。</p><p>//分区文件设置，设置的job的配置对象，不要是之前的conf.这里面的getconfiguration()是对最开始new的conf的拷贝，<br>TotalOrderPartitioner.setPartitionFile(job.getConfiguration(),new Path(“d:/mr/par.lst”));</p><h2 id="首先一段产生随机年份，温度的代码"><a href="#首先一段产生随机年份，温度的代码" class="headerlink" title="首先一段产生随机年份，温度的代码"></a>首先一段产生随机年份，温度的代码</h2><pre><code>public class PrepareTempData {    @Test    public void makeData() throws IOException {        FileWriter fw = new FileWriter(&quot;e:/mr/temp.txt&quot;);        for(int i=0;i&lt;6000;i++){            int year=1970+ new Random().nextInt(100);            int temp=-30 + new Random().nextInt(600);            fw.write(&quot; &quot;+ year + &quot; &quot;+ temp + &quot;\r\n&quot; );        }            fw.close();    }}</code></pre><hr><h2 id="全排序采样器代码"><a href="#全排序采样器代码" class="headerlink" title="全排序采样器代码"></a>全排序采样器代码</h2><pre><code>        public static void main(String[] args) throws Exception {    Configuration conf = new Configuration();    conf.set(&quot;fs.defaultFS&quot;, &quot;file:///&quot;);    Job job = Job.getInstance(conf);        设置作业的各种属性    job.setJobName(&quot;MaxTempApp&quot;);    //作业名称    job.setJarByClass(MaxTempApp.class); //搜索类路径    job.setInputFormatClass(SequenceFileInputFormat.class);//设置输入格式类    //添加输入路径    FileInputFormat.addInputPath(job, new Path(args[0]));    //设置输出路径    org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(args[1]));    job.setMapperClass(MaxTempMapper.class);         //mapper类    job.setReducerClass(MaxTempReducer.class);       //reduce类    job.setMapOutputKeyClass(IntWritable.class);    job.setMapOutputValueClass(IntWritable.class);    job.setOutputKeyClass(IntWritable.class);          //设置输出类型    job.setOutputValueClass(IntWritable.class);    //设置 全排序分区类    job.setPartitionerClass(TotalOrderPartitioner.class);    //将sample数据 写入分区文件    TotalOrderPartitioner.setPartitionFile(conf, new Path(&quot;e:/mr/par.lst&quot;));    //创建随机采样器对象    InputSampler.Sampler&lt;IntWritable, IntWritable&gt; sampler = new InputSampler.RandomSampler&lt;IntWritable, IntWritable&gt;(0.1, 10000, 10);    job.setNumReduceTasks(3);           //reduce个数    InputSampler.writePartitionFile(job, sampler);    job.waitForCompletion(true);    }}</code></pre><hr><pre><code>public class MaxTempMapper extends Mapper&lt;IntWritable, IntWritable, IntWritable, IntWritable&gt; {    public MaxTempMapper() {        System.out.println(&quot;new MaxTempMapper&quot;);    }    @Override    protected void map(IntWritable key, IntWritable value, Context context) throws IOException, InterruptedException {        context.write(key,value);    }}</code></pre><hr><pre><code>public class MaxTempReducer extends Reducer&lt;IntWritable, IntWritable, IntWritable, IntWritable&gt;{    /**     * reduce     */    protected void reduce(IntWritable key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {            int max =Integer.MIN_VALUE;            for(IntWritable iw : values){                max = max &gt; iw.get() ? max : iw.get() ;            }                context.write(key,new IntWritable(max));    }}</code></pre><hr><pre><code>public class YearPartitioner extends Partitioner&lt;IntWritable, IntWritable&gt; {    public int getPartition(IntWritable year, IntWritable temp, int parts) {        int y = year.get() - 1970;        if (y &lt; 33) {            return 0;        }        if (y &gt; 33 &amp;&amp; y &lt; 66) {            return 1;        }        else {            return 2;        }    }}</code></pre><hr><pre><code>全排序官方笔记:1.定义1个reduce2.自定义分区函数.    自行设置分解区间。3.使用hadoop采样机制。    通过采样器生成分区文件，结合hadoop的TotalOrderPartitioner进行分区划分。    TotalOrderPartitioner        //全排序分区类,读取外部生成的分区文件确定区间。    使用时采样代码在最后端,否则会出现错误。    //分区文件设置，设置的job的配置对象，不要是之前的conf.    TotalOrderPartitioner.setPartitionFile(job.getConfiguration(),new Path(&quot;d:/mr/par.lst&quot;));</code></pre><hr><hr><h2 id="全排序和部分排序二次排序在面试比例很重的"><a href="#全排序和部分排序二次排序在面试比例很重的" class="headerlink" title="全排序和部分排序二次排序在面试比例很重的"></a>全排序和部分排序二次排序在面试比例很重的</h2><p>分区在map端，分组在reduce端。<br>二次排序就是把value和key组合到一起，然后value就是nullwritable这样子。合在一起成为一个combokey。<br>Combokey不但要可比较还要串行化，hadoop的串行化机制是writable ，串行就是写入到流离去，反串行就是读出来。<br>IntWritable LongWritable这些都是经过串行化实现的类，Writable有很多子类。 </p><p>value本身不能排序，为了能让他排序，吧value做到key里面去。吧年份和气温都做到key里面去，这个key是自定义的key,然后在combokey中定义里面的排序规则。自定义key,自定义对比器。</p><p>wirtable是串行化机制，java本身也有串行化机制，本质是对象输出流和输入流objectInputStream,objectOutputStream。需要写入和读取就可以了，但是java的串行化效率比较低而且复杂，不能够夸语言，所以hadoop有一个自己的串行化Wriable过程。</p><p>这端是reduce端，经过map端的combokey输出的combokey类是这个样子的，如果不经过处理直接聚合，就会发先，每一个combo都是一个新的都不相同，但是我们想让相同年份的进入到同一个组里面去。所以要重写分组。根据年份写分组，只要是同一个年就是同一组</p><p>reduce端里面有reduce对象和reduce方法。我们说的分几个区进入到几个reduce的意思是进入到几个reduce对象。</p><p><img src="https://i.imgur.com/SSNM7tc.png" alt=""></p><p>现在讲一下下图：下图说在没有分组的情况下，一个key对应好多value,也就是一个1978对应好多个12。好多个12进入到迭代器里面去，不断it.next是下一个v。但是key也改变，其实每次都是变化的，<br><img src="https://i.imgur.com/R4Pj8tj.png" alt=""></p><p><strong>讲一下二次排序和全排序：</strong>全排序就是整个年份-温度数据，分成几个区，让第一个区的最大值小于第二个区的最小值，然后这样子排下去，既解决了分布式的问题，又解决了数据倾斜的问题，但是value是不能排序的，因为mapreduce天生value就不能够排序。那么如何解决让温度也排序呢，就是要把年份和温度做成一个key，传入，然后再排序。就是二次排序，既实现了年份的排序，又实现了温度的排序，这里说的温度的排序是指同一个年份温度升序降序的问题。</p><h1 id="二次排序"><a href="#二次排序" class="headerlink" title="二次排序"></a>二次排序</h1><p>代码如下：</p><pre><code>/** * 自定义组合key */public class ComboKey implements WritableComparable&lt;ComboKey&gt; {    private int year;    private int temp;    public int getYear() {        return year;    }    public void setYear(int year) {        this.year = year;    }    public int getTemp() {        return temp;    }    public void setTemp(int temp) {        this.temp = temp;    }    /**     * 对key进行比较实现     */    public int compareTo(ComboKey o) {        int y0 = o.getYear();        int t0 = o.getTemp();        //年份相同(升序)        if (year == y0) {            //气温降序            return -(temp - t0);            // //这个地方为什么说temp-t0是升序排列，因为这个temp-t0是默认的。默认就是升序排列加一个负号就是降序排列。本身升序排列的就是temp-t0是大于0的        } else {            return year - y0;        }    }    /**     * 串行化过程     */    public void write(DataOutput out) throws IOException {        //年份        out.writeInt(year);        //气温        out.writeInt(temp);    }    public void readFields(DataInput in) throws IOException {        year = in.readInt();        temp = in.readInt();    }}</code></pre><hr><pre><code>/** *ComboKeyComparator */public class ComboKeyComparator extends WritableComparator {    protected ComboKeyComparator() {        super(ComboKey.class, true);    }    public int compare(WritableComparable a, WritableComparable b) {        ComboKey k1 = (ComboKey) a;        ComboKey k2 = (ComboKey) b;        return k1.compareTo(k2);    }}</code></pre><hr><pre><code>    public static void main(String[] args) throws Exception {        Configuration conf = new Configuration();        conf.set(&quot;fs.defaultFS&quot;, &quot;file:///&quot;);        Job job = Job.getInstance(conf);//        设置作业的各种属性        job.setJobName(&quot;SecondarySortApp&quot;);    //作业名称        job.setJarByClass(MaxTempApp.class); //搜索类路径        job.setInputFormatClass(TextInputFormat.class);//设置输入格式类        //添加输入路径        FileInputFormat.addInputPath(job, new Path(args[0]));        //设置输出路径        org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(args[1]));        job.setMapperClass(MaxTempMapper.class);         //mapper类        job.setReducerClass(MaxTempReducer.class);       //reduce类        //设置map输出类型        job.setMapOutputKeyClass(ComboKey.class);        job.setMapOutputValueClass(NullWritable.class);        //设置Reduceoutput类型        job.setOutputKeyClass(IntWritable.class);          //设置输出类型        job.setOutputValueClass(IntWritable.class);        //设置分区类        job.setPartitionerClass(YearPartitioner.class);        //设置分组对比器。        job.setGroupingComparatorClass(YearGroupComparator.class);        //设置排序对比器，听说不写那个ComboKeyComparator不设置也可以        job.setSortComparatorClass(ComboKeyComparator.class);        //reduce个数        job.setNumReduceTasks(3);        job.waitForCompletion(true);    }}</code></pre><hr><pre><code>public class MaxTempMapper extends Mapper&lt;LongWritable, Text, ComboKey, NullWritable&gt; {    @Override    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {        String line = value.toString();        String[] arr = line.split(&quot; &quot;);        ComboKey keyout = new ComboKey();        keyout.setYear(Integer.parseInt(arr[0]));        keyout.setTemp(Integer.parseInt(arr[1]));        context.write(keyout, NullWritable.get());    }  }</code></pre><hr><pre><code>/** * Reducer */public class MaxTempReducer extends Reducer&lt;ComboKey, NullWritable, IntWritable, IntWritable&gt;{    protected void reduce(ComboKey key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException {            int year = key.getYear();            int temp = key.getTemp();            context.write(new IntWritable(year),new IntWritable(temp));    }}</code></pre><hr><pre><code>public class YearGroupComparator extends WritableComparator {    protected YearGroupComparator() {        super(ComboKey.class, true);    }    public int compare(WritableComparable a, WritableComparable b) {        ComboKey k1 = (ComboKey) a ;        ComboKey k2 = (ComboKey) b ;        return k1.getYear() - k2.getYear() ;    }}</code></pre><hr><pre><code>//自定义分区，在map端执行，是map中的一个阶段，mapkv进kv出，kv出去之后要有一个分区的过程。//默认是哈希分区，这里边是修改了分区规则。public class YearPartitioner extends Partitioner&lt;ComboKey,NullWritable&gt; {public int getPartition(ComboKey key, NullWritable nullWritable, int numPartitions) {    int year = key.getYear();    return year % numPartitions;}</code></pre><p>}</p><hr><hr><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;多输入问题&quot;&gt;&lt;a href=&quot;#多输入问题&quot; class=&quot;headerlink&quot; title=&quot;多输入问题&quot;&gt;&lt;/a&gt;多输入问题&lt;/h1&gt;&lt;p&gt;在IDEA里面代码：&lt;/p&gt;
&lt;p&gt;首先明确一点InputInputformat也就是文本输入格式的输入是Longw
      
    
    </summary>
    
      <category term="Hadoop" scheme="http://erichunn.github.io/categories/Hadoop/"/>
    
    
      <category term="多输入问题" scheme="http://erichunn.github.io/tags/%E5%A4%9A%E8%BE%93%E5%85%A5%E9%97%AE%E9%A2%98/"/>
    
      <category term="计数器使用" scheme="http://erichunn.github.io/tags/%E8%AE%A1%E6%95%B0%E5%99%A8%E4%BD%BF%E7%94%A8/"/>
    
      <category term="跟踪运行信息" scheme="http://erichunn.github.io/tags/%E8%B7%9F%E8%B8%AA%E8%BF%90%E8%A1%8C%E4%BF%A1%E6%81%AF/"/>
    
      <category term="产生随机数文件" scheme="http://erichunn.github.io/tags/%E4%BA%A7%E7%94%9F%E9%9A%8F%E6%9C%BA%E6%95%B0%E6%96%87%E4%BB%B6/"/>
    
      <category term="全排序" scheme="http://erichunn.github.io/tags/%E5%85%A8%E6%8E%92%E5%BA%8F/"/>
    
      <category term="全排序采样器" scheme="http://erichunn.github.io/tags/%E5%85%A8%E6%8E%92%E5%BA%8F%E9%87%87%E6%A0%B7%E5%99%A8/"/>
    
      <category term="二次排序" scheme="http://erichunn.github.io/tags/%E4%BA%8C%E6%AC%A1%E6%8E%92%E5%BA%8F/"/>
    
      <category term="倒排序" scheme="http://erichunn.github.io/tags/%E5%80%92%E6%8E%92%E5%BA%8F/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop第六天之Yarn作业提交</title>
    <link href="http://erichunn.github.io/2018/10/11/Hadoop%E7%AC%AC%E5%85%AD%E5%A4%A9%E4%B9%8BYarn%E4%BD%9C%E4%B8%9A%E6%8F%90%E4%BA%A4/"/>
    <id>http://erichunn.github.io/2018/10/11/Hadoop第六天之Yarn作业提交/</id>
    <published>2018-10-11T07:48:32.000Z</published>
    <updated>2018-10-16T12:27:50.548Z</updated>
    
    <content type="html"><![CDATA[<h2 id="本地模式job提交流程"><a href="#本地模式job提交流程" class="headerlink" title="本地模式job提交流程"></a>本地模式job提交流程</h2><pre><code>mr.Job = new Job();job.setxxx();JobSubmitter.提交LocalJobRunner.Job();start();</code></pre><h2 id="hdfs-write"><a href="#hdfs-write" class="headerlink" title="hdfs write"></a>hdfs write</h2><pre><code>packet,</code></pre><h2 id="hdfs-切片计算方式"><a href="#hdfs-切片计算方式" class="headerlink" title="hdfs 切片计算方式"></a>hdfs 切片计算方式</h2><pre><code>getFormatMinSplitSize() = 1//最小值(&gt;=1)                            1                        0long minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));//最大值(&lt;= Long.Max , mapreduce.input.fileinputformat.split.maxsize=)long maxSize = getMaxSplitSize(job);//得到block大小long blockSize = file.getBlockSize();//minSplit maxSplit blockSize//Math.max(minSize, Math.min(maxSize, blockSize));</code></pre><p>LF : Line feed,换行符<br>private static final byte CR = ‘\r’;<br>private static final byte LF = ‘\n’;</p><h2 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h2><pre><code>1.Windows    源文件大小:82.8k    源文件类型:txt    压缩性能比较                |    DeflateCodec    GzipCodec    BZip2Codec    Lz4Codec    SnappyCodec    |结论    ------------|-------------------------------------------------------------------|----------------------    压缩时间(ms)|    450                7            196            44            不支持        |Gzip &gt; Lz4 &gt; BZip2 &gt; Deflate    ------------|-------------------------------------------------------------------|----------------------    解压时间(ms)|    444                66            85            33                        |lz4  &gt; gzip &gt; bzip2 &gt; Deflate    ------------|-------------------------------------------------------------------|----------------------    占用空间(k)    |    19k                19k            17k            31k            不支持        |Bzip &gt; Deflate = Gzip &gt; Lz4                |                                                                    |2.CentOS    源文件大小:82.8k    源文件类型:txt                |    DeflateCodec    GzipCodec    BZip2Codec    Lz4Codec    LZO        SnappyCodec    |结论    ------------|---------------------------------------------------------------------------|----------------------    压缩时间(ms)|    944                77            261            53            77        不支持        |Gzip &gt; Lz4 &gt; BZip2 &gt; Deflate    ------------|---------------------------------------------------------------------------|----------------------    解压时间(ms)|    67                66            106            52            73                    |lz4  &gt; gzip &gt; Deflate&gt; lzo &gt; bzip2     ------------|---------------------------------------------------------------------------|----------------------    占用空间(k)    |    19k                19k            17k            31k            34k                    |Bzip &gt; Deflate = Gzip &gt; Lz4 &gt; lzo</code></pre><h2 id="远程调试"><a href="#远程调试" class="headerlink" title="远程调试"></a>远程调试</h2><pre><code>1.设置服务器java vm的-agentlib:jdwp选项.[server]//windwos//set JAVA_OPTS=%JAVA_OPTS% -agentlib:jdwp=transport=dt_socket,address=8888,server=y,suspend=n//linuxexport HADOOP_CLIENT_OPTS=-agentlib:jdwp=transport=dt_socket,address=8888,server=y,suspend=y2.在server启动java程序    hadoop jar HdfsDemo.jar com.it18zhang.hdfs.mr.compress.TestCompress3.server会暂挂在8888.    Listening ...4.客户端通过远程调试连接到远程主机的8888.5.客户端就可以调试了。</code></pre><p>hadoop jar<br>java </p><p>hadoop jar com.it18zhang.hdfs.mr.compress.TestCompress</p><p>export HADOOP_CLIENT_OPTS=-agentlib:jdwp=transport=dt_socket,address=8888,server=y,suspend=y</p><h2 id="在pom-xml中引入新的插件-maven-antrun-plugin-实现文件的复制"><a href="#在pom-xml中引入新的插件-maven-antrun-plugin-实现文件的复制" class="headerlink" title="在pom.xml中引入新的插件(maven-antrun-plugin),实现文件的复制."></a>在pom.xml中引入新的插件(maven-antrun-plugin),实现文件的复制.</h2><pre><code>[pom.xml]&lt;project&gt;    ...    &lt;build&gt;        &lt;plugins&gt;            ...            &lt;plugin&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-antrun-plugin&lt;/artifactId&gt;                &lt;version&gt;1.8&lt;/version&gt;                &lt;executions&gt;                    &lt;execution&gt;                        &lt;phase&gt;package&lt;/phase&gt;                        &lt;goals&gt;                            &lt;goal&gt;run&lt;/goal&gt;                        &lt;/goals&gt;                        &lt;configuration&gt;                            &lt;tasks&gt;                                &lt;echo&gt;---------开始复制jar包到共享目录下----------&lt;/echo&gt;                                &lt;delete file=&quot;D:\downloads\bigdata\data\HdfsDemo-1.0-SNAPSHOT.jar&quot;&gt;&lt;/delete&gt;                                &lt;copy file=&quot;target/HdfsDemo-1.0-SNAPSHOT.jar&quot; toFile=&quot;D:\downloads\bigdata\data\HdfsDemo.jar&quot;&gt;                                &lt;/copy&gt;                            &lt;/tasks&gt;                        &lt;/configuration&gt;                    &lt;/execution&gt;                &lt;/executions&gt;            &lt;/plugin&gt;        &lt;/plugins&gt;    &lt;/build&gt;    ...&lt;/project&gt;</code></pre><h2 id="在centos上使用yum安装snappy压缩库文件"><a href="#在centos上使用yum安装snappy压缩库文件" class="headerlink" title="在centos上使用yum安装snappy压缩库文件"></a>在centos上使用yum安装snappy压缩库文件</h2><pre><code>[google snappy]$&gt;sudo yum search snappy                #查看是否有snappy库$&gt;sudo yum install -y snappy.x86_64        #安装snappy压缩解压缩库</code></pre><h2 id="库文件"><a href="#库文件" class="headerlink" title="库文件"></a>库文件</h2><pre><code>windows    :dll(dynamic linked library)linux    :so(shared object)</code></pre><h2 id="LZO"><a href="#LZO" class="headerlink" title="LZO"></a>LZO</h2><pre><code>1.在pom.xml引入lzo依赖    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;    &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;             xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;             xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;        &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;        &lt;groupId&gt;com.it18zhang&lt;/groupId&gt;        &lt;artifactId&gt;HdfsDemo&lt;/artifactId&gt;        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;        &lt;packaging&gt;jar&lt;/packaging&gt;        &lt;build&gt;            &lt;plugins&gt;                &lt;plugin&gt;                    &lt;groupId&gt;org.apache.maven.plugins                    &lt;/groupId&gt;                    &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;                    &lt;configuration&gt;                        &lt;source&gt;1.8&lt;/source&gt;                        &lt;target&gt;1.8&lt;/target&gt;                    &lt;/configuration&gt;                &lt;/plugin&gt;                &lt;plugin&gt;                    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                    &lt;artifactId&gt;maven-antrun-plugin&lt;/artifactId&gt;                    &lt;version&gt;1.8&lt;/version&gt;                    &lt;executions&gt;                        &lt;execution&gt;                            &lt;phase&gt;package&lt;/phase&gt;                            &lt;goals&gt;                                &lt;goal&gt;run&lt;/goal&gt;                            &lt;/goals&gt;                            &lt;configuration&gt;                                &lt;tasks&gt;                                    &lt;echo&gt;---------开始复制jar包到共享目录下----------&lt;/echo&gt;                                    &lt;delete file=&quot;D:\downloads\bigdata\data\HdfsDemo-1.0-SNAPSHOT.jar&quot;&gt;&lt;/delete&gt;                                    &lt;copy file=&quot;target/HdfsDemo-1.0-SNAPSHOT.jar&quot; toFile=&quot;D:\downloads\bigdata\data\HdfsDemo.jar&quot;&gt;                                    &lt;/copy&gt;                                &lt;/tasks&gt;                            &lt;/configuration&gt;                        &lt;/execution&gt;                    &lt;/executions&gt;                &lt;/plugin&gt;            &lt;/plugins&gt;        &lt;/build&gt;        &lt;dependencies&gt;            &lt;dependency&gt;                &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;                &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;                &lt;version&gt;2.7.3&lt;/version&gt;            &lt;/dependency&gt;            &lt;dependency&gt;                &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;                &lt;artifactId&gt;hadoop-yarn-common&lt;/artifactId&gt;                &lt;version&gt;2.7.3&lt;/version&gt;            &lt;/dependency&gt;            &lt;dependency&gt;                &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;                &lt;artifactId&gt;hadoop-yarn-client&lt;/artifactId&gt;                &lt;version&gt;2.7.3&lt;/version&gt;            &lt;/dependency&gt;            &lt;dependency&gt;                &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;                &lt;artifactId&gt;hadoop-yarn-server-resourcemanager&lt;/artifactId&gt;                &lt;version&gt;2.7.3&lt;/version&gt;            &lt;/dependency&gt;            &lt;dependency&gt;                &lt;groupId&gt;org.anarres.lzo&lt;/groupId&gt;                &lt;artifactId&gt;lzo-hadoop&lt;/artifactId&gt;                &lt;version&gt;1.0.0&lt;/version&gt;                &lt;scope&gt;compile&lt;/scope&gt;            &lt;/dependency&gt;            &lt;dependency&gt;                &lt;groupId&gt;junit&lt;/groupId&gt;                &lt;artifactId&gt;junit&lt;/artifactId&gt;                &lt;version&gt;4.11&lt;/version&gt;            &lt;/dependency&gt;        &lt;/dependencies&gt;    &lt;/project&gt;2.在centos上安装lzo库    $&gt;sudo yum -y install lzo3.使用mvn命令下载工件中的所有依赖    进入pom.xml所在目录，运行cmd：    mvn -DoutputDirectory=./lib -DgroupId=com.it18zhang -DartifactId=HdfsDemo -Dversion=1.0-SNAPSHOT dependency:copy-dependencies4.在lib下存放依赖所有的第三方jar5.找出lzo-hadoop.jar + lzo-core.jar复制到hadoop的响应目录下。    $&gt;cp lzo-hadoop.jar lzo-core.jar /soft/hadoop/shared/hadoop/common/lib6.执行远程程序即可。</code></pre><h2 id="修改maven使用aliyun镜像。"><a href="#修改maven使用aliyun镜像。" class="headerlink" title="修改maven使用aliyun镜像。"></a>修改maven使用aliyun镜像。</h2><pre><code>[maven/conf/settings.xml]&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;settings xmlns=&quot;http://maven.apache.org/SETTINGS/1.0.0&quot;          xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;          xsi:schemaLocation=&quot;http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd&quot;&gt;  &lt;pluginGroups&gt;  &lt;/pluginGroups&gt;  &lt;proxies&gt;  &lt;/proxies&gt;&lt;servers&gt;    &lt;server&gt;        &lt;id&gt;releases&lt;/id&gt;        &lt;username&gt;admin&lt;/username&gt;        &lt;password&gt;admin123&lt;/password&gt;    &lt;/server&gt;    &lt;server&gt;        &lt;id&gt;snapshots&lt;/id&gt;        &lt;username&gt;admin&lt;/username&gt;        &lt;password&gt;admin123&lt;/password&gt;    &lt;/server&gt;    &lt;server&gt;        &lt;id&gt;Tomcat7&lt;/id&gt;        &lt;username&gt;tomcat&lt;/username&gt;        &lt;password&gt;tomcat&lt;/password&gt;    &lt;/server&gt;&lt;/servers&gt;&lt;mirrors&gt;     &lt;mirror&gt;        &lt;id&gt;nexus-aliyun&lt;/id&gt;        &lt;mirrorOf&gt;*&lt;/mirrorOf&gt;        &lt;name&gt;Nexus aliyun&lt;/name&gt;        &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt;    &lt;/mirror&gt; &lt;/mirrors&gt;&lt;/settings&gt;</code></pre><h2 id="文件格式-SequenceFile"><a href="#文件格式-SequenceFile" class="headerlink" title="文件格式:SequenceFile"></a>文件格式:SequenceFile</h2><pre><code>1.SequenceFile    Key-Value对方式。2.不是文本文件，是二进制文件。3.可切割    因为有同步点。    reader.sync(pos);    //定位到pos之后的第一个同步点。    writer.sync();        //写入同步点4.压缩方式    不压缩    record压缩            //只压缩value    块压缩                //按照多个record形成一个block.</code></pre><h2 id="文件格式-MapFile"><a href="#文件格式-MapFile" class="headerlink" title="文件格式:MapFile"></a>文件格式:MapFile</h2><pre><code>1.Key-value2.key按升序写入(可重复)。3.mapFile对应一个目录，目录下有index和data文件,都是序列文件。4.index文件划分key区间,用于快速定位。</code></pre><h2 id="自定义分区函数"><a href="#自定义分区函数" class="headerlink" title="自定义分区函数"></a>自定义分区函数</h2><pre><code>1.定义分区类    public class MyPartitioner extends Partitioner&lt;Text, IntWritable&gt;{        public int getPartition(Text text, IntWritable intWritable, int numPartitions) {            return 0;        }    }2.程序中配置使用分区类    job.setPartitionerClass(MyPartitioner.class);</code></pre><h2 id="combinerd（合成）-继承了Reducer-任何的Reducer类都能被他使用"><a href="#combinerd（合成）-继承了Reducer-任何的Reducer类都能被他使用" class="headerlink" title="combinerd（合成） 继承了Reducer 任何的Reducer类都能被他使用"></a>combinerd（合成） 继承了Reducer 任何的Reducer类都能被他使用</h2><p> Map端的Reducer  预先化简<br>1，为了减少网络带宽 将Map端发出的的数据进行聚合 并不是所有的都可以用combiner<br>2,combiner </p><hr><p>切片个数是四个，mapper就需要也是4个 </p><p>下图是一个客户端的写入过程：首先形成一个管线，因为有很多节点，节点形成管线，现在本地构建2个队列，一个发送队列sendQueue，一个是确认队列ackQueue，客户端吧消息封装成Packet放到队列里面，而且结束后会放一个空包进去，发送的话，是以管线的方式发给A，A在发给B，B在发给C，在发送的过程中有一个校验的过程chunk(4+512)就是4个校验位+512个数据字节，一个Packet是64K+33个头的字节，在确认队列里面是一个编号seqNo如果在丢列中ABC中都回执，就吧seqNo在ackQueue中移除。发送如果失败，就吧管线重新建立，吧没法送的包返回到队列之前的一个容错机制。<br><img src="https://i.imgur.com/43B2Ebp.png" alt=""></p><p>输入格式定位文件，在输入格式中间有一个交互reader，在Map和Reduce都有这么一些方法，在Mapper和Reducer中有回调方法，就是本来使用的是子类的方法，但是子类中调用有父类的的run方法，然后run方法里面有reduce方法也就又调用了子类的方法，这叫做回调 。Mapper是通过reader读取下一条数据传进来的。Map和Reduce中间有一个shuffer的混洗过程，网络间分发数据的过程， Map里面先分区，分区个数等于Reduce的个数，所以有几个reduce就有几个分区，默认情况下，Map分区通过哈希算法来分区的，对K哈希。只要K相同，一定会被哈希到一个分区里面，分区也就是桶的概念，也就是只要K相同进入到同一个桶里面，只要是同一个分区就是进入到同一个reduce，默认情况下inputformat的格式，他的K就是偏移量。如果是图片格式的话就可能是别的。只有在文本输入的时候才是偏移量，当然也可以自己定义Inputformat格式。在mr计算之前切片就已经算好了，产生的切片文件和配置XML文件放到了临时目录下了。<br><img src="https://i.imgur.com/7qjZLGJ.png" alt=""></p><p>看下图，首先客户端运行作业，第一步run job，第二部走ResourceManager，是Yarn层面，负责资源调度，所有集群的资源都属于ResourceManager，第二部get new application，首先得到应用的ID，第三部copyt job resources，复制切片信息和xml配置信息和jar包到HDFS，第四部提交给资源管理器，然后资源管理器start container，然后通过节点管理器启动一个MRAPPMaster（应用管理程序），第六步初始化Job，第七部检索切片信息，第八步请求资源管理器去分配一个资源列表，第九步启动节点管理器，通过节点管理器来启动一个JVM虚拟机，在虚拟机里面启动yarn子进程，第十部通过子进程读取切片信息，然后在运行MapTast或者ReduceTask<br><img src="https://i.imgur.com/lqVQLT1.png" alt=""></p><p>客户端提取一个作业，然后联系资源管理器，为了得到id,在完全分布式下客户端client叫job，一旦提交就叫做application了，先请求，然后得到了appid返回给了客户端，第三部将作业信息上传到HDFS的临时目录，存档切片信息和jar包，然后找到节点管理器，然后NM孵化一个MRAPPMaster，然后检索HDFS文件系统，获取到有多少个要执行的并发任务，拿到任务之后，要求资源管理器分配一个集合，哪些节点可以用。 然后MRAppmaster在联系其他的NM，让NM去开启Yarn子进程，子进程在去检索HDFS信息，在开启Map和Reduce</p><p><img src="https://i.imgur.com/bn7A9H6.png" alt=""></p><h2 id="hdfs-切片计算方式-1"><a href="#hdfs-切片计算方式-1" class="headerlink" title="hdfs 切片计算方式"></a>hdfs 切片计算方式</h2><pre><code>getFormatMinSplitSize() = 1//最小值(&gt;=1)                            1                        0long minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));//最大值(&lt;= Long.Max , mapreduce.input.fileinputformat.split.maxsize=)long maxSize = getMaxSplitSize(job);//得到block大小long blockSize = file.getBlockSize();//minSplit maxSplit blockSize//Math.max(minSize, Math.min(maxSize, blockSize));</code></pre><p>在计算切片的时候一般块大小不设置，是最好的，一般就是128M，采用本地优先策略，一个快大小是128m如果一旦把他边变1m那么切片大小就会变成1M，一个，然后128M的块大小会被分发128份切片分别传输给MR去计算，这样的话，就会增加网络间的传输，增加网络负载。所以128m是最合适的</p><hr><p>CR=‘\r’回车符<br>LF+’\n’换行符</p><p>windows系统里面是\r\n。回车带换行。<br>linux系统只有一个\n</p><p>切片是定义大方向的，阅读器recorderreader是控制细节的。切片定义任务数量，开几个map但是不一定是每个map都有任务。</p><p><strong>切片问题</strong>是物理设置，但是是逻辑读取。</p><p><img src="https://i.imgur.com/tgAbsGx.png" alt=""></p><p>打个比方说上图这个东西比如说第一个例子，前面的设置物理划分，根据字节数划分成了4个切片，也就是有4个Mapper,然后四个切片开始读取，其中第一个物理切割到t但是实际上，由于切片读取到换行符所以，第一个Mapper也就是读取到了hello world tom，然后第二个，第二个偏移量是13+13.偏移量是13，在物理读取13个直接，但是由于Mapper检测不是行首，所以直接从下一行读取也在读取一整行。这个就是每个切片128MB的缩影，吧128M变成了13而已。然后其实Textinputformat下一个环节是。recoderreader。这个会先检测是否是行首，<br><img src="https://i.imgur.com/7qjZLGJ.png" alt="">      </p><h2 id="压缩问题"><a href="#压缩问题" class="headerlink" title="压缩问题"></a>压缩问题</h2><pre><code>package com.it18zhang.hdfs.mr.compress;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.io.IOUtils;import org.apache.hadoop.io.compress.*;import org.apache.hadoop.util.ReflectionUtils;import org.junit.Test;import java.io.FileInputStream;import java.io.FileOutputStream;/** * @Title:TestCompress * @Description: * @Auther: Eric Hunn * @Version:1.0 * @Create 2018/10/14 19:39 */public class TestCompress {    @Test    public void deflateCompress() throws Exception {        Class[] zipClasses = {                DeflateCodec.class,                GzipCodec.class,                BZip2Codec.class,                Lz4Codec.class,        };        for(Class c : zipClasses){            zip(c);        }    }    /*压缩测试     *     * */    public void zip(Class codecClass) throws Exception {        long start = System.currentTimeMillis();        //实例化对象        CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, new Configuration());        //创建文件输出流，得到默认拓展名        FileOutputStream fos = new FileOutputStream(&quot;e:/comp/b&quot; + codec.getDefaultExtension());        //得到压缩流        CompressionOutputStream zipOut = codec.createOutputStream(fos);        IOUtils.copyBytes(new FileInputStream(&quot;e:/11111.txt/&quot;), zipOut, 1024);        zipOut.close();        System.out.println(codecClass.getSimpleName() + &quot; ：&quot;);    }}                                     </code></pre><p>压缩一般是在文件Map输出的时候压缩，Map输出的时候内存不够一般要往磁盘里面溢出，在溢出的时候可以让他压缩溢出，这样reduce的时候就要解压缩。上面代码测试过lz4不行，但是视频上是可以的，暂时不知道哪里出错了。</p><pre><code>package com.it18zhang.hdfs.mr.compress;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.io.IOUtils;import org.apache.hadoop.io.compress.*;import org.apache.hadoop.util.ReflectionUtils;import org.junit.Test;import java.io.FileInputStream;import java.io.FileOutputStream;/** * @Title:TestCompress * @Description: * @Auther: Eric Hunn * @Version:1.0 * @Create 2018/10/14 19:39 */public class TestCompress {    @Test    public void deflateCompress() throws Exception {        Class[] zipClasses = {                DeflateCodec.class,                GzipCodec.class,                BZip2Codec.class,//                Lz4Codec.class,//                SnappyCodec.class,        };        for(Class c : zipClasses){            unzip(c);        }    }    /*压缩测试     *     * */    public void zip(Class codecClass) throws Exception {        long start = System.currentTimeMillis();        //实例化对象        CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, new Configuration());        //创建文件输出流，得到默认拓展名        FileOutputStream fos = new FileOutputStream(&quot;e:/comp/b&quot; + codec.getDefaultExtension());        //得到压缩流        CompressionOutputStream zipOut = codec.createOutputStream(fos);        IOUtils.copyBytes(new FileInputStream(&quot;e:/11111.txt/&quot;), zipOut, 1024);        zipOut.close();        System.out.println(codecClass.getSimpleName() + &quot; ：&quot;);    }    public void unzip(Class codecClass) throws Exception {        long start = System.currentTimeMillis();        //实例化对象        CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, new Configuration());        //创建文件输出流，得到默认拓展名        FileInputStream fis = new FileInputStream(&quot;e:/comp/b&quot; + codec.getDefaultExtension());        //得到压缩流        CompressionInputStream zipIn = codec.createInputStream(fis);        IOUtils.copyBytes(zipIn,new FileOutputStream(&quot;e:/comp/b&quot; + codec.getDefaultExtension() + &quot;.txt&quot;), 1024);        zipIn.close();        System.out.println(codecClass.getSimpleName() + &quot; ：&quot; + (System.currentTimeMillis() - start));    }}</code></pre><h2 id="在集群上运"><a href="#在集群上运" class="headerlink" title="在集群上运"></a>在集群上运</h2><pre><code>package com.it18zhang.hdfs.mr.compress;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.io.IOUtils;import org.apache.hadoop.io.compress.*;import org.apache.hadoop.util.ReflectionUtils;import java.io.FileInputStream;import java.io.FileOutputStream;/** * @Title:TestCompress * @Description: * @Auther: Eric Hunn * @Version:1.0 * @Create 2018/10/14 19:39 */public class TestCompress {    public static void main(String[] args) throws Exception {        Class[] zipClasses = {                DeflateCodec.class,                GzipCodec.class,                BZip2Codec.class,//                Lz4Codec.class,//                SnappyCodec.class,        };        for(Class c : zipClasses){            zip(c);        }        System.out.println(&quot;==================================&quot;);        for(Class c : zipClasses){            unzip(c);        }    }    /*压缩测试     *     * */    public static void zip(Class codecClass) throws Exception {        long start = System.currentTimeMillis();        //实例化对象        CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, new Configuration());        //创建文件输出流，得到默认拓展名        FileOutputStream fos = new FileOutputStream(&quot;/home/centos/zip/b&quot; + codec.getDefaultExtension());        //得到压缩流        CompressionOutputStream zipOut = codec.createOutputStream(fos);        IOUtils.copyBytes(new FileInputStream(&quot;/home/centos/zip/a.txt&quot;), zipOut, 1024);        zipOut.close();        System.out.println(codecClass.getSimpleName() + &quot; ：&quot;);    }    public static void unzip(Class codecClass) throws Exception {        long start = System.currentTimeMillis();        //实例化对象        CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, new Configuration());        //创建文件输出流，得到默认拓展名        FileInputStream fis = new FileInputStream(&quot;/home/centos/zip/b&quot; + codec.getDefaultExtension());        //得到压缩流        CompressionInputStream zipIn = codec.createInputStream(fis);        IOUtils.copyBytes(zipIn,new FileOutputStream(&quot;/home/centos/zip/b&quot; + codec.getDefaultExtension() + &quot;.txt&quot;), 1024);        zipIn.close();        System.out.println(codecClass.getSimpleName() + &quot; ：&quot; + (System.currentTimeMillis() - start));    }}</code></pre><h2 id="远程调试-1"><a href="#远程调试-1" class="headerlink" title="远程调试"></a>远程调试</h2><pre><code>1.设置服务器java vm的-agentlib:jdwp选项.[server]//windwos//set JAVA_OPTS=%JAVA_OPTS% -agentlib:jdwp=transport=dt_socket,address=8888,server=y,suspend=n//linux2.在server启动java程序    hadoop jar HdfsDemo.jar com.it18zhang.hdfs.mr.compress.TestCompress3.server会暂挂在8888.    Listening ...4.客户端通过远程调试连接到远程主机的8888.5.客户端就可以调试了。</code></pre><hr><hr><h3 id="通过MapFile来写入"><a href="#通过MapFile来写入" class="headerlink" title="通过MapFile来写入"></a>通过MapFile来写入</h3><pre><code>    /*写操作 * */@Testpublic void save() throws Exception {    Configuration conf = new Configuration();    conf.set(&quot;fs.defaultFS &quot;, &quot;file:///&quot;);    FileSystem fs = FileSystem.get(conf);    Path p = new Path(&quot;e:/seq/1.seq&quot;);    MapFile.Writer writer = new MapFile.Writer(conf, fs, &quot;e:/map&quot;, IntWritable.class, Text.class);    for (int i = 0; i &lt; 100000; i++) {        writer.append(new IntWritable(i), new Text(&quot;tom&quot; + i));    }</code></pre><p>//        for(int i =0 ;i &lt; 10 ; i++){<br>//            writer.append(new IntWritable(i),new Text(“tom” + i));<br>//        }<br>        writer.close();<br>    }</p><hr><h3 id="通过MapFile来读取"><a href="#通过MapFile来读取" class="headerlink" title="通过MapFile来读取"></a>通过MapFile来读取</h3><pre><code>/*读取Mapfile文件 * */@Testpublic void readMapfile() throws Exception {    Configuration conf = new Configuration();    conf.set(&quot;fs.defaultFS &quot;, &quot;file:///&quot;);    FileSystem fs = FileSystem.get(conf);    Path p = new Path(&quot;e:/seq/1.seq&quot;);    MapFile.Reader reader = new MapFile.Reader(fs, &quot;e:/map&quot;, conf);    IntWritable key = new IntWritable();    Text value = new Text();    while (reader.next(key, value)) {        System.out.println(key.get() + &quot;:&quot; + value.toString());    }    reader.close();} </code></pre><hr><p>Map的分区是哈希分区</p><p>combiner的好处是：map端的reduce在map端做集合，减少了shuffle量。统计每个单词hello 1发了十万次，不如发一个hello1在map端做聚合发过去hello十万，即刻。就是在map端口预先reduce化简的过程。不是所有的作业都可以把reduce做成combiner。最大值最小值都可以combiner但是平均值就不行了。</p><p>第六天的最后两个视频没有往下写代码，因为太多太杂了。主要讲解MR计数器，数据倾斜，Mapfile 序列化读取，压缩这些。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;本地模式job提交流程&quot;&gt;&lt;a href=&quot;#本地模式job提交流程&quot; class=&quot;headerlink&quot; title=&quot;本地模式job提交流程&quot;&gt;&lt;/a&gt;本地模式job提交流程&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;mr.Job = new Job();
job.se
      
    
    </summary>
    
      <category term="Hadoop" scheme="http://erichunn.github.io/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="http://erichunn.github.io/tags/Hadoop/"/>
    
      <category term="Yan作业提交" scheme="http://erichunn.github.io/tags/Yan%E4%BD%9C%E4%B8%9A%E6%8F%90%E4%BA%A4/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop第五天01之hdfs写入剖析</title>
    <link href="http://erichunn.github.io/2018/10/05/Hadoop%E7%AC%AC%E4%BA%94%E5%A4%A901%E4%B9%8Bhdfs%E5%86%99%E5%85%A5%E5%89%96%E6%9E%90/"/>
    <id>http://erichunn.github.io/2018/10/05/Hadoop第五天01之hdfs写入剖析/</id>
    <published>2018-10-05T08:31:19.000Z</published>
    <updated>2018-10-08T13:12:40.378Z</updated>
    
    <content type="html"><![CDATA[<p>一段HDFS写入流源码分析。</p><p><img src="https://i.imgur.com/17cBkFO.png" alt=""></p><p>首先这个得到了一个抽象类，这个FileSystemm是一个抽象类。</p><p><img src="https://i.imgur.com/Rlbd5VC.png" alt=""></p><p>这个抽象类有子类，左边有一个箭头，点击得到了一个这个抽象类的所有的子类，简单看下，因为Hadoop包类的hadfs这个，得到的子类是DistributedFileSystem这个分布式文件系统。</p><p><img src="https://i.imgur.com/bzQ91XF.png" alt=""></p><p>也可以吧鼠标放到fs上会显示返回的类</p><p><img src="https://i.imgur.com/JOfPZvk.png" alt=""></p><p>也可以在IDEA的右下角的类标签里面找到：</p><p><img src="https://i.imgur.com/WSz2duV.png" alt=""></p><p>也就是说返回了一个DistributedFIleSystem,</p><p>然后文件系统fs.create调用了create方法返回了一个FSDataOutputStream流，这个是一个HDFS数据输出流</p><p><img src="https://i.imgur.com/SKFBqkH.png" alt=""></p><p><img src="https://i.imgur.com/0aIVTzf.png" alt=""></p><p>单机F5单部进入第一个：</p><p><img src="https://i.imgur.com/nnxWOri.png" alt=""></p><p><img src="https://i.imgur.com/f434RJR.png" alt=""></p><p><img src="https://i.imgur.com/4Cf1ljy.png" alt=""></p><p>看这个，进到下一步，调用checkClosed()，方法，这个方法其实是继承FSOutput流里面的东西。  </p><p><img src="https://i.imgur.com/lHPzWs4.png" alt=""><br>看到这个里面out=DFSOutputStream。这个是被装饰的流。调用这个流的write方法</p><p>HDFS流是对DFS输出流的包装</p><p>进去这个是装饰模式。</p><p>在这个构造模式中也声明了字段。</p><p><img src="https://i.imgur.com/8CsurDw.png" alt=""></p><p>下一步：</p><p><img src="https://i.imgur.com/0f1RcbV.png" alt=""></p><p>调用了Close方法因为是继承    都是FSoutput流的子类。一个检查的方法，判断是否数组越界。</p><p>下面这个for是个循环，循环写入，。 </p><p><img src="https://i.imgur.com/MTsauqO.png" alt=""></p><p>然后下一步，进入到write1方法。</p><p><img src="https://i.imgur.com/nAIKdn5.png" alt=""></p><p>里面的buf是一个缓冲区，count是一个成员常量</p><p><img src="https://i.imgur.com/aLCcnkl.png" alt=""></p><p>上图的buf看注释，是一个缓冲区用哦过来存储数据，在他被checksum校验和之前</p><p>校验和就是检查是否是比之前设置的最小块大小大，而且还必须是512字节的倍数</p><p><img src="https://i.imgur.com/pAFducf.png" alt=""></p><p>上图点击进入到一个校验和类的里面看注释，校验和用于数据传输之前。</p><p><img src="https://i.imgur.com/E8qEJy0.png" alt=""></p><p>上图这两行，通过一个算法来实现校验和。通过CRC32算法类似于哈希算法。</p><p><img src="https://i.imgur.com/RJ1vXHv.png" alt=""></p><p>不管他，回到buf缓冲这个地方，单部进入</p><p><img src="https://i.imgur.com/RKkQGry.png" alt=""></p><p>首先在缓冲区进行一个判定</p><p><img src="https://i.imgur.com/qfwP1Yn.png" alt=""></p><p>拷贝本地数据到本地的一个buf，显示Localbuffer=4608,count=0.要拷贝的字节是4608。<br>如果拷贝的内容没有缓冲区大就考数据内容，要是比缓冲区大的话，就拷贝缓冲区大小的内容。</p><p><img src="https://i.imgur.com/rk8wX6W.png" alt=""></p><p>在单部进入到上图。</p><p><img src="https://i.imgur.com/PjqlOXa.png" alt=""></p><p>返回到代码。进入到源代码中，如上图</p><p>在单部进入到这个Close里面：如下图：<br><img src="https://i.imgur.com/97VYh5X.png" alt=""></p><p>这个close是out.close。out是调用父类的close方法，而且父类还是一个包装类，所以进入了包装的列的close方法。<br>再进入到close()方法。如下图：</p><p><img src="https://i.imgur.com/2ExMNXU.png" alt=""></p><p>out的本类HDFSOutputStream，然后这个又是一个装饰类，本质上是DFSOutputStream这个类的方法。</p><p>单部进入到Out里面，看一下调用的到底是谁的out。再单部进入到Out里面</p><p><img src="https://i.imgur.com/RG98Z9F.png" alt=""></p><p>看注释，关闭输出流并且    释放与之相关联的系统资源。<br>上图最终进入到了DFSOutputstream的close()方法里面了。</p><p><img src="https://i.imgur.com/dDhSzjg.png" alt=""></p><p>接上图，看到在这个close(）方法里面有一个closeImp（)方法。调用自己的关闭实现方法<br>还在这个类里面执行呢：继续在这个类里面往下走:如下图：</p><p><img src="https://i.imgur.com/7Vkorih.png" alt=""></p><p>这个里面有一个flushbuffer()方法， 在单部进入到这个方法里面。如下图：</p><p><img src="https://i.imgur.com/YSCeFmf.png" alt=""></p><p>看到这个图里面this和当前类不太一样说明这两个东西也是继承关系。</p><p>清理缓冲区两个参数，看注释：强制任何输出流字节被校验和并且被写入底层的输出流。</p><p>再单步进入：</p><p><img src="https://i.imgur.com/jfWYEI1.png" alt=""></p><p><img src="https://i.imgur.com/4iqESya.png" alt=""></p><p>看到如上图的内容。然后到了writeChecksumChunks这个方法里面，单部进入到这个里面：</p><p><img src="https://i.imgur.com/FM0MpyT.png" alt=""></p><p>对给定的数据小块来生成校验和，并且根据小块和校验和给底层的数据输出流。 </p><p>单部进入到这个sum.calculateChunckedSum方法里面。</p><p><img src="https://i.imgur.com/EVQOpOL.png" alt=""></p><p><img src="https://i.imgur.com/UKsSHTO.png" alt=""></p><p>下一步</p><p><img src="https://i.imgur.com/ifq7oZR.png" alt=""></p><p>上图吧数据写入了底层里面去了。</p><p>下一步</p><p><img src="https://i.imgur.com/WLEldHU.png" alt=""></p><p>下一步</p><p><img src="https://i.imgur.com/TN0fU7T.png" alt=""></p><p>下一步</p><p><img src="https://i.imgur.com/X3dHeJt.png" alt=""></p><p><img src="https://i.imgur.com/W5NCoWA.png" alt=""></p><p>上图就是将底层的内容打成包，再去通过网络传输。在包里面加编号和序号，也就是加顺序。</p><p><img src="https://i.imgur.com/INeugMK.png" alt=""></p><p>往下不写了太多了。现在的视频位置是hdfs写入剖析2的开头。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;一段HDFS写入流源码分析。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/17cBkFO.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;首先这个得到了一个抽象类，这个FileSystemm是一个抽象类。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https:
      
    
    </summary>
    
      <category term="Hadoop" scheme="http://erichunn.github.io/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="http://erichunn.github.io/tags/Hadoop/"/>
    
      <category term="hdfs写入" scheme="http://erichunn.github.io/tags/hdfs%E5%86%99%E5%85%A5/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop第四天之滚动日志-安全模式</title>
    <link href="http://erichunn.github.io/2018/10/04/Hadoop%E7%AC%AC%E5%9B%9B%E5%A4%A9%E4%B9%8B%E6%BB%9A%E5%8A%A8%E6%97%A5%E5%BF%97-%E5%AE%89%E5%85%A8%E6%A8%A1%E5%BC%8F/"/>
    <id>http://erichunn.github.io/2018/10/04/Hadoop第四天之滚动日志-安全模式/</id>
    <published>2018-10-04T00:56:35.000Z</published>
    <updated>2018-10-04T00:56:35.958Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Hadoop第四天之最小块设置-指定副本数</title>
    <link href="http://erichunn.github.io/2018/10/04/Hadoop%E7%AC%AC%E5%9B%9B%E5%A4%A9%E4%B9%8B%E6%9C%80%E5%B0%8F%E5%9D%97%E8%AE%BE%E7%BD%AE-%E6%8C%87%E5%AE%9A%E5%89%AF%E6%9C%AC%E6%95%B0/"/>
    <id>http://erichunn.github.io/2018/10/04/Hadoop第四天之最小块设置-指定副本数/</id>
    <published>2018-10-04T00:21:11.000Z</published>
    <updated>2018-10-04T00:21:11.458Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>遇到未解决的问题</title>
    <link href="http://erichunn.github.io/2018/10/01/%E9%81%87%E5%88%B0%E6%9C%AA%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <id>http://erichunn.github.io/2018/10/01/遇到未解决的问题/</id>
    <published>2018-10-01T03:15:11.000Z</published>
    <updated>2018-11-09T11:48:46.244Z</updated>
    
    <content type="html"><![CDATA[<p>在eclipse中，ctrl+左键找到源码，为什么旁边在pakage explore中有有轮廓啊？</p><p><img src="https://i.imgur.com/oYjJ6In.png" alt=""></p><p><img src="https://i.imgur.com/IZVxlow.png" alt=""></p><p>点击两个箭头即可转换。</p><p>这个东西是在IDEA中怎么调出来的。</p><p><img src="https://i.imgur.com/fz08Y7d.png" alt=""></p><hr><p>克隆centos之后有时候出现这种情况</p><p><img src="https://i.imgur.com/G7CLxft.png" alt=""></p><hr><p>IDEA的使用问题：<br>这个是什么快捷键。</p><p><img src="https://i.imgur.com/hKrt9St.png" alt=""></p><hr><p>在增强虚拟机工具的时候按照视频的方法没有成功，然后通过挂载的方法实现了，但是hgfs文件夹和文件夹里面的内容都是root权限的，只能修改文件夹的所有者，但是不能修改里面文件的所有者。</p><hr><p>有一个分组的问题：<br>    分组就是说组合key出来本来是每一个组合key大多都是不同的key。相同年份不同温度也是不同的key，但是为了相同的年份的组合key能够进入到同一个reduce，所以要通过分组让他这个样子，但是为什么要这个样</p><pre><code>经过分区已经说达到了将不同年份的</code></pre><hr><p>在mapreduce阶段有一个大表和小表连接</p><p>hive里面也有一个小表和小表连接就在map端口，然后小表和大表连接在reduce端口。</p><p>这个地方的问题没有搞明白。具体的hive在hive第二天07mr的忘记在哪里了</p><hr><p>然后mr的最后一天hadoop第十一天的二次排序没有搞得特别明白。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在eclipse中，ctrl+左键找到源码，为什么旁边在pakage explore中有有轮廓啊？&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/oYjJ6In.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://i.im
      
    
    </summary>
    
      <category term="问题" scheme="http://erichunn.github.io/categories/%E9%97%AE%E9%A2%98/"/>
    
    
      <category term="未解决的问题" scheme="http://erichunn.github.io/tags/%E6%9C%AA%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>haoop第三天之脚本分析，单个进程启动</title>
    <link href="http://erichunn.github.io/2018/09/30/haoop%E7%AC%AC%E4%B8%89%E5%A4%A9%E4%B9%8B%E8%84%9A%E6%9C%AC%E5%88%86%E6%9E%90%EF%BC%8C%E5%8D%95%E4%B8%AA%E8%BF%9B%E7%A8%8B%E5%90%AF%E5%8A%A8/"/>
    <id>http://erichunn.github.io/2018/09/30/haoop第三天之脚本分析，单个进程启动/</id>
    <published>2018-09-30T01:33:38.000Z</published>
    <updated>2018-10-28T07:44:57.041Z</updated>
    
    <content type="html"><![CDATA[<h2 id="ssh权限问题"><a href="#ssh权限问题" class="headerlink" title="ssh权限问题"></a>ssh权限问题</h2><pre><code>1.~/.ssh/authorized_keys    6442.$/.ssh    7003.root</code></pre><h2 id="配置SSH"><a href="#配置SSH" class="headerlink" title="配置SSH"></a>配置SSH</h2><pre><code>生成密钥对$&gt;ssh-keygen -t rsa -P &apos;&apos; -f ~/.ssh/id_rsa添加认证文件$&gt;cat ~/.ssh/id_rsa.pub &gt; ~/.ssh/authorized_keys权限设置,文件和文件夹权限除了自己之外，别人不可写。$&gt;chmod 700 ~/.ssh$&gt;chmod 644 ~/.ssh/authorized_keys</code></pre><h2 id="scp"><a href="#scp" class="headerlink" title="scp"></a>scp</h2><pre><code>远程复制.</code></pre><h2 id="rsync"><a href="#rsync" class="headerlink" title="rsync"></a>rsync</h2><pre><code>远程同步,支持符号链接。rsync -lr xxx xxx</code></pre><h2 id="完全分布式"><a href="#完全分布式" class="headerlink" title="完全分布式"></a>完全分布式</h2><pre><code>1.配置文件[core-site.xml]fs.defaultFS=hdfs://s201:8020/[hdfs-site.xml]replication=1        //伪分布replication=3        //完全分布[mapred-site.xml]mapreduce.framework.name=yarn[yarn-site.xml]rm.name=s201[slaves]s202s203s2042.分发文件    a)ssh    openssh-server        //sshd    openssh-clients        //ssh    openssh                //ssh-keygen    b)scp/rsync3.格式化文件系统    $&gt;hadoop namenode -format4.启动hadoop所有进程    //start-dfs.sh + start-yarn.sh    $&gt;start-all.sh5.xcall.sh jps    /usr/local/bin/jps     /usr/local/bin/java6.查看jps进程    $&gt;xcall.sh jps7.关闭centos的防火墙    $&gt;sudo service firewalld stop        // &lt;=6.5    start/stop/status/restart    $&gt;sudo systemctl stop firewalld        // 7.0 停止    start/stop/status/restart    $&gt;sudo systemctl disable firewalld    //关闭    $&gt;sudo systemctl enable firewalld    //启用7.最终通过webui    http://s201:50070/</code></pre><h2 id="符号连接"><a href="#符号连接" class="headerlink" title="符号连接"></a>符号连接</h2><pre><code>1.修改符号连接的owner    $&gt;chown -h centos:centos xxx        //-h:针对连接本身，而不是所指文件.2.修改符号链接    $&gt;ln -sfT index.html index            //覆盖原有的连接。</code></pre><h2 id="hadoop模块"><a href="#hadoop模块" class="headerlink" title="hadoop模块"></a>hadoop模块</h2><pre><code>common        //hdfs        //mapreduce    //yarn        //</code></pre><h2 id="进程"><a href="#进程" class="headerlink" title="进程"></a>进程</h2><pre><code>[hdfs]start-dfs.shNameNode            NNDataNode            DNSecondaryNamenode    2NN[yarn]start-yarn.shResourceMananger    RMNodeManager            NM</code></pre><h2 id="脚本分析"><a href="#脚本分析" class="headerlink" title="脚本分析"></a>脚本分析</h2><pre><code>sbin/start-all.sh--------------    libexec/hadoop-config.sh    start-dfs.sh    start-yarn.shsbin/start-dfs.sh--------------    libexec/hadoop-config.sh    sbin/hadoop-daemons.sh --config .. --hostname .. start namenode ...    sbin/hadoop-daemons.sh --config .. --hostname .. start datanode ...    sbin/hadoop-daemons.sh --config .. --hostname .. start sescondarynamenode ...    sbin/hadoop-daemons.sh --config .. --hostname .. start zkfc ...            //sbin/start-yarn.sh--------------        libexec/yarn-config.sh    bin/yarn-daemon.sh start resourcemanager    bin/yarn-daemons.sh start nodemanagersbin/hadoop-daemons.sh----------------------    libexec/hadoop-config.sh    slaves    hadoop-daemon.shsbin/hadoop-daemon.sh-----------------------    libexec/hadoop-config.sh    bin/hdfs ....sbin/yarn-daemon.sh-----------------------    libexec/yarn-config.sh    bin/yarnbin/hadoop------------------------    hadoop verion        //版本    hadoop fs            //文件系统客户端.    hadoop jar            //    hadoop classpath    hadoop checknativebin/hdfs------------------------    dfs                        // === hadoop fs    classpath              namenode -format       secondarynamenode      namenode               journalnode            zkfc                   datanode               dfsadmin               haadmin                fsck                   balancer               jmxget                 mover                  oiv                    oiv_legacy             oev                    fetchdt                getconf                groups                 snapshotDiff           lsSnapshottableDir     portmap                nfs3                   cacheadmin             crypto                 storagepolicies        version </code></pre><h2 id="hdfs常用命令"><a href="#hdfs常用命令" class="headerlink" title="hdfs常用命令"></a>hdfs常用命令</h2><pre><code>$&gt;hdfs dfs -mkdir /user/centos/hadoop$&gt;hdfs dfs -ls -r /user/centos/hadoop$&gt;hdfs dfs -lsr /user/centos/hadoop$&gt;hdfs dfs -put index.html /user/centos/hadoop$&gt;hdfs dfs -get /user/centos/hadoop/index.html a.html$&gt;hdfs dfs -rm -r -f /user/centos/hadoop</code></pre><h2 id="no-route"><a href="#no-route" class="headerlink" title="no route "></a>no route </h2><pre><code>关闭防火墙。$&gt;su root$&gt;xcall.sh &quot;service firewalld stop&quot;$&gt;xcall.sh &quot;systemctl disable firewalld&quot;</code></pre><h2 id="hdfs"><a href="#hdfs" class="headerlink" title="hdfs"></a>hdfs</h2><pre><code>500G1024G = 2T/4T切割。寻址时间:10ms左右磁盘速率 : 100M /s64M128M            //让寻址时间占用读取时间的1%.1ms1 / 100size = 181260798block-0 : 134217728block-1 :  47043070 --------------------b0.no : 1073741829b1.no : 1073741830</code></pre><h2 id="HA"><a href="#HA" class="headerlink" title="HA"></a>HA</h2><pre><code>high availability,高可用性。通常用几个9衡量。99.999%</code></pre><h2 id="SPOF"><a href="#SPOF" class="headerlink" title="SPOF:"></a>SPOF:</h2><pre><code>single point of failure,单点故障。</code></pre><h2 id="secondarynamenode"><a href="#secondarynamenode" class="headerlink" title="secondarynamenode"></a>secondarynamenode</h2><h2 id="找到所有的配置文件"><a href="#找到所有的配置文件" class="headerlink" title="找到所有的配置文件"></a>找到所有的配置文件</h2><pre><code>1.tar开hadoop-2.7.3.tar.gzhadoop-2.7.3\share\hadoop\common\hadoop-common-2.7.3.jar\core-default.xmlhadoop-2.7.3\share\hadoop\hdfs\hadoop-hdfs-2.7.3.jar\hdfs-default.xmlhadoop-2.7.3\share\hadoop\mapreduce\hadoop-mapreduce-client-core-2.7.3.jar\mapred-default.xmlhadoop-2.7.3\share\hadoop\yarn\hadoop-yarn-common-2.7.3.jar\yarn-site.xml</code></pre><h2 id="本地模式"><a href="#本地模式" class="headerlink" title="本地模式"></a>本地模式</h2><pre><code>[core-site.xml]fs.defaultFS=file:///            //默认值</code></pre><h2 id="配置hadoop临时目录"><a href="#配置hadoop临时目录" class="headerlink" title="配置hadoop临时目录"></a>配置hadoop临时目录</h2><pre><code>1.配置[core-site.xml]文件&lt;configuration&gt;        &lt;property&gt;                &lt;name&gt;fs.defaultFS&lt;/name&gt;                &lt;value&gt;hdfs://s201/&lt;/value&gt;        &lt;/property&gt;        &lt;!--- 配置新的本地目录 --&gt;        &lt;property&gt;                &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;                &lt;value&gt;/home/centos/hadoop&lt;/value&gt;        &lt;/property&gt;&lt;/configuration&gt;//以下属性均由hadoop.tmp.dir决定,在hdfs-site.xml文件中配置。dfs.namenode.name.dir=file://${hadoop.tmp.dir}/dfs/namedfs.datanode.data.dir=file://${hadoop.tmp.dir}/dfs/datadfs.datanode.data.dir=file://${hadoop.tmp.dir}/dfs/datadfs.namenode.checkpoint.dir=file://${hadoop.tmp.dir}/dfs/namesecondarydfs.namenode.checkpoint.dir=file://${hadoop.tmp.dir}/dfs/namesecondary2.分发core-site.xml文件    $&gt;xsync core-site.xml3.格式化文件系统,只对namenode的本地目录进行初始化。    $&gt;hadoop namenode -format        //hdfs namenode -format4.启动hadoop    $&gt;start-dfs.sh</code></pre><h2 id="使用xcall-sh在所有节点上创建jps符号连接，指向-soft-jdk-bin-jps"><a href="#使用xcall-sh在所有节点上创建jps符号连接，指向-soft-jdk-bin-jps" class="headerlink" title="使用xcall.sh在所有节点上创建jps符号连接，指向/soft/jdk/bin/jps"></a>使用xcall.sh在所有节点上创建jps符号连接，指向/soft/jdk/bin/jps</h2><pre><code>1.切换到root用户    $&gt;su root2.创建符号连接    $&gt;xcall.sh &quot;ln -sfT /soft/jdk/bin/jps /usr/local/bin/jps&quot;3.修改jps符号连接的owner    $&gt;xcall.sh &quot;chown -h centos:centos /usr/local/bin/jps&quot;4.查看所有主机上的java进程    $&gt;xcall.sh jps</code></pre><h2 id="在centos桌面版中安装eclipse"><a href="#在centos桌面版中安装eclipse" class="headerlink" title="在centos桌面版中安装eclipse"></a>在centos桌面版中安装eclipse</h2><pre><code>1.下载eclipse linux版    eclipse-jee-mars-R-linux-gtk-x86_64.tar.gz2.tar开到/soft下,    $&gt;tar -xzvf eclipse-jee-mars-R-linux-gtk-x86_64.tar.gz -C /soft3.启动eclipse    $&gt;cd /soft/eclipse    $&gt;./eclipse &amp;            //后台启动4.创建桌面快捷方式    $&gt;ln -s /soft/eclipse/eclipse ~/Desktop/eclipse5.</code></pre><h2 id="收集hadoop的所有jar包"><a href="#收集hadoop的所有jar包" class="headerlink" title="收集hadoop的所有jar包"></a>收集hadoop的所有jar包</h2><h2 id="使用hadoop客户端api访问hdfs"><a href="#使用hadoop客户端api访问hdfs" class="headerlink" title="使用hadoop客户端api访问hdfs"></a>使用hadoop客户端api访问hdfs</h2><pre><code>1.创建java项目2.导入hadoop类库3.4.5.</code></pre><h2 id="网络拓扑"><a href="#网络拓扑" class="headerlink" title="网络拓扑"></a>网络拓扑</h2><pre><code>1.2.3.4.</code></pre><h2 id="作业"><a href="#作业" class="headerlink" title="作业"></a>作业</h2><pre><code>1.使用hadoop API递归输出整个文件系统2.</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;ssh权限问题&quot;&gt;&lt;a href=&quot;#ssh权限问题&quot; class=&quot;headerlink&quot; title=&quot;ssh权限问题&quot;&gt;&lt;/a&gt;ssh权限问题&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;1.~/.ssh/authorized_keys
    644
2.$/.ssh

      
    
    </summary>
    
      <category term="Hadoop" scheme="http://erichunn.github.io/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="http://erichunn.github.io/tags/Hadoop/"/>
    
      <category term="脚本分析" scheme="http://erichunn.github.io/tags/%E8%84%9A%E6%9C%AC%E5%88%86%E6%9E%90/"/>
    
      <category term="单个进程启动" scheme="http://erichunn.github.io/tags/%E5%8D%95%E4%B8%AA%E8%BF%9B%E7%A8%8B%E5%90%AF%E5%8A%A8/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop第二天之搭建</title>
    <link href="http://erichunn.github.io/2018/09/29/Hadoop%E7%AC%AC%E4%BA%8C%E5%A4%A9%E4%B9%8B%E6%90%AD%E5%BB%BA/"/>
    <id>http://erichunn.github.io/2018/09/29/Hadoop第二天之搭建/</id>
    <published>2018-09-29T09:21:35.000Z</published>
    <updated>2018-10-28T07:45:12.135Z</updated>
    
    <content type="html"><![CDATA[<h2 id="hadoop"><a href="#hadoop" class="headerlink" title="hadoop"></a>hadoop</h2><pre><code>1.独立模式(standalone|local)    nothing!    本地文件系统。    不需要启用单独进程。2.pesudo(伪分布模式)    等同于完全分布式，只有一个节点。    SSH:        //(Socket),                //public + private                //server : sshd ps -Af | grep sshd                //clint     : ssh                //ssh-keygen:生成公私秘钥。                //authorized_keys 需要使用644                //ssh 192.168.231.201  yes    [配置文件]        core-site.xml    //fs.defaultFS=hdfs://localhost/        hdfs-site.xml    //replication=1        mapred-site.xml    //        yarn-site.xml    //3.full distributed(完全分布式)</code></pre><h2 id="让命令行提示符显式完整路径"><a href="#让命令行提示符显式完整路径" class="headerlink" title="让命令行提示符显式完整路径"></a>让命令行提示符显式完整路径</h2><pre><code>1.编辑profile文件，添加环境变量PS1    [/etc/profile]    export PS1=&apos;[\u@\h `pwd`]\$&apos;2.source        $&gt;source /etc/profile</code></pre><h2 id="配置hadoop，使用符号连接的方式，让三种配置形态共存。"><a href="#配置hadoop，使用符号连接的方式，让三种配置形态共存。" class="headerlink" title="配置hadoop，使用符号连接的方式，让三种配置形态共存。"></a>配置hadoop，使用符号连接的方式，让三种配置形态共存。</h2><pre><code>1.创建三个配置目录,内容等同于hadoop目录    ${hadoop_home}/etc/local    ${hadoop_home}/etc/pesudo    ${hadoop_home}/etc/full2.创建符号连接    $&gt;ln -s 3.对hdfs进行格式化    $&gt;hadoop namenode -format4.修改hadoop配置文件，手动指定JAVA_HOME环境变量    [${hadoop_home}/etc/hadoop/hadoop-env.sh]    ...    export JAVA_HOME=/soft/jdk    ...5.启动hadoop的所有进程    $&gt;start-all.sh6.启动完成后，出现以下进程    $&gt;jps        33702 NameNode        33792 DataNode        33954 SecondaryNameNode        29041 ResourceManager        34191 NodeManager7.查看hdfs文件系统    $&gt;hdfs dfs -ls /8.创建目录    $&gt;hdfs dfs -mkdir -p /user/centos/hadoop9.通过webui查看hadoop的文件系统    http://localhost:50070/10.停止hadoop所有进程    $&gt;stop-all.sh11.centos防火墙操作    [cnetos 6.5之前的版本]    $&gt;sudo service firewalld stop        //停止服务    $&gt;sudo service firewalld start        //启动服务    $&gt;sudo service firewalld status        //查看状态    [centos7]    $&gt;sudo systemctl enable firewalld.service    //&quot;开机启动&quot;启用    $&gt;sudo systemctl disable firewalld.service    //&quot;开机自启&quot;禁用    $&gt;sudo systemctl start firewalld.service    //启动防火墙    $&gt;sudo systemctl stop firewalld.service        //停止防火墙    $&gt;sudo systemctl status firewalld.service    //查看防火墙状态    [开机自启]    $&gt;sudo chkconfig firewalld    on                //&quot;开启自启&quot;启用    $&gt;sudo chkconfig firewalld    off                //&quot;开启自启&quot;禁用</code></pre><h2 id="hadoop的端口"><a href="#hadoop的端口" class="headerlink" title="hadoop的端口"></a>hadoop的端口</h2><pre><code>50070        //namenode http port50075        //datanode http port50090        //2namenode    http port8020        //namenode rpc port50010        //datanode rpc port</code></pre><h2 id="hadoop四大模块"><a href="#hadoop四大模块" class="headerlink" title="hadoop四大模块"></a>hadoop四大模块</h2><pre><code>commonhdfs        //namenode + datanode + secondarynamenodemapredyarn        //resourcemanager + nodemanager</code></pre><h2 id="启动脚本"><a href="#启动脚本" class="headerlink" title="启动脚本"></a>启动脚本</h2><pre><code>1.start-all.sh        //启动所有进程2.stop-all.sh        //停止所有进程3.start-dfs.sh        //4.start-yarn.sh[hdfs]  start-dfs.sh stop-dfs.sh    NN    DN    2NN[yarn] start-yarn.sh stop-yarn.sh    RM    NM</code></pre><h2 id="修改主机名"><a href="#修改主机名" class="headerlink" title="修改主机名"></a>修改主机名</h2><pre><code>1./etc/hostname    s2012./etc/hosts    127.0.0.1 localhost    192.168.231.201 s201    192.168.231.202 s202    192.168.231.203 s203    192.168.231.204 s204</code></pre><h2 id="完全分布式"><a href="#完全分布式" class="headerlink" title="完全分布式"></a>完全分布式</h2><pre><code>1.克隆3台client(centos7)    右键centos-7--&gt;管理-&gt;克隆-&gt; ... -&gt; 完整克隆2.启动client3.启用客户机共享文件夹。4.修改hostname和ip地址文件    [/etc/hostname]    s202    [/etc/sysconfig/network-scripts/ifcfg-ethxxxx]    ...    IPADDR=..5.重启网络服务    $&gt;sudo service network restart6.修改/etc/resolv.conf文件    nameserver 192.168.231.27.重复以上3 ~ 6过程.</code></pre><h2 id="准备完全分布式主机的ssh"><a href="#准备完全分布式主机的ssh" class="headerlink" title="准备完全分布式主机的ssh"></a>准备完全分布式主机的ssh</h2><pre><code>1.删除所有主机上的/home/centos/.ssh/*2.在s201主机上生成密钥对    $&gt;ssh-keygen -t rsa -P &apos;&apos; -f ~/.ssh/id_rsa3.将s201的公钥文件id_rsa.pub远程复制到202 ~ 204主机上。  并放置/home/centos/.ssh/authorized_keys    $&gt;scp id_rsa.pub centos@s201:/home/centos/.ssh/authorized_keys    $&gt;scp id_rsa.pub centos@s202:/home/centos/.ssh/authorized_keys    $&gt;scp id_rsa.pub centos@s203:/home/centos/.ssh/authorized_keys    $&gt;scp id_rsa.pub centos@s204:/home/centos/.ssh/authorized_keys4.配置完全分布式(${hadoop_home}/etc/hadoop/)    [core-site.xml]    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;    &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;    &lt;configuration&gt;            &lt;property&gt;                    &lt;name&gt;fs.defaultFS&lt;/name&gt;                    &lt;value&gt;hdfs://s201/&lt;/value&gt;            &lt;/property&gt;    &lt;/configuration&gt;    [hdfs-site.xml]    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;    &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;    &lt;configuration&gt;            &lt;property&gt;                    &lt;name&gt;dfs.replication&lt;/name&gt;                    &lt;value&gt;3&lt;/value&gt;            &lt;/property&gt;    &lt;/configuration&gt;    [mapred-site.xml]        不变    [yarn-site.xml]    &lt;?xml version=&quot;1.0&quot;?&gt;    &lt;configuration&gt;            &lt;property&gt;                    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;                    &lt;value&gt;s201&lt;/value&gt;            &lt;/property&gt;            &lt;property&gt;                    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;                    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;            &lt;/property&gt;    &lt;/configuration&gt;    [slaves]    s202    s203    s204    [hadoop-env.sh]    ...    export JAVA_HOME=/soft/jdk    ...5.分发配置    $&gt;cd /soft/hadoop/etc/    $&gt;scp -r full centos@s202:/soft/hadoop/etc/    $&gt;scp -r full centos@s203:/soft/hadoop/etc/    $&gt;scp -r full centos@s204:/soft/hadoop/etc/6.删除符号连接    $&gt;cd /soft/hadoop/etc    $&gt;rm hadoop    $&gt;ssh s202 rm /soft/hadoop/etc/hadoop    $&gt;ssh s203 rm /soft/hadoop/etc/hadoop    $&gt;ssh s204 rm /soft/hadoop/etc/hadoop7.创建符号连接    $&gt;cd /soft/hadoop/etc/    $&gt;ln -s full hadoop    $&gt;ssh s202 ln -s /soft/hadoop/etc/full /soft/hadoop/etc/hadoop    $&gt;ssh s203 ln -s /soft/hadoop/etc/full /soft/hadoop/etc/hadoop    $&gt;ssh s204 ln -s /soft/hadoop/etc/full /soft/hadoop/etc/hadoop8.删除临时目录文件    $&gt;cd /tmp    $&gt;rm -rf hadoop-centos    $&gt;ssh s202 rm -rf /tmp/hadoop-centos    $&gt;ssh s203 rm -rf /tmp/hadoop-centos    $&gt;ssh s204 rm -rf /tmp/hadoop-centos9.删除hadoop日志    $&gt;cd /soft/hadoop/logs    $&gt;rm -rf *    $&gt;ssh s202 rm -rf /soft/hadoop/logs/*    $&gt;ssh s203 rm -rf /soft/hadoop/logs/*    $&gt;ssh s204 rm -rf /soft/hadoop/logs/*10.格式化文件系统    $&gt;hadoop namenode -format11.启动hadoop进程    $&gt;start-all.sh</code></pre><h2 id="rsync"><a href="#rsync" class="headerlink" title="rsync"></a>rsync</h2><pre><code>四个机器均安装rsync命令。远程同步.$&gt;sudo yum install rsync</code></pre><h2 id="将root用户实现无密登录"><a href="#将root用户实现无密登录" class="headerlink" title="将root用户实现无密登录"></a>将root用户实现无密登录</h2><pre><code>1.同</code></pre><h2 id="编写脚本"><a href="#编写脚本" class="headerlink" title="编写脚本"></a>编写脚本</h2><pre><code>1.xcall.sh2.xsync.sh    xsync.sh /home/etc/a.txt    rsync -lr /home/etc/a.txt centos@s202:/home/etc</code></pre><hr><pre><code>netstat -anop    查看进程</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;hadoop&quot;&gt;&lt;a href=&quot;#hadoop&quot; class=&quot;headerlink&quot; title=&quot;hadoop&quot;&gt;&lt;/a&gt;hadoop&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;1.独立模式(standalone|local)
    nothing!
    本地文件
      
    
    </summary>
    
      <category term="Hadoop" scheme="http://erichunn.github.io/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="http://erichunn.github.io/tags/Hadoop/"/>
    
      <category term="搭建" scheme="http://erichunn.github.io/tags/%E6%90%AD%E5%BB%BA/"/>
    
      <category term="第二天" scheme="http://erichunn.github.io/tags/%E7%AC%AC%E4%BA%8C%E5%A4%A9/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop安装配置</title>
    <link href="http://erichunn.github.io/2018/09/24/Hadoop%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/"/>
    <id>http://erichunn.github.io/2018/09/24/Hadoop安装配置/</id>
    <published>2018-09-24T08:10:47.000Z</published>
    <updated>2018-10-28T07:45:59.120Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="-Hadoop" scheme="http://erichunn.github.io/categories/Hadoop/"/>
    
    
      <category term="-Hadoop" scheme="http://erichunn.github.io/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Linux基础之Yum命令</title>
    <link href="http://erichunn.github.io/2018/09/20/Linux%E5%9F%BA%E7%A1%80%E4%B9%8BYum%E5%91%BD%E4%BB%A4/"/>
    <id>http://erichunn.github.io/2018/09/20/Linux基础之Yum命令/</id>
    <published>2018-09-20T10:17:43.000Z</published>
    <updated>2018-10-28T07:48:03.384Z</updated>
    
    <content type="html"><![CDATA[<h2 id="软件源"><a href="#软件源" class="headerlink" title="软件源"></a>软件源</h2><pre><code>Repository        //仓库.URL                //http:// .d                //directory目录xxxd            //daemon</code></pre><h2 id="查看仓库文件"><a href="#查看仓库文件" class="headerlink" title="查看仓库文件"></a>查看仓库文件</h2><pre><code>/etc/yum.repos.d/xxx.repo</code></pre><h2 id="curl"><a href="#curl" class="headerlink" title="curl"></a>curl</h2><p>传输url上的数据的。</p><p>[下载文件到指定目录]</p><pre><code>curl -o /etc/yum.repos.d/ali.repo http://mirrors.aliyun.com/repo/Centos-7.repo</code></pre><h2 id="更换centos的软件源"><a href="#更换centos的软件源" class="headerlink" title="更换centos的软件源"></a>更换centos的软件源</h2><p>1.下载源仓库文件,xxx.repo</p><pre><code>curl -o /etc/yum.repos.d/ali.repo http://mirrors.aliyun.com/repo/Centos-7.repo</code></pre><p>2.将repo文件保存到/etc/yum.repos.d/目录中。</p><h2 id="屏蔽软件仓库"><a href="#屏蔽软件仓库" class="headerlink" title="屏蔽软件仓库"></a>屏蔽软件仓库</h2><p>1.将/etc/yum.repos.d/xxx.repo文件删除或者更换扩展名即可。</p><h2 id="修改centos能够使用sudo命令"><a href="#修改centos能够使用sudo命令" class="headerlink" title="修改centos能够使用sudo命令"></a>修改centos能够使用sudo命令</h2><pre><code>[/etc/sudoers]$&gt;su root$&gt;nano /etc/sudoers    ...    centos ALL</code></pre><h2 id="使用yum进行软件包安装卸载"><a href="#使用yum进行软件包安装卸载" class="headerlink" title="使用yum进行软件包安装卸载"></a>使用yum进行软件包安装卸载</h2><pre><code>$&gt;yum list                            //列出所有软件包$&gt;yum list installed                //列出已经安装的软件包$&gt;yum list installed | grep nano    //列出已经安装的软件包$&gt;yum search nano                    //在yum的软件源中搜索软件 $&gt;yum remove nano                    //卸载软件$&gt;yum -y install nano                //直接安装，不需要yes确认.$&gt;yum list installed | grep nano    //查看是否安装了Nano$&gt;mkdir /home/centos/rpms$echo 以下命令只下载软件，不安装软件$&gt;sudo yum install --downloadonly                //只下载              --downloaddir=/home/centos/rpms    //指定下载目录              wget//下载已经安装的软件$&gt;sudo yum reinstall --downloadonly                         --downloaddir=/home/centos/rpms                     wget$&gt;sudo yum localinstall xxx.rpm    //从本地rpm文件直接安装软件$&gt;su root$&gt;yum search ifconfig$&gt;yum -y install net-tools        //安装网络工具#==========修改网络地址======================    //需要重启network服务$&gt;sudo nano /etc/sysconfig/network-scripts/ifcfg-eth1677736    [/etc/sysconfig/network-scripts/ifcfg-eth1677736]    ...    IPADDR=192.168.231.201    GATEWAY=192.168.231.2    DNS=192.168.231.2$&gt;service network restart                        //重启网络服务。$&gt;sudo nano /etc/resolv.conf                    //修改该文件不需要重启network服务    [/etc/resolv.conf]    nameserver 192.168.231.2</code></pre><h2 id="在没有nano时，使用自带的vi文本编辑器"><a href="#在没有nano时，使用自带的vi文本编辑器" class="headerlink" title="在没有nano时，使用自带的vi文本编辑器"></a>在没有nano时，使用自带的vi文本编辑器</h2><pre><code>1.vi xx.txt</code></pre><p>2.模式切换</p><pre><code>esc                //切换到命令模式,退出编辑模式                //:q!  不保存退出                //:wq  保存退出                //x        删除一个字符                //dd    删除一行insert            //切换到编辑模式,退出命令模式                //del backspace</code></pre><h2 id="Which命令"><a href="#Which命令" class="headerlink" title="Which命令"></a>Which命令</h2><p>which命令用于查找并显示给定命令的绝对路径，环境变量PATH中保存了查找命令时需要遍历的目录。</p><p>which指令会在环境变量$PATH设置的目录里查找符合条件的文件。也就是说，使用which命令，就可以看到某个系统命令是否存在，以及执行的到底是哪一个位置的命令。</p><h3 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h3><pre><code>which(选项)(参数)</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;软件源&quot;&gt;&lt;a href=&quot;#软件源&quot; class=&quot;headerlink&quot; title=&quot;软件源&quot;&gt;&lt;/a&gt;软件源&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;Repository        //仓库.
URL                //http:// 

.d 
      
    
    </summary>
    
    
      <category term="Linux基础" scheme="http://erichunn.github.io/tags/Linux%E5%9F%BA%E7%A1%80/"/>
    
      <category term="Yum命令" scheme="http://erichunn.github.io/tags/Yum%E5%91%BD%E4%BB%A4/"/>
    
  </entry>
  
  <entry>
    <title>Linux基础第之循环命令</title>
    <link href="http://erichunn.github.io/2018/09/19/Linux%E5%9F%BA%E7%A1%80%E7%AC%AC%E4%B9%8B%E5%BE%AA%E7%8E%AF%E5%91%BD%E4%BB%A4/"/>
    <id>http://erichunn.github.io/2018/09/19/Linux基础第之循环命令/</id>
    <published>2018-09-19T10:03:21.000Z</published>
    <updated>2018-09-19T15:39:46.393Z</updated>
    
    <content type="html"><![CDATA[<h2 id="访问文件-夹-物理位置"><a href="#访问文件-夹-物理位置" class="headerlink" title="访问文件(夹)物理位置"></a>访问文件(夹)物理位置</h2><p>pwd命令是显示当前的逻辑位置。</p><p>物理位置是针对符号连接也是针对软连接的。</p><p>//进入/t的物理位置</p><pre><code>$&gt;cd -P /t    </code></pre><p>//显式当前目录的物理位置</p><pre><code>$&gt;pwd -P            </code></pre><p><img src="https://i.imgur.com/uCWAgEF.png" alt=""></p><h2 id="访问环境变量"><a href="#访问环境变量" class="headerlink" title="访问环境变量"></a>访问环境变量</h2><pre><code>echo ${PATH}                //okecho $PATH                    //okecho &quot;$PATH&quot;                //okecho &apos;$PATH&apos;                //&apos;&apos;原样输出这个不行，显示“PATH”</code></pre><h2 id="export定义环境变量-只在session中有效-当前会话"><a href="#export定义环境变量-只在session中有效-当前会话" class="headerlink" title="export定义环境变量,只在session中有效 (当前会话)"></a>export定义环境变量,只在session中有效 (当前会话)</h2><pre><code>$&gt;export name=${PATH}:tom</code></pre><p>设置name为${Var1}的值，Var1没有设置为${Var2}的值。</p><pre><code>$&gt;export name=${Var1:-${Var2}}    </code></pre><p><img src="https://i.imgur.com/l1hKqkn.png" alt=""></p><h2 id="命令执行过程"><a href="#命令执行过程" class="headerlink" title="命令执行过程"></a>命令执行过程</h2><pre><code>$?        //命令的返回值存储变量,0:成功 1:失败。$#        //参数个数$1        //第几个参数$0        //当前脚本(命令)名称$@        //取出所有参数shift    //参数左移${a/b/c}    //</code></pre><p><img src="https://i.imgur.com/jhsCOo7.png" alt=""></p><p><img src="https://i.imgur.com/v4zjND7.png" alt=""></p><p><img src="https://i.imgur.com/qkZ4UhX.png" alt=""></p><p>下面一个例子</p><p><img src="https://i.imgur.com/i9F9fEr.png" alt=""></p><p><img src="https://i.imgur.com/6u7sNQx.png" alt=""></p><p>向左移位解释：</p><p><img src="https://i.imgur.com/erlgoWB.png" alt=""></p><h2 id="if-命令讲解"><a href="#if-命令讲解" class="headerlink" title="if 命令讲解"></a>if 命令讲解</h2><p>语法:</p><p>中括号是可以选择的：</p><pre><code>if COMMANDS; then COMMANDS; [ elif COMMANDS; then COMMANDS; ]... [ else COMMANDS; ] fiif [ $# -lt 3 ]; then xx ; fi3,5</code></pre><p><img src="https://i.imgur.com/oUNME5o.png" alt=""></p><p><img src="https://i.imgur.com/zj0NX7K.png" alt=""></p><p><img src="https://i.imgur.com/eRyv4Iw.png" alt=""></p><h2 id="使用for循环输出1-100个数"><a href="#使用for循环输出1-100个数" class="headerlink" title="使用for循环输出1 - 100个数"></a>使用for循环输出1 - 100个数</h2><p>看一下for的帮助文档，注意从冒号之后是开始的：</p><p><img src="https://i.imgur.com/drXeDUE.png" alt=""></p><pre><code>for NAME [in WORDS ... ] ; do COMMANDS; donefor x in a b c d ; do echo $x ; done ;</code></pre><p><img src="https://i.imgur.com/wrIcUZI.png" alt=""></p><p>通过for循环打印一个三角形：<br>首先看一下<br><img src="https://i.imgur.com/2ayzuZl.png" alt=""></p><p>然后这段是命令：</p><p><img src="https://i.imgur.com/IKLm90v.png" alt=""></p><h2 id="while语法"><a href="#while语法" class="headerlink" title="while语法"></a>while语法</h2><pre><code>for: for NAME [in WORDS ... ] ; do COMMANDS; donefor ((: for (( exp1; exp2; exp3 )); do COMMANDS; done</code></pre><p><img src="https://i.imgur.com/qewzeuC.png" alt=""></p><p>一个例子：</p><pre><code>#!/bin/bash((i=0))while ((i&lt;100)) ; do    echo $i;    i=$((i+1))done</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;访问文件-夹-物理位置&quot;&gt;&lt;a href=&quot;#访问文件-夹-物理位置&quot; class=&quot;headerlink&quot; title=&quot;访问文件(夹)物理位置&quot;&gt;&lt;/a&gt;访问文件(夹)物理位置&lt;/h2&gt;&lt;p&gt;pwd命令是显示当前的逻辑位置。&lt;/p&gt;
&lt;p&gt;物理位置是针对符号连
      
    
    </summary>
    
    
      <category term="linux" scheme="http://erichunn.github.io/tags/linux/"/>
    
      <category term="循环命令" scheme="http://erichunn.github.io/tags/%E5%BE%AA%E7%8E%AF%E5%91%BD%E4%BB%A4/"/>
    
  </entry>
  
  <entry>
    <title>Linux基础之hostname</title>
    <link href="http://erichunn.github.io/2018/09/19/Linux%E5%9F%BA%E7%A1%80%E4%B9%8Bhostname/"/>
    <id>http://erichunn.github.io/2018/09/19/Linux基础之hostname/</id>
    <published>2018-09-19T07:46:12.000Z</published>
    <updated>2018-10-28T07:48:10.512Z</updated>
    
    <content type="html"><![CDATA[<h2 id="命令嵌套"><a href="#命令嵌套" class="headerlink" title="命令嵌套"></a>命令嵌套</h2><h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用<code></code></h3><pre><code>$&gt;echo `cat b.txt`    //命令解析,无法嵌套$&gt;$(... $())        //支持命令的嵌套</code></pre><p><img src="https://i.imgur.com/wlI357k.png" alt=""></p><h2 id="创建用户"><a href="#创建用户" class="headerlink" title="创建用户"></a>创建用户</h2><p>0.用户和组之间，一个用户可以属于多个组。</p><pre><code>但是有一个首要组。</code></pre><p>1.adduser,等同于useradd</p><p>符号链接。</p><pre><code>/usr/sbin/adduser --&gt; /usr/sbin/useradd.</code></pre><p>2.useradd</p><p>输入新密码.</p><p>重复输入</p><pre><code>$&gt;su root$&gt;useradd -m centos2$&gt;su root$&gt;passwd centos2-m, --create-home             create the user&apos;s home directory-p, --password PASSWORD       encrypted password of the new account</code></pre><p>3.使用方法</p><pre><code>$&gt;su root$&gt;userdel -r centos2        //用户所在组目录也会被删除.在删除用户时候要用exit退出要删除的用户,删除的时候可能会exit好多次，因为会来回su。</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;命令嵌套&quot;&gt;&lt;a href=&quot;#命令嵌套&quot; class=&quot;headerlink&quot; title=&quot;命令嵌套&quot;&gt;&lt;/a&gt;命令嵌套&lt;/h2&gt;&lt;h3 id=&quot;使用&quot;&gt;&lt;a href=&quot;#使用&quot; class=&quot;headerlink&quot; title=&quot;使用&quot;&gt;&lt;/a&gt;使用&lt;co
      
    
    </summary>
    
    
      <category term="Linux基础" scheme="http://erichunn.github.io/tags/Linux%E5%9F%BA%E7%A1%80/"/>
    
      <category term="基础命令" scheme="http://erichunn.github.io/tags/%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4/"/>
    
      <category term="hostname" scheme="http://erichunn.github.io/tags/hostname/"/>
    
  </entry>
  
  <entry>
    <title>Linux基础之进程查看</title>
    <link href="http://erichunn.github.io/2018/09/18/Linux%E5%9F%BA%E7%A1%80%E4%B9%8B%E8%BF%9B%E7%A8%8B%E6%9F%A5%E7%9C%8B/"/>
    <id>http://erichunn.github.io/2018/09/18/Linux基础之进程查看/</id>
    <published>2018-09-18T12:10:44.000Z</published>
    <updated>2018-09-18T13:34:40.187Z</updated>
    
    <content type="html"><![CDATA[<h2 id="job"><a href="#job" class="headerlink" title="job"></a>job</h2><hr><p>放到后台运行的进程.<br>1.将程序放到后台运行,以&amp;结尾.</p><pre><code>$&gt;nano b.txt &amp;</code></pre><p>2.查看后台运行的jobs数</p><pre><code>$&gt;jobs</code></pre><p>3.切换后台作业到前台来.</p><pre><code>$&gt;fg %n                //n是job编号.</code></pre><p>4.前台正在的进程，放到后台。</p><pre><code>ctrl + z</code></pre><p>5.让后作业运行</p><pre><code>$&gt;bg %1        //</code></pre><p>6.杀死作业</p><pre><code>$&gt;kill %1    //</code></pre><hr><pre><code>man + 命令    ：查看该命令详细帮助</code></pre><hr><h2 id="进程查看-prcess-show"><a href="#进程查看-prcess-show" class="headerlink" title="进程查看,prcess show"></a>进程查看,prcess show</h2><hr><pre><code>$&gt;ps -Af |grep gnome        //-A:所有进程  -f:所有列格式.$&gt;top                        //动态显示进程信息。含有cpu、内存的使用情况.                            //q,按照q退出。</code></pre><h2 id="cut"><a href="#cut" class="headerlink" title="cut"></a>cut</h2><pre><code>剪切显示文件的每一行。$&gt;cut -c 1-5 a.txt                    //从第一个字符开始,下标从1开始。$&gt;ps -Af | cut -c 45-80 | more        //吧PS里面获得的内容剪切显示，显示没一行的45-80，翻页查看</code></pre><h2 id="查看帮助"><a href="#查看帮助" class="headerlink" title="查看帮助"></a>查看帮助</h2><pre><code>$&gt;help            //查看os内置的命令$&gt;man ifcon fig    //查看特定命令$&gt;ifconfig --help$&gt;ifconfig -h$&gt;info ifconfig    //</code></pre><h2 id="磁盘分区使用"><a href="#磁盘分区使用" class="headerlink" title="磁盘分区使用"></a>磁盘分区使用</h2><pre><code>$&gt;fdisk -l /dev/sda</code></pre><p>里面的中括号是可选的，尖括号是必须要写的</p><p><img src="https://i.imgur.com/kxOQiFw.png" alt=""></p><p>里面sad是磁盘，sda1是分区。sd1，sd2，sd3是磁盘的三个分区。</p><p><img src="https://i.imgur.com/OuJzwf1.png" alt=""></p><p><img src="https://i.imgur.com/dfgfkKL.png" alt=""></p><h2 id="查看磁盘使用情况-disk-free"><a href="#查看磁盘使用情况-disk-free" class="headerlink" title="查看磁盘使用情况(disk free)"></a>查看磁盘使用情况(disk free)</h2><pre><code>$&gt;df -ah /home/centos        //查看</code></pre><p><img src="https://i.imgur.com/JcXk8Fl.png" alt=""></p><p><img src="https://i.imgur.com/KuqoyN5.png" alt=""></p><p><img src="https://i.imgur.com/aibzJWn.png" alt=""></p><h2 id="dirname"><a href="#dirname" class="headerlink" title="dirname"></a>dirname</h2><p>取出指定地址的上级目录.</p><pre><code>$&gt;dirname /a/b/c/d$&gt;/a/b/c</code></pre><h2 id="basename"><a href="#basename" class="headerlink" title="basename"></a>basename</h2><p>取出当前地址的上级目录.</p><pre><code>$&gt;dirname /a/b/c/d$&gt;d</code></pre><h2 id="主机名"><a href="#主机名" class="headerlink" title="主机名"></a>主机名</h2><pre><code>$&gt;hostname        //显式主机名$&gt;修改主机名(sudo)    [/etc/hostname]    s200</code></pre><h2 id="关机重启命令"><a href="#关机重启命令" class="headerlink" title="关机重启命令"></a>关机重启命令</h2><pre><code>$&gt;reboot        //重启$&gt;halt            //停止,黑屏                //halt -p  === poweroff                //halt -r  === reboot$&gt;poweroff        //关机$&gt;shutdown        //shutdown now,    </code></pre><h2 id="命令嵌套"><a href="#命令嵌套" class="headerlink" title="命令嵌套"></a>命令嵌套</h2><p>1.使用<code></code></p><pre><code>$&gt;echo `cat b.txt`    //命令解析,无法嵌套$&gt;$(... $())        //支持命令的嵌套</code></pre><p>2.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;job&quot;&gt;&lt;a href=&quot;#job&quot; class=&quot;headerlink&quot; title=&quot;job&quot;&gt;&lt;/a&gt;job&lt;/h2&gt;&lt;hr&gt;
&lt;p&gt;放到后台运行的进程.&lt;br&gt;1.将程序放到后台运行,以&amp;amp;结尾.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$&amp;gt;nano 
      
    
    </summary>
    
    
      <category term="Linux基础" scheme="http://erichunn.github.io/tags/Linux%E5%9F%BA%E7%A1%80/"/>
    
      <category term="进程查看" scheme="http://erichunn.github.io/tags/%E8%BF%9B%E7%A8%8B%E6%9F%A5%E7%9C%8B/"/>
    
  </entry>
  
</feed>
