<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>无心是一首歌</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://erichunn.github.io/"/>
  <updated>2018-11-19T08:52:46.492Z</updated>
  <id>http://erichunn.github.io/</id>
  
  <author>
    <name>Eric Hunn</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hbase第一天</title>
    <link href="http://erichunn.github.io/2018/11/16/Hbase%E7%AC%AC%E4%BA%8C%E5%A4%A9/"/>
    <id>http://erichunn.github.io/2018/11/16/Hbase第二天/</id>
    <published>2018-11-16T01:33:51.000Z</published>
    <updated>2018-11-19T08:52:46.492Z</updated>
    
    <content type="html"><![CDATA[<p>start-hbase.sh<br>    hbase-daemon.sh start master<br>    habse-daemon.sh    start regionserver</p><p>hbase的ha设置：<br>    直接打开S202或者s203的master进程即可，启动命令如上图。</p><p>hbase shell操作：<br>    $&gt;hbaes shell<br>    $hbase&gt;help</p><p>namespace 类似于Mysql库的概念</p><p>insert into<br>nosql: not only SQL<br>key-value<br>put用来放kv对。<br>在建表的时候没有指定列明，指定的是列族名，然后这个列族下的列是可以增减的。<br>help ‘put’</p><p><img src="https://i.imgur.com/5JUqC0b.png" alt=""></p><p>habase shell 操作：</p><pre><code>$&gt;hbase shell                                    //登陆shell终端$hbase&gt;help                                        //    $hbase&gt;help &apos;list_namespace&apos;                    //查看特定 的命令帮助$hbase&gt;list_namespace                            //列出名字空间（数据库）$hbase&gt;list_namespace_tables &apos;default&apos;            //列出名字空间$hbase&gt;create_namespace &apos;ns1&apos;                    //创建名字空间$hbase&gt;help &apos;create&apos;                            //$hbase&gt;create &apos;ns1:t1&apos;,&apos;f1&apos;                        //创建表，指定空间下$hbase&gt;put &apos;ns1:t1&apos;,&apos;row1&apos;,&apos;f1:id&apos;,100$hbase&gt;put &apos;ns1:t1&apos;,&apos;row1&apos;,&apos;f1:name&apos;,tom$hbase&gt;get &apos;ns1:t1&apos;,&apos;row1&apos;                        //指定查询row$hbase&gt;scan &apos;ns1:t1&apos;                            //权标扫描扫描ns1列族的t1列</code></pre><p>三级坐标定位，一个是列族，一个是row一个是时间戳如下图;</p><p><img src="https://i.imgur.com/ipxtMbm.png" alt=""></p><p><img src="https://i.imgur.com/uz0TL9q.png" alt=""></p><p><img src="https://i.imgur.com/3HmcLCa.png" alt=""></p><p>通过java api操作hbase:<br>    package com.it18zhang.hbasedemo;</p><pre><code>import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.HBaseConfiguration;import org.apache.hadoop.hbase.TableName;import org.apache.hadoop.hbase.client.Connection;import org.apache.hadoop.hbase.client.ConnectionFactory;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.client.Table;import org.apache.hadoop.hbase.util.Bytes;import org.junit.Test;eate 2018/11/17 11:56 */public class TestCRUD {    @Test    public void put() throws Exception {        Configuration conf = HBaseConfiguration.create();        Connection conn = ConnectionFactory.createConnection(conf);        TableName tname = TableName.valueOf(&quot;ns1:t1&quot;);        Table table = conn.getTable(tname);        byte[] rowid = Bytes.toBytes(&quot;row2&quot;);        byte[] f1 = Bytes.toBytes(&quot;f1&quot;);        byte[] id = Bytes.toBytes(&quot;id&quot;);        byte[] value = Bytes.toBytes(102);        //创建put对象        Put put = new Put(rowid);        put.addColumn(f1, id, value);        table.put(put);    }}</code></pre><p>pom文件：</p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;groupId&gt;com.it18zhang&lt;/groupId&gt;    &lt;artifactId&gt;HbaseDemo&lt;/artifactId&gt;    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;dependencies&gt;        &lt;dependency&gt;            &lt;groupId&gt;junit&lt;/groupId&gt;            &lt;artifactId&gt;junit&lt;/artifactId&gt;            &lt;version&gt;4.11&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;            &lt;artifactId&gt;hbase-client&lt;/artifactId&gt;            &lt;version&gt;1.2.3&lt;/version&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;&lt;/project&gt;</code></pre><hr><p>hbase架构介绍：</p><p><img src="https://i.imgur.com/BaJwZ3z.png" alt=""></p><h1 id="关于区域服务器"><a href="#关于区域服务器" class="headerlink" title="关于区域服务器"></a>关于区域服务器</h1><p>  每个大表按照rowkey来切分，rowkey是可排序的，每个切分的被分派到每个区域分五期里面。key是有序的。而且每个取余服务器的表都是有范围的，所以在get的时候要指定rowkey，然后查询的时候定位到这个rowkey在哪个区域服务器上，因为所有的区域服务器信息都在meta表这个自带的表里面，叫元数据表。里面存储着rowkey。也就是每个区域的rowkey的范围。</p><p><img src="https://i.imgur.com/4nzxjz8.png" alt=""></p><p>看里面的内容hbase；namespace,,14….,74….<br>这个就是名字空间表，起始的位置，结束的rowkey位置。<br>前4大段，是指的是一大行的前4列。两个逗号意味着没有切割。就只是在一个区域服务器里面</p><p>再看下面的ns1:t1表的都是一行，然后3列不同的列，如果查询这部分直接联系s204,如果204挂了， 这个表就会更新，变成承接的新表的数据。这个ns1:t1的这个表就没有分割。column是列，info是列族，server是列</p><p>在hbase查询的时候是3级定位。时间戳，列族（列），rowkey（行）</p><p>看一下下面的这个目录：</p><p>hbase,数据，ns1名字空间，t1表，区域服务器,f1列族，然后列族下面就是文件了二进制存储，内容都在这个二进制文件里面。</p><p><img src="https://i.imgur.com/18bQKMj.png" alt=""></p><p>一个表可以有多个区域，一个区域服务器肯定有多个区域，由于有副本的存在所以一个区域可以在多个服务器上，区域服务器是来提供服务的，读写过程是由区域服务器完成，相当于数据节点，这2个都是完成数据的读写过程的。</p><p>在区域里面有个store在里面有MemStore。所以这些内容都是在内存当中，如果内存满了，就会被溢出到磁盘当中，Storefile就是在磁盘当中了，Hfile通过和底层交互</p><h1 id="hbase的写入过程："><a href="#hbase的写入过程：" class="headerlink" title="hbase的写入过程："></a>hbase的写入过程：</h1><p><img src="https://i.imgur.com/XfOpVug.png" alt=""></p><p>root这个地方写错了是老版本的，应该是meta表<br><img src="https://i.imgur.com/Imht8Oq.png" alt=""><br><img src="https://i.imgur.com/peKNm1w.png" alt=""></p><p>现在妖王一个表里写数据，需要先查询这个表在那个区域服务器上，要先查询hbase空间下的meta表，因为这个表里面标识了类似于一个目录的内容，标识那个区域子在那个服务器上，也就是rowkey的范围。这个meta表在哪里呢怎么找到呢？ meta表要通过zk来找到。因为zk集群在，所以没有zk的情况下hbase都起不来。</p><p>进入到hbase shell里面</p><p><img src="https://i.imgur.com/Rx4QHIF.png" alt=""></p><p>通过进入到shell里面发现这个meta表大概可能在这个里面，进去之后发现这个meta表在s203里面。如下图</p><p><img src="https://i.imgur.com/SHSVJwz.png" alt=""></p><p>所以实际上就是先联系zk然后通过zk找到他的位置在s203里面，这个meta表在s203里面</p><p>ta/hbase/meta/1588230740/info/da7a63e29d1c4fb588068ea9c187ae27</p><hr><h1 id="hbase基于hdfs"><a href="#hbase基于hdfs" class="headerlink" title="hbase基于hdfs"></a>hbase基于hdfs</h1><p>【表数据的存储结构目录构成】</p><pre><code>hdfs://s201:8020/hbase/data/${名字空间}/${表名}/区域名称/列族名称</code></pre><p>相同列族的数据存放在一个文件中，</p><p>【WAL写前日志目录结构构成】</p><pre><code>hdfs://s201:8020/hbase/wals/s202,16020,1542534586029/s202%2C16020%2C1542534586029.default.1542541792199hdfs://s201:8020/hbase/wals/${区域服务器名称}/主机名，端口号，时间戳/</code></pre><h1 id="client端交互过程"><a href="#client端交互过程" class="headerlink" title="client端交互过程"></a>client端交互过程</h1><p>0.集群启动时，master负责分配区域到指定的区域服务器</p><p>1.联系zk找出meta表所在的区域服务器rs(regionserver)<br>        /meta/meta-region-server<br>    定位到所在的服务器</p><p>2.定位rowkey，找到对应的rs(regionserver)</p><p>3.缓存信息到本地，</p><p>4.联系regionserver</p><p>5.HRegionServe负责open-HRegion对象打开H区域服务器对象，为每个列族创建store实例，说明store是和列族对应的，store包涵多个storefile实例，他们是对hfile的轻量级封装，每个store还对应一个memstroe（用于内存储速度快），</p><p><img src="https://i.imgur.com/ks0t9JW.png" alt=""></p><hr><h1 id="在百万数据存储的时候："><a href="#在百万数据存储的时候：" class="headerlink" title="在百万数据存储的时候："></a>在百万数据存储的时候：</h1><p>关闭WALS</p><p><img src="https://i.imgur.com/ITwCty6.png" alt=""></p><p>代码如下：</p><pre><code>@Test   public void biginsert() throws Exception {       long start=System.currentTimeMillis();       Configuration conf = HBaseConfiguration.create();       Connection conn = ConnectionFactory.createConnection(conf);       TableName tname = TableName.valueOf(&quot;ns1:t1&quot;);       HTable table = (HTable) conn.getTable(tname);       //不要自动清理缓冲区       table.setAutoFlushTo(false);       for (int i = 0; i &lt; 1000000; i++) {           Put put = new Put(Bytes.toBytes(&quot;row&quot; + i));           //关闭写前日志           put.setWriteToWAL(false);           put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;), Bytes.toBytes(i));           put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;), Bytes.toBytes(&quot;tom&quot; + i));           put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;), Bytes.toBytes(i % 100));           table.put(put);           if (i % 2000 == 0) {               table.flushCommits();           }       }       table.flushCommits();       System.out.println(System.currentTimeMillis()-start);   }</code></pre><hr><p>hbase shell命令：</p><p>要想删除表，先要禁用表。</p><pre><code>$hbase&gt;flush &apos;ns1:t1&apos;        //清理内存数据到磁盘$hbase&gt;count &apos;ns1:t1&apos;        //统计函数$hbase&gt;disable &apos;ns1:t1&apos;        //删除表之前要禁用表$hbase&gt;drop &apos;ns1:t1&apos;        //删除表 $hbase&gt;count &apos;hbase:meta&apos;     //查看元数据表</code></pre><p><img src="https://i.imgur.com/AAzzOmp.png" alt=""></p><hr><h1 id="格式化代码，设置固定数字格式"><a href="#格式化代码，设置固定数字格式" class="headerlink" title="格式化代码，设置固定数字格式"></a>格式化代码，设置固定数字格式</h1><pre><code> @Test    public void formatNum(){        DecimalFormat format =new DecimalFormat();        format.applyPattern(&quot;0000000&quot;);//        format.applyPattern(&quot;###,###,00&quot;);        System.out.println(format.format(8));    }</code></pre><hr><p>为什么要格式化rowid，因为rowid1008和8相比较8比1008还要大，所以要格式化一下。</p><p>经过格式化rowid的代码：</p><pre><code> @Testpublic void biginsert() throws Exception {    DecimalFormat format =new DecimalFormat();    format.applyPattern(&quot;0000000&quot;);    System.out.println(format.format(8));    long start=System.currentTimeMillis();    Configuration conf = HBaseConfiguration.create();    Connection conn = ConnectionFactory.createConnection(conf);    TableName tname = TableName.valueOf(&quot;ns1:t1&quot;);    HTable table = (HTable) conn.getTable(tname);    //不要自动清理缓冲区    table.setAutoFlushTo(false);    for (int i = 0; i &lt; 10000; i++) {        Put put = new Put(Bytes.toBytes(&quot;row&quot; + format.format(i)));        //关闭写前日志        put.setWriteToWAL(false);        put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;), Bytes.toBytes(i));        put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;), Bytes.toBytes(&quot;tom&quot; + i));        put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;), Bytes.toBytes(i % 100));        table.put(put);        if (i % 2000 == 0) {            table.flushCommits();        }    }    table.flushCommits();    System.out.println(System.currentTimeMillis()-start);}</code></pre><hr><h1 id="flush命令"><a href="#flush命令" class="headerlink" title="flush命令"></a>flush命令</h1><pre><code>$hbase:flush：清理内存数据到磁盘</code></pre><h1 id="region拆分切割"><a href="#region拆分切割" class="headerlink" title="region拆分切割"></a>region拆分切割</h1><p><img src="https://i.imgur.com/v0N169Q.png" alt=""><br><img src="https://i.imgur.com/XQesWs6.png" alt=""></p><p>hbase默认切割文件是10G，超过切割。</p><pre><code>$hbase&gt;count &apos;ns1:t1&apos;        //统计函数</code></pre><p><img src="https://i.imgur.com/sjQ5dq7.png" alt=""></p><pre><code>切割而一旦完成会保留原来的表结构，但是原来的数据内容已经没有了被删除了 </code></pre><p><img src="https://i.imgur.com/UhuexPk.png" alt=""></p><p><img src="https://i.imgur.com/0NfLQSJ.png" alt=""></p><p><img src="https://i.imgur.com/oJTAFPt.png" alt=""></p><hr><h1 id="hbase和hadoop的ha集成"><a href="#hbase和hadoop的ha集成" class="headerlink" title="hbase和hadoop的ha集成"></a>hbase和hadoop的ha集成</h1><p>1.在hbase-env.sh文件添加hadoop配置文件目录到hbase_classpath环境变量[/soft/hbase/conf/hbase-env]并且分发。</p><pre><code>export HBASE_CLASSPATH=$HBASE_CLASSPATH:/soft/hadoop/    etc/hadoop</code></pre><p>2.在hbase/conf目录下创建到hadoop的hdfs-site.xml的符号链接</p><pre><code>    $&gt;ln -s /soft/hadoop/etc/hadoop/hdfs-site.xml/soft/hbase/conf/hdfs-site.xml</code></pre><p>3.修改Hbase-site.xml文件中hbase.rootdir的目录值<br>        /soft/hbase/conf/hbase-site.xml<br>4.将之都分发出去。</p><p>继承了之后及时关闭了s201的namenode，hbase的Hmaster也不会关闭，也就是形成了高可用状态。 </p><hr><h1 id="hbase手动移动区域"><a href="#hbase手动移动区域" class="headerlink" title="hbase手动移动区域"></a>hbase手动移动区域</h1><p>手动移动区域<br><img src="https://i.imgur.com/UayeavF.png" alt=""></p><p>手动强行合并hbase块<br><img src="https://i.imgur.com/CrvZAFo.png" alt=""><br><img src="https://i.imgur.com/JCFdxeV.png" alt=""></p><p>手动切割：</p><h1 id="拆分风暴："><a href="#拆分风暴：" class="headerlink" title="拆分风暴："></a>拆分风暴：</h1><p><img src="https://i.imgur.com/wP6mfUJ.png" alt=""></p><p>在达到10G以后，几个区域同时增长，同时到达10G临界点，同时切割额，服务器工作瞬间增大。每个都在切割，就叫切割风暴，我们要避免这种情况，通过使用手动切割，避免拆分风暴。</p><hr><p>代码操作增删改查<br>    package com.it18zhang.hbasedemo;</p><pre><code>import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.*;import org.apache.hadoop.hbase.client.*;import org.apache.hadoop.hbase.util.Bytes;import org.junit.Test;import java.io.IOException;import java.text.DecimalFormat;import java.util.Iterator;import java.util.Map;import java.util.NavigableMap;/** * @Title:TestCRUD * @Description: * @Auther: Eric Hunn * @Version:1.0 * @Create 2018/11/17 11:56 */public class TestCRUD {    @Test    public void put() throws Exception {        Configuration conf = HBaseConfiguration.create();        Connection conn = ConnectionFactory.createConnection(conf);        TableName tname = TableName.valueOf(&quot;ns1:t1&quot;);        Table table = conn.getTable(tname);        byte[] rowid = Bytes.toBytes(&quot;row2&quot;);        byte[] f1 = Bytes.toBytes(&quot;f1&quot;);        byte[] id = Bytes.toBytes(&quot;id&quot;);        byte[] value = Bytes.toBytes(102);        //创建put对象        Put put = new Put(rowid);        put.addColumn(f1, id, value);        table.put(put);    }    @Test    public void biginsert() throws Exception {        DecimalFormat format = new DecimalFormat();        format.applyPattern(&quot;0000000&quot;);        System.out.println(format.format(8));        long start = System.currentTimeMillis();        Configuration conf = HBaseConfiguration.create();        Connection conn = ConnectionFactory.createConnection(conf);        TableName tname = TableName.valueOf(&quot;ns1:t1&quot;);        HTable table = (HTable) conn.getTable(tname);        //不要自动清理缓冲区        table.setAutoFlushTo(false);        for (int i = 0; i &lt; 10000; i++) {            Put put = new Put(Bytes.toBytes(&quot;row&quot; + format.format(i)));            //关闭写前日志            put.setWriteToWAL(false);            put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;), Bytes.toBytes(i));            put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;), Bytes.toBytes(&quot;tom&quot; + i));            put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;), Bytes.toBytes(i % 100));            table.put(put);            if (i % 2000 == 0) {                table.flushCommits();            }        }        table.flushCommits();        System.out.println(System.currentTimeMillis() - start);    }    @Test    public void formatNum() {        DecimalFormat format = new DecimalFormat();        format.applyPattern(&quot;0000000&quot;);//        format.applyPattern(&quot;###,###,00&quot;);        System.out.println(format.format(8));    }    @Test    public void createNamespace() throws Exception {        //创建conf对象        Configuration conf = HBaseConfiguration.create();        //通过工厂创建连接对象        Connection conn = ConnectionFactory.createConnection(conf);        Admin admin = conn.getAdmin();        NamespaceDescriptor nsd = NamespaceDescriptor.create(&quot;ns2&quot;).build();        admin.createNamespace(nsd);        NamespaceDescriptor[] ns = admin.listNamespaceDescriptors();        for (NamespaceDescriptor n : ns) {            System.out.println(n.getName());        }    }    @Test    public void listNamespaces() throws Exception {        //创建conf对象        Configuration conf = HBaseConfiguration.create();        //通过工厂创建连接对象        Connection conn = ConnectionFactory.createConnection(conf);        Admin admin = conn.getAdmin();        NamespaceDescriptor[] ns = admin.listNamespaceDescriptors();        for (NamespaceDescriptor n : ns) {            System.out.println(n.getName());        }    }    @Test    public void createTables() throws Exception {        //创建conf对象        Configuration conf = HBaseConfiguration.create();        //通过工厂创建连接对象        Connection conn = ConnectionFactory.createConnection(conf);        Admin admin = conn.getAdmin();        //创建表名对象        TableName tbn = TableName.valueOf(&quot;ns2:t2&quot;);        //创建表描述符对象        HTableDescriptor tbl = new HTableDescriptor(tbn);        //在表描述符中添加列族创建列族描述符        HColumnDescriptor col = new HColumnDescriptor(&quot;f1&quot;);        tbl.addFamily(col);        admin.createTable(tbl);        System.out.println(&quot;over&quot;);    }    @Test    public void disableTable() throws Exception {        Configuration conf = HBaseConfiguration.create();        Connection conn = ConnectionFactory.createConnection(conf);        Admin admin = conn.getAdmin();        admin.disableTable(TableName.valueOf(&quot;ns2:t2&quot;));    }    @Test    public void dropTable() throws Exception {        Configuration conf = HBaseConfiguration.create();        Connection conn = ConnectionFactory.createConnection(conf);        Admin admin = conn.getAdmin();        admin.disableTable(TableName.valueOf(&quot;ns2:t2&quot;));        admin.deleteTable(TableName.valueOf(&quot;ns2:t2&quot;));    }    @Test    public void deleteData() throws IOException {        Configuration conf = HBaseConfiguration.create();        Connection conn = ConnectionFactory.createConnection(conf);        TableName tname = TableName.valueOf(&quot;ns1:t1&quot;);        Table table = conn.getTable(tname);        Delete del = new Delete(Bytes.toBytes(&quot;row0000001&quot;));        del.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;id&quot;));        del.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;));        table.delete(del);        System.out.println(&quot;over&quot;);    }    @Test    public void scanall() throws IOException {        Configuration conf = HBaseConfiguration.create();        Connection conn = ConnectionFactory.createConnection(conf);        TableName tname = TableName.valueOf(&quot;ns1:t1&quot;);        Table table = conn.getTable(tname);        Scan scan = new Scan();        ResultScanner rs = table.getScanner(scan);        Iterator&lt;Result&gt; it = rs.iterator();        while (it.hasNext()) {            Result r = it.next();//理解这个r是对一整行的封装            byte[] value = r.getValue(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;));            System.out.println(Bytes.toString(value));        }    }    @Test    public void scan() throws IOException {        Configuration conf = HBaseConfiguration.create();        Connection conn = ConnectionFactory.createConnection(conf);        TableName tname = TableName.valueOf(&quot;ns1:t1&quot;);        Table table = conn.getTable(tname);        Scan scan = new Scan();        scan.setStartRow(Bytes.toBytes(&quot;row0005000&quot;));        scan.setStopRow(Bytes.toBytes(&quot;row0008000&quot;));        ResultScanner rs = table.getScanner(scan);        Iterator&lt;Result&gt; it = rs.iterator();        while (it.hasNext()) {            Result r = it.next();//理解这个r是对一整行的封装            byte[] value = r.getValue(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;name&quot;));            System.out.println(Bytes.toString(value));        }    }    //动态获取所有的列和列族，动态遍历，每个列和值的集合，由于强行getvalue转换成string类型，所以难免出现乱码情况    @Test    public void scan2() throws IOException {        Configuration conf = HBaseConfiguration.create();        Connection conn = ConnectionFactory.createConnection(conf);        TableName tname = TableName.valueOf(&quot;ns1:t1&quot;);        Table table = conn.getTable(tname);        Scan scan = new Scan();        ResultScanner rs = table.getScanner(scan);        Iterator&lt;Result&gt; it = rs.iterator();        while (it.hasNext()) {            Result r = it.next();//理解这个r是对一整行的封装            Map&lt;byte[], byte[]&gt; map = r.getFamilyMap(Bytes.toBytes(&quot;f1&quot;));            for (Map.Entry&lt;byte[], byte[]&gt; entrySet : map.entrySet()) {                String col = Bytes.toString(entrySet.getKey());                String val = Bytes.toString(entrySet.getValue());                System.out.println(col + &quot;:&quot; + val + &quot;,&quot;);            }            System.out.println();        }    }    @Test    public void scan3() throws IOException {        Configuration conf = HBaseConfiguration.create();        Connection conn = ConnectionFactory.createConnection(conf);        TableName tname = TableName.valueOf(&quot;ns1:t1&quot;);        Table table = conn.getTable(tname);        Scan scan = new Scan();        ResultScanner rs = table.getScanner(scan);        Iterator&lt;Result&gt; it = rs.iterator();        while (it.hasNext()) {            Result r = it.next();//理解这个r是对一整行的封装            //得到一行的所有map，key=f1,value=Map&lt;Col,Map&lt;Timestamp,value&gt;&gt;这个结构            NavigableMap&lt;byte[], NavigableMap&lt;byte[], NavigableMap&lt;Long, byte[]&gt;&gt;&gt; map = r.getMap();            for (Map.Entry&lt;byte[], NavigableMap&lt;byte[], NavigableMap&lt;Long, byte[]&gt;&gt;&gt; entry : map.entrySet()) {                //得到列族                String f = Bytes.toString(entry.getKey());                NavigableMap&lt;byte[], NavigableMap&lt;Long, byte[]&gt;&gt; colDataMap = entry.getValue();                for (Map.Entry&lt;byte[], NavigableMap&lt;Long, byte[]&gt;&gt; ets : colDataMap.entrySet()) {                    String c = Bytes.toString(ets.getKey());                    Map&lt;Long, byte[]&gt; tsValueMap = ets.getValue();                    for (Map.Entry&lt;Long, byte[]&gt; e : tsValueMap.entrySet()) {                        Long ts = e.getKey();                        String value = Bytes.toString(e.getValue());                        System.out.println(f + &quot;:&quot; + c + &quot;:&quot; + ts + &quot;=&quot; + value + &quot;,&quot;);                    }                }            }        }    }}</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;start-hbase.sh&lt;br&gt;    hbase-daemon.sh start master&lt;br&gt;    habse-daemon.sh    start regionserver&lt;/p&gt;
&lt;p&gt;hbase的ha设置：&lt;br&gt;    直接打开S202或者s203的
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Hbase第一天</title>
    <link href="http://erichunn.github.io/2018/11/16/Hbase%E7%AC%AC%E4%B8%80%E5%A4%A9/"/>
    <id>http://erichunn.github.io/2018/11/16/Hbase第一天/</id>
    <published>2018-11-16T01:33:51.000Z</published>
    <updated>2018-11-16T01:33:51.434Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Zookeeper第二天</title>
    <link href="http://erichunn.github.io/2018/11/15/Zookeeper%E7%AC%AC%E4%BA%8C%E5%A4%A9/"/>
    <id>http://erichunn.github.io/2018/11/15/Zookeeper第二天/</id>
    <published>2018-11-15T08:45:25.000Z</published>
    <updated>2018-11-16T03:36:00.757Z</updated>
    
    <content type="html"><![CDATA[<h2 id="leader推选过程-最小号选举法"><a href="#leader推选过程-最小号选举法" class="headerlink" title="leader推选过程(最小号选举法)"></a>leader推选过程(最小号选举法)</h2><pre><code>1.所有节点在同一目录下创建临时序列节点。2.节点下会生成/xxx/xx000000001等节点。3.序号最小的节点就是leader，其余就是follower.4.每个节点观察小于自己节点的主机。(注册观察者)5.如果leader挂了，对应znode删除了。6.观察者收到通知。</code></pre><p><img src="https://i.imgur.com/O6wXdY8.png" alt=""></p><h2 id="配置完全分布式zk集群"><a href="#配置完全分布式zk集群" class="headerlink" title="配置完全分布式zk集群"></a>配置完全分布式zk集群</h2><pre><code>1.挑选3台主机    s201 ~ s2032.每台机器都安装zk    tar    环境变量3.配置zk配置文件    s201 ~ s203    [/soft/zk/conf/zoo.cfg]    ...    dataDir=/home/centos/zookeeper    server.1=s201:2888:3888    server.2=s202:2888:3888     server.3=s203:2888:38884.在每台主机的/home/centos/zookeeper中添加myid,内容分别是1,2,3    [s201]    $&gt;echo 1 &gt; /home/centos/zookeeper/myid    [s202]    $&gt;echo 2 &gt; /home/centos/zookeeper/myid    [s203]    $&gt;echo 3 &gt; /home/centos/zookeeper/myid5.启动服务器集群     $&gt;zkServer.sh start    ...6.查看每台服务器的状态    $&gt;zkServer.sh status7.修改zk的log目录    vi /soft/zk/conf/log4j.properties</code></pre><p>修改如下：</p><p><img src="https://i.imgur.com/xaRGDXr.png" alt=""></p><pre><code>8.创建log目录：    xcall.sh &quot;mkdir /home/centos/zookeeper/log&quot;</code></pre><h2 id="部署细节"><a href="#部署细节" class="headerlink" title="部署细节"></a>部署细节</h2><pre><code>1.在jn节点分别启动jn进程    $&gt;hadoop-daemon.sh start journalnode2.启动jn之后，在两个NN之间进行disk元数据同步    a)如果是全新集群，先format文件系统,只需要在一个nn上执行。        [s201]        $&gt;hadoop namenode -format    b)如果将非HA集群转换成HA集群，复制原NN的metadata到另一个nn.        1.步骤一            [s201]            $&gt;scp -r /home/centos/hadoop/dfs centos@s206:/home/centos/hadoop/        2.步骤二            在新的nn(未格式化的nn)上运行一下命令，实现待命状态引导。            [s206]            $&gt;hdfs namenode -bootstrapStandby        //需要s201为启动状态,提示是否格式化,选择N.        3)在一个NN上执行以下命令，完成edit日志到jn节点的传输。            $&gt;hdfs namenode -initializeSharedEdits            #查看s202,s203是否有edit数据.        4)启动所有节点.            [s201]            $&gt;hadoop-daemon.sh start namenode        //启动名称节点            $&gt;hadoop-daemons.sh start datanode        //启动所有数据节点            [s206]            $&gt;hadoop-daemon.sh start namenode        //启动名称节点</code></pre><h2 id="HA管理"><a href="#HA管理" class="headerlink" title="HA管理"></a>HA管理</h2><pre><code>$&gt;hdfs haadmin -transitionToActive nn1                //切成激活态$&gt;hdfs haadmin -transitionToStandby nn1                //切成待命态$&gt;hdfs haadmin -transitionToActive --forceactive nn2//强行激活$&gt;hdfs haadmin -failover nn1 nn2                    //模拟容灾演示,从nn1切换到nn2</code></pre><h2 id="完全0开始部署hadoop-HDFS的HA集群，使用zk实现自动容灾"><a href="#完全0开始部署hadoop-HDFS的HA集群，使用zk实现自动容灾" class="headerlink" title="完全0开始部署hadoop HDFS的HA集群，使用zk实现自动容灾"></a>完全0开始部署hadoop HDFS的HA集群，使用zk实现自动容灾</h2><pre><code>1.停掉hadoop的所有进程2.删除所有节点的日志和本地数据.    删除/home/centos/hadoop下的所有和        /home/centos/journal下的所有3.改换hadoop符号连接为ha4.登录每台JN节点主机，启动JN进程.    [s202-s204]    $&gt;hadoop-daemon.sh start journalnode5.登录其中一个NN,格式化文件系统(s201)    $&gt;hadoop namenode -format6.复制201目录的下nn的元数据到s206    $&gt;scp -r ~/hadoop/* centos@s206:/home/centos/hadoop7.在未格式化的NN(s206)节点上做standby引导.    7.1)需要保证201的NN启动        $&gt;hadoop-daemon.sh start namenode    7.2)登录到s206节点，做standby引导.        $&gt;hdfs namenode -bootstrapStandby    7.3)登录201，将s201的edit日志初始化到JN节点。        $&gt;hdfs namenode -initializeSharedEdits8.启动所有数据节点.    $&gt;hadoop-daemons.sh start datanode9.登录到206,启动NN    $&gt;hadoop-daemon.sh start namenode10.查看webui    http://s201:50070/    http://s206:50070/11.自动容灾    11.1)介绍        自动容灾引入两个组件，zk quarum + zk容灾控制器(ZKFC)。        运行NN的主机还要运行ZKFC进程，主要负责:        a.健康监控        b.session管理        c.选举    11.2部署容灾        a.停止所有进程            $&gt;stop-all.sh        b.配置hdfs-site.xml，启用自动容灾.            [hdfs-site.xml]                &lt;property&gt;                    &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;                    &lt;value&gt;true&lt;/value&gt;                &lt;/property&gt;        c.配置core-site.xml，指定zk的连接地址.            &lt;property&gt;                &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;                &lt;value&gt;s201:2181,s202:2181,s203:2181&lt;/value&gt;            &lt;/property&gt;        d.分发以上两个文件到所有节点。12.登录其中的一台NN(s201),在ZK中初始化HA状态    $&gt;hdfs zkfc -formatZK13.启动hdfs进程.    $&gt;start-dfs.sh14.测试自动容在(206是活跃节点)    $&gt;kill -9</code></pre><h2 id="配置RM的HA自动容灾"><a href="#配置RM的HA自动容灾" class="headerlink" title="配置RM的HA自动容灾"></a>配置RM的HA自动容灾</h2><pre><code>1.配置yarn-site.xml    &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;        &lt;value&gt;true&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;        &lt;value&gt;cluster1&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;        &lt;value&gt;rm1,rm2&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;        &lt;value&gt;s201&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;        &lt;value&gt;s206&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt;        &lt;value&gt;s201:8088&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt;        &lt;value&gt;s206:8088&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;        &lt;value&gt;s201:2181,s202:2181,s203:2181&lt;/value&gt;    &lt;/property&gt;    2.使用管理命令    //查看状态    $&gt;yarn rmadmin -getServiceState rm1    //切换状态到standby    $&gt;yarn rmadmin -transitionToStandby rm13.启动yarn集群    $&gt;start-yarn.sh4.hadoop没有启动两个resourcemanager,需要手动启动另外一个    $&gt;yarn-daemon.sh start resourcemanager5.查看webui6.做容灾模拟.    kill -9</code></pre><h2 id="hive的注意事项"><a href="#hive的注意事项" class="headerlink" title="hive的注意事项"></a>hive的注意事项</h2><pre><code>如果配置hadoop HA之前，搭建了Hive的话，在HA之后，需要调整路径信息.主要是修改mysql中的dbs,tbls等相关表。</code></pre><h2 id="Hbase"><a href="#Hbase" class="headerlink" title="Hbase"></a>Hbase</h2><pre><code>hadoop数据库，分布式可伸缩大型数据存储。用户对随机、实时读写数据。十亿行 x 百万列。版本化、非关系型数据库。</code></pre><h2 id="Feature"><a href="#Feature" class="headerlink" title="Feature"></a>Feature</h2><pre><code>Linear and modular scalability.                    //线性模块化扩展方式。Strictly consistent reads and writes.            //严格一致性读写Automatic and configurable sharding of tables    //自动可配置表切割Automatic failover support between RegionServers.    //区域服务器之间自动容在Convenient base classes for backing Hadoop MapReduce jobs with Apache HBase tables.        //Easy to use Java API for client access.            //java APIBlock cache and Bloom Filters for real-time queries    //块缓存和布隆过滤器用于实时查询 Query predicate push down via server side Filters    //通过服务器端过滤器实现查询预测Thrift gateway and a REST-ful Web service that supports XML, Protobuf, and binary data encoding options    //Extensible jruby-based (JIRB) shell                    //Support for exporting metrics via the Hadoop metrics subsystem to files or Ganglia; or via JMX            //可视化面向列数据库。</code></pre><h2 id="hbase存储机制"><a href="#hbase存储机制" class="headerlink" title="hbase存储机制"></a>hbase存储机制</h2><pre><code>面向列存储，table是按row排序。</code></pre><h2 id="搭建hbase集群"><a href="#搭建hbase集群" class="headerlink" title="搭建hbase集群"></a>搭建hbase集群</h2><pre><code>0.选择安装的主机    s201 ~ s2041.jdk    略2.hadoop    略3.tar     略4.环境变量    略5.验证安装是否成功    $&gt;hbase version5.配置hbase模式    5.1)本地模式        [hbase/conf/hbase-env.sh]        EXPORT JAVA_HOME=/soft/jdk        [hbase/conf/hbase-site.xml]        ...        &lt;property&gt;            &lt;name&gt;hbase.rootdir&lt;/name&gt;            &lt;value&gt;file:/home/hadoop/HBase/HFiles&lt;/value&gt;        &lt;/property&gt;    5.2)伪分布式        [hbase/conf/hbase-env.sh]        EXPORT JAVA_HOME=/soft/jdk        [hbase/conf/hbase-site.xml]        &lt;property&gt;            &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;            &lt;value&gt;true&lt;/value&gt;        &lt;/property        &lt;property&gt;            &lt;name&gt;hbase.rootdir&lt;/name&gt;            &lt;value&gt;hdfs://localhost:8030/hbase&lt;/value&gt;        &lt;/property&gt;    5.3)完全分布式(必做)        [hbase/conf/hbase-env.sh]        export JAVA_HOME=/soft/jdk        export HBASE_MANAGES_ZK=false        [hbse-site.xml]        &lt;!-- 使用完全分布式 --&gt;        &lt;property&gt;            &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;            &lt;value&gt;true&lt;/value&gt;        &lt;/property&gt;        &lt;!-- 指定hbase数据在hdfs上的存放路径 --&gt;        &lt;property&gt;            &lt;name&gt;hbase.rootdir&lt;/name&gt;            &lt;value&gt;hdfs://s201:8020/hbase&lt;/value&gt;        &lt;/property&gt;        &lt;!-- 配置zk地址 --&gt;        &lt;property&gt;            &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;            &lt;value&gt;s201:2181,s202:2181,s203:2181&lt;/value&gt;        &lt;/property&gt;        &lt;!-- zk的本地目录 --&gt;        &lt;property&gt;            &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;            &lt;value&gt;/home/centos/zookeeper&lt;/value&gt;        &lt;/property&gt;6.配置regionservers    [hbase/conf/regionservers]    s202    s203    s2047.启动hbase集群(s201)    $&gt;start-hbase.sh8.登录hbase的webui    http://s201:16010</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;leader推选过程-最小号选举法&quot;&gt;&lt;a href=&quot;#leader推选过程-最小号选举法&quot; class=&quot;headerlink&quot; title=&quot;leader推选过程(最小号选举法)&quot;&gt;&lt;/a&gt;leader推选过程(最小号选举法)&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>SSM</title>
    <link href="http://erichunn.github.io/2018/11/12/SSM/"/>
    <id>http://erichunn.github.io/2018/11/12/SSM/</id>
    <published>2018-11-12T07:39:39.000Z</published>
    <updated>2018-11-14T12:38:39.328Z</updated>
    
    <content type="html"><![CDATA[<h2 id="SSM"><a href="#SSM" class="headerlink" title="SSM"></a>SSM</h2><pre><code>Spring        //业务层框架Spring MVC    //开发web程序应用的模块model + view + controllerMybatis        //持久化。jdbc , mysql</code></pre><h2 id="mybatis"><a href="#mybatis" class="headerlink" title="mybatis"></a>mybatis</h2><pre><code>ibatis.</code></pre><h2 id="体验"><a href="#体验" class="headerlink" title="体验"></a>体验</h2><pre><code>1.创建项目和模块2.添加pom.xml    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;    &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;             xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;             xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;        &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;        &lt;groupId&gt;com.it18zhang&lt;/groupId&gt;        &lt;artifactId&gt;mybatisdemo&lt;/artifactId&gt;        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;        &lt;dependencies&gt;            &lt;dependency&gt;                &lt;groupId&gt;org.mybatis&lt;/groupId&gt;                &lt;artifactId&gt;mybatis&lt;/artifactId&gt;                &lt;version&gt;3.2.1&lt;/version&gt;            &lt;/dependency&gt;            &lt;dependency&gt;                &lt;groupId&gt;mysql&lt;/groupId&gt;                &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;                &lt;version&gt;5.1.17&lt;/version&gt;            &lt;/dependency&gt;            &lt;dependency&gt;                &lt;groupId&gt;junit&lt;/groupId&gt;                &lt;artifactId&gt;junit&lt;/artifactId&gt;                &lt;version&gt;4.11&lt;/version&gt;            &lt;/dependency&gt;        &lt;/dependencies&gt;    &lt;/project&gt;3.添加配置    [resoucecs/mybatis-config.xml]    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;    &lt;!DOCTYPE configuration      PUBLIC &quot;-//mybatis.org//DTD Config 3.0//EN&quot;      &quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot;&gt;    &lt;configuration&gt;      &lt;environments default=&quot;development&quot;&gt;        &lt;environment id=&quot;development&quot;&gt;          &lt;transactionManager type=&quot;JDBC&quot;/&gt;          &lt;dataSource type=&quot;POOLED&quot;&gt;            &lt;property name=&quot;driver&quot; value=&quot;${driver}&quot;/&gt;            &lt;property name=&quot;url&quot; value=&quot;${url}&quot;/&gt;            &lt;property name=&quot;username&quot; value=&quot;${username}&quot;/&gt;            &lt;property name=&quot;password&quot; value=&quot;${password}&quot;/&gt;          &lt;/dataSource&gt;        &lt;/environment&gt;      &lt;/environments&gt;      &lt;mappers&gt;        &lt;mapper resource=&quot;org/mybatis/example/BlogMapper.xml&quot;/&gt;      &lt;/mappers&gt;    &lt;/configuration&gt;4.创建库和表    mysql&gt;create database mybatis ;    mysql&gt;use mybatis ;    mysql&gt;create table users(id int primary key auto_increment , name varchar(20) ,age int) ;    mysql&gt;desc users ;5.测试连接    package com.it18zhang.mybatisdemo;    import org.apache.ibatis.io.Resources;    import org.apache.ibatis.session.SqlSession;    import org.apache.ibatis.session.SqlSessionFactory;    import org.apache.ibatis.session.SqlSessionFactoryBuilder;    import java.io.IOException;    import java.io.InputStream;    /**     *     */    public class App {        public static void main(String[] args) {            try {                //指定配置文件的路径(类路径)                String resource = &quot;mybatis-config.xml&quot;;                //加载文件                InputStream inputStream = Resources.getResourceAsStream(resource);                //创建会话工厂Builder,相当于连接池                SqlSessionFactory sf = new SqlSessionFactoryBuilder().build(inputStream);                //通过sf开启会话，相当于打开连接。                SqlSession s = sf.openSession();                System.out.println(s);            } catch (Exception e) {                e.printStackTrace();            }        }    }6.编写mapper文件。    a)创建User类，和users对应        public class User {            private Integer id ;            private String name ;            private int age ;            //get/set        }    b)创建UserMapper.xml,存放在resources/目录下        包名[resources/UserMapper.xml]        &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;        &lt;!DOCTYPE mapper                PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot;                &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;        &lt;!-- 定义名字空间 --&gt;        &lt;mapper namespace=&quot;users&quot;&gt;            &lt;!-- 定义insert语句 --&gt;            &lt;insert id=&quot;insert&quot;&gt;              insert into users(name,age) values(#{name},#{age})            &lt;/insert&gt;        &lt;/mapper&gt;7.在resources/mybatis-config.xml文件中引入mapper的xml文件.    [resources/mybatis-config.xml]    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;    &lt;!DOCTYPE configuration            PUBLIC &quot;-//mybatis.org//DTD Config 3.0//EN&quot;            &quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot;&gt;    &lt;configuration&gt;        &lt;environments default=&quot;development&quot;&gt;            &lt;environment id=&quot;development&quot;&gt;                &lt;transactionManager type=&quot;JDBC&quot;/&gt;                &lt;dataSource type=&quot;POOLED&quot;&gt;                    &lt;property name=&quot;driver&quot; value=&quot;com.mysql.jdbc.Driver&quot;/&gt;                    &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/mybatis&quot;/&gt;                    &lt;property name=&quot;username&quot; value=&quot;root&quot;/&gt;                    &lt;property name=&quot;password&quot; value=&quot;root&quot;/&gt;                &lt;/dataSource&gt;            &lt;/environment&gt;        &lt;/environments&gt;        &lt;!-- *****引入映射文件(新增部分)***** --&gt;        &lt;mappers&gt;            &lt;mapper resource=&quot;UserMapper.xml&quot;/&gt;        &lt;/mappers&gt;    &lt;/configuration&gt;8.编写单元测试，实现插入.    [test/com.it18zhang.mybatis.test.TestCRUD.java]    /**     * insert     */    @Test    public void insert() throws Exception {        //指定配置文件的路径(类路径)        String resource = &quot;mybatis-config.xml&quot;;        //加载文件        InputStream inputStream = Resources.getResourceAsStream(resource);        //创建会话工厂Builder,相当于连接池        SqlSessionFactory sf = new SqlSessionFactoryBuilder().build(inputStream);        //通过sf开启会话，相当于打开连接。        SqlSession s = sf.openSession();        User u = new User();        u.setName(&quot;jerry&quot;);        u.setAge(2);        s.insert(&quot;users.insert&quot;, u);        s.commit();        s.close();    }9.完成update-selectOne-selectAll操作。    9.1)编写UserMapper.xml，添加相应的元素        &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;        &lt;!DOCTYPE mapper                PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot;                &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;        &lt;mapper namespace=&quot;users&quot;&gt;            &lt;insert id=&quot;insert&quot;&gt;              insert into users(name,age) values(#{name},#{age})            &lt;/insert&gt;            &lt;update id=&quot;update&quot;&gt;                update users set name = #{name} , age = #{age} where id = #{id}            &lt;/update&gt;            &lt;!-- selectOne --&gt;            &lt;select id=&quot;selectOne&quot; parameterType=&quot;int&quot; resultType=&quot;com.it18zhang.mybatisdemo.domain.User&quot;&gt;                select * from users where id = #{id}            &lt;/select&gt;            &lt;!-- selectAll --&gt;            &lt;select id=&quot;selectAll&quot; resultType=&quot;com.it18zhang.mybatisdemo.domain.User&quot;&gt;                select * from users            &lt;/select&gt;        &lt;/mapper&gt;    9.2)编写测试程序        /**         * Created by Administrator on 2017/4/6.         */        public class TestCRUD {            /**             * insert             */            @Test            public void insert() throws Exception {                //指定配置文件的路径(类路径)                String resource = &quot;mybatis-config.xml&quot;;                //加载文件                InputStream inputStream = Resources.getResourceAsStream(resource);                //创建会话工厂Builder,相当于连接池                SqlSessionFactory sf = new SqlSessionFactoryBuilder().build(inputStream);                //通过sf开启会话，相当于打开连接。                SqlSession s = sf.openSession();                User u = new User();                u.setName(&quot;jerry&quot;);                u.setAge(2);                s.insert(&quot;users.insert&quot;, u);                s.commit();                s.close();            }            /**             * update             */            @Test            public void update() throws Exception {                String resource = &quot;mybatis-config.xml&quot;;                InputStream inputStream = Resources.getResourceAsStream(resource);                SqlSessionFactory sf = new SqlSessionFactoryBuilder().build(inputStream);                SqlSession s = sf.openSession();                User u = new User();                u.setId(1);                u.setName(&quot;tomas&quot;);                u.setAge(32);                s.update(&quot;users.update&quot;, u);                s.commit();                s.close();            }            /**             * selectOne             */            @Test            public void selectOne() throws Exception {                String resource = &quot;mybatis-config.xml&quot;;                InputStream inputStream = Resources.getResourceAsStream(resource);                SqlSessionFactory sf = new SqlSessionFactoryBuilder().build(inputStream);                SqlSession s = sf.openSession();                User user = s.selectOne(&quot;users.selectOne&quot;,1);                System.out.println(user.getName());                s.commit();                s.close();            }            /**             * selectOne             */            @Test            public void selectAll() throws Exception {                String resource = &quot;mybatis-config.xml&quot;;                InputStream inputStream = Resources.getResourceAsStream(resource);                SqlSessionFactory sf = new SqlSessionFactoryBuilder().build(inputStream);                SqlSession s = sf.openSession();                List&lt;User&gt; users = s.selectList(&quot;users.selectAll&quot;);                for(User uu : users){                    System.out.println(uu.getName() + &quot;,&quot; + uu.getAge());                }                s.commit();                s.close();            }        }</code></pre><h1 id="SSM第二天"><a href="#SSM第二天" class="headerlink" title="SSM第二天"></a>SSM第二天</h1><p>三大框架的整合：</p><p>数据库层：<br>mybatis数据持久化层：dao在交互mybatis<br>dao:数据访问层：数据访问对象，存放简单的增删改查。每个表都有这个业务<br>service层：业务层，和dao交互，对dao的方法进行组合调用。形成套餐<br>springmvc展现层：做网页，做web开发的，springmvc应该和业务层交互，在springmvc里面的controller层和业务层交互，展现的东西就是jsp做web开发做网站的，做web应用的。通过web架构来实现。springmvc就是web架构里面的一个。</p><p><img src="https://i.imgur.com/ygpfvcz.png" alt=""></p><h2 id="复杂应用"><a href="#复杂应用" class="headerlink" title="复杂应用"></a>复杂应用</h2><pre><code>1.准备数据    sql.sql2.创建java类.    [Order.java]    public class Order {        private Integer id ;        private String orderNo ;        //简历关联关系        private User user ;        //get/set    }    [Item.java]    public class Item {        private Integer id;        private String itemName;        //订单项和订单之间的关联关系        private Order order;        //get/set    }3.创建Order映射文件    [resource/OrderMapper.xml]    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;    &lt;!DOCTYPE mapper            PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot;            &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;    &lt;mapper namespace=&quot;orders&quot;&gt;        &lt;insert id=&quot;insert&quot;&gt;          insert into orders(orderno,uid) values(#{orderNo},#{user.id})        &lt;/insert&gt;        &lt;!-- findById --&gt;        &lt;select id=&quot;selectOne&quot; parameterType=&quot;int&quot; resultMap=&quot;RM_Order&quot;&gt;          select            o.id oid ,            o.orderno oorderno ,            o.uid uid ,            u.name uname ,            u.age uage          from orders o            left outer join users u on o.uid = u.id where o.id = #{id}        &lt;/select&gt;        &lt;!-- findAll --&gt;        &lt;select id=&quot;selectAll&quot; resultMap=&quot;RM_Order&quot;&gt;          select            o.id oid ,            o.orderno oorderno ,            o.uid uid ,            u.name uname ,            u.age uage          from orders o            left outer join users u on o.uid = u.id        &lt;/select&gt;        &lt;!-- 自定义结果映射 --&gt;        &lt;resultMap id=&quot;RM_Order&quot; type=&quot;com.it18zhang.mybatisdemo.domain.Order&quot;&gt;            &lt;id property=&quot;id&quot; column=&quot;oid&quot;/&gt;            &lt;result property=&quot;orderNo&quot; column=&quot;oorderno&quot;/&gt;            &lt;association property=&quot;user&quot; javaType=&quot;com.it18zhang.mybatisdemo.domain.User&quot;&gt;                &lt;id property=&quot;id&quot; column=&quot;uid&quot; /&gt;                &lt;result property=&quot;name&quot; column=&quot;uname&quot; /&gt;                &lt;result property=&quot;age&quot; column=&quot;uage&quot; /&gt;            &lt;/association&gt;        &lt;/resultMap&gt;    &lt;/mapper&gt;4.修改配置文件,添加映射。    [resource/mybatis-config.xml]    &lt;!-- 引入映射文件 --&gt;    &lt;mappers&gt;        &lt;mapper resource=&quot;*Mapper.xml&quot;/&gt;    &lt;/mappers&gt;5.测试类    public class TestOrder {        /**         * insert         */        @Test        public void insert() throws Exception {            String resource = &quot;mybatis-config.xml&quot;;            InputStream inputStream = Resources.getResourceAsStream(resource);            SqlSessionFactory sf = new SqlSessionFactoryBuilder().build(inputStream);            SqlSession s = sf.openSession();            User u = new User();            u.setId(2);            Order o = new Order();            o.setOrderNo(&quot;No005&quot;);            o.setUser(u);            s.insert(&quot;orders.insert&quot;,o);            s.commit();            s.close();        }        @Test        public void selectOne() throws Exception {            String resource = &quot;mybatis-config.xml&quot;;            InputStream inputStream = Resources.getResourceAsStream(resource);            SqlSessionFactory sf = new SqlSessionFactoryBuilder().build(inputStream);            SqlSession s = sf.openSession();            Order order = s.selectOne(&quot;orders.selectOne&quot;,1);            System.out.println(order.getOrderNo());            s.commit();            s.close();        }        @Test        public void selectAll() throws Exception {            String resource = &quot;mybatis-config.xml&quot;;            InputStream inputStream = Resources.getResourceAsStream(resource);            SqlSessionFactory sf = new SqlSessionFactoryBuilder().build(inputStream);            SqlSession s = sf.openSession();            List&lt;Order&gt; list = s.selectList(&quot;orders.selectAll&quot;);            for(Order o : list){                System.out.println(o.getOrderNo() + &quot; : &quot; + o.getUser().getName());            }            s.commit();            s.close();        }    }</code></pre><h2 id="配置一对多"><a href="#配置一对多" class="headerlink" title="配置一对多"></a>配置一对多</h2><pre><code>1.在User中增加orders集合。    public class User {        ...        private List&lt;Order&gt; orders ;        //get/set    }2.改造UserMapper.xml</code></pre><h2 id="组合多对一和一对多关联关系到一个实体-Order-中"><a href="#组合多对一和一对多关联关系到一个实体-Order-中" class="headerlink" title="组合多对一和一对多关联关系到一个实体(Order)中"></a>组合多对一和一对多关联关系到一个实体(Order)中</h2><pre><code>1.关系    Order(*) -&gt; (1)User    Order(1) -&gt; (*)Item2.Order.java    class Order{        ...        List&lt;Item&gt; items ;        //get/set        }2&apos;.修改配置文件增加别名    [resources/mybatis-config.xml]    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;    &lt;!DOCTYPE configuration            PUBLIC &quot;-//mybatis.org//DTD Config 3.0//EN&quot;            &quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot;&gt;    &lt;configuration&gt;        &lt;typeAliases&gt;            &lt;typeAlias type=&quot;com.it18zhang.mybatisdemo.domain.User&quot; alias=&quot;_User&quot;/&gt;            &lt;typeAlias type=&quot;com.it18zhang.mybatisdemo.domain.Order&quot; alias=&quot;_Order&quot;/&gt;            &lt;typeAlias type=&quot;com.it18zhang.mybatisdemo.domain.Item&quot; alias=&quot;_Item&quot;/&gt;        &lt;/typeAliases&gt;        &lt;environments default=&quot;development&quot;&gt;            &lt;environment id=&quot;development&quot;&gt;                &lt;transactionManager type=&quot;JDBC&quot;/&gt;                &lt;dataSource type=&quot;POOLED&quot;&gt;                    &lt;property name=&quot;driver&quot; value=&quot;com.mysql.jdbc.Driver&quot;/&gt;                    &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/mybatis&quot;/&gt;                    &lt;property name=&quot;username&quot; value=&quot;root&quot;/&gt;                    &lt;property name=&quot;password&quot; value=&quot;root&quot;/&gt;                &lt;/dataSource&gt;            &lt;/environment&gt;        &lt;/environments&gt;        &lt;!-- 引入映射文件 --&gt;        &lt;mappers&gt;            &lt;mapper resource=&quot;UserMapper.xml&quot;/&gt;            &lt;mapper resource=&quot;OrderMapper.xml&quot;/&gt;        &lt;/mappers&gt;    &lt;/configuration&gt;3.OrderMapper.xml    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;    &lt;!DOCTYPE mapper            PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot;            &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;    &lt;mapper namespace=&quot;orders&quot;&gt;        &lt;insert id=&quot;insert&quot;&gt;          insert into orders(orderno,uid) values(#{orderNo},#{user.id})        &lt;/insert&gt;        &lt;!-- findById --&gt;        &lt;select id=&quot;selectOne&quot; parameterType=&quot;int&quot; resultMap=&quot;RM_Order&quot;&gt;          select            o.id      oid ,            o.orderno oorderno ,            o.uid     uid ,            u.name    uname ,            u.age     uage ,            i.id      iid,            i.itemname iitemname          from orders o            left outer join users u on o.uid = u.id            left outer join items i on o.id = i.oid          where o.id = #{id}        &lt;/select&gt;        &lt;!-- findAll --&gt;        &lt;select id=&quot;selectAll&quot; resultMap=&quot;RM_Order&quot;&gt;          select            o.id      oid ,            o.orderno oorderno ,            o.uid     uid ,            u.name    uname ,            u.age     uage ,            i.id      iid,            i.itemname iitemname          from orders o            left outer join users u on o.uid = u.id            left outer join items i on o.id = i.oid        &lt;/select&gt;        &lt;!-- 自定义结果映射 --&gt;        &lt;resultMap id=&quot;RM_Order&quot; type=&quot;com.it18zhang.mybatisdemo.domain.Order&quot;&gt;            &lt;id property=&quot;id&quot; column=&quot;oid&quot;/&gt;            &lt;result property=&quot;orderNo&quot; column=&quot;oorderno&quot;/&gt;            &lt;association property=&quot;user&quot; javaType=&quot;com.it18zhang.mybatisdemo.domain.User&quot;&gt;                &lt;id property=&quot;id&quot; column=&quot;uid&quot; /&gt;                &lt;result property=&quot;name&quot; column=&quot;uname&quot; /&gt;                &lt;result property=&quot;age&quot; column=&quot;uage&quot; /&gt;            &lt;/association&gt;            &lt;collection property=&quot;items&quot; ofType=&quot;_Item&quot;&gt;                &lt;id property=&quot;id&quot; column=&quot;iid&quot; /&gt;                &lt;result property=&quot;itemName&quot; column=&quot;iitemname&quot; /&gt;            &lt;/collection&gt;        &lt;/resultMap&gt;    &lt;/mapper&gt;4.测试    @Test    public void selectOne() throws Exception {        String resource = &quot;mybatis-config.xml&quot;;        InputStream inputStream = Resources.getResourceAsStream(resource);        SqlSessionFactory sf = new SqlSessionFactoryBuilder().build(inputStream);        SqlSession s = sf.openSession();        Order order = s.selectOne(&quot;orders.selectOne&quot;,1);        System.out.println(order.getOrderNo() + order.getUser().getName());        for(Item i : order.getItems()){            System.out.println(i.getId() + &quot;:&quot; + i.getItemName());        }        s.commit();        s.close();    }</code></pre><h2 id="改造项目"><a href="#改造项目" class="headerlink" title="改造项目"></a>改造项目</h2><pre><code>1.引入Util类    package com.it18zhang.mybatisdemo.util;    import org.apache.ibatis.io.Resources;    import org.apache.ibatis.session.SqlSession;    import org.apache.ibatis.session.SqlSessionFactory;    import org.apache.ibatis.session.SqlSessionFactoryBuilder;    import java.io.InputStream;    /**     * 工具类     */    public class Util {        //        private static SqlSessionFactory sf ;        static{            try {                String resource = &quot;mybatis-config.xml&quot;;                InputStream inputStream = Resources.getResourceAsStream(resource);                sf = new SqlSessionFactoryBuilder().build(inputStream);            } catch (Exception e) {                e.printStackTrace();            }        }        /**         * 开启会话         */        public static SqlSession openSession(){            return sf.openSession() ;        }        /**         * 关闭会话         */        public static void closeSession(SqlSession s){            if(s != null){                s.close();            }        }        /**         * 关闭会话         */        public static void rollbackTx(SqlSession s) {            if (s != null) {                s.rollback();            }        }    }2.设计模板类DaoTemplate和回调MybatisCallback接口    [DaoTemplate.java]    package com.it18zhang.mybatisdemo.dao;    import com.it18zhang.mybatisdemo.util.Util;    import org.apache.ibatis.session.SqlSession;    /**     * 模板类     */    public class DaoTemplate {        /**         * 执行         */        public static Object execute(MybatisCallback cb){            SqlSession s = null;            try {                s = Util.openSession();                Object ret = cb.doInMybatis(s);                s.commit();                return ret ;            } catch (Exception e) {                Util.rollbackTx(s);            } finally {                Util.closeSession(s);            }            return null ;        }    }    [MybatisCallback.java]    package com.it18zhang.mybatisdemo.dao;    import org.apache.ibatis.session.SqlSession;    /**     * 回调接口     */    public interface MybatisCallback {        public Object doInMybatis(SqlSession s);    }3.通过模板类+回调接口实现UserDao.java    [UserDao.java]    package com.it18zhang.mybatisdemo.dao;    import com.it18zhang.mybatisdemo.domain.User;    import com.it18zhang.mybatisdemo.util.Util;    import org.apache.ibatis.session.SqlSession;    import java.util.List;    /**     * UserDao     */    public class UserDao {        /**         * 插入操作         */        public void insert(final User user){            DaoTemplate.execute(new MybatisCallback() {                public Object doInMybatis(SqlSession s) {                    s.insert(&quot;users.insert&quot;,user);                    return null ;                }            });        }        /**         * 插入操作         */        public void update(final User user){            DaoTemplate.execute(new MybatisCallback() {                public Object doInMybatis(SqlSession s) {                    s.update(&quot;users.update&quot;, user);                    return null ;                }            });        }        public User selctOne(final Integer id){            return (User)DaoTemplate.execute(new MybatisCallback() {                public Object doInMybatis(SqlSession s) {                    return s.selectOne(&quot;users.selectOne&quot;,id);                }            });        }        public List&lt;User&gt; selctAll(){            return (List&lt;User&gt;)DaoTemplate.execute(new MybatisCallback() {                public Object doInMybatis(SqlSession s) {                    return s.selectList(&quot;users.selectAll&quot;);                }            });        }    }4.App测试    public static void main(String[] args) {        UserDao dao = new UserDao();        User u = dao.selctOne(1);        System.out.println(u.getName());    }</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;SSM&quot;&gt;&lt;a href=&quot;#SSM&quot; class=&quot;headerlink&quot; title=&quot;SSM&quot;&gt;&lt;/a&gt;SSM&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;Spring        //业务层框架
Spring MVC    //开发web程序应用的模块model + 
      
    
    </summary>
    
    
      <category term="SSM" scheme="http://erichunn.github.io/tags/SSM/"/>
    
  </entry>
  
  <entry>
    <title>Avro&amp;protobuf</title>
    <link href="http://erichunn.github.io/2018/11/11/Avro-protobuf/"/>
    <id>http://erichunn.github.io/2018/11/11/Avro-protobuf/</id>
    <published>2018-11-11T11:52:10.000Z</published>
    <updated>2018-11-13T12:56:38.859Z</updated>
    
    <content type="html"><![CDATA[<h2 id="hive"><a href="#hive" class="headerlink" title="hive"></a>hive</h2><pre><code>数据倾斜.$hive&gt;SET hive.optimize.skewjoin=true;$hive&gt;SET hive.skewjoin.key=100000;$hive&gt;SET hive.groupby.skewindata=true;</code></pre><p>CREATE TABLE mydb.doc(line string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’ ;</p><p>select t.word,count(*) from (select explode(split(line,’ ‘)) word from doc ) t group by t.word ;</p><h2 id="java串行化"><a href="#java串行化" class="headerlink" title="java串行化"></a>java串行化</h2><h2 id="串行化系统"><a href="#串行化系统" class="headerlink" title="串行化系统"></a>串行化系统</h2><p>protobuf，协议缓冲区。</p><p>在Hadoop里面的代码很多是通过相关语言自动生成的。</p><p>hadoop底层的rpc都是自动生成的。</p><p>java也有串行化，但是hadoop不适用，因为相对来讲比protobuf和avro复杂而且性能差一些。</p><p>当吧对象写入磁盘文件或者用于网络传输的时候要把对象变成字节数组。</p><p>串行化从本质上来讲是数据格式，就是一种格式，吧对象变成字节数组，也就是说所有数据都要落实到字节数组上，都需要这样，不管是影音文件还是别的文件，通过串行化吧他边恒字节数组    </p><h1 id="关于Javabean"><a href="#关于Javabean" class="headerlink" title="关于Javabean:"></a>关于Javabean:</h1><p>标准javabean(pojo,plain old java object)</p><p>任何一个Java类也可以叫javabean.广义上。</p><p>狭义上的javabean：就是普通古老的java对象pojo:plain old java object </p><p>也就是私有的属性，getset方法，空的构造函数。比如person,学生，人。都是javabean。现实生活中的名词，在开发中都被定义成一个类，也就是说的javabean。</p><p>下面的代码就是一段javabean<br>    class Person{<br>        public Person(){<br>    }<br>    private String name;<br>    public  void setName(String name){<br>        this.name=name;<br>    }<br>    publc String genName(){<br>        return name; }<br>}</p><h2 id="google-protobuf"><a href="#google-protobuf" class="headerlink" title="google protobuf"></a>google protobuf</h2><pre><code>1.下载google protobuf.配置环境    protoc-2.5.0-win32.zip</code></pre><p><img src="https://i.imgur.com/7QVjPjY.png" alt=""></p><pre><code>1&apos;.pom.xml    &lt;dependencies&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.google.protobuf&lt;/groupId&gt;            &lt;artifactId&gt;protobuf-java&lt;/artifactId&gt;            &lt;version&gt;2.5.0&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;junit&lt;/groupId&gt;            &lt;artifactId&gt;junit&lt;/artifactId&gt;            &lt;version&gt;4.11&lt;/version&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;2.设计对象    ...3.描述对象    package tutorial;    option java_package = &quot;com.example.tutorial&quot;;    option java_outer_classname = &quot;AddressBookProtos&quot;;    //这个是一个javabean，在protobuf里面叫message        message Person {        required string name = 1;        required int32 id = 2;        optional string email = 3;    //下面这个是一个phonetype的枚举类        enum PhoneType {            MOBILE = 0;            HOME = 1;            WORK = 2;        }    //这个也是一个javabean。        message PhoneNumber {            required string number = 1;            optional PhoneType type = 2 [default = HOME];        }        repeated PhoneNumber phone = 4;    }    message AddressBook {        repeated Person person = 1;    }4.编译描述    cmd&gt;protoc --java_out . xxx.proto5.导入源代码到项目中    ...6.使用对象    public class TestProtoBuf {        @Test        public void write() throws Exception{            AddressBookProtos.Person john = AddressBookProtos.Person.newBuilder()                    .setId(12345)                    .setName(&quot;tomas&quot;)                    .setEmail(&quot;123@123.123&quot;)                    .addPhone(AddressBookProtos.Person.PhoneNumber.newBuilder()                            .setNumber(&quot;+351 999 999 999&quot;)                            .setType(AddressBookProtos.Person.PhoneType.HOME)                            .build())                    .build();            john.writeTo(new FileOutputStream(&quot;d:/prototbuf.data&quot;));        }        @Test        public void read() throws Exception{            AddressBookProtos.Person john = AddressBookProtos.Person.parseFrom(new FileInputStream(&quot;d:/prototbuf.data&quot;));            System.out.println(john.getName());        }    }</code></pre><p>上一段是用Protobuf进行编译和反编译的过程。然后下面这段图片是用java进行编译和反编译的过程：</p><p><img src="https://i.imgur.com/8rzRjOu.png" alt=""></p><h2 id="xml"><a href="#xml" class="headerlink" title="xml"></a>xml</h2><pre><code>&lt;?xml version=&quot;1.0&quot;?&gt;&lt;persons&gt;    &lt;person id=&quot;&quot; name=&quot;&quot;&gt;        &lt;age&gt;12&lt;/age&gt;    &lt;/person&gt;&lt;/person&gt;</code></pre><h2 id="json"><a href="#json" class="headerlink" title="json"></a>json</h2><pre><code>[{    &quot;id&quot; : 1,    &quot;nmae&quot; : &quot;tom&quot;,    &quot;age&quot; : 20},{    &quot;id&quot; : 2,    &quot;nmae&quot; : &quot;tomas&quot;,    &quot;age&quot; : 30}]</code></pre><h2 id="avro-doug-cutting"><a href="#avro-doug-cutting" class="headerlink" title="avro (doug cutting)"></a>avro (doug cutting)</h2><pre><code>1.数据串行化系统2.自描述语言.    数据结构和数据都存在文件中。跨语言。    使用json格式存储数据。3.可压缩 + 可切割。4.使用avro    a)定义schema    b)编译schema，生成java类        {    //名字空间是这个，然后类型，然后名字，然后是字段数组            &quot;namespace&quot;: &quot;tutorialspoint.com&quot;,            &quot;type&quot;: &quot;record&quot;,            &quot;name&quot;: &quot;emp&quot;,            &quot;fields&quot;: [                {&quot;name&quot;: &quot;name&quot;, &quot;type&quot;: &quot;string&quot;},                {&quot;name&quot;: &quot;id&quot;, &quot;type&quot;: &quot;int&quot;},                {&quot;name&quot;: &quot;salary&quot;, &quot;type&quot;: &quot;int&quot;},                {&quot;name&quot;: &quot;age&quot;, &quot;type&quot;: &quot;int&quot;},                {&quot;name&quot;: &quot;address&quot;, &quot;type&quot;: &quot;string&quot;}            ]        }     c)使用java类        cmd&gt;java -jar avro-tools-1.7.7.jar compile schema emp.avsc .    d)单元测试        package com.it18zhang.avrodemo.test;        import org.apache.avro.Schema;        import org.apache.avro.file.DataFileReader;        import org.apache.avro.file.DataFileWriter;        import org.apache.avro.generic.GenericData;        import org.apache.avro.generic.GenericRecord;        import org.apache.avro.io.DatumWriter;        import org.apache.avro.specific.SpecificDatumReader;        import org.apache.avro.specific.SpecificDatumWriter;        import org.junit.Test;        import java.io.File;        import java.io.IOException;        import java.util.Iterator;        /**         * Created by Administrator on 2017/3/23.         */        public class TestAvro {        //    @Test        //    public void write() throws Exception {        //        //创建writer对象        //        SpecificDatumWriter empDatumWriter = new SpecificDatumWriter&lt;Employee&gt;(Employee.class);        //        //写入文件        //        DataFileWriter&lt;Employee&gt; empFileWriter = new DataFileWriter&lt;Employee&gt;(empDatumWriter);        //        //        //创建对象        //        Employee e1 = new Employee();        //        e1.setName(&quot;tomas&quot;);        //        e1.setAge(12);        //        //        //串行化数据到磁盘        //        empFileWriter.create(e1.getSchema(), new File(&quot;d:/avro/data/e1.avro&quot;));        //        empFileWriter.append(e1);        //        empFileWriter.append(e1);        //        empFileWriter.append(e1);        //        empFileWriter.append(e1);        //        //关闭流        //        empFileWriter.close();        //    }        //        //    @Test        //    public void read() throws Exception {        //        //创建writer对象        //        SpecificDatumReader empDatumReader = new SpecificDatumReader&lt;Employee&gt;(Employee.class);        //        //写入文件        //        DataFileReader&lt;Employee&gt; dataReader = new DataFileReader&lt;Employee&gt;(new File(&quot;d:/avro/data/e1.avro&quot;)  ,empDatumReader);        //        Iterator&lt;Employee&gt; it = dataReader.iterator();        //        while(it.hasNext()){        //            System.out.println(it.next().getName());        //        }        //    }            /**             * 直接使用schema文件进行读写，不需要编译             */            @Test            public void writeInSchema() throws  Exception {                //指定定义的avsc文件。                Schema schema = new Schema.Parser().parse(new File(&quot;d:/avro/emp.avsc&quot;));                //创建GenericRecord,相当于Employee                GenericRecord e1 = new GenericData.Record(schema);                //设置javabean属性                e1.put(&quot;Name&quot;, &quot;ramu&quot;);        //        e1.put(&quot;id&quot;, 001);        //        e1.put(&quot;salary&quot;, 30000);                e1.put(&quot;age&quot;, 25);        //        e1.put(&quot;address&quot;, &quot;chennai&quot;);                //                DatumWriter&lt;GenericRecord&gt; empDatumWriter = new SpecificDatumWriter&lt;GenericRecord&gt;(GenericRecord.class);                DataFileWriter&lt;GenericRecord&gt; empFileWriter = new DataFileWriter&lt;GenericRecord&gt;(empDatumWriter);                empFileWriter.create(schema,new File(&quot;d:/avro/data/e2.avro&quot;)) ;                empFileWriter.append(e1);                empFileWriter.append(e1);                empFileWriter.append(e1);                empFileWriter.append(e1);                empFileWriter.close();            }        }</code></pre><p>看一下AVSC这个编译好的avro文件里面的是什么结构：</p><p>他其实是一个json格式的结构：</p><p><img src="https://i.imgur.com/dKcBw8b.png" alt=""></p><pre><code>非编译模式---------------    @Test    public void writeInSchema() throws  Exception {        //指定定义的avsc文件。        Schema schema = new Schema.Parser().parse(new File(&quot;d:/avro/emp.avsc&quot;));        //创建GenericRecord,相当于Employee        GenericRecord e1 = new GenericData.Record(schema);        //设置javabean属性        e1.put(&quot;name&quot;, &quot;ramu&quot;);//        e1.put(&quot;id&quot;, 001);//        e1.put(&quot;salary&quot;, 30000);        e1.put(&quot;age&quot;, 25);//        e1.put(&quot;address&quot;, &quot;chennai&quot;);        //        DatumWriter w1 = new SpecificDatumWriter (schema);        DataFileWriter w2 = new DataFileWriter(w1);        w2.create(schema,new File(&quot;d:/avro/data/e2.avro&quot;)) ;        w2.append(e1);        w2.append(e1);        w2.close();    }    /**     * 反串行avro数据     */    @Test    public void readInSchema() throws  Exception {        //指定定义的avsc文件。        Schema schema = new Schema.Parser().parse(new File(&quot;d:/avro/emp.avsc&quot;));        GenericRecord e1 = new GenericData.Record(schema);        DatumReader r1 = new SpecificDatumReader (schema);        DataFileReader r2 = new DataFileReader(new File(&quot;d:/avro/data/e2.avro&quot;),r1);        while(r2.hasNext()){            GenericRecord rec = (GenericRecord)r2.next();            System.out.println(rec.get(&quot;name&quot;));        }        r2.close();    }</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;hive&quot;&gt;&lt;a href=&quot;#hive&quot; class=&quot;headerlink&quot; title=&quot;hive&quot;&gt;&lt;/a&gt;hive&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;数据倾斜.
$hive&amp;gt;SET hive.optimize.skewjoin=true;
$hive&amp;
      
    
    </summary>
    
      <category term="Avro&amp;Protobuf" scheme="http://erichunn.github.io/categories/Avro-Protobuf/"/>
    
    
      <category term="Avro" scheme="http://erichunn.github.io/tags/Avro/"/>
    
      <category term="Protobuf" scheme="http://erichunn.github.io/tags/Protobuf/"/>
    
  </entry>
  
  <entry>
    <title>笔记重点总结</title>
    <link href="http://erichunn.github.io/2018/11/11/%E7%AC%94%E8%AE%B0%E9%87%8D%E7%82%B9%E6%80%BB%E7%BB%93/"/>
    <id>http://erichunn.github.io/2018/11/11/笔记重点总结/</id>
    <published>2018-11-11T11:51:41.000Z</published>
    <updated>2018-11-19T08:52:06.228Z</updated>
    
    <content type="html"><![CDATA[<p>在Hadoop中重点是，全排序，二次排序。<br>在视频的最后有一天是讲解一个二次排序的实例，我没有细看。</p><p>在Hive中我认为重点的是关于查询的内容，因为在hive中一般都是加载数据然后查询，查询之后有一个自定义函数UDF的内容，这部分的内容看的不是很清晰。</p><p>在avro和Protoc里面主要讲解2中反编译的方法，主要是一个面向对象开发的一个过程，还是java编程不熟悉，然后代码跟着走了一遍但是肯定还是有问题，视频里面的老师是在一遍看外文的书籍，根据里面的代码模板来写的。在自己敲的过程中protoc的代码引入maven依赖包之后，还是没有能够将编译好的文件识别出来，不知道什么问题，但是avo就引入maven之后解决了这个问题。</p><p>在讲解zookeeper的时候我觉得在选举算法的时候他没有细致的讲解，<br>然后这边主要是zookeeper部署在hadoop集群和使用API控制zookeeper。在集群部署高可用的时候是201202203部署zk202203204部署journalnode节点，（journalnode节点已经快忘记了）。</p><p>容灾管理器进程ZKFC在配置的时候后来跟着yarn启动了起来。下面图是整个的进程图：</p><p><img src="https://i.imgur.com/osWW4ZM.png" alt=""></p><p>这个zookeeper的地方看的比较快，对于每个目录做什么的比较忘记，然后在配置的时候没有细究配置的意义。就是跟着走了一遍。但是最后配置了起来了。配置hdfs高可用的时候比较费时费力，配置内容比较多，配置yarn的时候比较简单，配置一下即可。</p><hr><p>在hbase安装的过程中，由于开始的时候设置了zookeeper的HA自动容灾，所以最开始的时候s206自动设置为active模式，要把s206设置为<br>standby模式才能在s201里面设置s201为HMaster。</p><p>也就是说要杀死掉s206里面的namenode或者说通过转换吧他变成standby然后s201设置为active模式。不然会导致出错</p><hr><p>几个端口2181 8080 50070  8020 16010</p><hr><p>在讲解hbase 第二天的动态遍历并没有看明白，也就是说Java的基础还是很薄弱的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在Hadoop中重点是，全排序，二次排序。&lt;br&gt;在视频的最后有一天是讲解一个二次排序的实例，我没有细看。&lt;/p&gt;
&lt;p&gt;在Hive中我认为重点的是关于查询的内容，因为在hive中一般都是加载数据然后查询，查询之后有一个自定义函数UDF的内容，这部分的内容看的不是很清晰。&lt;
      
    
    </summary>
    
      <category term="笔记总结" scheme="http://erichunn.github.io/categories/%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="笔记" scheme="http://erichunn.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="重点" scheme="http://erichunn.github.io/tags/%E9%87%8D%E7%82%B9/"/>
    
      <category term="总结" scheme="http://erichunn.github.io/tags/%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>心情日记———————2018.11.110</title>
    <link href="http://erichunn.github.io/2018/11/11/%E5%BF%83%E6%83%85%E6%97%A5%E8%AE%B0%E2%80%94%E2%80%94%E2%80%94%E2%80%94%E2%80%94%E2%80%94%E2%80%942018-11-11/"/>
    <id>http://erichunn.github.io/2018/11/11/心情日记———————2018-11-11/</id>
    <published>2018-11-11T11:11:03.000Z</published>
    <updated>2018-11-11T11:11:03.819Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>心情日记——2018.11.09</title>
    <link href="http://erichunn.github.io/2018/11/09/%E4%BA%BA%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%8A%AA%E5%8A%9B%EF%BC%9F%E2%80%94%E2%80%942018-11-09/"/>
    <id>http://erichunn.github.io/2018/11/09/人为什么要努力？——2018-11-09/</id>
    <published>2018-11-09T14:38:08.000Z</published>
    <updated>2018-11-09T14:39:47.344Z</updated>
    
    <content type="html"><![CDATA[<p>无意之间从知乎里面看见的题目是：人这一辈子为什么要努力？</p><p>泪目，与诸君共勉。</p><pre><code>奶奶五十岁才来的美国，身无分文，也没有一技之长，随身携带的只有长年辛勤劳作留下的一身病根。那是九十年代初，她去了唐人街的扎花厂上班，每天工作十个小时以上，一年工作三百六十五天。扎花按件算钱，她眼神虽然不好，却比谁扎得都快。有人弹吉他磨出了茧，有人搬砖头磨出了茧，她被针头扎出了茧。回国看我的时候，她给我带了费列罗巧克力，翘胡子薯片，Jif花生酱。她给我买好看的小西装，给我买一斤几十元的黄螺。她带我去动物园，游乐园，森林公园，带我去北京看长城，看天安门，看毛主席。“奶奶，美国比北京还好吗？”她用最朴实的语言向我描述纽约的繁华，告诉我美国好极了，一切都在等着我。知乎上经常讨论富养女孩，我有一个男孩被富养的故事。有一年，我的巧克力吃完了，薯片吃完了，花生酱吃完了，奶奶还是没有回来。那是我第一个不愿意知道的生活真相，她中风了，瘫了半边身子。奶奶移民美国时带去了三个未成年的儿子，二叔，三叔，四叔。二叔和三叔在登陆美国的第二天就打工去了，年幼的四叔读了几年书后也离开了学校。他们在亲戚家的餐馆打工，每天工作十二个小时，每周工作六天。餐馆一年营业三百六十四天，只在感恩节那天歇业。奶奶瘫痪后，叔叔们轮流回国，给我带好吃的，带我出去玩。我喜欢打乒乓球，二叔给我买了乒乓球，乒乓球拍，乒乓球桌。有一年流行四驱车，三叔给我买了一辆遥控越野车，助我碾压所有的小伙伴。四叔年少风流，出门把妹的时候总不忘带上我，要把我培养成下一代情圣。他们绝口不提在美国生存的艰辛，那是大人们的秘密，和我无关。奶奶出国五年后，爸妈也去了美国。怎一个落魄了得？夫妻俩连属于自己的房间都没有。扎花已经被时代淘汰，只有重活可以干。我爸当过屠夫，货柜工人，货运司机。细节不必赘述，无非就是 12小时 x 365天的陈词滥调。后来，他们四兄弟聚首，开了一家小超市，一家餐馆，后来又开了第二家小超市，第二家餐馆。钱赚得越来越多，老家伙们拼命工作的老毛病却没有得到丝毫缓解。爸妈出国五年后，我也来了美国，看清了生活本来的面目。我在纽约生活了几个月，带着半身不遂的奶奶看遍了世界之都的繁华。在这之前，她几乎没有走出过唐人街的范围，以前对我描绘纽约时一半是转述，一半靠想象。后来，我回到了父母的身边，结束了长达五年的骨肉分离。我爸来车站接我，把我带到了一栋小别墅前，很得意地告诉我：“这是我们家，知道你要来，刚买的。”我去过奶奶刚来美国时住过的那个阴暗破旧的小公寓楼，也去过爸妈栖身过的那个散发着霉味的地下室，我知道我在新家会有一张床，一个房间，但我没想到，我会有一个别墅，一个前院，一个后院，一整个铺垫好的未来。人这一生为什么要努力？奶奶的答案是我，爸妈的答案是我，我的答案是他们，以及把身家性命托付于我的媳妇和孩子。对于我来说，长大了，责任多了，自然而然地想要努力，不是为了赚很多很多钱，而是为了过上自己想要的生活，过上奶奶和父母不曾拥有过的生活。他们替我吃完了所有的苦，帮我走了九十九步，我自己只需再走一步，哪敢迟疑？从外曾祖父母算起，我们家族四代八十多口人已经在美国奋斗了半个多世纪。我们人人都在努力，我们一代好过一代。如果你的人生起点不高，不曾有人为你走过人生百步中的任何一步，不打紧，你尽管努力，多出来的步数不会被浪费掉，总有你在乎的人用得着，而你迟早会遇到你在乎的人。与诸君共勉。顾宇的知乎回答索引</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;无意之间从知乎里面看见的题目是：人这一辈子为什么要努力？&lt;/p&gt;
&lt;p&gt;泪目，与诸君共勉。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;奶奶五十岁才来的美国，身无分文，也没有一技之长，随身携带的只有长年辛勤劳作留下的一身病根。

那是九十年代初，她去了唐人街的扎花厂上班，每天工作十个小时
      
    
    </summary>
    
      <category term="心情记" scheme="http://erichunn.github.io/categories/%E5%BF%83%E6%83%85%E8%AE%B0/"/>
    
    
      <category term="心情记" scheme="http://erichunn.github.io/tags/%E5%BF%83%E6%83%85%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Hive第二天</title>
    <link href="http://erichunn.github.io/2018/11/08/Hive%E7%AC%AC%E4%BA%8C%E5%A4%A9/"/>
    <id>http://erichunn.github.io/2018/11/08/Hive第二天/</id>
    <published>2018-11-08T08:17:04.000Z</published>
    <updated>2018-11-11T11:47:34.578Z</updated>
    
    <content type="html"><![CDATA[<h2 id="hive"><a href="#hive" class="headerlink" title="hive"></a>hive</h2><pre><code>数据仓库,在线分析处理。HiveQL,类似sql语言。表,metadata-&gt;rdbms.hive处理的数据是hdfs.MR,聚合操作。</code></pre><h1 id="hive的2个重点一个是分区一个是桶表"><a href="#hive的2个重点一个是分区一个是桶表" class="headerlink" title="hive的2个重点一个是分区一个是桶表"></a>hive的2个重点一个是分区一个是桶表</h1><h1 id="内部表-管理表-托管表"><a href="#内部表-管理表-托管表" class="headerlink" title="内部表,管理表,托管表"></a>内部表,管理表,托管表</h1><pre><code>hive,drop ,数据也删除</code></pre><h1 id="外部表"><a href="#外部表" class="headerlink" title="外部表"></a>外部表</h1><pre><code>hive表结构。</code></pre><h1 id="分区表"><a href="#分区表" class="headerlink" title="分区表"></a>分区表</h1><pre><code>目录.where 缩小查询范围。</code></pre><h1 id="bucket表"><a href="#bucket表" class="headerlink" title="bucket表"></a>bucket表</h1><pre><code>文件。hashclustered by &apos;&apos;</code></pre><h1 id="join"><a href="#join" class="headerlink" title="join"></a>join</h1><pre><code>横向连接，也就是作外连接和右外连接</code></pre><h1 id="union"><a href="#union" class="headerlink" title="union"></a>union</h1><pre><code>竖向连接。只能指定相匹配的字段select id ,name from a1 union select id ,cid from a2;</code></pre><h1 id="hive-union操作"><a href="#hive-union操作" class="headerlink" title="hive union操作"></a>hive union操作</h1><pre><code>select id,name from customers union select id,orderno from orders ;$&gt;hive                            //hive --service cli $&gt;hive --servic hiveserver2        //启动hiveserver2，10000 [thriftServer]$&gt;hive --service beeline        //beeline    </code></pre><h1 id="hive使用jdbc协议实现远程访问"><a href="#hive使用jdbc协议实现远程访问" class="headerlink" title="hive使用jdbc协议实现远程访问"></a>hive使用jdbc协议实现远程访问</h1><h1 id="hive-1"><a href="#hive-1" class="headerlink" title="hive"></a>hive</h1><pre><code>$hive&gt;CREATE TABLE t3(id int,name string,age int) PARTITIONED BY (Year INT, Month INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;</code></pre><h1 id="export"><a href="#export" class="headerlink" title="export"></a>export</h1><pre><code>$hive&gt;EXPORT TABLE customers TO &apos;/user/centos/tmp.txt&apos;;        //导出表结构+数据到hdfs目录。看一下导出来的东西，是一个目录，包括表结构和表内容。</code></pre><p><img src="https://i.imgur.com/pFsQ5fZ.png" alt=""></p><h1 id="order全排序"><a href="#order全排序" class="headerlink" title="/order全排序"></a>/order全排序</h1><pre><code>$hive&gt;select * from orders order by id asc ;</code></pre><h1 id="sort-map端排序-本地有序。"><a href="#sort-map端排序-本地有序。" class="headerlink" title="sort,map端排序,本地有序。"></a>sort,map端排序,本地有序。</h1><pre><code>$hive&gt;select * from orders sort by id asc ;</code></pre><h1 id="distribute-by-amp-cluster-by-amp-sort-by"><a href="#distribute-by-amp-cluster-by-amp-sort-by" class="headerlink" title="distribute by &amp; cluster by  &amp; sort by"></a>distribute by &amp; cluster by  &amp; sort by</h1><p>类似于mysql的group by,进行分区操作。</p><pre><code>//select cid , ... from orders distribute by cid sort by name ;            //注意顺序.$hive&gt;select id,orderno,cid from orders distribute by cid sort by cid desc ;//cluster by ===&gt;  distribute by cid sort by cid</code></pre><p><strong>destribut by 和cluster by图解对比：</strong></p><p><img src="https://i.imgur.com/BQ7NHQU.png" alt=""></p><p>这个地方没有细讲，说主要应用还是group by 只要记住这个cluster是dirstribute by 和sort by 的组合极客</p><h2 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h2><pre><code>mysql&gt;select concat(&apos;tom&apos;,1000) ;$hive&gt;select current_database(),current_user() ;$hive&gt;tab                                //查看帮助</code></pre><h2 id="设置作业参数"><a href="#设置作业参数" class="headerlink" title="设置作业参数"></a>设置作业参数</h2><pre><code>$hive&gt;set hive.exec.reducers.bytes.per.reducer=xxx            //设置reducetask的字节数。$hive&gt;set hive.exec.reducers.max=0                            //设置reduce task的最大任务数$hive&gt;set mapreduce.job.reduces=0                            //设置reducetask个数。</code></pre><h2 id="动态分区-严格模式-非严格模式"><a href="#动态分区-严格模式-非严格模式" class="headerlink" title="动态分区-严格模式-非严格模式"></a>动态分区-严格模式-非严格模式</h2><pre><code>动态分区模式:strict-严格模式，插入时至少指定一个静态分区，nonstrict-非严格模式-可以不指定静态分区。set hive.exec.dynamic.partition.mode=nonstrict            //设置非严格模式$hive&gt;INSERT [OVERWRITE] into TABLE employees PARTITION (country, state) SELECT ..., se.cnty, se.st FROM staged_employees se WHERE se.cnty = &apos;US&apos;;</code></pre><p><img src="https://i.imgur.com/8zgzuuO.png" alt=""></p><p>本来有一个已经分区好了的表，然后再建立一个表，比如也是相同的字段，然后新表没有分区要插进去老表，让他自动分区，就要用到动态分区，动态分区分为严格模式和非严格模式。如下图先使用了严格模式的情况，出错，说要至少指定一个分区也就是</p><pre><code>$hive&gt;INSERT [OVERWRITE] into TABLE employees PARTITION (country=&apos;US&apos;, state) SELECT ..., se.cnty, se.st FROM staged_employees;</code></pre><p>但是严格模式还是要制定这个分区，所以用一下非严格模式先设置一下：</p><pre><code>set hive.exec.dynamic.partition.mode=nonstrict            //设置非严格模式</code></pre><p>然后我们开始插入：</p><pre><code>$hive&gt;INSERT [OVERWRITE] into TABLE employees PARTITION (country, state) SELECT ..., se.cnty, se.st FROM staged_employees;</code></pre><p>这样就实现了非严格模式下的动态分区</p><h1 id="hive事务处理在-gt-0-13-0之后支持行级事务。-（这个地方老师没讲清楚自己没搞出来）"><a href="#hive事务处理在-gt-0-13-0之后支持行级事务。-（这个地方老师没讲清楚自己没搞出来）" class="headerlink" title="hive事务处理在&gt;0.13.0之后支持行级事务。 #（这个地方老师没讲清楚自己没搞出来）"></a>hive事务处理在&gt;0.13.0之后支持行级事务。 #（这个地方老师没讲清楚自己没搞出来）</h1><pre><code>1.所有事务自动提交。2.只支持orc格式。3.使用bucket表。4.配置hive参数，使其支持事务。</code></pre><p>$hive&gt;SET hive.support.concurrency = true;<br>$hive&gt;SET hive.enforce.bucketing = true;<br>$hive&gt;SET hive.exec.dynamic.partition.mode = nonstrict;<br>$hive&gt;SET hive.txn.manager = org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;<br>$hive&gt;SET hive.compactor.initiator.on = true;<br>$hive&gt;SET hive.compactor.worker.threads = 1;</p><pre><code>5.使用事务性操作    $&gt;CREATE TABLE tx(id int,name string,age int) CLUSTERED BY (id) INTO 3 BUCKETS ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; stored as orc TBLPROPERTIES (&apos;transactional&apos;=&apos;true&apos;);</code></pre><p><img src="https://i.imgur.com/UWJd7M5.png" alt=""></p><hr><p>mysql支持这样的操作，左边选择的查询的列，并不出现在右边的分组当中这样可以查询，但是hive中不允许这样。</p><p><img src="https://i.imgur.com/vpfbmjD.png" alt=""></p><p>但是在hive当中只可以如下图查询：</p><p><img src="https://i.imgur.com/oOqn35d.png" alt=""></p><h2 id="聚合处理"><a href="#聚合处理" class="headerlink" title="聚合处理"></a>聚合处理</h2><pre><code>每个用户购买商品订单大于1的：这个按照cid分组的，查询每个分组里面的count(*)，也就是每个不同的cid 的$hive&gt;select cid,count(*) c ,max(price) from orders group by cid having c &gt; 1 ; </code></pre><h1 id="wordcount"><a href="#wordcount" class="headerlink" title="wordcount"></a>wordcount</h1><p>创建新表:stats(word string,c int) ;<br>    将查询结果插入到指定表中。</p><p><img src="https://i.imgur.com/wxA99my.png" alt=""></p><p>按照下图这个切割方法，用一个split的函数切割，切割之后形成一个数组类型的东西</p><p><img src="https://i.imgur.com/NVQuyss.png" alt=""></p><p>explode这个函数传入的需要是一个经过split切割之后的数组，传入之后再去展开得到如上图所示的东西。但是切出来还是需要聚合，查询每个单词的个数</p><p>对下面这个SQL语句进行讲解。先看括号里面的最里面的括号，select explode(split(line,’ ‘)) as word from doc这句话都能理解，就是先切割成数组，然后对这个数据进行explode炸裂成单词，然后as word就是别名叫word，然后再把整个查询的as t别名t，然后select t.word,count(*) c from …group by t.word order by c desc limit2;就是对t的word字段进行分组，对分组的t的字段word和计数总和进行查询，然后按照降序排序，然后limit 2取最高的2个数。</p><pre><code>$hive&gt;select t.word,count(*) c from ((select explode(split(line, &apos; &apos;)) as word from doc) as t) group by t.word order by c desc limit 2 ;</code></pre><h1 id="view-视图-虚表"><a href="#view-视图-虚表" class="headerlink" title="view:视图,虚表"></a>view:视图,虚表</h1><p>是一个虚拟的表，类似于对这个语句的封装，或者说起了一个别名，并不是真的表。</p><p>这个地方创建视图的时候不能</p><pre><code>$hive&gt;create view v1 as select a.*,b.*from customers a left outer join default.tt b on a.id = b.cid ;</code></pre><p>上图这样是错误的，因为在a和b里面有相同的字段，要把相同的字段修改为不同的字段，如下图：</p><pre><code>//创建视图$hive&gt;create view v1 as select a.id aid,a.name ,b.id bid , b.order from customers a left outer join default.tt b on a.id = b.cid ;</code></pre><p>可以在续表的基础上在查询，如下所示：</p><pre><code>//查看视图$hive&gt;show tables ;$hive&gt;select * from v1 ;</code></pre><p>view这个在hdfs上没有哦，而是存放在了mysql里面</p><h2 id="一个mysql里面的操作：-G-行转列"><a href="#一个mysql里面的操作：-G-行转列" class="headerlink" title="一个mysql里面的操作：\G 行转列"></a>一个mysql里面的操作：\G 行转列</h2><p>select * from tbls \G：结果如下：</p><p><img src="https://i.imgur.com/AbE1mvx.png" alt=""></p><h2 id="Map端连接"><a href="#Map端连接" class="headerlink" title="Map端连接"></a>Map端连接</h2><pre><code>$hive&gt;set hive.auto.convert.join=true            //设置自动转换连接,默认开启了。//使用mapjoin连接暗示实现mapjoin$hive&gt;select /*+ mapjoin(customers) */ a.*,b.* from customers a left outer join orders b on a.id = b.cid ;</code></pre><h2 id="调优"><a href="#调优" class="headerlink" title="调优"></a>调优</h2><pre><code>1.explain    使用explain查看查询计划    hive&gt;explain [extended] select count(*) from customers ;    hive&gt;explain select t.name , count(*) from (select a.name ,b.id,b.orderno from customers a ,orders b where a.id = b.cid) t group by t.name ;    //设置limit优化测，避免全部查询.但是树上说因为是随机取样，可能会出问题。    hive&gt;set hive.limit.optimize.enable=true    //本地模式    $hive&gt;set mapred.job.tracker=local;            //    $hive&gt;set hive.exec.mode.local.auto=true    //自动本地模式,主要用于测试不用于实战    //并行执行,同时执行不存在依赖关系的阶段。??    $hive&gt;set hive.exec.parallel=true            //是自动设置好的    //严格模式,    $hive&gt;set hive.mapred.mode=strict            //手动设置之后。1.分区表必须指定分区进行查询，否则不让查询。                                                //2.order by时必须使用limit子句。                                                //3.不允许笛卡尔积.    //设置MR的数量    hive&gt; set hive.exec.reducers.bytes.per.reducer=750000000;    //设置reduce处理的字节数。    //JVM重用    $hive&gt;set mapreduce.job.jvm.numtasks=1        //-1没有限制，使用大量小文件。    //UDF    //User define function,用户自定义函数    //current_database(),current_user();    //显式所有函数    $hive&gt;show functions;    $hive&gt;select array(1,2,3) ;    //显式指定函数帮助    $hive&gt;desc function current_database();    //表生成函数,多行函数。    $hive&gt;explode(str,exp);            //按照exp切割str.</code></pre><h2 id="自定义函数"><a href="#自定义函数" class="headerlink" title="自定义函数"></a>自定义函数</h2><p><img src="https://i.imgur.com/lvT1nkj.png" alt=""></p><p>上图带括号的都是函数，不带括号的是命令</p><pre><code>1.创建类，继承UDF    package com.it18zhang.hivedemo.udf;    import org.apache.hadoop.hive.ql.exec.Description;    import org.apache.hadoop.hive.ql.exec.UDF;    /**     * 自定义hive函数     */    @Description(name = &quot;myadd&quot;,            value = &quot;myadd(int a , int b) ==&gt; return a + b &quot;,            extended = &quot;Example:\n&quot;                    + &quot; myadd(1,1) ==&gt; 2 \n&quot;                    + &quot; myadd(1,2,3) ==&gt; 6;&quot;)    public class AddUDF extends UDF {        public int evaluate(int a ,int b) {            return a + b ;        }        public int evaluate(int a ,int b , int c) {            return a + b + c;        }    }2.打成jar包。    cmd&gt;cd {classes所在目录}    cmd&gt;jar cvf HiveDemo.jar -C x/x/x/x/classes/ .3.添加jar包到hive的类路径    //添加jar到类路径    $&gt;cp /mnt/hgfs/downloads/bigdata/data/HiveDemo.jar /soft/hive/lib3.重进入hive    $&gt;....4.创建临时函数    //    CREATE TEMPORARY FUNCTION myadd AS &apos;com.it18zhang.hivedemo.udf.AddUDF&apos;;5.在查询中使用自定义函数    $hive&gt;select myadd(1,2)  ;6.定义日期函数    1)定义类    public class ToCharUDF extends UDF {        /**         * 取出服务器的当前系统时间 2017/3/21 16:53:55         */        public String evaluate() {            Date date = new Date();            SimpleDateFormat sdf = new SimpleDateFormat();            sdf.applyPattern(&quot;yyyy/MM/dd hh:mm:ss&quot;);            return sdf.format(date) ;        }        public String evaluate(Date date) {            SimpleDateFormat sdf = new SimpleDateFormat();            sdf.applyPattern(&quot;yyyy/MM/dd hh:mm:ss&quot;);            return sdf.format(date) ;        }        public String evaluate(Date date,String frt) {            SimpleDateFormat sdf = new SimpleDateFormat();            sdf.applyPattern(frt);            return sdf.format(date) ;        }    }    2)导出jar包，通过命令添加到hive的类路径(不需要重进hive)。        $hive&gt;add jar /mnt/hgfs/downloads/bigdata/data/HiveDemo-1.0-SNAPSHOT.jar    3)注册函数        $hive&gt;CREATE TEMPORARY FUNCTION to_char AS &apos;com.it18zhang.hivedemo.udf.ToCharUDF&apos;;        $hive&gt;CREATE TEMPORARY FUNCTION to_date AS &apos;com.it18zhang.hivedemo.udf.ToCharUDF&apos;;</code></pre><h2 id="定义Nvl函数-这个是在英文版的Hadoop权威指南上给的"><a href="#定义Nvl函数-这个是在英文版的Hadoop权威指南上给的" class="headerlink" title="定义Nvl函数(这个是在英文版的Hadoop权威指南上给的)"></a>定义Nvl函数(这个是在英文版的Hadoop权威指南上给的)</h2><pre><code>package com.it18zhang.hivedemo.udf;import org.apache.hadoop.hive.ql.exec.UDFArgumentException;import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;import org.apache.hadoop.hive.ql.metadata.HiveException;import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;import org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils;import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;/** * 自定义null值处理函数 */public class Nvl extends GenericUDF {    private GenericUDFUtils.ReturnObjectInspectorResolver returnOIResolver;    private ObjectInspector[] argumentOIs;    public ObjectInspector initialize(ObjectInspector[] arguments)            throws UDFArgumentException {        argumentOIs = arguments;        //检查参数个数        if (arguments.length != 2) {            throw new UDFArgumentLengthException(                    &quot;The operator &apos;NVL&apos; accepts 2 arguments.&quot;);        }        returnOIResolver = new GenericUDFUtils.ReturnObjectInspectorResolver(true);        //检查参数类型        if (!(returnOIResolver.update(arguments[0]) &amp;&amp; returnOIResolver                .update(arguments[1]))) {            throw new UDFArgumentTypeException(2,                    &quot;The 1st and 2nd args of function NLV should have the same type, &quot;                            + &quot;but they are different: \&quot;&quot; + arguments[0].getTypeName()                            + &quot;\&quot; and \&quot;&quot; + arguments[1].getTypeName() + &quot;\&quot;&quot;);        }        return returnOIResolver.get();    }    public Object evaluate(DeferredObject[] arguments) throws HiveException {        Object retVal = returnOIResolver.convertIfNecessary(arguments[0].get(), argumentOIs[0]);        if (retVal == null) {            retVal = returnOIResolver.convertIfNecessary(arguments[1].get(),                    argumentOIs[1]);        }        return retVal;    }    public String getDisplayString(String[] children) {        StringBuilder sb = new StringBuilder();        sb.append(&quot;if &quot;);        sb.append(children[0]);        sb.append(&quot; is null &quot;);        sb.append(&quot;returns&quot;);        sb.append(children[1]);        return sb.toString();    }}2)添加jar到类路径    ...3)注册函数    $hive&gt;CREATE TEMPORARY FUNCTION nvl AS &apos;org.apache.hadoop.hive.ql.udf.generic.GenericUDFNvl&apos;;</code></pre><h1 id="一些课上用的PPT"><a href="#一些课上用的PPT" class="headerlink" title="一些课上用的PPT"></a>一些课上用的PPT</h1><p><img src="https://i.imgur.com/6OyT9i3.png" alt=""><br><img src="https://i.imgur.com/CLZNHSV.png" alt=""><br><img src="https://i.imgur.com/Rx5Hslt.png" alt=""><br><img src="https://i.imgur.com/fb0PfG0.png" alt=""><br><img src="https://i.imgur.com/P8kwdax.png" alt=""><br><img src="https://i.imgur.com/avoDPKV.png" alt=""><br><img src="https://i.imgur.com/OwHiTIn.png" alt=""><br><img src="https://i.imgur.com/x4qXG7v.png" alt=""><br><img src="https://i.imgur.com/mIQbSR1.png" alt=""><br><img src="https://i.imgur.com/2bMpEMd.png" alt=""><br><img src="https://i.imgur.com/bSF4lMN.png" alt=""><br><img src="https://i.imgur.com/C3IMpOQ.png" alt=""><br><img src="https://i.imgur.com/6xpn8Ul.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;hive&quot;&gt;&lt;a href=&quot;#hive&quot; class=&quot;headerlink&quot; title=&quot;hive&quot;&gt;&lt;/a&gt;hive&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;数据仓库,在线分析处理。
HiveQL,类似sql语言。
表,metadata-&amp;gt;rdbms.
hiv
      
    
    </summary>
    
      <category term="Hive" scheme="http://erichunn.github.io/categories/Hive/"/>
    
    
      <category term="Hive" scheme="http://erichunn.github.io/tags/Hive/"/>
    
      <category term="内部表" scheme="http://erichunn.github.io/tags/%E5%86%85%E9%83%A8%E8%A1%A8/"/>
    
      <category term="外部表" scheme="http://erichunn.github.io/tags/%E5%A4%96%E9%83%A8%E8%A1%A8/"/>
    
      <category term="托管表" scheme="http://erichunn.github.io/tags/%E6%89%98%E7%AE%A1%E8%A1%A8/"/>
    
      <category term="分区" scheme="http://erichunn.github.io/tags/%E5%88%86%E5%8C%BA/"/>
    
      <category term="分桶" scheme="http://erichunn.github.io/tags/%E5%88%86%E6%A1%B6/"/>
    
      <category term="join" scheme="http://erichunn.github.io/tags/join/"/>
    
      <category term="union" scheme="http://erichunn.github.io/tags/union/"/>
    
      <category term="export" scheme="http://erichunn.github.io/tags/export/"/>
    
      <category term="order" scheme="http://erichunn.github.io/tags/order/"/>
    
      <category term="sort" scheme="http://erichunn.github.io/tags/sort/"/>
    
      <category term="动态分区" scheme="http://erichunn.github.io/tags/%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA/"/>
    
  </entry>
  
  <entry>
    <title>Hive第一天</title>
    <link href="http://erichunn.github.io/2018/11/06/Hive%E7%AC%AC%E4%B8%80%E5%A4%A9/"/>
    <id>http://erichunn.github.io/2018/11/06/Hive第一天/</id>
    <published>2018-11-06T08:35:55.000Z</published>
    <updated>2018-11-07T13:13:48.789Z</updated>
    
    <content type="html"><![CDATA[<h2 id="hive"><a href="#hive" class="headerlink" title="hive"></a>hive</h2><pre><code>在hadoop处理结构化数据的数据仓库。不是:    关系数据库        不是OLTP        实时查询和行级更新。</code></pre><h2 id="hive特点"><a href="#hive特点" class="headerlink" title="hive特点"></a>hive特点</h2><pre><code>hive存储数据结构(schema)在数据库中,处理的数据进入hdfs.OLAPHQL / HiveQL</code></pre><h2 id="hive安装"><a href="#hive安装" class="headerlink" title="hive安装"></a>hive安装</h2><pre><code>1.下载hive2.1-tar.gz2.tar开    $&gt;tar -xzvf hive-2.1.0.tar.gz -C /soft    //tar开    $&gt;cd /soft/hive-2.1.0                    //    $&gt;ln -s hive-2.1.0 hive                    //符号连接3.配置环境变量    [/etc/profile]    HIVE_HOME=/soft/hive    PATH=...:$HIVE_HOME/bin4.验证hive安装成功    $&gt;hive --v5.配置hive,使用win7的mysql存放hive的元数据.    a)复制mysql驱动程序到hive的lib目录下。        ...    b)配置hive-site.xml        复制hive-default.xml.template为hive-site.xml        修改连接信息为mysql链接地址，将${system:...字样替换成具体路径。        [hive/conf/hive-site.xml]        &lt;property&gt;            &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;            &lt;value&gt;root&lt;/value&gt;            &lt;description&gt;password to use against metastore database&lt;/description&gt;        &lt;/property&gt;        &lt;property&gt;            &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;            &lt;value&gt;root&lt;/value&gt;            &lt;description&gt;Username to use against metastore database&lt;/description&gt;        &lt;/property&gt;        &lt;property&gt;            &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;            &lt;value&gt;jdbc:mysql://192.168.231.1:3306/hive2&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;            &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;            &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;            &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;        &lt;/property&gt;    6)在msyql中创建存放hive信息的数据库        mysql&gt;create database hive2 ;    6)初始化hive的元数据(表结构)到mysql中。        $&gt;cd /soft/hive/bin        $&gt;schematool -dbType mysql -initSchema</code></pre><h1 id="hive命令行操作"><a href="#hive命令行操作" class="headerlink" title="hive命令行操作"></a>hive命令行操作</h1><h2 id="1-创建hive的数据库"><a href="#1-创建hive的数据库" class="headerlink" title="1.创建hive的数据库"></a>1.创建hive的数据库</h2><pre><code>$hive&gt;hive --version                //$hive&gt;hive --help                    //$hive&gt;create database mydb2 ;                //$hive&gt;show databases ;$hive&gt;use mydb2 ;$hive&gt;create table mydb2.t(id int,name string,age int);$hive&gt;drop table t ;$hive&gt;drop table mydb2.t ;$hive&gt;select * from mydb2.t ;        //查看指定库的表$hive&gt;exit ;                        //退出$&gt;hive                                //hive --service cli$&gt;hive                                //hive --service cli</code></pre><h2 id="2-通过远程jdbc方式连接到hive数据仓库"><a href="#2-通过远程jdbc方式连接到hive数据仓库" class="headerlink" title="2.通过远程jdbc方式连接到hive数据仓库"></a>2.通过远程jdbc方式连接到hive数据仓库</h2><pre><code>1.启动hiveserver2服务器，监听端口10000    $&gt;hive --service hiveserver2 &amp;2.通过beeline命令行连接到hiveserver2    $&gt;beeline                                            //进入beeline命令行(于hive --service beeline)    $beeline&gt;!help                                        //查看帮助    $beeline&gt;!quit                                        //退出    $beeline&gt;!connect jdbc:hive2://localhost:10000/mydb2//连接到hibve数据    $beeline&gt;show databases ;    $beeline&gt;use mydb2 ;    $beeline&gt;show tables;                                //显式表</code></pre><h2 id="使用Hive-jdbc驱动程序采用jdbc方式访问远程数据仓库"><a href="#使用Hive-jdbc驱动程序采用jdbc方式访问远程数据仓库" class="headerlink" title="使用Hive-jdbc驱动程序采用jdbc方式访问远程数据仓库"></a>使用Hive-jdbc驱动程序采用jdbc方式访问远程数据仓库</h2><pre><code>1.创建java模块2.引入maven3.添加hive-jdbc依赖    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;    &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;             xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;             xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;        &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;        &lt;groupId&gt;com.it18zhang&lt;/groupId&gt;        &lt;artifactId&gt;HiveDemo&lt;/artifactId&gt;        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;        &lt;dependencies&gt;            &lt;dependency&gt;                &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;                &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt;                &lt;version&gt;2.1.0&lt;/version&gt;            &lt;/dependency&gt;        &lt;/dependencies&gt;    &lt;/project&gt;4.App    package com.it18zhang.hivedemo;    import java.sql.Connection;    import java.sql.DriverManager;    import java.sql.ResultSet;    import java.sql.Statement;    /**     * 使用jdbc方式连接到hive数据仓库，数据仓库需要开启hiveserver2服务。     */    public class App {        public static void main(String[] args) throws  Exception {            Class.forName(&quot;org.apache.hive.jdbc.HiveDriver&quot;);            Connection conn = DriverManager.getConnection(&quot;jdbc:hive2://192.168.231.201:10000/mydb2&quot;);            Statement st = conn.createStatement();            ResultSet rs = st.executeQuery(&quot;select id , name ,age from t&quot;);            while(rs.next()){                System.out.println(rs.getInt(1) + &quot;,&quot; + rs.getString(2)) ;            }            rs.close();            st.close();            conn.close();        }    }</code></pre><h1 id="hive中表"><a href="#hive中表" class="headerlink" title="hive中表"></a>hive中表</h1><h2 id="1-managed-table"><a href="#1-managed-table" class="headerlink" title="1.managed table"></a>1.managed table</h2><pre><code>托管表。删除表时，数据也删除了。</code></pre><h2 id="2-external-table"><a href="#2-external-table" class="headerlink" title="2.external table"></a>2.external table</h2><pre><code>外部表。删除表时，数据不删。</code></pre><h1 id="hive命令"><a href="#hive命令" class="headerlink" title="hive命令"></a>hive命令</h1><pre><code>//创建表,external 外部表$hive&gt;CREATE external TABLE IF NOT EXISTS t2(id int,name string,age int)COMMENT &apos;xx&apos; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; STORED AS TEXTFILE ; //查看表数据$hive&gt;desc t2 ;$hive&gt;desc formatted t2 ;//加载数据到hive表$hive&gt;load data local inpath &apos;/home/centos/customers.txt&apos; into table t2 ;    //local上传文件$hive&gt;load data inpath &apos;/user/centos/customers.txt&apos; [overwrite] into table t2 ;    //移动文件//复制表mysql&gt;create table tt as select * from users ;        //携带数据和表结构mysql&gt;create table tt like users ;            //不带数据，只有表结构hive&gt;create table tt as select * from users ;    hive&gt;create table tt like users ;    //count()查询要转成mr$hive&gt;select count(*) from t2 ;$hive&gt;select id,name from t2 ;//$hive&gt;select * from t2 order by id desc ;                //MR//启用/禁用表$hive&gt;ALTER TABLE t2 ENABLE NO_DROP;    //不允许删除$hive&gt;ALTER TABLE t2 DISABLE NO_DROP;    //允许删除</code></pre><h1 id="分区表"><a href="#分区表" class="headerlink" title="分区表"></a>分区表</h1><pre><code>优化手段之一，从目录的层面控制搜索数据的范围。//创建分区表.$hive&gt;CREATE TABLE t3(id int,name string,age int) PARTITIONED BY (Year INT, Month INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;//显式表的分区信息$hive&gt;SHOW PARTITIONS t3;//添加分区,创建目录$hive&gt; alter table t3 add partition (year=2014, month=12) partition (year=2014,month=11);//删除分区hive&gt;ALTER TABLE employee_partitioned DROP IF EXISTS PARTITION (year=2014, month=11);//分区结构hive&gt;/user/hive/warehouse/mydb2.db/t3/year=2014/month=11hive&gt;/user/hive/warehouse/mydb2.db/t3/year=2014/month=12//加载数据到分区表hive&gt;load data local inpath &apos;/home/centos/customers.txt&apos; into table t3 partition(year=2014,month=11);//查询分区表hive&gt;select * from t3 where year = 2014 and month =11;</code></pre><p><img src="https://i.imgur.com/Ad6JAI7.png" alt=""></p><p>分区对应的是文件夹是目录，分桶对应的是文件，按照id分桶的意思就是按照id哈希不同的哈希进入不同的桶</p><pre><code>//创建桶表$hive&gt;CREATE TABLE t4(id int,name string,age int) CLUSTERED BY (id) INTO 3 BUCKETS ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;//加载数据不会进行分桶操作$hive&gt;load data local inpath &apos;/home/centos/customers.txt&apos; into table t4 ;//查询t3表数据插入到t4中。t3是分区表还多了年份和月份的2个字段所以要投影查询不能直接insert into t4 select * from t3;这是错的$hive&gt;insert into t4 select id,name,age from t3 ;</code></pre><p><img src="https://i.imgur.com/5Rpz8su.png" alt=""></p><pre><code>//桶表的数量如何设置?//评估数据量，保证每个桶的数据量block的2倍大小。//连接查询$hive&gt;CREATE TABLE customers(id int,name string,age int) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;$hive&gt;CREATE TABLE orders(id int,orderno string,price float,cid int) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;//加载数据到表//内连接查询hive&gt;select a.*,b.* from customers a , orders b where a.id = b.cid ;//左外hive&gt;select a.*,b.* from customers a left outer join orders b on a.id = b.cid ;hive&gt;select a.*,b.* from customers a right outer join orders b on a.id = b.cid ;hive&gt;select a.*,b.* from customers a full outer join orders b on a.id = b.cid ;//explode,炸裂,表生成函数。//使用hive实现单词统计//1.建表$hive&gt;CREATE TABLE doc(line string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;hive&quot;&gt;&lt;a href=&quot;#hive&quot; class=&quot;headerlink&quot; title=&quot;hive&quot;&gt;&lt;/a&gt;hive&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;在hadoop处理结构化数据的数据仓库。
不是:    关系数据库
        不是OLTP
     
      
    
    </summary>
    
      <category term="Hive" scheme="http://erichunn.github.io/categories/Hive/"/>
    
    
      <category term="Hive" scheme="http://erichunn.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop第十一天</title>
    <link href="http://erichunn.github.io/2018/11/01/Hadoop%E7%AC%AC%E5%8D%81%E4%B8%80%E5%A4%A9/"/>
    <id>http://erichunn.github.io/2018/11/01/Hadoop第十一天/</id>
    <published>2018-11-01T10:11:38.000Z</published>
    <updated>2018-11-01T10:17:09.635Z</updated>
    
    <content type="html"><![CDATA[<h2 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h2><pre><code>[customers.txt]1,tom,122,tom,133,tom,144,tom,15[orders.txt]1,no001,12.23,12,no001,12.23,13,no001,12.23,24,no001,12.23,25,no001,12.23,26,no001,12.23,37,no001,12.23,38,no001,12.23,39,no001,12.23,3</code></pre><h2 id="map端join"><a href="#map端join" class="headerlink" title="map端join"></a>map端join</h2><pre><code>1.创建Mapper    package com.it18zhang.hdfs.mr.mapjoin;    import org.apache.hadoop.conf.Configuration;    import org.apache.hadoop.fs.FSDataInputStream;    import org.apache.hadoop.fs.FileSystem;    import org.apache.hadoop.fs.Path;    import org.apache.hadoop.io.LongWritable;    import org.apache.hadoop.io.NullWritable;    import org.apache.hadoop.io.Text;    import org.apache.hadoop.mapreduce.Mapper;    import java.io.BufferedReader;    import java.io.IOException;    import java.io.InputStreamReader;    import java.util.HashMap;    import java.util.Map;    /**     * join操作，map端连接。     */    public class MapJoinMapper extends Mapper&lt;LongWritable,Text,Text,NullWritable&gt; {        private Map&lt;String,String&gt; allCustomers = new HashMap&lt;String,String&gt;();        //启动,初始化客户信息        protected void setup(Context context) throws IOException, InterruptedException {            try {                Configuration conf = context.getConfiguration();                FileSystem fs = FileSystem.get(conf);                FSDataInputStream fis = fs.open(new Path(&quot;file:///d:/mr/mapjoin/customers.txt&quot;));                //得到缓冲区阅读器                BufferedReader br = new BufferedReader(new InputStreamReader(fis));                String line = null ;                while((line = br.readLine()) != null){                    //得到cid                    String cid = line.substring(0,line.indexOf(&quot;,&quot;));                    allCustomers.put(cid,line);                }            } catch (Exception e) {                e.printStackTrace();            }        }        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {            //订单信息            String line = value.toString();            //提取customer id            String cid = line.substring(line.lastIndexOf(&quot;,&quot;) + 1);            //订单信息            String orderInfo = line.substring(0,line.lastIndexOf(&quot;,&quot;));            //连接customer + &quot;,&quot; + order            String customerInfo = allCustomers.get(cid);            context.write(new Text(customerInfo + &quot;,&quot; + orderInfo),NullWritable.get());        }    }2.创建App    package com.it18zhang.hdfs.mr.mapjoin;    import org.apache.hadoop.conf.Configuration;    import org.apache.hadoop.fs.Path;    import org.apache.hadoop.io.NullWritable;    import org.apache.hadoop.io.Text;    import org.apache.hadoop.mapreduce.Job;    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;    /**     *     */    public class MapJoinApp {        public static void main(String[] args) throws Exception {            Configuration conf = new Configuration();            conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;);            Job job = Job.getInstance(conf);            //设置job的各种属性            job.setJobName(&quot;MapJoinApp&quot;);                        //作业名称            job.setJarByClass(MapJoinApp.class);                 //搜索类            //添加输入路径            FileInputFormat.addInputPath(job,new Path(args[0]));            //设置输出路径            FileOutputFormat.setOutputPath(job,new Path(args[1]));            //没有reduce            job.setNumReduceTasks(0);            job.setMapperClass(MapJoinMapper.class);             //mapper类            job.setMapOutputKeyClass(Text.class);           //            job.setMapOutputValueClass(NullWritable.class);  //            job.waitForCompletion(true);        }    }</code></pre><h2 id="join端连接"><a href="#join端连接" class="headerlink" title="join端连接"></a>join端连接</h2><pre><code>1.自定义key    package com.it18zhang.hdfs.mr.mapjoin.reducejoin;    import org.apache.hadoop.io.WritableComparable;    import java.io.DataInput;    import java.io.DataOutput;    import java.io.IOException;    /**     */    public class ComboKey2 implements WritableComparable&lt;ComboKey2&gt; {        //0-customer 1-order        private int type ;        private int cid ;        private int oid ;        private String customerInfo = &quot;&quot; ;        private String orderInfo = &quot;&quot; ;        public int compareTo(ComboKey2 o) {            int type0 = o.type ;            int cid0= o.cid;            int oid0 = o.oid;            String customerInfo0 = o.customerInfo;            String orderInfo0 = o.orderInfo ;            //是否同一个customer的数据            if(cid == cid0){                //同一个客户的两个订单                if(type == type0){                    return oid - oid0 ;                }                //一个Customer + 他的order                else{                    if(type ==0)                        return -1 ;                    else                        return 1 ;                }            }            //cid不同            else{                return cid - cid0 ;            }        }        public void write(DataOutput out) throws IOException {            out.writeInt(type);            out.writeInt(cid);            out.writeInt(oid);            out.writeUTF(customerInfo);            out.writeUTF(orderInfo);        }        public void readFields(DataInput in) throws IOException {            this.type = in.readInt();            this.cid = in.readInt();            this.oid = in.readInt();            this.customerInfo = in.readUTF();            this.orderInfo = in.readUTF();        }    }2.自定义分区类    public class CIDPartitioner extends Partitioner&lt;ComboKey2,NullWritable&gt;{        public int getPartition(ComboKey2 key, NullWritable nullWritable, int numPartitions) {            return key.getCid() % numPartitions;        }    }3.创建Mapper    package com.it18zhang.hdfs.mr.mapjoin.reducejoin;    import org.apache.hadoop.io.LongWritable;    import org.apache.hadoop.io.NullWritable;    import org.apache.hadoop.io.Text;    import org.apache.hadoop.mapreduce.InputSplit;    import org.apache.hadoop.mapreduce.Mapper;    import org.apache.hadoop.mapreduce.lib.input.FileSplit;    import java.io.IOException;    /**     * mapper     */    public class ReduceJoinMapper extends Mapper&lt;LongWritable,Text,ComboKey2,NullWritable&gt; {        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {            //            String line = value.toString() ;            //判断是customer还是order            FileSplit split = (FileSplit)context.getInputSplit();            String path = split.getPath().toString();            //客户信息            ComboKey2 key2 = new ComboKey2();            if(path.contains(&quot;customers&quot;)){                String cid = line.substring(0,line.indexOf(&quot;,&quot;));                String custInfo = line ;                key2.setType(0);                key2.setCid(Integer.parseInt(cid));                key2.setCustomerInfo(custInfo);            }            //order info            else{                String cid = line.substring(line.lastIndexOf(&quot;,&quot;) + 1);                String oid = line.substring(0, line.indexOf(&quot;,&quot;));                String oinfo = line.substring(0, line.lastIndexOf(&quot;,&quot;));                key2.setType(1);                key2.setCid(Integer.parseInt(cid));                key2.setOid(Integer.parseInt(oid));                key2.setOrderInfo(oinfo);            }            context.write(key2,NullWritable.get());        }    }4.创建Reducer    package com.it18zhang.hdfs.mr.mapjoin.reducejoin;    import org.apache.hadoop.io.NullWritable;    import org.apache.hadoop.io.Text;    import org.apache.hadoop.mapreduce.Reducer;    import java.io.IOException;    import java.util.Iterator;    /**     * ReduceJoinReducer,reducer端连接实现。     */    public class ReduceJoinReducer extends Reducer&lt;ComboKey2,NullWritable,Text,NullWritable&gt; {        protected void reduce(ComboKey2 key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException {            Iterator&lt;NullWritable&gt; it = values.iterator();            it.next();            int type = key.getType();            int cid = key.getCid() ;            String cinfo = key.getCustomerInfo() ;            while(it.hasNext()){                it.next();                String oinfo = key.getOrderInfo();                context.write(new Text(cinfo + &quot;,&quot; + oinfo),NullWritable.get());            }        }    }5.创建排序对比器    package com.it18zhang.hdfs.mr.mapjoin.reducejoin;    import com.it18zhang.hdfs.maxtemp.allsort.secondarysort.ComboKey;    import org.apache.hadoop.io.WritableComparable;    import org.apache.hadoop.io.WritableComparator;    /**     * 组合Key排序对比器     */    public class ComboKey2Comparator extends WritableComparator {        protected ComboKey2Comparator() {            super(ComboKey2.class, true);        }        public int compare(WritableComparable a, WritableComparable b) {            ComboKey2 k1 = (ComboKey2) a;            ComboKey2 k2 = (ComboKey2) b;            return k1.compareTo(k2);        }    }6.分组对比器    package com.it18zhang.hdfs.mr.mapjoin.reducejoin;    import com.it18zhang.hdfs.maxtemp.allsort.secondarysort.ComboKey;    import org.apache.hadoop.io.WritableComparable;    import org.apache.hadoop.io.WritableComparator;    /**     * CID分组对比器     */    public class CIDGroupComparator extends WritableComparator{        protected CIDGroupComparator() {            super(ComboKey2.class, true);        }        public int compare(WritableComparable a, WritableComparable b) {            ComboKey2 k1 = (ComboKey2) a;            ComboKey2 k2 = (ComboKey2) b;            return k1.getCid() - k2.getCid();        }    }7.App    package com.it18zhang.hdfs.mr.mapjoin.reducejoin;    import com.it18zhang.hdfs.maxtemp.allsort.secondarysort.*;    import org.apache.hadoop.conf.Configuration;    import org.apache.hadoop.fs.Path;    import org.apache.hadoop.io.IntWritable;    import org.apache.hadoop.io.NullWritable;    import org.apache.hadoop.io.Text;    import org.apache.hadoop.mapreduce.Job;    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;    import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;    /**     *     */    public class ReduceJoinApp {        public static void main(String[] args) throws Exception {            Configuration conf = new Configuration();            conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;);            Job job = Job.getInstance(conf);            //设置job的各种属性            job.setJobName(&quot;ReduceJoinApp&quot;);                        //作业名称            job.setJarByClass(ReduceJoinApp.class);                 //搜索类            //添加输入路径            FileInputFormat.addInputPath(job,new Path(&quot;D:\\mr\\reducejoin&quot;));            //设置输出路径            FileOutputFormat.setOutputPath(job,new Path(&quot;D:\\mr\\reducejoin\\out&quot;));            job.setMapperClass(ReduceJoinMapper.class);             //mapper类            job.setReducerClass(ReduceJoinReducer.class);           //reducer类            //设置Map输出类型            job.setMapOutputKeyClass(ComboKey2.class);            //            job.setMapOutputValueClass(NullWritable.class);      //            //设置ReduceOutput类型            job.setOutputKeyClass(Text.class);            job.setOutputValueClass(NullWritable.class);         //            //设置分区类            job.setPartitionerClass(CIDPartitioner.class);            //设置分组对比器            job.setGroupingComparatorClass(CIDGroupComparator.class);            //设置排序对比器            job.setSortComparatorClass(ComboKey2Comparator.class);            job.setNumReduceTasks(2);                           //reduce个数            job.waitForCompletion(true);        }    }</code></pre><h2 id="hive"><a href="#hive" class="headerlink" title="hive"></a>hive</h2><pre><code>在hadoop处理结构化数据的数据仓库。不是:    关系数据库        不是OLTP        实时查询和行级更新。</code></pre><h2 id="hive特点"><a href="#hive特点" class="headerlink" title="hive特点"></a>hive特点</h2><pre><code>hive存储数据结构(schema)在数据库中,处理的数据进入hdfs.OLAPHQL / HiveQL</code></pre><h2 id="hive安装"><a href="#hive安装" class="headerlink" title="hive安装"></a>hive安装</h2><pre><code>1.下载hive2.1-tar.gz2.tar开    $&gt;tar -xzvf hive-2.1.0.tar.gz -C /soft    //tar开    $&gt;cd /soft/hive-2.1.0                    //    $&gt;ln -s hive-2.1.0 hive                    //符号连接3.配置环境变量    [/etc/profile]    HIVE_HOME=/soft/hive    PATH=...:$HIVE_HOME/bin4.验证hive安装成功    $&gt;hive --v5.配置hive,使用win7的mysql存放hive的元数据.    a)复制mysql驱动程序到hive的lib目录下。        ...    b)配置hive-site.xml        复制hive-default.xml.template为hive-site.xml        修改连接信息为mysql链接地址，将${system:...字样替换成具体路径。        [hive/conf/hive-site.xml]        &lt;property&gt;            &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;            &lt;value&gt;root&lt;/value&gt;            &lt;description&gt;password to use against metastore database&lt;/description&gt;        &lt;/property&gt;        &lt;property&gt;            &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;            &lt;value&gt;root&lt;/value&gt;            &lt;description&gt;Username to use against metastore database&lt;/description&gt;        &lt;/property&gt;        &lt;property&gt;            &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;            &lt;value&gt;jdbc:mysql://192.168.231.1:3306/hive2&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;            &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;            &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;            &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;        &lt;/property&gt;    6)在msyql中创建存放hive信息的数据库        mysql&gt;create database hive2 ;    6)初始化hive的元数据(表结构)到mysql中。        $&gt;cd /soft/hive/bin        $&gt;schematool -dbType mysql -initSchema</code></pre><h2 id="hive命令行操作"><a href="#hive命令行操作" class="headerlink" title="hive命令行操作"></a>hive命令行操作</h2><pre><code>1.创建hive的数据库    $hive&gt;hive --version                //    $hive&gt;hive --help                    //    $hive&gt;create database mydb2 ;                //    $hive&gt;show databases ;    $hive&gt;use mydb2 ;    $hive&gt;create table mydb2.t(id int,name string,age int);    $hive&gt;drop table t ;    $hive&gt;drop table mydb2.t ;    $hive&gt;select * from mydb2.t ;        //查看指定库的表    $hive&gt;exit ;                        //退出    $&gt;hive                                //hive --service cli    $&gt;hive                                //hive --service cli</code></pre><h2 id="通过远程jdbc方式连接到hive数据仓库"><a href="#通过远程jdbc方式连接到hive数据仓库" class="headerlink" title="通过远程jdbc方式连接到hive数据仓库"></a>通过远程jdbc方式连接到hive数据仓库</h2><pre><code>1.启动hiveserver2服务器，监听端口10000    $&gt;hive --service hiveserver2 &amp;2.通过beeline命令行连接到hiveserver2    $&gt;beeline                                            //进入beeline命令行(于hive --service beeline)    $beeline&gt;!help                                        //查看帮助    $beeline&gt;!quit                                        //退出    $beeline&gt;!connect jdbc:hive2://localhost:10000/mydb2//连接到hibve数据    $beeline&gt;show databases ;    $beeline&gt;use mydb2 ;    $beeline&gt;show tables;                                //显式表</code></pre><h2 id="使用Hive-jdbc驱动程序采用jdbc方式访问远程数据仓库"><a href="#使用Hive-jdbc驱动程序采用jdbc方式访问远程数据仓库" class="headerlink" title="使用Hive-jdbc驱动程序采用jdbc方式访问远程数据仓库"></a>使用Hive-jdbc驱动程序采用jdbc方式访问远程数据仓库</h2><pre><code>1.创建java模块2.引入maven3.添加hive-jdbc依赖    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;    &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;             xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;             xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;        &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;        &lt;groupId&gt;com.it18zhang&lt;/groupId&gt;        &lt;artifactId&gt;HiveDemo&lt;/artifactId&gt;        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;        &lt;dependencies&gt;            &lt;dependency&gt;                &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;                &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt;                &lt;version&gt;2.1.0&lt;/version&gt;            &lt;/dependency&gt;        &lt;/dependencies&gt;    &lt;/project&gt;4.App    package com.it18zhang.hivedemo;    import java.sql.Connection;    import java.sql.DriverManager;    import java.sql.ResultSet;    import java.sql.Statement;    /**     * 使用jdbc方式连接到hive数据仓库，数据仓库需要开启hiveserver2服务。     */    public class App {        public static void main(String[] args) throws  Exception {            Class.forName(&quot;org.apache.hive.jdbc.HiveDriver&quot;);            Connection conn = DriverManager.getConnection(&quot;jdbc:hive2://192.168.231.201:10000/mydb2&quot;);            Statement st = conn.createStatement();            ResultSet rs = st.executeQuery(&quot;select id , name ,age from t&quot;);            while(rs.next()){                System.out.println(rs.getInt(1) + &quot;,&quot; + rs.getString(2)) ;            }            rs.close();            st.close();            conn.close();        }    }</code></pre><h2 id="hive中表"><a href="#hive中表" class="headerlink" title="hive中表"></a>hive中表</h2><pre><code>1.managed table    托管表。    删除表时，数据也删除了。2.external table    外部表。    删除表时，数据不删。</code></pre><h2 id="hive命令"><a href="#hive命令" class="headerlink" title="hive命令"></a>hive命令</h2><pre><code>//创建表,external 外部表$hive&gt;CREATE external TABLE IF NOT EXISTS t2(id int,name string,age int)COMMENT &apos;xx&apos; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; STORED AS TEXTFILE ; //查看表数据$hive&gt;desc t2 ;$hive&gt;desc formatted t2 ;//加载数据到hive表$hive&gt;load data local inpath &apos;/home/centos/customers.txt&apos; into table t2 ;    //local上传文件$hive&gt;load data inpath &apos;/user/centos/customers.txt&apos; [overwrite] into table t2 ;    //移动文件//复制表mysql&gt;create table tt as select * from users ;        //携带数据和表结构mysql&gt;create table tt like users ;            //不带数据，只有表结构hive&gt;create table tt as select * from users ;    hive&gt;create table tt like users ;    //count()查询要转成mr$hive&gt;select count(*) from t2 ;$hive&gt;select id,name from t2 ;//$hive&gt;select * from t2 order by id desc ;                //MR//启用/禁用表$hive&gt;ALTER TABLE t2 ENABLE NO_DROP;    //不允许删除$hive&gt;ALTER TABLE t2 DISABLE NO_DROP;    //允许删除//分区表,优化手段之一，从目录的层面控制搜索数据的范围。//创建分区表.$hive&gt;CREATE TABLE t3(id int,name string,age int) PARTITIONED BY (Year INT, Month INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;//显式表的分区信息$hive&gt;SHOW PARTITIONS t3;//添加分区,创建目录$hive&gt;alter table t3 add partition (year=2014, month=12);//删除分区hive&gt;ALTER TABLE employee_partitioned DROP IF EXISTS PARTITION (year=2014, month=11);//分区结构hive&gt;/user/hive/warehouse/mydb2.db/t3/year=2014/month=11hive&gt;/user/hive/warehouse/mydb2.db/t3/year=2014/month=12//加载数据到分区表hive&gt;load data local inpath &apos;/home/centos/customers.txt&apos; into table t3 partition(year=2014,month=11);//创建桶表$hive&gt;CREATE TABLE t4(id int,name string,age int) CLUSTERED BY (id) INTO 3 BUCKETS ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;//加载数据不会进行分桶操作$hive&gt;load data local inpath &apos;/home/centos/customers.txt&apos; into table t4 ;//查询t3表数据插入到t4中。$hive&gt;insert into t4 select id,name,age from t3 ;//桶表的数量如何设置?//评估数据量，保证每个桶的数据量block的2倍大小。//连接查询$hive&gt;CREATE TABLE customers(id int,name string,age int) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;$hive&gt;CREATE TABLE orders(id int,orderno string,price float,cid int) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;//加载数据到表//内连接查询hive&gt;select a.*,b.* from customers a , orders b where a.id = b.cid ;//左外hive&gt;select a.*,b.* from customers a left outer join orders b on a.id = b.cid ;hive&gt;select a.*,b.* from customers a right outer join orders b on a.id = b.cid ;hive&gt;select a.*,b.* from customers a full outer join orders b on a.id = b.cid ;//explode,炸裂,表生成函数。//使用hive实现单词统计//1.建表$hive&gt;CREATE TABLE doc(line string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; ;</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;数据&quot;&gt;&lt;a href=&quot;#数据&quot; class=&quot;headerlink&quot; title=&quot;数据&quot;&gt;&lt;/a&gt;数据&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;[customers.txt]
1,tom,12
2,tom,13
3,tom,14
4,tom,15

[orders.t
      
    
    </summary>
    
      <category term="Hadoop" scheme="http://erichunn.github.io/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="http://erichunn.github.io/tags/Hadoop/"/>
    
      <category term="join" scheme="http://erichunn.github.io/tags/join/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop第十天</title>
    <link href="http://erichunn.github.io/2018/10/28/Hadoop%E7%AC%AC%E5%8D%81%E5%A4%A9/"/>
    <id>http://erichunn.github.io/2018/10/28/Hadoop第十天/</id>
    <published>2018-10-28T02:31:16.000Z</published>
    <updated>2018-10-31T12:40:18.671Z</updated>
    
    <content type="html"><![CDATA[<h2 id="HA"><a href="#HA" class="headerlink" title="HA"></a>HA</h2><pre><code>3个JournalNode,edit log两个NN，active | standby2NN</code></pre><h2 id="ha的管理命令"><a href="#ha的管理命令" class="headerlink" title="ha的管理命令"></a>ha的管理命令</h2><pre><code>hdfs haadmin -getServiceState nn1        //查看服务hdfs haadmin -transitionToActive nn1    //激活hdfs haadmin -transitionToStandby nn2    //待命hdfs haadmin -failover nn1 nn2            //对调</code></pre><h2 id="数据仓库"><a href="#数据仓库" class="headerlink" title="数据仓库"></a>数据仓库</h2><pre><code>OLAP        //online analyze process,在线分析处理            //延迟性高.在于后期海量数据的查询统计hive是延迟性比较高的操作实时性不好都是批量计算</code></pre><h2 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h2><pre><code>OLTP        //online transaction process在线事务处理.            //实时性好。延迟性很低        mysql针对事务性处理保证ACI特性。        </code></pre><h2 id="HA-1"><a href="#HA-1" class="headerlink" title="HA"></a>HA</h2><pre><code>3个JournalNode,edit log两个NN，active | standby2NN</code></pre><h2 id="ha的管理命令-1"><a href="#ha的管理命令-1" class="headerlink" title="ha的管理命令"></a>ha的管理命令</h2><pre><code>hdfs haadmin -getServiceState nn1        //查看服务hdfs haadmin -transitionToActive nn1    //激活hdfs haadmin -transitionToStandby nn2    //待命hdfs haadmin -failover nn1 nn2            //对调</code></pre><h2 id="数据仓库-1"><a href="#数据仓库-1" class="headerlink" title="数据仓库"></a>数据仓库</h2><pre><code>OLAP        //online analyze process,在线分析处理            //延迟性高.</code></pre><h2 id="数据库-1"><a href="#数据库-1" class="headerlink" title="数据库"></a>数据库</h2><pre><code>OLTP        //online transaction process在线事务处理.            //实时性好。</code></pre><h2 id="jdbc"><a href="#jdbc" class="headerlink" title="jdbc"></a>jdbc</h2><pre><code>java database connection,java数据库连接。</code></pre><p>java数据库连接设计套接字编程吗：面向接口编程，依托mysql驱动程序和mysql操作， jdbc就是socket，musql就是serversocket。</p><p><img src="https://i.imgur.com/5s8oEhk.png" alt=""><br>0</p><pre><code>1.创建mysql数据库和表     create table users(id int primary key auto_increment , name varchar(20) , age int);2.idea中创建jdbcDemo模块</code></pre><h2 id="事务-transaction"><a href="#事务-transaction" class="headerlink" title="事务:transaction,"></a>事务:transaction,</h2><pre><code>和数据库之间的一组操作。特点.a        //atomic,原子性,不可分割.c        //consistent,不能破坏掉i        //isolate,隔离型.d        //durable.永久性</code></pre><h2 id="truncate"><a href="#truncate" class="headerlink" title="truncate"></a>truncate</h2><pre><code>截断表，类似于delete操作，速度快，数据无法回滚。在Mysql里面操作truncate table users ;</code></pre><h2 id="sql语句"><a href="#sql语句" class="headerlink" title="sql语句"></a>sql语句</h2><pre><code>1.2.3.</code></pre><h2 id="Transaction"><a href="#Transaction" class="headerlink" title="Transaction"></a>Transaction</h2><pre><code>commit            //提交rollback        //回滚savePoint        //保存点</code></pre><h1 id="插入10万条数据不用预处理语句"><a href="#插入10万条数据不用预处理语句" class="headerlink" title="插入10万条数据不用预处理语句"></a>插入10万条数据不用预处理语句</h1><pre><code>import org.junit.Test;import java.sql.Connection;import java.sql.DriverManager;import java.sql.Statement;//测试增删改查基本功能public class TestCRUD {    @Test    public void testStatement() throws Exception{        long start=System.currentTimeMillis();        //创建连接        String diverClass = &quot;com.mysql.jdbc.Driver&quot;;        String url = &quot;jdbc:mysql://localhost:3306/big4&quot;;        String username = &quot;root&quot;;        String password = &quot;root&quot;;        Class.forName(diverClass);        Connection conn = DriverManager.getConnection(url, username, password);        //关闭自动提交        conn.setAutoCommit(false);        //创建语句对象        Statement st = conn.createStatement();        for(int i=0;i&lt;100000;i++){            String sql = &quot;insert into users(name,age) values(&apos;tomas&quot; + i + &quot;&apos;,&quot; + (i % 100) + &quot;)&quot;;            st.execute(sql);        }        conn.commit();        st.close();        conn.close();        System.out.println(System.currentTimeMillis() - start);    }}</code></pre><h1 id="使用预处理语句："><a href="#使用预处理语句：" class="headerlink" title="使用预处理语句："></a>使用预处理语句：</h1><pre><code>import org.junit.Test;import java.sql.Connection;import java.sql.DriverManager;import java.sql.PreparedStatement;//测试增删改查基本功能public class TestCRUD {    @Test    public void testPreparedStatement() throws Exception{        long start=System.currentTimeMillis();        //创建连接        String diverClass = &quot;com.mysql.jdbc.Driver&quot;;        String url = &quot;jdbc:mysql://localhost:3306/big4&quot;;        String username = &quot;root&quot;;        String password = &quot;root&quot;;        Class.forName(diverClass);        Connection conn = DriverManager.getConnection(url, username, password);        //关闭自动提交        conn.setAutoCommit(false);        //创建语句对象        String sql = &quot;insert into users(name,age) value(?,?)&quot;;        PreparedStatement ppst = conn.prepareStatement(sql);        for(int i =0;i &lt; 10000;i++){                ppst.setString(1,&quot;tom&quot;+i);                ppst.setInt(2,i%100);                ppst.executeUpdate();//执行更新             }        conn.commit();        ppst.close();        conn.close();        System.out.println(System.currentTimeMillis() - start);    }}</code></pre><h2 id="上述代码加一个批处理："><a href="#上述代码加一个批处理：" class="headerlink" title="上述代码加一个批处理："></a>上述代码加一个批处理：</h2><pre><code>import org.junit.Test;import java.sql.Connection;import java.sql.DriverManager;import java.sql.PreparedStatement;//测试增删改查基本功能public class TestCRUD {    @Test    public void testPreparedStatement() throws Exception {        long start = System.currentTimeMillis();        //创建连接        String diverClass = &quot;com.mysql.jdbc.Driver&quot;;        String url = &quot;jdbc:mysql://localhost:3306/big4&quot;;        String username = &quot;root&quot;;        String password = &quot;root&quot;;        Class.forName(diverClass);        Connection conn = DriverManager.getConnection(url, username, password);        //关闭自动提交        conn.setAutoCommit(false);        //创建语句对象        String sql = &quot;insert into users(name,age) value(?,?)&quot;;        PreparedStatement ppst = conn.prepareStatement(sql);        for (int i = 0; i &lt; 10000; i++) {            ppst.setString(1, &quot;tom&quot; + i);            ppst.setInt(2, i % 100);            ppst.addBatch();//这个地方相当于一个缓冲区，相当于缓冲区里面放了2000个在传出去，减少了网络带宽速度会变快            if (i % 200 == 0) {                ppst.executeUpdate();//执行更新            }        }        ppst.addBatch();//最后不够2000在进行一个批处理        conn.commit();        ppst.close();        conn.close();        System.out.println(System.currentTimeMillis() - start);    }}</code></pre><p>这边的一个通过自回环传输一百万次插入，传输量巨大。<br>但是要是直接在Mysql端直接给个命令做一百万次插入，就很快。不用和服务器端交互很多的数据量。通过存储过程来写，存储过程就是一组sql语句。这些语句已经预先在数据库中存放了。无非就是循环了。要执行这些语句，就要发一条执行存储过程的指令，服务器已收到命令，就直接执行。 速度很快的。</p><p><img src="https://i.imgur.com/7TubkBX.png" alt=""></p><h1 id="100000条数据通过普通事务，预处理，批处理时间"><a href="#100000条数据通过普通事务，预处理，批处理时间" class="headerlink" title="100000条数据通过普通事务，预处理，批处理时间"></a>100000条数据通过普通事务，预处理，批处理时间</h1><pre><code>Statement                //46698PreparedStatent            //43338CallableStatement        //14385</code></pre><p><img src="https://i.imgur.com/YJmcnEF.png" alt=""></p><h1 id="mysql存储过程"><a href="#mysql存储过程" class="headerlink" title="mysql存储过程"></a>mysql存储过程</h1><p>msyql&gt;– 定义新的终止符,<strong>*</strong>不要带空格这个是注释<strong>*</strong><br>mysql&gt;delimiter //</p><p>mysql&gt;– 创建存储过程<br>mysql&gt;CREATE PROCEDURE simpleproc (OUT param1 INT)<br>        BEGIN<br>        SELECT COUNT(*) INTO param1 FROM users;        – into 是赋值方式之一<br>        END<br>        //</p><p>mysql&gt;– 查看存储过程的状态<br>mysql&gt;show procedure status //</p><p>mysql&gt;– 查看指定存储过程创建语句<br>mysql&gt;show create procedure simpleproc ;</p><p>mysql&gt;– 调用存储过程,@a在命令中定义变量<br>mysql&gt;call simpleproc(@a)</p><p>mysql&gt;– 删除存储过程<br>mysql&gt;show drop procedure simpleproc ;</p><p>mysql&gt;– 定义加法存储过程,set赋值语句 :=<br>mysql&gt;create procedure sp_add(in a int,in b int, out c int)<br>            begin<br>            set c := a + b ;<br>        end<br>        //</p><h2 id="java访问存储过程（调用的是上一步c-a-b这个过程）"><a href="#java访问存储过程（调用的是上一步c-a-b这个过程）" class="headerlink" title="java访问存储过程（调用的是上一步c=a+b这个过程）"></a>java访问存储过程（调用的是上一步c=a+b这个过程）</h2><pre><code>import org.junit.Test;import java.sql.*;/** * 测试基本操作 */public class TestCRUD {    /**     * 存储过程     */    @Test    public void testCallableStatement() throws Exception {        long start = System.currentTimeMillis();        //创建连接        String driverClass = &quot;com.mysql.jdbc.Driver&quot;;        String url = &quot;jdbc:mysql://localhost:3306/big4&quot;;        String username = &quot;root&quot;;        String password = &quot;root&quot;;        Class.forName(driverClass);        Connection conn = DriverManager.getConnection(url, username, password);        //关闭自动提交        conn.setAutoCommit(false);        //创建可调用语句，调用存储过程        CallableStatement cst = conn.prepareCall(&quot;{call sp_add(?,?,?)}&quot;);        cst.setInt(1,2);        //绑定参数        cst.setInt(2,3);        //注册输出参数类型        cst.registerOutParameter(3,Types.INTEGER);        cst.execute();        int sum = cst.getInt(3);        System.out.println(sum);        conn.commit();        conn.close();        System.out.println(System.currentTimeMillis() - start);    }}</code></pre><h2 id="百万数据插入，存储过程的性能"><a href="#百万数据插入，存储过程的性能" class="headerlink" title="百万数据插入，存储过程的性能"></a>百万数据插入，存储过程的性能</h2><pre><code>1.创建存储过程    mysql&gt;create procedure sp_batchinsert(in n int)        begin        DECLARE name0 varchar(20);    -- 定义在begin内部        DECLARE age0 int;        DECLARE i int default 0 ;            while i &lt; n do                set name0 := concat(&apos;tom&apos;,i) ;                set age0 := i % 100 ;                insert into users(name,age) values(name0,age0);                set i := i + 1 ;            end while ;        end         //2.java代码    @Test    public void testCallableStatement() throws Exception {        long start = System.currentTimeMillis();        //创建连接        String driverClass = &quot;com.mysql.jdbc.Driver&quot;;        String url = &quot;jdbc:mysql://localhost:3306/big4&quot;;        String username = &quot;root&quot;;        String password = &quot;root&quot;;        Class.forName(driverClass);        Connection conn = DriverManager.getConnection(url, username, password);        //关闭自动提交        conn.setAutoCommit(false);        //创建可调用语句，调用存储过程        CallableStatement cst = conn.prepareCall(&quot;{call sp_batchinsert(?)}&quot;);        cst.setInt(1,1000000);        //绑定参数        //注册输出参数类型        cst.execute();        conn.commit();        conn.close();        System.out.println(System.currentTimeMillis() - start);    }</code></pre><h1 id="mysql函数"><a href="#mysql函数" class="headerlink" title="mysql函数"></a>mysql函数</h1><p>这个是创建函数的SQL参考手册：</p><p><img src="https://i.imgur.com/1gvxZzF.png" alt=""></p><pre><code>1.函数和存储过程相似，只是多了返回值声明.2.创建函数    mysql&gt;create function sf_add(a int ,b int) returns int        begin            return a + b ;        end        //3.显式创建的函数    mysql&gt;show function status                --     mysql&gt;show function status like &apos;%add%&apos;    --     mysql&gt;select sf_add(1,2)                --4.java调用函数    @Test    public void testFunction() throws Exception {        long start = System.currentTimeMillis();        //创建连接        String driverClass = &quot;com.mysql.jdbc.Driver&quot;;        String url = &quot;jdbc:mysql://localhost:3306/big4&quot;;        String username = &quot;root&quot;;        String password = &quot;root&quot;;        Class.forName(driverClass);        Connection conn = DriverManager.getConnection(url, username, password);        //关闭自动提交        conn.setAutoCommit(false);        //创建可调用语句，调用存储过程        CallableStatement cst = conn.prepareCall(&quot;{? = call sf_add(?,?)}&quot;);        cst.setInt(2,100);//设置第二个参数是100第三个参数是200，第一个参数是输出。        cst.setInt(3,200);        cst.registerOutParameter(1,Types.INTEGER);        //注册输出参数类型        cst.execute();        System.out.println(cst.getInt(1));        conn.commit();        conn.close();        System.out.println(System.currentTimeMillis() - start);    }</code></pre><h2 id="MVCC"><a href="#MVCC" class="headerlink" title="MVCC"></a>MVCC</h2><pre><code>multiple version concurrent control,多版本并发控制。数据库的记录存储有多条，每一条都附加版本。所以可以实现隔离级别。是因为有MVCC多版本并发控制</code></pre><h2 id="事务的并发执行，容易出现的几个现象"><a href="#事务的并发执行，容易出现的几个现象" class="headerlink" title="事务的并发执行，容易出现的几个现象"></a>事务的并发执行，容易出现的几个现象</h2><pre><code>1.脏读    读未提交,一个事务读取了另外一个事务改写还没有提交的数据，如果另外一个    事务在稍后的时候回滚。2.不可重复读    一个事务进行相同条件查询连续的两次或者两次以上，每次结果都不同。这个是对数据进行了更改    有其他事务做了update操作。3.幻读    和(2)很像，其他事务做了insert操作.1时间查询到了10个，2时刻查询到了20个，是因为中间有做了插入操作</code></pre><h2 id="隔离级别（由于有MVCC）"><a href="#隔离级别（由于有MVCC）" class="headerlink" title="隔离级别（由于有MVCC）"></a>隔离级别（由于有MVCC）</h2><pre><code>为了避免出现哪种并发现象的。1        //read uncommitted    ,读未提交        导致脏读导致不可重复读2        //read committed    ,读已提交        这里避免了脏读4        //repeatable read    ,可以重复读        这里避免了不可重复读。还是可以让他插入更新，但是读取的是更新之前的值8        //serializable        ,串行化(悲观锁)        这里避免了换读，串行化不支持并发了就已经。</code></pre><h2 id="演示mysql事务隔离级别"><a href="#演示mysql事务隔离级别" class="headerlink" title="演示mysql事务隔离级别"></a>演示mysql事务隔离级别</h2><pre><code>1.开启mysql客户端    mysql&gt;2.关闭自动提交    mysql&gt;set autocommit 0 ;3.每次操作数据,都要开启事务，提交事务。</code></pre><h2 id="脏读现象"><a href="#脏读现象" class="headerlink" title="脏读现象"></a>脏读现象</h2><pre><code>[A]    1)mysql&gt;start transaction ;                                -- 开始事务    2)msyql&gt;update users set age = age + 1 where id = 1 ;    -- 更新数据,没有提交    6)mysql&gt;rollback ;                                        -- 回滚    7)mysql&gt;select * from users ;[B]    3)mysql&gt;set session transaction isolation level read uncommitted ;    -- 读未提交    4)msyql&gt;start transaction ;        -- 开始事务    5)mysql&gt;select * from users ;    -- 13</code></pre><h2 id="避免脏读"><a href="#避免脏读" class="headerlink" title="避免脏读"></a>避免脏读</h2><pre><code>[A]    1)mysql&gt;start transaction ;                                -- 开始事务    2)msyql&gt;update users set age = age + 1 where id = 1 ;    -- 更新数据,没有提交    6)mysql&gt;rollback ;                                        -- 回滚    7)mysql&gt;select * from users ;[B]    3)mysql&gt;set session transaction isolation level read committed ;    -- 读已提交    4)msyql&gt;start transaction ;        -- 开始事务    5)mysql&gt;select * from users ;    -- 13</code></pre><h2 id="测试不可重复读-隔离级别设置为读已提交不能避免不可重复读。"><a href="#测试不可重复读-隔离级别设置为读已提交不能避免不可重复读。" class="headerlink" title="测试不可重复读(隔离级别设置为读已提交不能避免不可重复读。)"></a>测试不可重复读(隔离级别设置为读已提交不能避免不可重复读。)</h2><pre><code>[A]    1)mysql&gt;commit ;    2)mysql&gt;set session transaction isolation level read committed ;    -- 读已提交    3)mysql&gt;start transaction ;                                            -- 开始事务    4)mysql&gt;select * from users    ;                                        -- 查询    9)mysql&gt;select * from users    ;[B]    5)mysql&gt;commit;    6)mysql&gt;start transaction ;        7)mysql&gt;update users set age = 15 where id = 1 ;                    -- 更新    8)mysql&gt;commit;</code></pre><h2 id="测试避免不可重复读-隔离级别设置为读已提交不能避免不可重复读。"><a href="#测试避免不可重复读-隔离级别设置为读已提交不能避免不可重复读。" class="headerlink" title="测试避免不可重复读(隔离级别设置为读已提交不能避免不可重复读。)"></a>测试避免不可重复读(隔离级别设置为读已提交不能避免不可重复读。)</h2><pre><code>[A]    1)mysql&gt;commit ;    2)mysql&gt;set session transaction isolation level repeatable read ;    -- 可以重复读    3)mysql&gt;start transaction ;                                            -- 开始事务    4)mysql&gt;select * from users    ;                                        -- 查询    9)mysql&gt;select * from users    ;[B]    5)mysql&gt;commit;    6)mysql&gt;start transaction ;        7)mysql&gt;update users set age = 15 where id = 1 ;                    -- 更新    8)mysql&gt;commit;</code></pre><h2 id="测试幻读-隔离级别设置为repeatable"><a href="#测试幻读-隔离级别设置为repeatable" class="headerlink" title="测试幻读(隔离级别设置为repeatable)"></a>测试幻读(隔离级别设置为repeatable)</h2><pre><code>[A]    1)mysql&gt;commit ;    2)mysql&gt;set session transaction isolation level serializable;        -- 串行化    3)mysql&gt;start transaction ;                                            -- 开始事务    4)mysql&gt;select * from users    ;                                        -- 查询    9)mysql&gt;select * from users    ;[B]    5)mysql&gt;commit;    6)mysql&gt;start transaction ;        7)mysql&gt;insert into users(name,age) values(&apos;tomas&apos;,13);                -- 更新    8)mysql&gt;commit;</code></pre><h2 id="ANSI-SQL"><a href="#ANSI-SQL" class="headerlink" title="ANSI SQL"></a>ANSI SQL</h2><pre><code>美国国家标准结构SQL组select * from users for update ;</code></pre><h2 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h2><pre><code>1.支持四种隔离级别。2.默认隔离级别是可以重复读。3.隔离级别是seriable,不支持并发写。</code></pre><h2 id="表级锁"><a href="#表级锁" class="headerlink" title="表级锁"></a>表级锁</h2><pre><code>LOCK TABLE t WRITE;    -- 加锁(表级锁,read)UNLOCK TABLES ;        -- 解除自己所有的所有表级锁表级锁只能通过命令来解锁。</code></pre><p><img src="https://i.imgur.com/cLufeUV.png" alt=""></p><h2 id="编程实现脏读现象"><a href="#编程实现脏读现象" class="headerlink" title="编程实现脏读现象"></a>编程实现脏读现象</h2><pre><code>package com.it18zhang.jdbcdemo.test;import org.junit.Test;import java.sql.*;/** * 测试隔离级别 */public class TestIsolationLevel {    /**     * 执行写，不提交     */    @Test    public void testA() throws  Exception{        //创建连接        String driverClass = &quot;com.mysql.jdbc.Driver&quot;;        String url = &quot;jdbc:mysql://localhost:3306/big4&quot;;        String username = &quot;root&quot;;        String password = &quot;root&quot;;        Class.forName(driverClass);        Connection conn = DriverManager.getConnection(url, username, password);        conn.setAutoCommit(false);        Statement st = conn.createStatement();        st.execute(&quot;update users set age = 80 where id = 1&quot;);        System.out.println(&quot;===============&quot;);        conn.commit();        conn.close();    }    /**     * 查询，查到别人没有提交的数据     */    @Test    public void testB() throws  Exception{        //创建连接        String driverClass = &quot;com.mysql.jdbc.Driver&quot;;        String url = &quot;jdbc:mysql://localhost:3306/big4&quot;;        String username = &quot;root&quot;;        String password = &quot;root&quot;;        Class.forName(driverClass);        Connection conn = DriverManager.getConnection(url, username, password);        //设置隔离级别读未提交==&gt;导致脏读        /************************** 设置隔离级别 ***************************************/        conn.setTransactionIsolation(Connection.TRANSACTION_READ_UNCOMMITTED);        conn.setAutoCommit(false);        Statement st = conn.createStatement();        ResultSet rs = st.executeQuery(&quot;select age from users where id = 1&quot;);        rs.next();        int age = rs.getInt(1) ;        System.out.println(age);        System.out.println(&quot;===============&quot;);        conn.commit();        conn.close();    }</code></pre><h2 id="共享读锁"><a href="#共享读锁" class="headerlink" title="共享读锁"></a>共享读锁</h2><h1 id="独占写锁"><a href="#独占写锁" class="headerlink" title="独占写锁"></a>独占写锁</h1><pre><code>一个事务写操作，另一个塞住。行级别的锁在独占锁这块，开启事务之后，资源存在征用的情况，如果A修改&gt;=2，b修改&lt;=3那么就出现先来先用的情况，没有提交事务之前，B一直要等待状态。然后事务是you原子性的，肯定就是非交集和交集的部分都不能更改。因为原子性。查询是由隔离级别控制。在查询的时候为了不让别人更新设置隔离级别为最高级别，就是8串行化，但是很难受，连接不关掉放到连接池里面，被下一个调用，还是串行化状态，不能写入，性能差。除了设置隔离级别，我们还可以加上一个select * from users for update。这样的话就代表仅仅是对这一个查询语句的时候执行独占写锁。这个时候不需要再设置隔离级别为8了。</code></pre><h2 id="SQL"><a href="#SQL" class="headerlink" title="SQL"></a>SQL</h2><pre><code>//insert into users(name,age,...) values(&apos;&apos;,12,..) ;                -- insertupdate users set name = &apos;xxx&apos;,age = xxx ,... where id = xxx ;    -- updatedelete from users where id = xxx                                -- delete-- 投影查询 projection.select id,name from users where ... order by limit xxx            --select-- 查询时直接上独占写锁select * from users for update ;    </code></pre><h2 id="连接查询"><a href="#连接查询" class="headerlink" title="连接查询"></a>连接查询</h2><pre><code>1.准备表[mysql.sql]drop table if exists customers; -- 删除表drop table if exists orders ;    -- 删除表create table customers(id int primary key auto_increment , name varchar(20) , age int);                    -- 创建customers表create table orders(id int primary key auto_increment , orderno varchar(20) , price float , cid int);    -- 创建orders表-- 插入数据insert into customers(name,age) values(&apos;tom&apos;,12);insert into customers(name,age) values(&apos;tomas&apos;,13);insert into customers(name,age) values(&apos;tomasLee&apos;,14);insert into customers(name,age) values(&apos;tomason&apos;,15);-- 插入订单数据insert into orders(orderno,price,cid) values(&apos;No001&apos;,12.25,1);insert into orders(orderno,price,cid) values(&apos;No002&apos;,12.30,1);insert into orders(orderno,price,cid) values(&apos;No003&apos;,12.25,2);insert into orders(orderno,price,cid) values(&apos;No004&apos;,12.25,2);insert into orders(orderno,price,cid) values(&apos;No005&apos;,12.25,2);insert into orders(orderno,price,cid) values(&apos;No006&apos;,12.25,3);insert into orders(orderno,price,cid) values(&apos;No007&apos;,12.25,3);insert into orders(orderno,price,cid) values(&apos;No008&apos;,12.25,3);insert into orders(orderno,price,cid) values(&apos;No009&apos;,12.25,3);insert into orders(orderno,price,cid) values(&apos;No0010&apos;,12.25,NULL);---执行SQL文件    source d:/SQL/mysql.sql2.查询--连接查询mysql&gt;-- 笛卡尔积查询,无连接条件查询mysql&gt;select a.*,b.* from customers a , orders b ; </code></pre><p><img src="https://i.imgur.com/4hC9GS4.png" alt=""></p><pre><code>mysql&gt;-- 内连接,查询符合条件的记录.mysql&gt;select a.*,b.* from customers a , orders b where a.id = b.cid ; </code></pre><p><img src="https://i.imgur.com/2bP8sx4.png" alt=""></p><pre><code>mysql&gt;-- 左外连接,查询符合条件的记录.mysql&gt;select a.*,b.* from customers a left outer join orders b on a.id = b.cid ; </code></pre><p><img src="https://i.imgur.com/XKWihaX.png" alt=""></p><pre><code>mysql&gt;-- 右外连接,查询符合条件的记录.mysql&gt;select a.*,b.* from customers a right outer join orders b on a.id = b.cid ; </code></pre><p><img src="https://i.imgur.com/LiR1yV0.png" alt=""></p><p><img src="https://i.imgur.com/6GdI0kM.png" alt=""></p><pre><code>mysql&gt;-- 全外连接,查询符合条件的记录(mysql不支持全外链接)mysql&gt;select a.*,b.* from customers a full outer join orders b on a.id = b.cid ; </code></pre><p>2.查询–分组</p><pre><code>             字段列表  表       条件        分组        组内条件       排序     分页mysql&gt;select ... from ... where ... group by ... having ...  order by ... limit ..mysql&gt;-- 去重查询mysql&gt;select distinct price,cid from orders ;mysql&gt;-- 条件查询mysql&gt;select price,cid from orders where price &gt; 12.27 ;mysql&gt;-- 聚集查询mysql&gt;select max(price) from orders ;mysql&gt;select min(price) from orders ;mysql&gt;select avg(price) from orders ;mysql&gt;select sum(price) from orders ;mysql&gt;select count(id) from orders ;mysql&gt;-- 分组查询mysql&gt;select max(price) from orders where cid is not null group by cid ;mysql&gt;-- 分组查询(组内过滤)mysql&gt;select cid ,orderno,max(price) as max_price,min(price) from orders where cid is not null group by cid having max_price &gt; 20 ;mysql&gt;-- 降序查询mysql&gt;select cid ,orderno,max(price) as max_price,min(price) from orders where cid is not null group by cid having max_price &gt; 20 order by max_price desc;mysql&gt;-- 模糊查询mysql&gt;select  * from customers where name like &apos;toma%&apos;        mysql&gt;select  * from customers where name not like &apos;toma%&apos;        mysql&gt;-- 范围查询mysql&gt;select  * from customers where id in (1,2,3)        mysql&gt;select  * from customers where id not in (1,2,3)        mysql&gt;-- between 1 and 10,闭区间mysql&gt;select  * from customers where id between 1 and 3 ;mysql&gt;select  * from customers where id &gt;= 1 and id &lt;= 3 ;mysql&gt;-- 嵌套子查询(查询没有订单的客户)mysql&gt;select  * from customers where id not in (select distinct cid from orders where cid is not null);mysql&gt;-- 嵌套子查询(查询订单数量&gt;2的客户)mysql&gt;select * from customers where id in (select cid from orders group by cid having count(cid) &gt; 2);mysql&gt;select * from customers where id in ( select t.cid from (select cid,count(*) as c from orders group by cid having c &gt; 2) as t);mysql&gt;--向已有表中添加列mysql&gt;--alter table orders add column area int;mysql&gt;--设置area字段的值mysql&gt;--update orders set area = 2 where id in(1,3,,6,7);mysql&gt;--设置area字段的值mysql&gt;--update orders set area = 1 where id not in(1,3,,6,7);mysql&gt;--设置area字段的值mysql&gt;--update orders set area = 1 where id not in(1,3,,6,7);稍微看一下下面这个SQL语句：mysq&gt;--select cid,area,count(price) from orders group by cid,area;//这句话的意思是对cid和area联合分组，什么意思：就是说要cid和area要都相同才能进入到同一个组里面去。mysq&gt;--select cid,area,count(price),max(price),min(price) from orders group by cid,area order by cid asc, area desc  ;//这句话的意思是对cid和area联合分组，什么意思：就是说要cid和area要都相同才能进入到同一个组里面去。然后这个cid升序area降序是在1里面对area降序，也就是说先对cid整体升序，然后对升序完之后相同的cid里面进行降序。</code></pre><p><img src="https://i.imgur.com/QbThVj2.png" alt=""></p><pre><code>mysql&gt;-- 嵌套子查询(查询客户id,客户name,订单数量,最贵订单，最便宜订单，平均订单价格 where 订单数量&gt;2的客户)mysql&gt;select a.id,a.name,b.c,b.max,b.min,b.avg       from customers a,((select cid,count(cid) c , max(price) max ,min(price) min,avg(price) avg from orders group by cid having c &gt; 2) as b)       where a.id = b.cid ;</code></pre><h2 id="hadoop"><a href="#hadoop" class="headerlink" title="hadoop"></a>hadoop</h2><pre><code>MR左外连接.</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;HA&quot;&gt;&lt;a href=&quot;#HA&quot; class=&quot;headerlink&quot; title=&quot;HA&quot;&gt;&lt;/a&gt;HA&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;3个JournalNode,edit log
两个NN，active | standby
2NN
&lt;/code&gt;&lt;/pre&gt;
      
    
    </summary>
    
      <category term="Hadoop" scheme="http://erichunn.github.io/categories/Hadoop/"/>
    
      <category term="Mysql" scheme="http://erichunn.github.io/categories/Hadoop/Mysql/"/>
    
    
      <category term="jdbc加强" scheme="http://erichunn.github.io/tags/jdbc%E5%8A%A0%E5%BC%BA/"/>
    
      <category term="事务操作" scheme="http://erichunn.github.io/tags/%E4%BA%8B%E5%8A%A1%E6%93%8D%E4%BD%9C/"/>
    
      <category term="批处理" scheme="http://erichunn.github.io/tags/%E6%89%B9%E5%A4%84%E7%90%86/"/>
    
      <category term="预处理" scheme="http://erichunn.github.io/tags/%E9%A2%84%E5%A4%84%E7%90%86/"/>
    
      <category term="自定义mysql存储过程" scheme="http://erichunn.github.io/tags/%E8%87%AA%E5%AE%9A%E4%B9%89mysql%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B/"/>
    
      <category term="mysql函数" scheme="http://erichunn.github.io/tags/mysql%E5%87%BD%E6%95%B0/"/>
    
      <category term="百万数据插入" scheme="http://erichunn.github.io/tags/%E7%99%BE%E4%B8%87%E6%95%B0%E6%8D%AE%E6%8F%92%E5%85%A5/"/>
    
      <category term="HA管理命令" scheme="http://erichunn.github.io/tags/HA%E7%AE%A1%E7%90%86%E5%91%BD%E4%BB%A4/"/>
    
      <category term="数据仓库" scheme="http://erichunn.github.io/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"/>
    
      <category term="数据库" scheme="http://erichunn.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
      <category term="mysql事务隔离级别" scheme="http://erichunn.github.io/tags/mysql%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB/"/>
    
      <category term="独占写锁" scheme="http://erichunn.github.io/tags/%E7%8B%AC%E5%8D%A0%E5%86%99%E9%94%81/"/>
    
      <category term="共享读锁" scheme="http://erichunn.github.io/tags/%E5%85%B1%E4%BA%AB%E8%AF%BB%E9%94%81/"/>
    
      <category term="脏读" scheme="http://erichunn.github.io/tags/%E8%84%8F%E8%AF%BB/"/>
    
      <category term="不可重复读" scheme="http://erichunn.github.io/tags/%E4%B8%8D%E5%8F%AF%E9%87%8D%E5%A4%8D%E8%AF%BB/"/>
    
      <category term="幻读" scheme="http://erichunn.github.io/tags/%E5%B9%BB%E8%AF%BB/"/>
    
      <category term="串行化" scheme="http://erichunn.github.io/tags/%E4%B8%B2%E8%A1%8C%E5%8C%96/"/>
    
      <category term="行级锁" scheme="http://erichunn.github.io/tags/%E8%A1%8C%E7%BA%A7%E9%94%81/"/>
    
      <category term="连接查询" scheme="http://erichunn.github.io/tags/%E8%BF%9E%E6%8E%A5%E6%9F%A5%E8%AF%A2/"/>
    
      <category term="模糊查询" scheme="http://erichunn.github.io/tags/%E6%A8%A1%E7%B3%8A%E6%9F%A5%E8%AF%A2/"/>
    
      <category term="外连接查询" scheme="http://erichunn.github.io/tags/%E5%A4%96%E8%BF%9E%E6%8E%A5%E6%9F%A5%E8%AF%A2/"/>
    
      <category term="笛卡尔积查询" scheme="http://erichunn.github.io/tags/%E7%AC%9B%E5%8D%A1%E5%B0%94%E7%A7%AF%E6%9F%A5%E8%AF%A2/"/>
    
      <category term="分组查询" scheme="http://erichunn.github.io/tags/%E5%88%86%E7%BB%84%E6%9F%A5%E8%AF%A2/"/>
    
      <category term="嵌套子查询" scheme="http://erichunn.github.io/tags/%E5%B5%8C%E5%A5%97%E5%AD%90%E6%9F%A5%E8%AF%A2/"/>
    
      <category term="条件查询" scheme="http://erichunn.github.io/tags/%E6%9D%A1%E4%BB%B6%E6%9F%A5%E8%AF%A2/"/>
    
      <category term="去重查询" scheme="http://erichunn.github.io/tags/%E5%8E%BB%E9%87%8D%E6%9F%A5%E8%AF%A2/"/>
    
      <category term="范围查询" scheme="http://erichunn.github.io/tags/%E8%8C%83%E5%9B%B4%E6%9F%A5%E8%AF%A2/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop第九天</title>
    <link href="http://erichunn.github.io/2018/10/25/Hadoop%E7%AC%AC%E4%B9%9D%E5%A4%A9/"/>
    <id>http://erichunn.github.io/2018/10/25/Hadoop第九天/</id>
    <published>2018-10-25T01:52:10.000Z</published>
    <updated>2018-10-27T09:06:13.510Z</updated>
    
    <content type="html"><![CDATA[<h1 id="复习："><a href="#复习：" class="headerlink" title="复习："></a>复习：</h1><p>1.链式job编程</p><pre><code>MR        //Mapper+ / Reduce Mapper*</code></pre><p>2.DBWritable</p><pre><code>和数据库交互。</code></pre><p>3.Sqoop</p><p>4.全排序</p><pre><code>对reduce输出的所有结果进行排序。</code></pre><p>5.二次排序</p><pre><code>对value进行排序。</code></pre><p>6.数据倾斜</p><pre><code>1.reduce2.自定义分区函数    数据结果错 + 二次job3.重新设计key    数据结果错 + 二次job</code></pre><h1 id="机架感知"><a href="#机架感知" class="headerlink" title="机架感知"></a>机架感知</h1><p>机架有一块网卡。可以连接很多主机，机架上每个刀片服务器主机都连接到这个机架网卡上的。</p><p>比如一台刀片服务器上有2个进程，2个进程之间通信需要走机架上的交换机吗。不需要直接走自回环网络即可。走Localhost即可。所以2个进程之间不需要网卡交互数据。2个进程之间的距离就是0。每个进程所在的主机节点，连接他们之间的网络通路的跃点数就是他们之间的距离</p><p>如果b中绿色进程和a中红色进程需要交互就需要通过交换机，距离就是2.怎么算的，就是从每个服务器到交换机之间的距离相加，就是1+1；</p><p>如果是2个机架也要通过一个交换机连接起来。左边机架a中绿色和右边机架红色通信。距离就是4.</p><p>同一个机房通信最多就是4。也就是在通过一个交换机。</p><p>Hadoop都有机架感知能力：是他里面的一种策略，都需人为维护。在默认情况下所有节点都在同一个机架上。/default-rack。<br><img src="https://i.imgur.com/jdRJXuc.png" alt=""><br>但是在临近节点数据量非常大的时候，也是不会使用的，就近原则只是一种策略，还有其他策略。</p><h2 id="fault-tolerance"><a href="#fault-tolerance" class="headerlink" title="fault tolerance"></a>fault tolerance</h2><pre><code>容错.针对业务。map或reduce任务失败，的这种错误。</code></pre><h2 id="fail-over"><a href="#fail-over" class="headerlink" title="fail over"></a>fail over</h2><pre><code>容灾.针对硬件故障。</code></pre><h2 id="master-slave"><a href="#master-slave" class="headerlink" title="master / slave"></a>master / slave</h2><pre><code>主(master,namenode)从(slave,datanode)结构.</code></pre><p> topology.node.switch.mapping.impl</p><h1 id="客户端请求Namenode来读取datanodes的过程"><a href="#客户端请求Namenode来读取datanodes的过程" class="headerlink" title="客户端请求Namenode来读取datanodes的过程"></a>客户端请求Namenode来读取datanodes的过程</h1><p><img src="https://i.imgur.com/XFcQNFV.png" alt=""><br>Namenode只存放文件系统的数据信息，而不存放各个节点的IP地址。存放块ID，块列表。</p><h2 id="可靠性"><a href="#可靠性" class="headerlink" title="可靠性"></a>可靠性</h2><pre><code>提供数据安全的能力。</code></pre><h2 id="可用性"><a href="#可用性" class="headerlink" title="可用性"></a>可用性</h2><pre><code>提供持续服务的能力。</code></pre><h2 id="默认的副本放置策略"><a href="#默认的副本放置策略" class="headerlink" title="默认的副本放置策略"></a>默认的副本放置策略</h2><pre><code>首选在本地机架的一个node存放副本,另一个副本在本地机架的另一个不同节点。最后一个副本在不同机架的不同节点上。</code></pre><p>hads oiv        //image data metadata.    离线镜像查看器<br>hads oev        //edit        编辑日志</p><p>镜像文件存放的是元信息，元信息包括块，块ID，块列表，是不是上下级关系，副本数，权限，块大小。除了数据内容本身内容之外的。</p><p>通过实现接口改变配置实现一个机架感知。</p><h2 id="自定义机架感知-优化hadoop集群一种方式"><a href="#自定义机架感知-优化hadoop集群一种方式" class="headerlink" title="自定义机架感知(优化hadoop集群一种方式)"></a>自定义机架感知(优化hadoop集群一种方式)</h2><pre><code>1.自定义实现类package com.it18zhang.hdfs.rackaware;import org.apache.hadoop.net.DNSToSwitchMapping;import java.io.FileWriter;import java.io.IOException;import java.util.ArrayList;import java.util.List;/*机架感知实现类吧203以下的机器设置为机架1，吧203以上的机架设置为机架2 */public class MyRackAware implements DNSToSwitchMapping {    public List&lt;String&gt; resolve(List&lt;String&gt; names) {        ArrayList&lt;String&gt; list = new ArrayList&lt;String&gt;();        //true表示是不是追加模式        FileWriter fw = null;        try {            fw = new FileWriter(&quot;/home/centos/rackaware.txt&quot;, true);            for (String str : names) {                fw.write(str + &quot;\r\n&quot;);                if (str.startsWith(&quot;192&quot;)) {                    //192.168.192.202                    String ip = str.substring(str.lastIndexOf(&quot;.&quot;) + 1);                    if (Integer.parseInt(ip) &lt;= 203) {                        list.add(&quot;/rack1/&quot; + ip);                    } else {                        list.add(&quot;/rack2&quot; + ip);                    }                } else if (str.startsWith(&quot;s&quot;)) {                    String ip = str.substring(1);                    if (Integer.parseInt(ip) &lt;= 203) {                        list.add(&quot;/rack1/&quot; + ip);                    } else {                        list.add(&quot;/rack2&quot; + ip);                    }                }            }        } catch (IOException e) {            e.printStackTrace();        }        return list;    }    public void reloadCachedMappings() {    }    public void reloadCachedMappings(List&lt;String&gt; names) {    }}2.配置core-site.xml    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;    &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;    &lt;configuration&gt;            &lt;property&gt;                    &lt;name&gt;fs.defaultFS&lt;/name&gt;                    &lt;value&gt;hdfs://192.168.231.201/&lt;/value&gt;            &lt;/property&gt;            &lt;property&gt;                    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;                    &lt;value&gt;/home/centos/hadoop&lt;/value&gt;            &lt;/property&gt;            &lt;property&gt;                    &lt;name&gt;topology.node.switch.mapping.impl&lt;/name&gt;                    &lt;value&gt;com.it18zhang.hdfs.rackaware.MyRackAware&lt;/value&gt;            &lt;/property&gt;    &lt;/configuration&gt;3.导出jar包4.复制jar到/soft/hadoop/shared/hadoop/common/lib（这个是hadoop的类路径）5.分发jar.(可以不做)    实际上不需要分发，只在名称节点上运行。6.重启名称节点    $&gt;hadoop-daemon.sh stop namenode    $&gt;hadoop-daemon.sh start namenode</code></pre><hr><p>在s202上传一个文件，最后得出来确实和副本存放策略一致：</p><pre><code>首选在本地机架的一个node存放副本,另一个副本在本地机架的另一个不同节点。最后一个副本在不同机架的不同节点上。</code></pre><p><img src="https://i.imgur.com/agPBlr6.png" alt=""></p><hr><h1 id="关于HDFS"><a href="#关于HDFS" class="headerlink" title="关于HDFS"></a>关于HDFS</h1><p>下面这一段是关于HDFS的讲解，HDFS包括namenode和datanode，名称节点和数据节点。名称节点只是保存名称信息。那么如果在HDFS中文件移动的话怎么办，就是名称节点改变目录来移动即可。1kb和1G的速度是一样的。在HDFS中移动只是改变路径而已，块根本就没变，只能重新删掉在put才是真正移动了块。这一节就是通过实验验证了这个问题。</p><p><img src="https://i.imgur.com/JozXTOc.png" alt=""></p><hr><h2 id="去IOE"><a href="#去IOE" class="headerlink" title="去IOE"></a>去IOE</h2><pre><code>IBM            //Oracle        //EMC            //</code></pre><h2 id="HA"><a href="#HA" class="headerlink" title="HA"></a>HA</h2><pre><code>1.NFS    网络共享存储设备。2.QJM    Quorum Journal Manager3.两个名称节点    active            //激活    standby            //待命</code></pre><p>active            //激活<br>deactive        //钝化</p><h2 id="SPOF"><a href="#SPOF" class="headerlink" title="SPOF"></a>SPOF</h2><pre><code>single point of failure,单点故障。</code></pre><h2 id="事务是个特性"><a href="#事务是个特性" class="headerlink" title="事务是个特性"></a>事务是个特性</h2><pre><code>a        //atomic    原子性c        //consistent一致性i        //isolate    隔离型d        //durable    永久性·</code></pre><h2 id="majority"><a href="#majority" class="headerlink" title="majority "></a>majority </h2><pre><code>大部分.</code></pre><h2 id="HA高可用配置"><a href="#HA高可用配置" class="headerlink" title="HA高可用配置"></a>HA高可用配置</h2><pre><code>high availability,高可用./home/centos/hadoop/dfs/data/current/BP-2100834435-192.168.231.201-1489328949370/current/finalized/subdir0/subdir0两个名称节点，一个active(激活态)，一个是standby(slave待命),slave节点维护足够多状态以便于容灾。和客户端交互的active节点,standby不交互.两个节点都和JN守护进程构成组的进行通信。数据节点配置两个名称节点，分别报告各自的信息。同一时刻只能有一个激活态名称节点。脑裂:两个节点都是激活态。为防止脑裂，JNs只允许同一时刻只有一个节点向其写数据。容灾发生时，成为active节点的namenode接管向jn的写入工作。</code></pre><h2 id="硬件资源"><a href="#硬件资源" class="headerlink" title="硬件资源"></a>硬件资源</h2><pre><code>名称节点:    硬件配置相同。JN节点    :    轻量级进程，至少3个节点,允许挂掉的节点数 (n - 1) / 2.            不需要再运行辅助名称节点。</code></pre><h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><h2 id="配置细节"><a href="#配置细节" class="headerlink" title="配置细节"></a>配置细节</h2><pre><code>0.s201和s206具有完全一致的配置，尤其是ssh.1.配置nameservice    [hdfs-site.xml]    &lt;property&gt;        &lt;name&gt;dfs.nameservices&lt;/name&gt;        &lt;value&gt;mycluster&lt;/value&gt;    &lt;/property&gt;2.dfs.ha.namenodes.[nameservice ID]    [hdfs-site.xml]    &lt;!-- myucluster下的名称节点两个id --&gt;    &lt;property&gt;        &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt;        &lt;value&gt;nn1,nn2&lt;/value&gt;    &lt;/property&gt;3.dfs.namenode.rpc-address.[nameservice ID].[name node ID]     [hdfs-site.xml]    配置每个nn的rpc地址。    &lt;property&gt;        &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt;        &lt;value&gt;s201:8020&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt;        &lt;value&gt;s206:8020&lt;/value&gt;    &lt;/property&gt;4.dfs.namenode.http-address.[nameservice ID].[name node ID]    配置webui端口    [hdfs-site.xml]    &lt;property&gt;        &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt;        &lt;value&gt;s201:50070&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt;        &lt;value&gt;s206:50070&lt;/value&gt;    &lt;/property&gt;5.dfs.namenode.shared.edits.dir    名称节点共享编辑目录.    [hdfs-site.xml]    &lt;property&gt;        &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;        &lt;value&gt;qjournal://s202:8485;s203:8485;s204:8485/mycluster&lt;/value&gt;    &lt;/property&gt;6.dfs.client.failover.proxy.provider.[nameservice ID]    java类，client使用它判断哪个节点是激活态。    [hdfs-site.xml]    &lt;property&gt;        &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt;        &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;    &lt;/property&gt;7.dfs.ha.fencing.methods    脚本列表或者java类，在容灾保护激活态的nn.    [hdfs-site.xml]    &lt;property&gt;        &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;        &lt;value&gt;                sshfence                shell(/bin/true)        &lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;        &lt;value&gt;/home/centos/.ssh/id_rsa&lt;/value&gt;    &lt;/property&gt;8.fs.defaultFS     配置hdfs文件系统名称服务。    [core-site.xml]    &lt;property&gt;        &lt;name&gt;fs.defaultFS&lt;/name&gt;        &lt;value&gt;hdfs://mycluster&lt;/value&gt;    &lt;/property&gt; 9.dfs.journalnode.edits.dir    配置JN存放edit的本地路径。    [hdfs-site.xml]    &lt;property&gt;        &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;        &lt;value&gt;/home/centos/hadoop/journal&lt;/value&gt;    &lt;/property&gt;</code></pre><h2 id="部署细节"><a href="#部署细节" class="headerlink" title="部署细节"></a>部署细节</h2><pre><code>1.在jn节点分别启动jn进程    $&gt;hadoop-daemon.sh start journalnode2.启动jn之后，在两个NN之间进行disk元数据同步    a)如果是全新集群，先format文件系统,只需要在一个nn上执行。        [s201]        $&gt;hadoop namenode -format</code></pre><p>格式化文件系统主要就是生成一个新的VERSION，里面内容就是生成一个名称空间ID集群ID时间块池ID</p><p><img src="https://i.imgur.com/9loeVXF.png" alt=""></p><pre><code>b)如果将非HA集群转换成HA集群，复制原NN的metadata到另一个nn.    1.步骤一        [s201]        $&gt;scp -r /home/centos/hadoop/dfs centos@s206:/home/centos/hadoop/    2.步骤二        在新的nn(未格式化的nn)上运行一下命令，实现待命状态引导。        [s206]        $&gt;hdfs namenode -bootstrapStandby        //需要s201为启动状态,提示是否格式化,选择N.    3)在一个NN上执行以下命令，完成edit日志到jn节点的传输。        $&gt;hdfs namenode -initializeSharedEdits        #查看s202,s203是否有edit数据.    4)启动所有节点.        [s201]        $&gt;hadoop-daemon.sh start namenode        //启动名称节点        $&gt;hadoop-daemons.sh start datanode        //启动所有数据节点        [s206]        $&gt;hadoop-daemon.sh start namenode        //启动名称节点</code></pre><h2 id="HA管理"><a href="#HA管理" class="headerlink" title="HA管理"></a>HA管理</h2><pre><code>$&gt;hdfs haadmin -transitionToActive nn1                //切成激活态$&gt;hdfs haadmin -transitionToStandby nn1                //切成待命态$&gt;hdfs haadmin -transitionToActive --forceactive nn2//强行激活$&gt;hdfs haadmin -failover nn1 nn2                    //模拟容灾演示,从nn1切换到nn2</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;复习：&quot;&gt;&lt;a href=&quot;#复习：&quot; class=&quot;headerlink&quot; title=&quot;复习：&quot;&gt;&lt;/a&gt;复习：&lt;/h1&gt;&lt;p&gt;1.链式job编程&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;MR        //Mapper+ / Reduce Mapper*
&lt;/co
      
    
    </summary>
    
      <category term="Hadoop" scheme="http://erichunn.github.io/categories/Hadoop/"/>
    
    
      <category term="机架感知" scheme="http://erichunn.github.io/tags/%E6%9C%BA%E6%9E%B6%E6%84%9F%E7%9F%A5/"/>
    
      <category term="机架感知实现" scheme="http://erichunn.github.io/tags/%E6%9C%BA%E6%9E%B6%E6%84%9F%E7%9F%A5%E5%AE%9E%E7%8E%B0/"/>
    
      <category term="HA" scheme="http://erichunn.github.io/tags/HA/"/>
    
      <category term="手动移动数据块" scheme="http://erichunn.github.io/tags/%E6%89%8B%E5%8A%A8%E7%A7%BB%E5%8A%A8%E6%95%B0%E6%8D%AE%E5%9D%97/"/>
    
      <category term="HDFS" scheme="http://erichunn.github.io/tags/HDFS/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop第八天</title>
    <link href="http://erichunn.github.io/2018/10/22/Hadoop%E7%AC%AC%E5%85%AB%E5%A4%A9/"/>
    <id>http://erichunn.github.io/2018/10/22/Hadoop第八天/</id>
    <published>2018-10-22T11:40:51.000Z</published>
    <updated>2018-10-24T11:12:28.389Z</updated>
    
    <content type="html"><![CDATA[<h1 id="二次排序链条化"><a href="#二次排序链条化" class="headerlink" title="二次排序链条化"></a>二次排序链条化</h1><p>分组不能解决数据倾斜问题，已经进入到了同一个reduce对象里面了，分一个组出来就进入同一个reduce方法里面。每次分组只是决定调用不调用同一个reduce方法。已经晚了，数据已经进入到了同一个reduce了，分区可以决定数据倾斜问题。</p><p>reduce分为方法和对象，有的时候说reduce是指的是方法，有的时候指的是对象，要分开对待。</p><p>单词统计做统计肯定不能哈希分区，因为哈希同一个值进入到同一个reduce对象利民区去了</p><h1 id="数据倾斜问题-随机分区-二次MR"><a href="#数据倾斜问题-随机分区-二次MR" class="headerlink" title="数据倾斜问题 随机分区 二次MR"></a>数据倾斜问题 随机分区 二次MR</h1><pre><code>1.2.3.4.</code></pre><p>如果正常按照wordcount来处理会分为</p><p>reduce不设置就是1，map没机会设置数量。但是也可以设置map，但是意义，以为mr提交的时候map的个数取决于切片的个数。</p><p>切片的计算公式：min block、maxsplit、 blocksize取中间值<br>等于blocksize。</p><p>现在文件如下，现在3个文件每个文件有一百万数据且有一百万个hello就对应了3个map。（这里没明白为什么3个切片也就是为啥3个map）</p><p>每个切片里，分区个数等于reduce个数，分区的个数取决于reduce个数，之前自己已经在WCAPP里面设置好了reduce个数。</p><p>我有3个map，4个reduce。那么每个map里面就有4个分区了。</p><p><strong>要解决哈希分区带来的数据倾斜问题，可以通过随机分区，然后在进行二次mr来解决这个问题</strong></p><p><img src="https://i.imgur.com/VoHpfBH.png" alt=""></p><p>组合combineor是对分区里面的预先聚合，下面的hello就不会发送一百万次，而是提前聚合了发送了hello 一百万次。这个样子。能解决目前的倾斜问题，但是不可能这么多数据，可能量很大</p><p>下面这种情况下，所有的hello根据哈希分区就进入到同一个分区，然后一百万个tom分4个分区</p><p>通过随机分区可以解决数据倾斜问题，但是会产生新的问题，就是说hello被分到4个不同的reduce里面，导致reduce1产生hello10reduce2产生hello13这种问题，所以通过二次mr来解决这个问题，让hello这个单词统计，进入到同一个reduce结果当中。所以也就是通过随即分区和二次mr解决</p><pre><code>[1.txt]1000000hello tom1hello tom2hello tom3hello tom4hello tom5hello tom6hello tom7hello tom8hello tom9hello tom10[2.txt]1000000hello tom11hello tom12hello tom13hello tom14hello tom15hello tom16hello tom17hello tom18hello tom19hello tom20[3.txt]1000000hello tom21hello tom22hello tom23hello tom24hello tom25hello tom26hello tom27hello tom28hello tom29hello tom30</code></pre><p>代码如下：<br>    //自定义分区函数<br>    public class RandomPartitioner extends Partitioner&lt;Text,IntWritable&gt; {<br>        @Override<br>        public int getPartition(Text text, IntWritable intWritable, int numPartitions) {<br>            return new Random().nextInt(numPartitions);<br>        }<br>    }</p><hr><pre><code>//解决数据倾斜问题：public class WCSkueApp {    public static void main(String[] args) throws Exception {        Configuration conf = new Configuration();        conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;);        Job job = Job.getInstance(conf);//        设置作业的各种属性        job.setJobName(&quot;MaxTempApp&quot;);    //作业名称        job.setJarByClass(WCSkueApp.class); //搜索类路径        job.setInputFormatClass(TextInputFormat.class);//设置输入格式类        //添加输入路径        FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew&quot;));        //设置输出路径        org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(&quot;e:/mr/skew/out&quot;));        //设置随机分区：        job.setPartitionerClass(RandomPartitioner.class);        job.setMapperClass(WCSkueMapper.class);         //mapper类        job.setReducerClass(WCSkueReducer.class);       //reduce类        job.setNumReduceTasks(4);        job.setMapOutputKeyClass(Text.class);        job.setMapOutputValueClass(IntWritable.class);        job.setOutputKeyClass(Text.class);          //设置输出类型        job.setOutputValueClass(IntWritable.class);        job.waitForCompletion(true);    }}</code></pre><hr><pre><code>public class WCSkueMapper extends Mapper &lt;LongWritable, Text, Text, IntWritable&gt;{    public WCSkueMapper(){        System.out.println(&quot;new WCMapper&quot;);    }    @Override    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {        String[] arr = value.toString().split(&quot; &quot;);        Text keyout= new Text();        IntWritable valueout = new IntWritable();        for(String s:arr){            keyout.set(s);            valueout.set(1);            context.write(keyout,valueout);        }    }}</code></pre><hr><pre><code>public class WCSkueReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{    /**     * reduce     */    protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {        int count = 0 ;        for(IntWritable iw : values){            count = count + iw.get() ;        }        context.write(key,new IntWritable(count));    }}</code></pre><hr><p>//解决数据倾斜问题：<br>    public class WCSkueApp2 {</p><pre><code>    public static void main(String[] args) throws Exception {        Configuration conf = new Configuration();        conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;);        Job job = Job.getInstance(conf);//        设置作业的各种属性        job.setJobName(&quot;MaxTempApp&quot;);    //作业名称        job.setJarByClass(WCSkueApp2.class); //搜索类路径        job.setInputFormatClass(TextInputFormat.class);//设置输入格式类        //添加输入路径        FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00000&quot;));        FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00001&quot;));        FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00002&quot;));        FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00003&quot;));        //设置输出路径        org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(&quot;e:/mr/skew/out2&quot;));        job.setMapperClass(WCSkueMapper2.class);         //mapper类        job.setReducerClass(WCSkueReducer2.class);       //reduce类        job.setNumReduceTasks(4);        job.setMapOutputKeyClass(Text.class);        job.setMapOutputValueClass(IntWritable.class);        job.setOutputKeyClass(Text.class);          //设置输出类型        job.setOutputValueClass(IntWritable.class);        job.waitForCompletion(true);    }}</code></pre><hr><pre><code>public class WCSkueMapper2 extends Mapper &lt;LongWritable, Text, Text, IntWritable&gt;{    public WCSkueMapper2(){        System.out.println(&quot;new WCMapper&quot;);    }    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {        String[] arr = value.toString().split(&quot;\t&quot;);        Text keyout= new Text();        IntWritable valueout = new IntWritable();        context.write(new Text(arr[0]),new IntWritable(Integer.parseInt(arr[1])));    }}</code></pre><hr><pre><code>public class WCSkueReducer2 extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{    /**     * reduce     */    protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {        int count = 0 ;        for(IntWritable iw : values){            count = count + iw.get() ;        }        context.write(key,new IntWritable(count));    }}</code></pre><hr><p>上面这个就是通过2次mr解决数据倾斜问题    但是不用那么麻烦的去切分了直接使用Keyvalueinputformat即可实现：如下代码：<br>    //解决数据倾斜问题：<br>    public class WCSkueApp2 {</p><pre><code>    public static void main(String[] args) throws Exception {        Configuration conf = new Configuration();        conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;);        Job job = Job.getInstance(conf);//        设置作业的各种属性        job.setJobName(&quot;MaxTempApp&quot;);    //作业名称        job.setJarByClass(WCSkueApp2.class); //搜索类路径        job.setInputFormatClass(KeyValueTextInputFormat.class);//设置输入格式类        //添加输入路径        FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00000&quot;));        FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00001&quot;));        FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00002&quot;));        FileInputFormat.addInputPath(job,new Path(&quot;e:/mr/skew/out/part-r-00003&quot;));        //设置输出路径        org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(&quot;e:/mr/skew/out2&quot;));        job.setMapperClass(WCSkueMapper2.class);         //mapper类        job.setReducerClass(WCSkueReducer2.class);       //reduce类        job.setNumReduceTasks(4);        job.setMapOutputKeyClass(Text.class);        job.setMapOutputValueClass(IntWritable.class);        job.setOutputKeyClass(Text.class);          //设置输出类型        job.setOutputValueClass(IntWritable.class);        job.waitForCompletion(true);    }}</code></pre><hr><pre><code>public class WCSkueMapper2 extends Mapper &lt;Text, Text, Text, IntWritable&gt;{    public WCSkueMapper2(){        System.out.println(&quot;new WCMapper&quot;);    }    protected void map(Text key, Text value, Context context) throws IOException, InterruptedException {        context.write(key,new IntWritable(Integer.parseInt(value.toString())));    }}</code></pre><hr><pre><code>public class WCSkueReducer2 extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{    /**     * reduce     */    protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {        int count = 0 ;        for(IntWritable iw : values){            count = count + iw.get() ;        }        context.write(key,new IntWritable(count));    }}</code></pre><hr><hr><hr><h1 id="链条式编程"><a href="#链条式编程" class="headerlink" title="链条式编程"></a>链条式编程</h1><p><img src="https://i.imgur.com/VwHIXax.png" alt=""></p><p>讲解上图：<br>首先通过MapMapper1来切割，然后通过MapMapper1过滤掉法轮功，真个是一个链条，在map端，执行完之后开始聚合，在reduce端，在聚合的时候，每个单词都有各自的个数，对产生的结果再次过滤，过滤掉少于5的单词，当然可以放到同一个map里面，但是如果变成模块方法，易于维护。更加方便使用。<br>m阶段可以有多个m但是R阶段只能有一个r。但是r阶段可以有多个m。</p><p>代码如下图：<br>//链条式job任务<br>    public class WCChainApp {</p><pre><code>    public static void main(String[] args) throws Exception {        Configuration conf = new Configuration();        conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;);        Job job = Job.getInstance(conf);//        设置作业的各种属性        job.setJobName(&quot;MaxTempApp&quot;);    //作业名称        job.setJarByClass(WCChainApp.class); //搜索类路径        job.setInputFormatClass(TextInputFormat.class);//设置输入格式类        //添加输入路径        FileInputFormat.addInputPath(job, new Path(&quot;e:/mr/skew&quot;));        //设置输出路径        org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(&quot;e:mr/skew/out&quot;));        //在map链条上添加一个mapper的环节        ChainMapper.addMapper(job,WCMapMapper1.class,LongWritable.class,Text.class,Text.class,IntWritable.class,conf);        ChainMapper.addMapper(job,WCMapMapper2.class,Text.class,IntWritable.class,Text.class,IntWritable.class,conf);        //在reduce链条上设置reduce        ChainReducer.setReducer(job,WCReducer.class,Text.class,IntWritable.class,Text.class,IntWritable.class,conf);        ChainReducer.addMapper(job,WCReduceMapper1.class,Text.class,IntWritable.class,Text.class,IntWritable.class,conf);        job.setNumReduceTasks(3);        job.waitForCompletion(true);    }}</code></pre><hr><pre><code>public class WCMapMapper1 extends Mapper &lt;LongWritable, Text, Text, IntWritable&gt;{    protected   void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {        Text keyout= new Text();        IntWritable valueout = new IntWritable();        String[] arr = value.toString().split(&quot; &quot;);        for(String s:arr){            keyout.set(s);            valueout.set(1);            context.write(keyout,valueout);        }    }}</code></pre><hr><pre><code>public class WCMapMapper2 extends Mapper &lt;Text, IntWritable, Text, IntWritable&gt;{    protected void map(Text key, IntWritable value, Context context) throws IOException, InterruptedException {        if(!key.toString().equals(&quot;falungong&quot;)){            context.write(key , value);        }    }}</code></pre><hr><pre><code>//过滤单词个数public class WCReduceMapper1 extends Mapper&lt;Text,IntWritable,Text,IntWritable&gt; {    @Override    protected void map(Text key, IntWritable value, Context context) throws IOException, InterruptedException {        if(value.get() &gt; 5){            context.write(key,value);        }    }}</code></pre><hr><pre><code>/** * Reducer */public class WCReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{    /**     * reduce     */    protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {        int count = 0 ;        for(IntWritable iw : values){            count = count + iw.get() ;        }        context.write(key,new IntWritable(count));    }}</code></pre><hr><hr><hr><h2 id="FileInputFormat（读源码）"><a href="#FileInputFormat（读源码）" class="headerlink" title="FileInputFormat（读源码）"></a>FileInputFormat（读源码）</h2><pre><code>获取切片集合。子类都要重写方法isSplittable();负责创建RecordReader对象。设置IO路径。</code></pre><h2 id="RecordReader（读源码）"><a href="#RecordReader（读源码）" class="headerlink" title="RecordReader（读源码）"></a>RecordReader（读源码）</h2><pre><code>负责从InputSplit中读取KV对。</code></pre><h2 id="jdbc笔记模板："><a href="#jdbc笔记模板：" class="headerlink" title="jdbc笔记模板："></a>jdbc笔记模板：</h2><pre><code>[写操作]Class.forName(&quot;com.mysql.jdbc.Driver&quot;);Connection conn = DriverMananger.getConnection(&quot;jdbc:mysql://localhost:3306/big4&quot;,&quot;root&quot;,&quot;root&quot;);//预处理语句PreparedStatement ppst = conn.preparedStatement(&quot;insert into test(id,name,age) values(?,?,?)&quot;);//绑定参数ppst.setInteger(1,1);ppst.setInteger(2,&quot;tom&quot;);ppst.setInteger(3,12);ppst.executeUpdate();ppst.close();conn.close();[读操作]Class.forName(&quot;com.mysql.jdbc.Driver&quot;);Connection conn = DriverMananger.getConnection(&quot;jdbc:mysql://localhost:3306/big4&quot;,&quot;root&quot;,&quot;root&quot;);ppst = conn.preparedStatement(&quot;select id,name from test &quot;);//结果集ResultSet rs = ppst.executeQuery();while(rs.next()){    int id = rs.getInt(&quot;id&quot;);    String name = rs.getInt(&quot;name&quot;);}rs.close();conn.close();</code></pre><p>看一下下源码：<br><img src="https://i.imgur.com/5ZrVZsW.png" alt=""><br>看一下写和读入都把结果集和预处理语句都已经封装进去了，剩下的只要填空就好了。</p><p>以下是从数据库中读入的代码模板：</p><h1 id="使用DBWritable向数据库从数据库中读取"><a href="#使用DBWritable向数据库从数据库中读取" class="headerlink" title="使用DBWritable向数据库从数据库中读取"></a>使用DBWritable向数据库从数据库中读取</h1><h2 id="1-准备数据库"><a href="#1-准备数据库" class="headerlink" title="1.准备数据库"></a>1.准备数据库</h2><pre><code>create database big4 ;use big4 ;create table words(id int primary key auto_increment , name varchar(20) , txt varchar(255));insert into words(name,txt) values(&apos;tomas&apos;,&apos;hello world tom&apos;);insert into words(txt) values(&apos;hello tom world&apos;);insert into words(txt) values(&apos;world hello tom&apos;);insert into words(txt) values(&apos;world tom hello&apos;);</code></pre><h2 id="2-编写hadoop-MyDBWritable"><a href="#2-编写hadoop-MyDBWritable" class="headerlink" title="2.编写hadoop MyDBWritable."></a>2.编写hadoop MyDBWritable.</h2><pre><code>import org.apache.hadoop.mapreduce.lib.db.DBWritable;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import java.sql.PreparedStatement;import java.sql.ResultSet;import java.sql.SQLException;/** * MyDBWritable */public class MyDBWritable implements DBWritable,Writable {    private int id ;    private String name ;    private String txt ;    public int getId() {        return id;    }    public void setId(int id) {        this.id = id;    }    public String getName() {        return name;    }    public void setName(String name) {        this.name = name;    }    public String getTxt() {        return txt;    }    public void setTxt(String txt) {        this.txt = txt;    }    public void write(DataOutput out) throws IOException {        out.writeInt(id);        out.writeUTF(name);        out.writeUTF(txt);    }    public void readFields(DataInput in) throws IOException {        id = in.readInt();        name = in.readUTF();        txt = in.readUTF();    }    /**     * 写入db     */    public void write(PreparedStatement ppst) throws SQLException {        ppst.setInt(1,id);        ppst.setString(2,name);        ppst.setString(3,txt);    }    /**     * 从db读取     */    public void readFields(ResultSet rs) throws SQLException {        id = rs.getInt(1);        name = rs.getString(2);        txt = rs.getString(3);    }}</code></pre><h2 id="3-WcMapper"><a href="#3-WcMapper" class="headerlink" title="3.WcMapper"></a>3.WcMapper</h2><pre><code>public class WCMapper extends Mapper&lt;LongWritable,MyDBWritable,Text,IntWritable&gt; {    protected void map(LongWritable key, MyDBWritable value, Context context) throws IOException, InterruptedException {        System.out.println(key);        String line = value.getTxt();        System.out.println(value.getId() + &quot;,&quot; + value.getName());        String[] arr = line.split(&quot; &quot;);        for(String s : arr){            context.write(new Text(s),new IntWritable(1));        }    }}</code></pre><h2 id="4-WCReducer"><a href="#4-WCReducer" class="headerlink" title="4.WCReducer"></a>4.WCReducer</h2><pre><code>protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {    int count = 0 ;    for(IntWritable w : values){        count = count + w.get() ;    }    context.write(key,new IntWritable(count));}</code></pre><h2 id="5-WCApp"><a href="#5-WCApp" class="headerlink" title="5.WCApp"></a>5.WCApp</h2><pre><code>public static void main(String[] args) throws Exception {    Configuration conf = new Configuration();    Job job = Job.getInstance(conf);    //设置job的各种属性    job.setJobName(&quot;MySQLApp&quot;);                        //作业名称    job.setJarByClass(WCApp.class);                 //搜索类    //配置数据库信息    String driverclass = &quot;com.mysql.jdbc.Driver&quot; ;    String url = &quot;jdbc:mysql://localhost:3306/big4&quot; ;    String username= &quot;root&quot; ;    String password = &quot;root&quot; ;    //设置数据库配置    DBConfiguration.configureDB(job.getConfiguration(),driverclass,url,username,password);    //设置数据输入内容    DBInputFormat.setInput(job,MyDBWritable.class,&quot;select id,name,txt from words&quot;,&quot;select count(*) from words&quot;);    //设置输出路径    FileOutputFormat.setOutputPath(job,new Path(&quot;e:/mr/sql/out&quot;));    //设置分区类    job.setMapperClass(WCMapper.class);             //mapper类    job.setReducerClass(WCReducer.class);           //reducer类    job.setNumReduceTasks(3);                       //reduce个数    job.setMapOutputKeyClass(Text.class);           //    job.setMapOutputValueClass(IntWritable.class);  //    job.setOutputKeyClass(Text.class);    job.setOutputValueClass(IntWritable.class);     //    job.waitForCompletion(true);}</code></pre><h2 id="6-pom-xml增加mysql驱动"><a href="#6-pom-xml增加mysql驱动" class="headerlink" title="6.pom.xml增加mysql驱动"></a>6.pom.xml增加mysql驱动</h2><pre><code>&lt;dependency&gt;    &lt;groupId&gt;mysql&lt;/groupId&gt;    &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;    &lt;version&gt;5.1.17&lt;/version&gt;&lt;/dependency&gt;</code></pre><h2 id="7-将mr的统计结果写入mysql数据库"><a href="#7-将mr的统计结果写入mysql数据库" class="headerlink" title="7.将mr的统计结果写入mysql数据库"></a>7.将mr的统计结果写入mysql数据库</h2><pre><code>a)准备表    create table stats(word varchar(50),c int);b)设置App的DBOutputFormat类    com.it18zhang.hdfs.mr.mysql.WCAppd)e)f)</code></pre><hr><h1 id="mysql分页查询"><a href="#mysql分页查询" class="headerlink" title="mysql分页查询"></a>mysql分页查询</h1><p>如图所示吧：</p><p><img src="https://i.imgur.com/0hHJN3B.png" alt=""></p><p><img src="https://i.imgur.com/FAbkIff.png" alt=""></p><p><img src="https://i.imgur.com/9wCCeH2.png" alt=""></p><h1 id="使用DBWritable向数据库从数据库中写入"><a href="#使用DBWritable向数据库从数据库中写入" class="headerlink" title="使用DBWritable向数据库从数据库中写入"></a>使用DBWritable向数据库从数据库中写入</h1><pre><code>public class MyDBWritalbe implements DBWritable,Writable {    private int id=0;    private String name=&quot;&quot;;    private String txt=&quot;&quot;;    private  String word=&quot;&quot;;    private int wordcount=0;    public String getWord() {        return word;    }    public void setWord(String word) {        this.word = word;    }    public int getWordcount() {        return wordcount;    }    public void setWordcount(int wordcount) {        this.wordcount = wordcount;    }    public int getId() {        return id;    }    public void setId(int id) {        this.id = id;    }    public String getName() {        return name;    }    public void setName(String name) {        this.name = name;    }    public String getTxt() {        return txt;    }    public void setTxt(String txt) {        this.txt = txt;    }    public void write(DataOutput out) throws IOException {        out.writeInt(id);        out.writeUTF(name);        out.writeUTF(txt);        out.writeUTF(word);        out.writeInt(wordcount);    }    public void readFields(DataInput in) throws IOException {        id=in.readInt();        name=in.readUTF();        txt=in.readUTF();        word=in.readUTF();        wordcount=in.readInt();    }    //向数据库中写入DB    public void write(PreparedStatement ppst) throws SQLException {        //要求定制字段列表的时候先单词后个数。        ppst.setString(1,word);        ppst.setInt(2,wordcount);    }    //从DB中读出    public void readFields(ResultSet rs) throws SQLException {        id=rs.getInt(1);        name=rs.getString(2);        txt=rs.getString(3);    }}</code></pre><hr><pre><code>public class WCApp {    ////    public static void main(String[] args) throws Exception {//        Configuration conf = new Configuration();////        Job job = Job.getInstance(conf);//////        设置作业的各种属性//        job.setJobName(&quot;MySQLApp&quot;);    //作业名称//        job.setJarByClass(WCApp.class); //搜索类路径////        //配置数据库信息//        String driverclass = &quot;com.mysql.jdbc.Driver&quot;;//        String url = &quot;jdbc:mysql://localhost:3306/big4&quot;;//        String usrname = &quot;root&quot;;//        String password = &quot;root&quot;;//        DBConfiguration.configureDB(conf,driverclass,url,usrname,password);//        //设置数据输入内容//        DBInputFormat.setInput(job,DBWritable.class,&quot;select id,name,txt from words&quot;,&quot;select count(*) from words&quot;);//        //设置输出路径//        FileOutputFormat.setOutputPath(job, new Path(&quot;d:/mr/sql/out&quot;));////        job.setMapperClass(WCMapper.class);         //mapper类//        job.setReducerClass(WCReducer.class);       //reduce类////        job.setNumReduceTasks(3);//////        job.setMapOutputKeyClass(Text.class);//        job.setMapOutputValueClass(IntWritable.class);////        job.setOutputKeyClass(Text.class);          //设置输出类型//        job.setOutputValueClass(IntWritable.class);////        job.waitForCompletion(true);////    }//}    public static void main(String[] args) throws Exception {        Configuration conf = new Configuration();        Job job = Job.getInstance(conf);        //设置job的各种属性        job.setJobName(&quot;MySQLApp&quot;);                        //作业名称        job.setJarByClass(WCApp.class);                 //搜索类        //配置数据库信息        String driverclass = &quot;com.mysql.jdbc.Driver&quot;;        String url = &quot;jdbc:mysql://localhost:3306/big4&quot;;        String username = &quot;root&quot;;        String password = &quot;root&quot;;        //设置数据库配置        DBConfiguration.configureDB(job.getConfiguration(), driverclass, url, username, password);        //设置数据输入内容        DBInputFormat.setInput(job, MyDBWritalbe.class, &quot;select id,name,txt from words&quot;, &quot;select count(*) from words&quot;);        //这里定制字段列表，先单词后个数        DBOutputFormat.setOutput(job,&quot;stats&quot;,&quot;word&quot;,&quot;c&quot;);        //设置分区类        job.setMapperClass(WCMapper.class);             //mapper类        job.setReducerClass(WCReducer.class);           //reducer类        job.setNumReduceTasks(3);                       //reduce个数        job.setMapOutputKeyClass(Text.class);           //        job.setMapOutputValueClass(IntWritable.class);  //        job.setOutputKeyClass(Text.class);        job.setOutputValueClass(IntWritable.class);     //        job.waitForCompletion(true);    }}</code></pre><hr><pre><code>public class WCMapper extends Mapper&lt;LongWritable, MyDBWritalbe, Text, IntWritable&gt; {    @Override    protected void map(LongWritable key, MyDBWritalbe value, Context context) throws IOException, InterruptedException {        System.out.println(key);        String line = value.getTxt();        System.out.println(value.getId() + &quot;,&quot; + value.getName());        String[] arr = line.split(&quot; &quot;);        for (String s : arr) {            context.write(new Text(s), new IntWritable(1));        }    }}</code></pre><hr><pre><code>public class WCReducer extends Reducer&lt;Text, IntWritable, MyDBWritalbe, NullWritable&gt; {    protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {        int count = 0;        for (IntWritable w : values) {            count = count + w.get();        }        MyDBWritalbe keyout = new MyDBWritalbe();        keyout.setWord(key.toString());        keyout.setWordcount(count);        context.write(keyout, NullWritable.get());    }}</code></pre><hr><h1 id="在虚拟机中跑wordcount写入数据库问题："><a href="#在虚拟机中跑wordcount写入数据库问题：" class="headerlink" title="在虚拟机中跑wordcount写入数据库问题："></a>在虚拟机中跑wordcount写入数据库问题：</h1><h2 id="需要修改的地方："><a href="#需要修改的地方：" class="headerlink" title="需要修改的地方："></a>需要修改的地方：</h2><p>1.修改url</p><p><img src="https://i.imgur.com/8lkYD5v.png" alt=""></p><p>2.修改core-site.xml或者删除掉</p><p><img src="https://i.imgur.com/weFYMBJ.png" alt=""></p><p>3.在虚拟机中分发mysql-connector-java-5.1.7-bin.jar将之分发到每个节点的/soft/hadoop/share/hadoop/common/lib目录下。</p><p><img src="https://i.imgur.com/ou52Qhr.png" alt=""></p><p>4.在运行</p><pre><code>hadoop jar HdfsDemo.jar com.it18zhang.hdfs.mr.mysql.WCApp</code></pre><h2 id="出现的问题："><a href="#出现的问题：" class="headerlink" title="出现的问题："></a>出现的问题：</h2><p>1.出现连接不上的报错：</p><p><img src="https://i.imgur.com/JgE2Q3e.png" alt=""></p><p>百度了一下完美解决，是由于mysql没有对所有用户开启权限导致：</p><p><a href="https://blog.csdn.net/OldTogether/article/details/79612627?utm_source=blogxgwz1" target="_blank" rel="noopener">https://blog.csdn.net/OldTogether/article/details/79612627?utm_source=blogxgwz1</a></p><p>2.出现一下问题：</p><p><img src="https://i.imgur.com/vgUGlEc.png" alt=""></p><p>原因是有一个s205的防火墙没有关掉导致的：</p><p><a href="https://blog.csdn.net/shirdrn/article/details/7280040" title="防火墙没关掉导致的" target="_blank" rel="noopener">https://blog.csdn.net/shirdrn/article/details/7280040</a></p><hr><p>完</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;二次排序链条化&quot;&gt;&lt;a href=&quot;#二次排序链条化&quot; class=&quot;headerlink&quot; title=&quot;二次排序链条化&quot;&gt;&lt;/a&gt;二次排序链条化&lt;/h1&gt;&lt;p&gt;分组不能解决数据倾斜问题，已经进入到了同一个reduce对象里面了，分一个组出来就进入同一个redu
      
    
    </summary>
    
      <category term="Hadoop" scheme="http://erichunn.github.io/categories/Hadoop/"/>
    
    
      <category term="二次排序链条化" scheme="http://erichunn.github.io/tags/%E4%BA%8C%E6%AC%A1%E6%8E%92%E5%BA%8F%E9%93%BE%E6%9D%A1%E5%8C%96/"/>
    
      <category term="数据倾斜" scheme="http://erichunn.github.io/tags/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop第七天</title>
    <link href="http://erichunn.github.io/2018/10/17/Hadoop%E7%AC%AC%E4%B8%83%E5%A4%A9/"/>
    <id>http://erichunn.github.io/2018/10/17/Hadoop第七天/</id>
    <published>2018-10-17T08:17:15.000Z</published>
    <updated>2018-11-02T13:58:54.355Z</updated>
    
    <content type="html"><![CDATA[<h1 id="多输入问题"><a href="#多输入问题" class="headerlink" title="多输入问题"></a>多输入问题</h1><p>在IDEA里面代码：</p><p>首先明确一点InputInputformat也就是文本输入格式的输入是Longwritblele和Text，然后SequenceFileputFormat的输入是intwritble和text。</p><pre><code>public class WCApp {    public static void main(String[] args) throws Exception {        Configuration conf = new Configuration();        conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;);        Job job = Job.getInstance(conf);//       设置作业的各种属性        job.setJobName(&quot;WCAppMulti&quot;);    //作业名称        job.setJarByClass(WCApp.class); //搜索类路径        //多个输入        MultipleInputs.addInputPath(job,new Path(&quot;file:///e:/mr/txt&quot;),TextInputFormat.class,WCTextMapper.class);        MultipleInputs.addInputPath(job,new Path(&quot;file:///e:/mr/seq&quot;),SequenceFileInputFormat.class,WCSeqMapper.class);        //设置输出        FileOutputFormat.setOutputPath(job,new Path(args[0]));        job.setReducerClass(WCReducer.class);//reducer类        job.setNumReduceTasks(3);//reducer个数        job.setOutputKeyClass(Text.class);          //设置输出类型        job.setOutputValueClass(IntWritable.class);        job.waitForCompletion(true);    }}</code></pre><hr><pre><code>ublic class WCReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;{/** * reduce */protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {    int count = 0 ;    for(IntWritable iw : values){        count = count + iw.get() ;    }    String tno = Thread.currentThread().getName();    System.out.println(tno + &quot;WCReducer:&quot; + key.toString() + &quot;=&quot; + count);    context.write(key,new IntWritable(count));}</code></pre><p>}</p><hr><pre><code>public class WCTextMapper extends Mapper &lt;LongWritable, Text, Text, IntWritable&gt;{    @Override    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {        String[] arr = value.toString().split(&quot; &quot;);        Text keyout= new Text();        IntWritable valueout = new IntWritable();        for(String s:arr){            keyout.set(s);            valueout.set(1);            context.write(keyout,valueout);        }    }}</code></pre><hr><pre><code>public class WCTextMapper extends Mapper &lt;LongWritable, Text, Text, IntWritable&gt;{    @Override    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {        String[] arr = value.toString().split(&quot; &quot;);        Text keyout= new Text();        IntWritable valueout = new IntWritable();        for(String s:arr){            keyout.set(s);            valueout.set(1);            context.write(keyout,valueout);        }    }}</code></pre><hr><p>下图是配置本地的路径。要明确其中的arg[]也就是这个里配置的那个Program arguments的路径。因为只需要一个参数就可以了，因为在代码中已经写好了输入路径了。</p><p><img src="https://i.imgur.com/vO9wETa.png" alt=""></p><hr><hr><hr><h1 id="计数器"><a href="#计数器" class="headerlink" title="计数器"></a>计数器</h1><p> 日志目录：<br>/soft/hadoop/logs/userlogs</p><p>用户打印的日志。Hadoop输出的日志都是在外层的文件之中。userlogs是自己的日志打印。</p><h2 id="计数器-1"><a href="#计数器-1" class="headerlink" title="计数器"></a>计数器</h2><p>是其他的内容都不做更改，在Mapper和Reducer里面添加这个语句即可<br>    context.getCounter(“r”, “WCReducer.reduce”).increment(1);</p><p>然后扔到虚拟机里面去运行：<br>    hadoop jar HdfsDemo.jar com.it18zhang.hdfs.mr.WCApp hdfs://user/centos/data/ hdfs://user/centos/data/out</p><h1 id="单独配置2nn到独立节点"><a href="#单独配置2nn到独立节点" class="headerlink" title="单独配置2nn到独立节点"></a>单独配置2nn到独立节点</h1><p>配置core-site文件</p><pre><code>[hdfs-site.xml]&lt;property&gt;        &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;        &lt;value&gt;s206:50090&lt;/value&gt;&lt;/property&gt;</code></pre><h1 id="跟踪运行器信息"><a href="#跟踪运行器信息" class="headerlink" title="跟踪运行器信息"></a>跟踪运行器信息</h1><p><img src="https://i.imgur.com/UVIcFDb.png" alt=""></p><p>添加一个工具类：</p><pre><code>public class Util {public static String getInfo(Object o,String msg ){    return getHostname() + &quot;:&quot; + getPID() + &quot;:&quot; + getTID()+&quot;:&quot;+getObjInfo(o) +msg;}//得到主机名public static String getHostname() {    try {        return InetAddress.getLocalHost().getHostName();    } catch (UnknownHostException e) {        e.printStackTrace();    }    return  null;}    //获得当前程序的所在的进程ID。    public static int getPID()  {        String info = ManagementFactory.getRuntimeMXBean().getName();        return Integer.parseInt(info.substring(0,info.indexOf(&quot;@&quot;)));    }    //返回当前线程ID，    public static String getTID(){    return Thread.currentThread().getName();    }    public static  String getObjInfo(Object o){        String sname = o.getClass().getSimpleName();        return sname + &quot;@&quot;+o.hashCode();    }}</code></pre><p>然后在map和reduce阶段添加：</p><pre><code>//每执行一次，计数器对这个组+1context.getCounter(&quot;m&quot;,Util.getInfo(this,&quot;map&quot;)).increment(1);</code></pre><p>效果如下图<br><img src="https://i.imgur.com/XeGcZvS.png" alt=""></p><h1 id="全排序"><a href="#全排序" class="headerlink" title="全排序"></a>全排序</h1><pre><code>## 普通排序求最高年份温度 ##</code></pre><p>代码如下：</p><pre><code>public class MaxTempApp {    public static void main(String[] args) throws Exception {        Configuration conf = new Configuration();        conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;);        Job job = Job.getInstance(conf);//        设置作业的各种属性        job.setJobName(&quot;MaxTempApp&quot;);    //作业名称        job.setJarByClass(MaxTempApp.class); //搜索类路径        job.setInputFormatClass(TextInputFormat.class);//设置输入格式类        //添加输入路径        FileInputFormat.addInputPath(job, new Path(args[0]));        //设置输出路径        org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(args[1]));        job.setMapperClass(MaxTempMapper.class);         //mapper类        job.setReducerClass(MaxTempReducer.class);       //reduce类        job.setNumReduceTasks(3);           //reduce个数        job.setMapOutputKeyClass(IntWritable.class);        job.setMapOutputValueClass(IntWritable.class);        job.setOutputKeyClass(IntWritable.class);          //设置输出类型        job.setOutputValueClass(IntWritable.class);        job.waitForCompletion(true);    }}</code></pre><hr><pre><code>public class MaxTempMapper extends Mapper&lt;LongWritable, Text, IntWritable, IntWritable&gt; {    public MaxTempMapper() {        System.out.println(&quot;new MaxTempMapper&quot;);    }    @Override    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {        String line = value.toString();        String arr[] = line.split(&quot; &quot;);        context.write(new IntWritable(Integer.parseInt(arr[0])), new IntWritable((Integer.parseInt(arr[1]))));    }}</code></pre><hr><pre><code>public class MaxTempReducer extends Reducer&lt;IntWritable, IntWritable, IntWritable, IntWritable&gt;{    /**     * reduce     */    protected void reduce(IntWritable key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {            int max =Integer.MIN_VALUE;            for(IntWritable iw : values){                max = max &gt; iw.get() ? max : iw.get() ;            }                context.write(key,new IntWritable(max));    }}</code></pre><hr><h2 id="全排序代码"><a href="#全排序代码" class="headerlink" title="全排序代码"></a>全排序代码</h2><p>上面代码产生的会是3个文件，文件内部各自排序，但是不是全排序的文件。全排序2种办法：</p><p>1、设置分区数是1，但是数据倾斜</p><ol start="2"><li><p>在每个分区里设置0-30，30-60，60-100即可,然后每个分区返回0,1,2进入分成不同的区<br>代码如下图：<br> public class YearPartitioner extends Partitioner&lt;IntWritable, IntWritable&gt; {</p><pre><code>public int getPartition(IntWritable year, IntWritable temp, int parts) {    int y = year.get() - 1970;    if (y &lt; 33) {        return 0;    }    if (y &gt; 33 &amp;&amp; y &lt; 66) {        return 1;    }    else {        return 2;    }}</code></pre><p> }</p></li></ol><hr><pre><code>public class MaxTempReducer extends Reducer&lt;IntWritable, IntWritable, IntWritable, IntWritable&gt;{    /**     * reduce     */    protected void reduce(IntWritable key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {            int max =Integer.MIN_VALUE;            for(IntWritable iw : values){                max = max &gt; iw.get() ? max : iw.get() ;            }                context.write(key,new IntWritable(max));    }}</code></pre><hr><pre><code>public class MaxTempMapper extends Mapper&lt;LongWritable, Text, IntWritable, IntWritable&gt; {    public MaxTempMapper() {        System.out.println(&quot;new MaxTempMapper&quot;);    }    @Override    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {        String line = value.toString();        String arr[] = line.split(&quot; &quot;);        context.write(new IntWritable(Integer.parseInt(arr[0])), new IntWritable((Integer.parseInt(arr[1]))));    }}</code></pre><hr><pre><code>public class MaxTempApp {    public static void main(String[] args) throws Exception {        Configuration conf = new Configuration();        conf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;);        Job job = Job.getInstance(conf);//        设置作业的各种属性        job.setJobName(&quot;MaxTempApp&quot;);    //作业名称        job.setJarByClass(MaxTempApp.class); //搜索类路径        job.setInputFormatClass(TextInputFormat.class);//设置输入格式类        job.setPartitionerClass(YearPartitioner.class);        //添加输入路径        FileInputFormat.addInputPath(job, new Path(args[0]));        //设置输出路径        org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(args[1]));        job.setMapperClass(MaxTempMapper.class);         //mapper类        job.setReducerClass(MaxTempReducer.class);       //reduce类        job.setNumReduceTasks(3);           //reduce个数        job.setMapOutputKeyClass(IntWritable.class);        job.setMapOutputValueClass(IntWritable.class);        job.setOutputKeyClass(IntWritable.class);          //设置输出类型        job.setOutputValueClass(IntWritable.class);        job.waitForCompletion(true);    }}</code></pre><hr><p>其实无非就是加一个partitioner的这个类而已。</p><hr><hr><hr><h1 id="全排序采样器"><a href="#全排序采样器" class="headerlink" title="全排序采样器"></a>全排序采样器</h1><p>1.定义1个reduce</p><p>2.自定义分区函数。<br>：        自行设置分解区间。</p><p>3.使用hadoop采样机制。</p><p>通过采样器生成分区文件，结合hadoop的TotalOrderPartitioner进行分区划分</p><p>TotalOrderPartitioner        //全排序分区类,读取外部生成的分区文件确定区间。</p><p>使用时采样代码在最后端,否则会出现错误。</p><p>//分区文件设置，设置的job的配置对象，不要是之前的conf.这里面的getconfiguration()是对最开始new的conf的拷贝，<br>TotalOrderPartitioner.setPartitionFile(job.getConfiguration(),new Path(“d:/mr/par.lst”));</p><h2 id="首先一段产生随机年份，温度的代码"><a href="#首先一段产生随机年份，温度的代码" class="headerlink" title="首先一段产生随机年份，温度的代码"></a>首先一段产生随机年份，温度的代码</h2><pre><code>public class PrepareTempData {    @Test    public void makeData() throws IOException {        FileWriter fw = new FileWriter(&quot;e:/mr/temp.txt&quot;);        for(int i=0;i&lt;6000;i++){            int year=1970+ new Random().nextInt(100);            int temp=-30 + new Random().nextInt(600);            fw.write(&quot; &quot;+ year + &quot; &quot;+ temp + &quot;\r\n&quot; );        }            fw.close();    }}</code></pre><hr><h2 id="全排序采样器代码"><a href="#全排序采样器代码" class="headerlink" title="全排序采样器代码"></a>全排序采样器代码</h2><pre><code>        public static void main(String[] args) throws Exception {    Configuration conf = new Configuration();    conf.set(&quot;fs.defaultFS&quot;, &quot;file:///&quot;);    Job job = Job.getInstance(conf);        设置作业的各种属性    job.setJobName(&quot;MaxTempApp&quot;);    //作业名称    job.setJarByClass(MaxTempApp.class); //搜索类路径    job.setInputFormatClass(SequenceFileInputFormat.class);//设置输入格式类    //添加输入路径    FileInputFormat.addInputPath(job, new Path(args[0]));    //设置输出路径    org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(args[1]));    job.setMapperClass(MaxTempMapper.class);         //mapper类    job.setReducerClass(MaxTempReducer.class);       //reduce类    job.setMapOutputKeyClass(IntWritable.class);    job.setMapOutputValueClass(IntWritable.class);    job.setOutputKeyClass(IntWritable.class);          //设置输出类型    job.setOutputValueClass(IntWritable.class);    //设置 全排序分区类    job.setPartitionerClass(TotalOrderPartitioner.class);    //将sample数据 写入分区文件    TotalOrderPartitioner.setPartitionFile(conf, new Path(&quot;e:/mr/par.lst&quot;));    //创建随机采样器对象    InputSampler.Sampler&lt;IntWritable, IntWritable&gt; sampler = new InputSampler.RandomSampler&lt;IntWritable, IntWritable&gt;(0.1, 10000, 10);    job.setNumReduceTasks(3);           //reduce个数    InputSampler.writePartitionFile(job, sampler);    job.waitForCompletion(true);    }}</code></pre><hr><pre><code>public class MaxTempMapper extends Mapper&lt;IntWritable, IntWritable, IntWritable, IntWritable&gt; {    public MaxTempMapper() {        System.out.println(&quot;new MaxTempMapper&quot;);    }    @Override    protected void map(IntWritable key, IntWritable value, Context context) throws IOException, InterruptedException {        context.write(key,value);    }}</code></pre><hr><pre><code>public class MaxTempReducer extends Reducer&lt;IntWritable, IntWritable, IntWritable, IntWritable&gt;{    /**     * reduce     */    protected void reduce(IntWritable key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {            int max =Integer.MIN_VALUE;            for(IntWritable iw : values){                max = max &gt; iw.get() ? max : iw.get() ;            }                context.write(key,new IntWritable(max));    }}</code></pre><hr><pre><code>public class YearPartitioner extends Partitioner&lt;IntWritable, IntWritable&gt; {    public int getPartition(IntWritable year, IntWritable temp, int parts) {        int y = year.get() - 1970;        if (y &lt; 33) {            return 0;        }        if (y &gt; 33 &amp;&amp; y &lt; 66) {            return 1;        }        else {            return 2;        }    }}</code></pre><hr><pre><code>全排序官方笔记:1.定义1个reduce2.自定义分区函数.    自行设置分解区间。3.使用hadoop采样机制。    通过采样器生成分区文件，结合hadoop的TotalOrderPartitioner进行分区划分。    TotalOrderPartitioner        //全排序分区类,读取外部生成的分区文件确定区间。    使用时采样代码在最后端,否则会出现错误。    //分区文件设置，设置的job的配置对象，不要是之前的conf.    TotalOrderPartitioner.setPartitionFile(job.getConfiguration(),new Path(&quot;d:/mr/par.lst&quot;));</code></pre><hr><hr><h2 id="全排序和部分排序二次排序在面试比例很重的"><a href="#全排序和部分排序二次排序在面试比例很重的" class="headerlink" title="全排序和部分排序二次排序在面试比例很重的"></a>全排序和部分排序二次排序在面试比例很重的</h2><p>分区在map端，分组在reduce端。<br>二次排序就是把value和key组合到一起，然后value就是nullwritable这样子。合在一起成为一个combokey。<br>Combokey不但要可比较还要串行化，hadoop的串行化机制是writable ，串行就是写入到流离去，反串行就是读出来。<br>IntWritable LongWritable这些都是经过串行化实现的类，Writable有很多子类。 </p><p>value本身不能排序，为了能让他排序，吧value做到key里面去。吧年份和气温都做到key里面去，这个key是自定义的key,然后在combokey中定义里面的排序规则。自定义key,自定义对比器。</p><p>wirtable是串行化机制，java本身也有串行化机制，本质是对象输出流和输入流objectInputStream,objectOutputStream。需要写入和读取就可以了，但是java的串行化效率比较低而且复杂，不能够夸语言，所以hadoop有一个自己的串行化Wriable过程。</p><p>这端是reduce端，经过map端的combokey输出的combokey类是这个样子的，如果不经过处理直接聚合，就会发先，每一个combo都是一个新的都不相同，但是我们想让相同年份的进入到同一个组里面去。所以要重写分组。根据年份写分组，只要是同一个年就是同一组</p><p>reduce端里面有reduce对象和reduce方法。我们说的分几个区进入到几个reduce的意思是进入到几个reduce对象。</p><p><img src="https://i.imgur.com/SSNM7tc.png" alt=""></p><p>现在讲一下下图：下图说在没有分组的情况下，一个key对应好多value,也就是一个1978对应好多个12。好多个12进入到迭代器里面去，不断it.next是下一个v。但是key也改变，其实每次都是变化的，<br><img src="https://i.imgur.com/R4Pj8tj.png" alt=""></p><p><strong>讲一下二次排序和全排序：</strong>全排序就是整个年份-温度数据，分成几个区，让第一个区的最大值小于第二个区的最小值，然后这样子排下去，既解决了分布式的问题，又解决了数据倾斜的问题，但是value是不能排序的，因为mapreduce天生value就不能够排序。那么如何解决让温度也排序呢，就是要把年份和温度做成一个key，传入，然后再排序。就是二次排序，既实现了年份的排序，又实现了温度的排序，这里说的温度的排序是指同一个年份温度升序降序的问题。</p><h1 id="二次排序"><a href="#二次排序" class="headerlink" title="二次排序"></a>二次排序</h1><p>代码如下：</p><pre><code>/** * 自定义组合key */public class ComboKey implements WritableComparable&lt;ComboKey&gt; {    private int year;    private int temp;    public int getYear() {        return year;    }    public void setYear(int year) {        this.year = year;    }    public int getTemp() {        return temp;    }    public void setTemp(int temp) {        this.temp = temp;    }    /**     * 对key进行比较实现     */    public int compareTo(ComboKey o) {        int y0 = o.getYear();        int t0 = o.getTemp();        //年份相同(升序)        if (year == y0) {            //气温降序            return -(temp - t0);            // //这个地方为什么说temp-t0是升序排列，因为这个temp-t0是默认的。默认就是升序排列加一个负号就是降序排列。本身升序排列的就是temp-t0是大于0的        } else {            return year - y0;        }    }    /**     * 串行化过程     */    public void write(DataOutput out) throws IOException {        //年份        out.writeInt(year);        //气温        out.writeInt(temp);    }    public void readFields(DataInput in) throws IOException {        year = in.readInt();        temp = in.readInt();    }}</code></pre><hr><pre><code>/** *ComboKeyComparator */public class ComboKeyComparator extends WritableComparator {    protected ComboKeyComparator() {        super(ComboKey.class, true);    }    public int compare(WritableComparable a, WritableComparable b) {        ComboKey k1 = (ComboKey) a;        ComboKey k2 = (ComboKey) b;        return k1.compareTo(k2);    }}</code></pre><hr><pre><code>    public static void main(String[] args) throws Exception {        Configuration conf = new Configuration();        conf.set(&quot;fs.defaultFS&quot;, &quot;file:///&quot;);        Job job = Job.getInstance(conf);//        设置作业的各种属性        job.setJobName(&quot;SecondarySortApp&quot;);    //作业名称        job.setJarByClass(MaxTempApp.class); //搜索类路径        job.setInputFormatClass(TextInputFormat.class);//设置输入格式类        //添加输入路径        FileInputFormat.addInputPath(job, new Path(args[0]));        //设置输出路径        org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(args[1]));        job.setMapperClass(MaxTempMapper.class);         //mapper类        job.setReducerClass(MaxTempReducer.class);       //reduce类        //设置map输出类型        job.setMapOutputKeyClass(ComboKey.class);        job.setMapOutputValueClass(NullWritable.class);        //设置Reduceoutput类型        job.setOutputKeyClass(IntWritable.class);          //设置输出类型        job.setOutputValueClass(IntWritable.class);        //设置分区类        job.setPartitionerClass(YearPartitioner.class);        //设置分组对比器。        job.setGroupingComparatorClass(YearGroupComparator.class);        //设置排序对比器，听说不写那个ComboKeyComparator不设置也可以        job.setSortComparatorClass(ComboKeyComparator.class);        //reduce个数        job.setNumReduceTasks(3);        job.waitForCompletion(true);    }}</code></pre><hr><pre><code>public class MaxTempMapper extends Mapper&lt;LongWritable, Text, ComboKey, NullWritable&gt; {    @Override    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {        String line = value.toString();        String[] arr = line.split(&quot; &quot;);        ComboKey keyout = new ComboKey();        keyout.setYear(Integer.parseInt(arr[0]));        keyout.setTemp(Integer.parseInt(arr[1]));        context.write(keyout, NullWritable.get());    }  }</code></pre><hr><pre><code>/** * Reducer */public class MaxTempReducer extends Reducer&lt;ComboKey, NullWritable, IntWritable, IntWritable&gt;{    protected void reduce(ComboKey key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException {            int year = key.getYear();            int temp = key.getTemp();            context.write(new IntWritable(year),new IntWritable(temp));    }}</code></pre><hr><pre><code>public class YearGroupComparator extends WritableComparator {    protected YearGroupComparator() {        super(ComboKey.class, true);    }    public int compare(WritableComparable a, WritableComparable b) {        ComboKey k1 = (ComboKey) a ;        ComboKey k2 = (ComboKey) b ;        return k1.getYear() - k2.getYear() ;    }}</code></pre><hr><pre><code>//自定义分区，在map端执行，是map中的一个阶段，mapkv进kv出，kv出去之后要有一个分区的过程。//默认是哈希分区，这里边是修改了分区规则。public class YearPartitioner extends Partitioner&lt;ComboKey,NullWritable&gt; {public int getPartition(ComboKey key, NullWritable nullWritable, int numPartitions) {    int year = key.getYear();    return year % numPartitions;}</code></pre><p>}</p><hr><hr><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;多输入问题&quot;&gt;&lt;a href=&quot;#多输入问题&quot; class=&quot;headerlink&quot; title=&quot;多输入问题&quot;&gt;&lt;/a&gt;多输入问题&lt;/h1&gt;&lt;p&gt;在IDEA里面代码：&lt;/p&gt;
&lt;p&gt;首先明确一点InputInputformat也就是文本输入格式的输入是Longw
      
    
    </summary>
    
      <category term="Hadoop" scheme="http://erichunn.github.io/categories/Hadoop/"/>
    
    
      <category term="多输入问题" scheme="http://erichunn.github.io/tags/%E5%A4%9A%E8%BE%93%E5%85%A5%E9%97%AE%E9%A2%98/"/>
    
      <category term="计数器使用" scheme="http://erichunn.github.io/tags/%E8%AE%A1%E6%95%B0%E5%99%A8%E4%BD%BF%E7%94%A8/"/>
    
      <category term="跟踪运行信息" scheme="http://erichunn.github.io/tags/%E8%B7%9F%E8%B8%AA%E8%BF%90%E8%A1%8C%E4%BF%A1%E6%81%AF/"/>
    
      <category term="产生随机数文件" scheme="http://erichunn.github.io/tags/%E4%BA%A7%E7%94%9F%E9%9A%8F%E6%9C%BA%E6%95%B0%E6%96%87%E4%BB%B6/"/>
    
      <category term="全排序" scheme="http://erichunn.github.io/tags/%E5%85%A8%E6%8E%92%E5%BA%8F/"/>
    
      <category term="全排序采样器" scheme="http://erichunn.github.io/tags/%E5%85%A8%E6%8E%92%E5%BA%8F%E9%87%87%E6%A0%B7%E5%99%A8/"/>
    
      <category term="二次排序" scheme="http://erichunn.github.io/tags/%E4%BA%8C%E6%AC%A1%E6%8E%92%E5%BA%8F/"/>
    
      <category term="倒排序" scheme="http://erichunn.github.io/tags/%E5%80%92%E6%8E%92%E5%BA%8F/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop第六天之Yarn作业提交</title>
    <link href="http://erichunn.github.io/2018/10/11/Hadoop%E7%AC%AC%E5%85%AD%E5%A4%A9%E4%B9%8BYarn%E4%BD%9C%E4%B8%9A%E6%8F%90%E4%BA%A4/"/>
    <id>http://erichunn.github.io/2018/10/11/Hadoop第六天之Yarn作业提交/</id>
    <published>2018-10-11T07:48:32.000Z</published>
    <updated>2018-10-16T12:27:50.548Z</updated>
    
    <content type="html"><![CDATA[<h2 id="本地模式job提交流程"><a href="#本地模式job提交流程" class="headerlink" title="本地模式job提交流程"></a>本地模式job提交流程</h2><pre><code>mr.Job = new Job();job.setxxx();JobSubmitter.提交LocalJobRunner.Job();start();</code></pre><h2 id="hdfs-write"><a href="#hdfs-write" class="headerlink" title="hdfs write"></a>hdfs write</h2><pre><code>packet,</code></pre><h2 id="hdfs-切片计算方式"><a href="#hdfs-切片计算方式" class="headerlink" title="hdfs 切片计算方式"></a>hdfs 切片计算方式</h2><pre><code>getFormatMinSplitSize() = 1//最小值(&gt;=1)                            1                        0long minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));//最大值(&lt;= Long.Max , mapreduce.input.fileinputformat.split.maxsize=)long maxSize = getMaxSplitSize(job);//得到block大小long blockSize = file.getBlockSize();//minSplit maxSplit blockSize//Math.max(minSize, Math.min(maxSize, blockSize));</code></pre><p>LF : Line feed,换行符<br>private static final byte CR = ‘\r’;<br>private static final byte LF = ‘\n’;</p><h2 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h2><pre><code>1.Windows    源文件大小:82.8k    源文件类型:txt    压缩性能比较                |    DeflateCodec    GzipCodec    BZip2Codec    Lz4Codec    SnappyCodec    |结论    ------------|-------------------------------------------------------------------|----------------------    压缩时间(ms)|    450                7            196            44            不支持        |Gzip &gt; Lz4 &gt; BZip2 &gt; Deflate    ------------|-------------------------------------------------------------------|----------------------    解压时间(ms)|    444                66            85            33                        |lz4  &gt; gzip &gt; bzip2 &gt; Deflate    ------------|-------------------------------------------------------------------|----------------------    占用空间(k)    |    19k                19k            17k            31k            不支持        |Bzip &gt; Deflate = Gzip &gt; Lz4                |                                                                    |2.CentOS    源文件大小:82.8k    源文件类型:txt                |    DeflateCodec    GzipCodec    BZip2Codec    Lz4Codec    LZO        SnappyCodec    |结论    ------------|---------------------------------------------------------------------------|----------------------    压缩时间(ms)|    944                77            261            53            77        不支持        |Gzip &gt; Lz4 &gt; BZip2 &gt; Deflate    ------------|---------------------------------------------------------------------------|----------------------    解压时间(ms)|    67                66            106            52            73                    |lz4  &gt; gzip &gt; Deflate&gt; lzo &gt; bzip2     ------------|---------------------------------------------------------------------------|----------------------    占用空间(k)    |    19k                19k            17k            31k            34k                    |Bzip &gt; Deflate = Gzip &gt; Lz4 &gt; lzo</code></pre><h2 id="远程调试"><a href="#远程调试" class="headerlink" title="远程调试"></a>远程调试</h2><pre><code>1.设置服务器java vm的-agentlib:jdwp选项.[server]//windwos//set JAVA_OPTS=%JAVA_OPTS% -agentlib:jdwp=transport=dt_socket,address=8888,server=y,suspend=n//linuxexport HADOOP_CLIENT_OPTS=-agentlib:jdwp=transport=dt_socket,address=8888,server=y,suspend=y2.在server启动java程序    hadoop jar HdfsDemo.jar com.it18zhang.hdfs.mr.compress.TestCompress3.server会暂挂在8888.    Listening ...4.客户端通过远程调试连接到远程主机的8888.5.客户端就可以调试了。</code></pre><p>hadoop jar<br>java </p><p>hadoop jar com.it18zhang.hdfs.mr.compress.TestCompress</p><p>export HADOOP_CLIENT_OPTS=-agentlib:jdwp=transport=dt_socket,address=8888,server=y,suspend=y</p><h2 id="在pom-xml中引入新的插件-maven-antrun-plugin-实现文件的复制"><a href="#在pom-xml中引入新的插件-maven-antrun-plugin-实现文件的复制" class="headerlink" title="在pom.xml中引入新的插件(maven-antrun-plugin),实现文件的复制."></a>在pom.xml中引入新的插件(maven-antrun-plugin),实现文件的复制.</h2><pre><code>[pom.xml]&lt;project&gt;    ...    &lt;build&gt;        &lt;plugins&gt;            ...            &lt;plugin&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-antrun-plugin&lt;/artifactId&gt;                &lt;version&gt;1.8&lt;/version&gt;                &lt;executions&gt;                    &lt;execution&gt;                        &lt;phase&gt;package&lt;/phase&gt;                        &lt;goals&gt;                            &lt;goal&gt;run&lt;/goal&gt;                        &lt;/goals&gt;                        &lt;configuration&gt;                            &lt;tasks&gt;                                &lt;echo&gt;---------开始复制jar包到共享目录下----------&lt;/echo&gt;                                &lt;delete file=&quot;D:\downloads\bigdata\data\HdfsDemo-1.0-SNAPSHOT.jar&quot;&gt;&lt;/delete&gt;                                &lt;copy file=&quot;target/HdfsDemo-1.0-SNAPSHOT.jar&quot; toFile=&quot;D:\downloads\bigdata\data\HdfsDemo.jar&quot;&gt;                                &lt;/copy&gt;                            &lt;/tasks&gt;                        &lt;/configuration&gt;                    &lt;/execution&gt;                &lt;/executions&gt;            &lt;/plugin&gt;        &lt;/plugins&gt;    &lt;/build&gt;    ...&lt;/project&gt;</code></pre><h2 id="在centos上使用yum安装snappy压缩库文件"><a href="#在centos上使用yum安装snappy压缩库文件" class="headerlink" title="在centos上使用yum安装snappy压缩库文件"></a>在centos上使用yum安装snappy压缩库文件</h2><pre><code>[google snappy]$&gt;sudo yum search snappy                #查看是否有snappy库$&gt;sudo yum install -y snappy.x86_64        #安装snappy压缩解压缩库</code></pre><h2 id="库文件"><a href="#库文件" class="headerlink" title="库文件"></a>库文件</h2><pre><code>windows    :dll(dynamic linked library)linux    :so(shared object)</code></pre><h2 id="LZO"><a href="#LZO" class="headerlink" title="LZO"></a>LZO</h2><pre><code>1.在pom.xml引入lzo依赖    &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;    &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;             xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;             xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;        &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;        &lt;groupId&gt;com.it18zhang&lt;/groupId&gt;        &lt;artifactId&gt;HdfsDemo&lt;/artifactId&gt;        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;        &lt;packaging&gt;jar&lt;/packaging&gt;        &lt;build&gt;            &lt;plugins&gt;                &lt;plugin&gt;                    &lt;groupId&gt;org.apache.maven.plugins                    &lt;/groupId&gt;                    &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;                    &lt;configuration&gt;                        &lt;source&gt;1.8&lt;/source&gt;                        &lt;target&gt;1.8&lt;/target&gt;                    &lt;/configuration&gt;                &lt;/plugin&gt;                &lt;plugin&gt;                    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                    &lt;artifactId&gt;maven-antrun-plugin&lt;/artifactId&gt;                    &lt;version&gt;1.8&lt;/version&gt;                    &lt;executions&gt;                        &lt;execution&gt;                            &lt;phase&gt;package&lt;/phase&gt;                            &lt;goals&gt;                                &lt;goal&gt;run&lt;/goal&gt;                            &lt;/goals&gt;                            &lt;configuration&gt;                                &lt;tasks&gt;                                    &lt;echo&gt;---------开始复制jar包到共享目录下----------&lt;/echo&gt;                                    &lt;delete file=&quot;D:\downloads\bigdata\data\HdfsDemo-1.0-SNAPSHOT.jar&quot;&gt;&lt;/delete&gt;                                    &lt;copy file=&quot;target/HdfsDemo-1.0-SNAPSHOT.jar&quot; toFile=&quot;D:\downloads\bigdata\data\HdfsDemo.jar&quot;&gt;                                    &lt;/copy&gt;                                &lt;/tasks&gt;                            &lt;/configuration&gt;                        &lt;/execution&gt;                    &lt;/executions&gt;                &lt;/plugin&gt;            &lt;/plugins&gt;        &lt;/build&gt;        &lt;dependencies&gt;            &lt;dependency&gt;                &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;                &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;                &lt;version&gt;2.7.3&lt;/version&gt;            &lt;/dependency&gt;            &lt;dependency&gt;                &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;                &lt;artifactId&gt;hadoop-yarn-common&lt;/artifactId&gt;                &lt;version&gt;2.7.3&lt;/version&gt;            &lt;/dependency&gt;            &lt;dependency&gt;                &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;                &lt;artifactId&gt;hadoop-yarn-client&lt;/artifactId&gt;                &lt;version&gt;2.7.3&lt;/version&gt;            &lt;/dependency&gt;            &lt;dependency&gt;                &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;                &lt;artifactId&gt;hadoop-yarn-server-resourcemanager&lt;/artifactId&gt;                &lt;version&gt;2.7.3&lt;/version&gt;            &lt;/dependency&gt;            &lt;dependency&gt;                &lt;groupId&gt;org.anarres.lzo&lt;/groupId&gt;                &lt;artifactId&gt;lzo-hadoop&lt;/artifactId&gt;                &lt;version&gt;1.0.0&lt;/version&gt;                &lt;scope&gt;compile&lt;/scope&gt;            &lt;/dependency&gt;            &lt;dependency&gt;                &lt;groupId&gt;junit&lt;/groupId&gt;                &lt;artifactId&gt;junit&lt;/artifactId&gt;                &lt;version&gt;4.11&lt;/version&gt;            &lt;/dependency&gt;        &lt;/dependencies&gt;    &lt;/project&gt;2.在centos上安装lzo库    $&gt;sudo yum -y install lzo3.使用mvn命令下载工件中的所有依赖    进入pom.xml所在目录，运行cmd：    mvn -DoutputDirectory=./lib -DgroupId=com.it18zhang -DartifactId=HdfsDemo -Dversion=1.0-SNAPSHOT dependency:copy-dependencies4.在lib下存放依赖所有的第三方jar5.找出lzo-hadoop.jar + lzo-core.jar复制到hadoop的响应目录下。    $&gt;cp lzo-hadoop.jar lzo-core.jar /soft/hadoop/shared/hadoop/common/lib6.执行远程程序即可。</code></pre><h2 id="修改maven使用aliyun镜像。"><a href="#修改maven使用aliyun镜像。" class="headerlink" title="修改maven使用aliyun镜像。"></a>修改maven使用aliyun镜像。</h2><pre><code>[maven/conf/settings.xml]&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;settings xmlns=&quot;http://maven.apache.org/SETTINGS/1.0.0&quot;          xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;          xsi:schemaLocation=&quot;http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd&quot;&gt;  &lt;pluginGroups&gt;  &lt;/pluginGroups&gt;  &lt;proxies&gt;  &lt;/proxies&gt;&lt;servers&gt;    &lt;server&gt;        &lt;id&gt;releases&lt;/id&gt;        &lt;username&gt;admin&lt;/username&gt;        &lt;password&gt;admin123&lt;/password&gt;    &lt;/server&gt;    &lt;server&gt;        &lt;id&gt;snapshots&lt;/id&gt;        &lt;username&gt;admin&lt;/username&gt;        &lt;password&gt;admin123&lt;/password&gt;    &lt;/server&gt;    &lt;server&gt;        &lt;id&gt;Tomcat7&lt;/id&gt;        &lt;username&gt;tomcat&lt;/username&gt;        &lt;password&gt;tomcat&lt;/password&gt;    &lt;/server&gt;&lt;/servers&gt;&lt;mirrors&gt;     &lt;mirror&gt;        &lt;id&gt;nexus-aliyun&lt;/id&gt;        &lt;mirrorOf&gt;*&lt;/mirrorOf&gt;        &lt;name&gt;Nexus aliyun&lt;/name&gt;        &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt;    &lt;/mirror&gt; &lt;/mirrors&gt;&lt;/settings&gt;</code></pre><h2 id="文件格式-SequenceFile"><a href="#文件格式-SequenceFile" class="headerlink" title="文件格式:SequenceFile"></a>文件格式:SequenceFile</h2><pre><code>1.SequenceFile    Key-Value对方式。2.不是文本文件，是二进制文件。3.可切割    因为有同步点。    reader.sync(pos);    //定位到pos之后的第一个同步点。    writer.sync();        //写入同步点4.压缩方式    不压缩    record压缩            //只压缩value    块压缩                //按照多个record形成一个block.</code></pre><h2 id="文件格式-MapFile"><a href="#文件格式-MapFile" class="headerlink" title="文件格式:MapFile"></a>文件格式:MapFile</h2><pre><code>1.Key-value2.key按升序写入(可重复)。3.mapFile对应一个目录，目录下有index和data文件,都是序列文件。4.index文件划分key区间,用于快速定位。</code></pre><h2 id="自定义分区函数"><a href="#自定义分区函数" class="headerlink" title="自定义分区函数"></a>自定义分区函数</h2><pre><code>1.定义分区类    public class MyPartitioner extends Partitioner&lt;Text, IntWritable&gt;{        public int getPartition(Text text, IntWritable intWritable, int numPartitions) {            return 0;        }    }2.程序中配置使用分区类    job.setPartitionerClass(MyPartitioner.class);</code></pre><h2 id="combinerd（合成）-继承了Reducer-任何的Reducer类都能被他使用"><a href="#combinerd（合成）-继承了Reducer-任何的Reducer类都能被他使用" class="headerlink" title="combinerd（合成） 继承了Reducer 任何的Reducer类都能被他使用"></a>combinerd（合成） 继承了Reducer 任何的Reducer类都能被他使用</h2><p> Map端的Reducer  预先化简<br>1，为了减少网络带宽 将Map端发出的的数据进行聚合 并不是所有的都可以用combiner<br>2,combiner </p><hr><p>切片个数是四个，mapper就需要也是4个 </p><p>下图是一个客户端的写入过程：首先形成一个管线，因为有很多节点，节点形成管线，现在本地构建2个队列，一个发送队列sendQueue，一个是确认队列ackQueue，客户端吧消息封装成Packet放到队列里面，而且结束后会放一个空包进去，发送的话，是以管线的方式发给A，A在发给B，B在发给C，在发送的过程中有一个校验的过程chunk(4+512)就是4个校验位+512个数据字节，一个Packet是64K+33个头的字节，在确认队列里面是一个编号seqNo如果在丢列中ABC中都回执，就吧seqNo在ackQueue中移除。发送如果失败，就吧管线重新建立，吧没法送的包返回到队列之前的一个容错机制。<br><img src="https://i.imgur.com/43B2Ebp.png" alt=""></p><p>输入格式定位文件，在输入格式中间有一个交互reader，在Map和Reduce都有这么一些方法，在Mapper和Reducer中有回调方法，就是本来使用的是子类的方法，但是子类中调用有父类的的run方法，然后run方法里面有reduce方法也就又调用了子类的方法，这叫做回调 。Mapper是通过reader读取下一条数据传进来的。Map和Reduce中间有一个shuffer的混洗过程，网络间分发数据的过程， Map里面先分区，分区个数等于Reduce的个数，所以有几个reduce就有几个分区，默认情况下，Map分区通过哈希算法来分区的，对K哈希。只要K相同，一定会被哈希到一个分区里面，分区也就是桶的概念，也就是只要K相同进入到同一个桶里面，只要是同一个分区就是进入到同一个reduce，默认情况下inputformat的格式，他的K就是偏移量。如果是图片格式的话就可能是别的。只有在文本输入的时候才是偏移量，当然也可以自己定义Inputformat格式。在mr计算之前切片就已经算好了，产生的切片文件和配置XML文件放到了临时目录下了。<br><img src="https://i.imgur.com/7qjZLGJ.png" alt=""></p><p>看下图，首先客户端运行作业，第一步run job，第二部走ResourceManager，是Yarn层面，负责资源调度，所有集群的资源都属于ResourceManager，第二部get new application，首先得到应用的ID，第三部copyt job resources，复制切片信息和xml配置信息和jar包到HDFS，第四部提交给资源管理器，然后资源管理器start container，然后通过节点管理器启动一个MRAPPMaster（应用管理程序），第六步初始化Job，第七部检索切片信息，第八步请求资源管理器去分配一个资源列表，第九步启动节点管理器，通过节点管理器来启动一个JVM虚拟机，在虚拟机里面启动yarn子进程，第十部通过子进程读取切片信息，然后在运行MapTast或者ReduceTask<br><img src="https://i.imgur.com/lqVQLT1.png" alt=""></p><p>客户端提取一个作业，然后联系资源管理器，为了得到id,在完全分布式下客户端client叫job，一旦提交就叫做application了，先请求，然后得到了appid返回给了客户端，第三部将作业信息上传到HDFS的临时目录，存档切片信息和jar包，然后找到节点管理器，然后NM孵化一个MRAPPMaster，然后检索HDFS文件系统，获取到有多少个要执行的并发任务，拿到任务之后，要求资源管理器分配一个集合，哪些节点可以用。 然后MRAppmaster在联系其他的NM，让NM去开启Yarn子进程，子进程在去检索HDFS信息，在开启Map和Reduce</p><p><img src="https://i.imgur.com/bn7A9H6.png" alt=""></p><h2 id="hdfs-切片计算方式-1"><a href="#hdfs-切片计算方式-1" class="headerlink" title="hdfs 切片计算方式"></a>hdfs 切片计算方式</h2><pre><code>getFormatMinSplitSize() = 1//最小值(&gt;=1)                            1                        0long minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));//最大值(&lt;= Long.Max , mapreduce.input.fileinputformat.split.maxsize=)long maxSize = getMaxSplitSize(job);//得到block大小long blockSize = file.getBlockSize();//minSplit maxSplit blockSize//Math.max(minSize, Math.min(maxSize, blockSize));</code></pre><p>在计算切片的时候一般块大小不设置，是最好的，一般就是128M，采用本地优先策略，一个快大小是128m如果一旦把他边变1m那么切片大小就会变成1M，一个，然后128M的块大小会被分发128份切片分别传输给MR去计算，这样的话，就会增加网络间的传输，增加网络负载。所以128m是最合适的</p><hr><p>CR=‘\r’回车符<br>LF+’\n’换行符</p><p>windows系统里面是\r\n。回车带换行。<br>linux系统只有一个\n</p><p>切片是定义大方向的，阅读器recorderreader是控制细节的。切片定义任务数量，开几个map但是不一定是每个map都有任务。</p><p><strong>切片问题</strong>是物理设置，但是是逻辑读取。</p><p><img src="https://i.imgur.com/tgAbsGx.png" alt=""></p><p>打个比方说上图这个东西比如说第一个例子，前面的设置物理划分，根据字节数划分成了4个切片，也就是有4个Mapper,然后四个切片开始读取，其中第一个物理切割到t但是实际上，由于切片读取到换行符所以，第一个Mapper也就是读取到了hello world tom，然后第二个，第二个偏移量是13+13.偏移量是13，在物理读取13个直接，但是由于Mapper检测不是行首，所以直接从下一行读取也在读取一整行。这个就是每个切片128MB的缩影，吧128M变成了13而已。然后其实Textinputformat下一个环节是。recoderreader。这个会先检测是否是行首，<br><img src="https://i.imgur.com/7qjZLGJ.png" alt="">      </p><h2 id="压缩问题"><a href="#压缩问题" class="headerlink" title="压缩问题"></a>压缩问题</h2><pre><code>package com.it18zhang.hdfs.mr.compress;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.io.IOUtils;import org.apache.hadoop.io.compress.*;import org.apache.hadoop.util.ReflectionUtils;import org.junit.Test;import java.io.FileInputStream;import java.io.FileOutputStream;/** * @Title:TestCompress * @Description: * @Auther: Eric Hunn * @Version:1.0 * @Create 2018/10/14 19:39 */public class TestCompress {    @Test    public void deflateCompress() throws Exception {        Class[] zipClasses = {                DeflateCodec.class,                GzipCodec.class,                BZip2Codec.class,                Lz4Codec.class,        };        for(Class c : zipClasses){            zip(c);        }    }    /*压缩测试     *     * */    public void zip(Class codecClass) throws Exception {        long start = System.currentTimeMillis();        //实例化对象        CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, new Configuration());        //创建文件输出流，得到默认拓展名        FileOutputStream fos = new FileOutputStream(&quot;e:/comp/b&quot; + codec.getDefaultExtension());        //得到压缩流        CompressionOutputStream zipOut = codec.createOutputStream(fos);        IOUtils.copyBytes(new FileInputStream(&quot;e:/11111.txt/&quot;), zipOut, 1024);        zipOut.close();        System.out.println(codecClass.getSimpleName() + &quot; ：&quot;);    }}                                     </code></pre><p>压缩一般是在文件Map输出的时候压缩，Map输出的时候内存不够一般要往磁盘里面溢出，在溢出的时候可以让他压缩溢出，这样reduce的时候就要解压缩。上面代码测试过lz4不行，但是视频上是可以的，暂时不知道哪里出错了。</p><pre><code>package com.it18zhang.hdfs.mr.compress;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.io.IOUtils;import org.apache.hadoop.io.compress.*;import org.apache.hadoop.util.ReflectionUtils;import org.junit.Test;import java.io.FileInputStream;import java.io.FileOutputStream;/** * @Title:TestCompress * @Description: * @Auther: Eric Hunn * @Version:1.0 * @Create 2018/10/14 19:39 */public class TestCompress {    @Test    public void deflateCompress() throws Exception {        Class[] zipClasses = {                DeflateCodec.class,                GzipCodec.class,                BZip2Codec.class,//                Lz4Codec.class,//                SnappyCodec.class,        };        for(Class c : zipClasses){            unzip(c);        }    }    /*压缩测试     *     * */    public void zip(Class codecClass) throws Exception {        long start = System.currentTimeMillis();        //实例化对象        CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, new Configuration());        //创建文件输出流，得到默认拓展名        FileOutputStream fos = new FileOutputStream(&quot;e:/comp/b&quot; + codec.getDefaultExtension());        //得到压缩流        CompressionOutputStream zipOut = codec.createOutputStream(fos);        IOUtils.copyBytes(new FileInputStream(&quot;e:/11111.txt/&quot;), zipOut, 1024);        zipOut.close();        System.out.println(codecClass.getSimpleName() + &quot; ：&quot;);    }    public void unzip(Class codecClass) throws Exception {        long start = System.currentTimeMillis();        //实例化对象        CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, new Configuration());        //创建文件输出流，得到默认拓展名        FileInputStream fis = new FileInputStream(&quot;e:/comp/b&quot; + codec.getDefaultExtension());        //得到压缩流        CompressionInputStream zipIn = codec.createInputStream(fis);        IOUtils.copyBytes(zipIn,new FileOutputStream(&quot;e:/comp/b&quot; + codec.getDefaultExtension() + &quot;.txt&quot;), 1024);        zipIn.close();        System.out.println(codecClass.getSimpleName() + &quot; ：&quot; + (System.currentTimeMillis() - start));    }}</code></pre><h2 id="在集群上运"><a href="#在集群上运" class="headerlink" title="在集群上运"></a>在集群上运</h2><pre><code>package com.it18zhang.hdfs.mr.compress;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.io.IOUtils;import org.apache.hadoop.io.compress.*;import org.apache.hadoop.util.ReflectionUtils;import java.io.FileInputStream;import java.io.FileOutputStream;/** * @Title:TestCompress * @Description: * @Auther: Eric Hunn * @Version:1.0 * @Create 2018/10/14 19:39 */public class TestCompress {    public static void main(String[] args) throws Exception {        Class[] zipClasses = {                DeflateCodec.class,                GzipCodec.class,                BZip2Codec.class,//                Lz4Codec.class,//                SnappyCodec.class,        };        for(Class c : zipClasses){            zip(c);        }        System.out.println(&quot;==================================&quot;);        for(Class c : zipClasses){            unzip(c);        }    }    /*压缩测试     *     * */    public static void zip(Class codecClass) throws Exception {        long start = System.currentTimeMillis();        //实例化对象        CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, new Configuration());        //创建文件输出流，得到默认拓展名        FileOutputStream fos = new FileOutputStream(&quot;/home/centos/zip/b&quot; + codec.getDefaultExtension());        //得到压缩流        CompressionOutputStream zipOut = codec.createOutputStream(fos);        IOUtils.copyBytes(new FileInputStream(&quot;/home/centos/zip/a.txt&quot;), zipOut, 1024);        zipOut.close();        System.out.println(codecClass.getSimpleName() + &quot; ：&quot;);    }    public static void unzip(Class codecClass) throws Exception {        long start = System.currentTimeMillis();        //实例化对象        CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, new Configuration());        //创建文件输出流，得到默认拓展名        FileInputStream fis = new FileInputStream(&quot;/home/centos/zip/b&quot; + codec.getDefaultExtension());        //得到压缩流        CompressionInputStream zipIn = codec.createInputStream(fis);        IOUtils.copyBytes(zipIn,new FileOutputStream(&quot;/home/centos/zip/b&quot; + codec.getDefaultExtension() + &quot;.txt&quot;), 1024);        zipIn.close();        System.out.println(codecClass.getSimpleName() + &quot; ：&quot; + (System.currentTimeMillis() - start));    }}</code></pre><h2 id="远程调试-1"><a href="#远程调试-1" class="headerlink" title="远程调试"></a>远程调试</h2><pre><code>1.设置服务器java vm的-agentlib:jdwp选项.[server]//windwos//set JAVA_OPTS=%JAVA_OPTS% -agentlib:jdwp=transport=dt_socket,address=8888,server=y,suspend=n//linux2.在server启动java程序    hadoop jar HdfsDemo.jar com.it18zhang.hdfs.mr.compress.TestCompress3.server会暂挂在8888.    Listening ...4.客户端通过远程调试连接到远程主机的8888.5.客户端就可以调试了。</code></pre><hr><hr><h3 id="通过MapFile来写入"><a href="#通过MapFile来写入" class="headerlink" title="通过MapFile来写入"></a>通过MapFile来写入</h3><pre><code>    /*写操作 * */@Testpublic void save() throws Exception {    Configuration conf = new Configuration();    conf.set(&quot;fs.defaultFS &quot;, &quot;file:///&quot;);    FileSystem fs = FileSystem.get(conf);    Path p = new Path(&quot;e:/seq/1.seq&quot;);    MapFile.Writer writer = new MapFile.Writer(conf, fs, &quot;e:/map&quot;, IntWritable.class, Text.class);    for (int i = 0; i &lt; 100000; i++) {        writer.append(new IntWritable(i), new Text(&quot;tom&quot; + i));    }</code></pre><p>//        for(int i =0 ;i &lt; 10 ; i++){<br>//            writer.append(new IntWritable(i),new Text(“tom” + i));<br>//        }<br>        writer.close();<br>    }</p><hr><h3 id="通过MapFile来读取"><a href="#通过MapFile来读取" class="headerlink" title="通过MapFile来读取"></a>通过MapFile来读取</h3><pre><code>/*读取Mapfile文件 * */@Testpublic void readMapfile() throws Exception {    Configuration conf = new Configuration();    conf.set(&quot;fs.defaultFS &quot;, &quot;file:///&quot;);    FileSystem fs = FileSystem.get(conf);    Path p = new Path(&quot;e:/seq/1.seq&quot;);    MapFile.Reader reader = new MapFile.Reader(fs, &quot;e:/map&quot;, conf);    IntWritable key = new IntWritable();    Text value = new Text();    while (reader.next(key, value)) {        System.out.println(key.get() + &quot;:&quot; + value.toString());    }    reader.close();} </code></pre><hr><p>Map的分区是哈希分区</p><p>combiner的好处是：map端的reduce在map端做集合，减少了shuffle量。统计每个单词hello 1发了十万次，不如发一个hello1在map端做聚合发过去hello十万，即刻。就是在map端口预先reduce化简的过程。不是所有的作业都可以把reduce做成combiner。最大值最小值都可以combiner但是平均值就不行了。</p><p>第六天的最后两个视频没有往下写代码，因为太多太杂了。主要讲解MR计数器，数据倾斜，Mapfile 序列化读取，压缩这些。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;本地模式job提交流程&quot;&gt;&lt;a href=&quot;#本地模式job提交流程&quot; class=&quot;headerlink&quot; title=&quot;本地模式job提交流程&quot;&gt;&lt;/a&gt;本地模式job提交流程&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;mr.Job = new Job();
job.se
      
    
    </summary>
    
      <category term="Hadoop" scheme="http://erichunn.github.io/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="http://erichunn.github.io/tags/Hadoop/"/>
    
      <category term="Yan作业提交" scheme="http://erichunn.github.io/tags/Yan%E4%BD%9C%E4%B8%9A%E6%8F%90%E4%BA%A4/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop第五天01之hdfs写入剖析</title>
    <link href="http://erichunn.github.io/2018/10/05/Hadoop%E7%AC%AC%E4%BA%94%E5%A4%A901%E4%B9%8Bhdfs%E5%86%99%E5%85%A5%E5%89%96%E6%9E%90/"/>
    <id>http://erichunn.github.io/2018/10/05/Hadoop第五天01之hdfs写入剖析/</id>
    <published>2018-10-05T08:31:19.000Z</published>
    <updated>2018-10-08T13:12:40.378Z</updated>
    
    <content type="html"><![CDATA[<p>一段HDFS写入流源码分析。</p><p><img src="https://i.imgur.com/17cBkFO.png" alt=""></p><p>首先这个得到了一个抽象类，这个FileSystemm是一个抽象类。</p><p><img src="https://i.imgur.com/Rlbd5VC.png" alt=""></p><p>这个抽象类有子类，左边有一个箭头，点击得到了一个这个抽象类的所有的子类，简单看下，因为Hadoop包类的hadfs这个，得到的子类是DistributedFileSystem这个分布式文件系统。</p><p><img src="https://i.imgur.com/bzQ91XF.png" alt=""></p><p>也可以吧鼠标放到fs上会显示返回的类</p><p><img src="https://i.imgur.com/JOfPZvk.png" alt=""></p><p>也可以在IDEA的右下角的类标签里面找到：</p><p><img src="https://i.imgur.com/WSz2duV.png" alt=""></p><p>也就是说返回了一个DistributedFIleSystem,</p><p>然后文件系统fs.create调用了create方法返回了一个FSDataOutputStream流，这个是一个HDFS数据输出流</p><p><img src="https://i.imgur.com/SKFBqkH.png" alt=""></p><p><img src="https://i.imgur.com/0aIVTzf.png" alt=""></p><p>单机F5单部进入第一个：</p><p><img src="https://i.imgur.com/nnxWOri.png" alt=""></p><p><img src="https://i.imgur.com/f434RJR.png" alt=""></p><p><img src="https://i.imgur.com/4Cf1ljy.png" alt=""></p><p>看这个，进到下一步，调用checkClosed()，方法，这个方法其实是继承FSOutput流里面的东西。  </p><p><img src="https://i.imgur.com/lHPzWs4.png" alt=""><br>看到这个里面out=DFSOutputStream。这个是被装饰的流。调用这个流的write方法</p><p>HDFS流是对DFS输出流的包装</p><p>进去这个是装饰模式。</p><p>在这个构造模式中也声明了字段。</p><p><img src="https://i.imgur.com/8CsurDw.png" alt=""></p><p>下一步：</p><p><img src="https://i.imgur.com/0f1RcbV.png" alt=""></p><p>调用了Close方法因为是继承    都是FSoutput流的子类。一个检查的方法，判断是否数组越界。</p><p>下面这个for是个循环，循环写入，。 </p><p><img src="https://i.imgur.com/MTsauqO.png" alt=""></p><p>然后下一步，进入到write1方法。</p><p><img src="https://i.imgur.com/nAIKdn5.png" alt=""></p><p>里面的buf是一个缓冲区，count是一个成员常量</p><p><img src="https://i.imgur.com/aLCcnkl.png" alt=""></p><p>上图的buf看注释，是一个缓冲区用哦过来存储数据，在他被checksum校验和之前</p><p>校验和就是检查是否是比之前设置的最小块大小大，而且还必须是512字节的倍数</p><p><img src="https://i.imgur.com/pAFducf.png" alt=""></p><p>上图点击进入到一个校验和类的里面看注释，校验和用于数据传输之前。</p><p><img src="https://i.imgur.com/E8qEJy0.png" alt=""></p><p>上图这两行，通过一个算法来实现校验和。通过CRC32算法类似于哈希算法。</p><p><img src="https://i.imgur.com/RJ1vXHv.png" alt=""></p><p>不管他，回到buf缓冲这个地方，单部进入</p><p><img src="https://i.imgur.com/RKkQGry.png" alt=""></p><p>首先在缓冲区进行一个判定</p><p><img src="https://i.imgur.com/qfwP1Yn.png" alt=""></p><p>拷贝本地数据到本地的一个buf，显示Localbuffer=4608,count=0.要拷贝的字节是4608。<br>如果拷贝的内容没有缓冲区大就考数据内容，要是比缓冲区大的话，就拷贝缓冲区大小的内容。</p><p><img src="https://i.imgur.com/rk8wX6W.png" alt=""></p><p>在单部进入到上图。</p><p><img src="https://i.imgur.com/PjqlOXa.png" alt=""></p><p>返回到代码。进入到源代码中，如上图</p><p>在单部进入到这个Close里面：如下图：<br><img src="https://i.imgur.com/97VYh5X.png" alt=""></p><p>这个close是out.close。out是调用父类的close方法，而且父类还是一个包装类，所以进入了包装的列的close方法。<br>再进入到close()方法。如下图：</p><p><img src="https://i.imgur.com/2ExMNXU.png" alt=""></p><p>out的本类HDFSOutputStream，然后这个又是一个装饰类，本质上是DFSOutputStream这个类的方法。</p><p>单部进入到Out里面，看一下调用的到底是谁的out。再单部进入到Out里面</p><p><img src="https://i.imgur.com/RG98Z9F.png" alt=""></p><p>看注释，关闭输出流并且    释放与之相关联的系统资源。<br>上图最终进入到了DFSOutputstream的close()方法里面了。</p><p><img src="https://i.imgur.com/dDhSzjg.png" alt=""></p><p>接上图，看到在这个close(）方法里面有一个closeImp（)方法。调用自己的关闭实现方法<br>还在这个类里面执行呢：继续在这个类里面往下走:如下图：</p><p><img src="https://i.imgur.com/7Vkorih.png" alt=""></p><p>这个里面有一个flushbuffer()方法， 在单部进入到这个方法里面。如下图：</p><p><img src="https://i.imgur.com/YSCeFmf.png" alt=""></p><p>看到这个图里面this和当前类不太一样说明这两个东西也是继承关系。</p><p>清理缓冲区两个参数，看注释：强制任何输出流字节被校验和并且被写入底层的输出流。</p><p>再单步进入：</p><p><img src="https://i.imgur.com/jfWYEI1.png" alt=""></p><p><img src="https://i.imgur.com/4iqESya.png" alt=""></p><p>看到如上图的内容。然后到了writeChecksumChunks这个方法里面，单部进入到这个里面：</p><p><img src="https://i.imgur.com/FM0MpyT.png" alt=""></p><p>对给定的数据小块来生成校验和，并且根据小块和校验和给底层的数据输出流。 </p><p>单部进入到这个sum.calculateChunckedSum方法里面。</p><p><img src="https://i.imgur.com/EVQOpOL.png" alt=""></p><p><img src="https://i.imgur.com/UKsSHTO.png" alt=""></p><p>下一步</p><p><img src="https://i.imgur.com/ifq7oZR.png" alt=""></p><p>上图吧数据写入了底层里面去了。</p><p>下一步</p><p><img src="https://i.imgur.com/WLEldHU.png" alt=""></p><p>下一步</p><p><img src="https://i.imgur.com/TN0fU7T.png" alt=""></p><p>下一步</p><p><img src="https://i.imgur.com/X3dHeJt.png" alt=""></p><p><img src="https://i.imgur.com/W5NCoWA.png" alt=""></p><p>上图就是将底层的内容打成包，再去通过网络传输。在包里面加编号和序号，也就是加顺序。</p><p><img src="https://i.imgur.com/INeugMK.png" alt=""></p><p>往下不写了太多了。现在的视频位置是hdfs写入剖析2的开头。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;一段HDFS写入流源码分析。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/17cBkFO.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;首先这个得到了一个抽象类，这个FileSystemm是一个抽象类。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https:
      
    
    </summary>
    
      <category term="Hadoop" scheme="http://erichunn.github.io/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="http://erichunn.github.io/tags/Hadoop/"/>
    
      <category term="hdfs写入" scheme="http://erichunn.github.io/tags/hdfs%E5%86%99%E5%85%A5/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop第四天之滚动日志-安全模式</title>
    <link href="http://erichunn.github.io/2018/10/04/Hadoop%E7%AC%AC%E5%9B%9B%E5%A4%A9%E4%B9%8B%E6%BB%9A%E5%8A%A8%E6%97%A5%E5%BF%97-%E5%AE%89%E5%85%A8%E6%A8%A1%E5%BC%8F/"/>
    <id>http://erichunn.github.io/2018/10/04/Hadoop第四天之滚动日志-安全模式/</id>
    <published>2018-10-04T00:56:35.000Z</published>
    <updated>2018-10-04T00:56:35.958Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Hadoop第四天之最小块设置-指定副本数</title>
    <link href="http://erichunn.github.io/2018/10/04/Hadoop%E7%AC%AC%E5%9B%9B%E5%A4%A9%E4%B9%8B%E6%9C%80%E5%B0%8F%E5%9D%97%E8%AE%BE%E7%BD%AE-%E6%8C%87%E5%AE%9A%E5%89%AF%E6%9C%AC%E6%95%B0/"/>
    <id>http://erichunn.github.io/2018/10/04/Hadoop第四天之最小块设置-指定副本数/</id>
    <published>2018-10-04T00:21:11.000Z</published>
    <updated>2018-10-04T00:21:11.458Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>遇到未解决的问题</title>
    <link href="http://erichunn.github.io/2018/10/01/%E9%81%87%E5%88%B0%E6%9C%AA%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <id>http://erichunn.github.io/2018/10/01/遇到未解决的问题/</id>
    <published>2018-10-01T03:15:11.000Z</published>
    <updated>2018-11-14T13:05:16.969Z</updated>
    
    <content type="html"><![CDATA[<p>在eclipse中，ctrl+左键找到源码，为什么旁边在pakage explore中有有轮廓啊？</p><p><img src="https://i.imgur.com/oYjJ6In.png" alt=""></p><p><img src="https://i.imgur.com/IZVxlow.png" alt=""></p><p>点击两个箭头即可转换。</p><p>这个东西是在IDEA中怎么调出来的。</p><p><img src="https://i.imgur.com/fz08Y7d.png" alt=""></p><hr><p>克隆centos之后有时候出现这种情况</p><p><img src="https://i.imgur.com/G7CLxft.png" alt=""></p><hr><p>IDEA的使用问题：<br>这个是什么快捷键。</p><p><img src="https://i.imgur.com/hKrt9St.png" alt=""></p><hr><p>在增强虚拟机工具的时候按照视频的方法没有成功，然后通过挂载的方法实现了，但是hgfs文件夹和文件夹里面的内容都是root权限的，只能修改文件夹的所有者，但是不能修改里面文件的所有者。</p><hr><p>有一个分组的问题：<br>    分组就是说组合key出来本来是每一个组合key大多都是不同的key。相同年份不同温度也是不同的key，但是为了相同的年份的组合key能够进入到同一个reduce，所以要通过分组让他这个样子，但是为什么要这个样</p><pre><code>经过分区已经说达到了将不同年份的</code></pre><hr><p>在mapreduce阶段有一个大表和小表连接</p><p>hive里面也有一个小表和小表连接就在map端口，然后小表和大表连接在reduce端口。</p><p>这个地方的问题没有搞明白。具体的hive在hive第二天07mr的忘记在哪里了</p><hr><p>然后mr的最后一天hadoop第十一天的二次排序没有搞得特别明白。<br>和hive阶段的自定义函数UDF没有特别清楚</p><hr><p>在avro和rotobuf第一天</p><p>在讲解protobuf的时候写代码写到下图的时候听说这个com.example.tuorial是源码包里面的东西，我是没找到，我觉得可能是源码包里面的例子，里面包括的一些类，但是还是没有这个东西。</p><p><img src="https://i.imgur.com/jaaEFTJ.png" alt=""></p><p>已经解决，见大坑</p><hr><p>在一个spring项目里面有这样一个问题：</p><p>dao层类dao一个方法，然后在servicce层里面没有new出一个dao类的对象，直接写一个属性Private Dao d;然后直接d.insert();</p><p>这种方法为什么可以</p><hr><p>这种是什么调用方法：</p><p><img src="https://i.imgur.com/sE9UKl1.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在eclipse中，ctrl+左键找到源码，为什么旁边在pakage explore中有有轮廓啊？&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/oYjJ6In.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://i.im
      
    
    </summary>
    
      <category term="问题" scheme="http://erichunn.github.io/categories/%E9%97%AE%E9%A2%98/"/>
    
    
      <category term="未解决的问题" scheme="http://erichunn.github.io/tags/%E6%9C%AA%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    
  </entry>
  
</feed>
